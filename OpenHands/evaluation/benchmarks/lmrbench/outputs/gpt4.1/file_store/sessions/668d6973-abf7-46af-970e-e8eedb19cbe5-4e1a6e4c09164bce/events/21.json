{"id": 21, "timestamp": "2025-05-19T19:29:47.140185", "source": "agent", "message": "Code executed in IPython cell.", "cause": 20, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_0Hpwkgjw2UqeKyECpVmgqDSP", "model_response": {"id": "chatcmpl-BZ59VnnwQWzjiv2t1efhyS46NhTKg", "created": 1747700985, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Extract the section that describes the probing methodology in detail\\n# Look for section 3.1 and surrounding context\\nstart = paper_text.find('3.1 Linear Classifier Probing')\\nend = paper_text.find('3.2', start)\\nprobing_section = paper_text[start:end if end != -1 else None]\\nprint(probing_section)\"}", "name": "execute_ipython_cell"}, "id": "call_0Hpwkgjw2UqeKyECpVmgqDSP", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 91, "prompt_tokens": 7155, "total_tokens": 7246, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 6400}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "3.1 Linear Classifier Probing\nProbe technology (Alain and Bengio, 2016) is a\nmethod for analyzing and evaluating the internal\nrepresentations of a neural network by applying\na specific probe task, typically a classification or\nregression task, to a particular layer of the model.\nThis technique measures the layer\u2019s ability to repre-\nsent information for the given task, thereby reveal-\ning the features and information captured by dif-\nferent layers of the model. Our approach involves\nextracting the representations from each layer of\nthe large model, training a binary classifier on these\nrepresentations, and validating its accuracy.\nFor one specific task wthat contains nquestions,\nthe hidden feature set in LLMs is x\u2208Rn\u00d7dmodel ,\nwhere ndenotes number of samples, and x(i)\u2208\nR1\u00d7dmodel represent the representation at a certain\nlayer, where dmodel donate the dimension for the\nhidden layer representation. Binary label y(i)is\nset as 0 or 1. The objective function of such a\nbinary Logistic regression classifier probes with L2\nregularization can be written as:\nJ(\u03b8) =\u22121\nnnX\ni=1Cost (\u03c3(x(i)), y(i)) +\u03bb\n2nmX\nj=1\u03b82\nj (1)\nCost (\u03c3(x(i)), y(i)) =y(i)log\u0010\n\u03c3(\u03b8Tx(i))\u0011\n+(1\u2212y(i)) log\u0010\n1\u2212\u03c3(\u03b8Tx(i))\u0011 (2)\nwhere \u03b8is the parameter in this Logistic regression\nmodel, \u03bbis the regularization parameter. The lin-\near model predicts LLM\u2019s response to the test set,\ncompared with the true label. This yields a quan-\ntification of how well LLMs understand the current\ndepth. If the binary model gets good accuracy at a\ncertain layer, that means the LLM can distinguish\ntrue or false in this layer.\n4 Experimental Setting\nOur experiments used nine datasets containing\nthree aspects (emotion understanding, reasoning,\nand fact-checking). We categorized these nine\ndatasets from easy to complex levels according to\nthe performance of LLaMA3-8B-Instruct (Dubey\net al., 2024), GPT-4o-mini (OpenAI, 2024), and\nQWen2-7B-Instruct (Yang et al., 2024) on each\ndataset (see Section 4.2.1) to anchor the difficulty\nof the datasets. Specifically, the datasets in which\nthe linear probes can obtain high classification accu-\nracy at the initial or middle depth of the LLMs arecategorized as easy levels. Other datasets where lin-\near probes can only accurately classify at a deeper\nlayer of the model or even fail to classify accurately\nare categorized as complex levels. The average ac-\ncuracy of these datasets on the three models was\nconsistent with the probe results and had a signifi-\ncant correlation. In Section 4.1, we introduce the\nLLMs used for experiments. The nine datasets are\ndescribed in Section 4.2.\n4.1 Models\nIn this paper, we employ three open-source model\nfamilies: Gemma (2B, 7B) (Team et al., 2024),\nLLaMA-2 (7B, 13B) (Touvron et al., 2023), and\nQwen (0.5B, 1.8B, 4B, 7B, and 14B) (Bai et al.,\n2023) to support our analysis. Table 1 shows the\nnumber of layers in each model. We choose a lin-\near classifier probe for the experiments during the\nprobing analysis. The ratio of the training set to\nthe test set is 8:2, following the usual approach\nof LLMs probing classifier (Duan et al., 2024; Pal\net al., 2023). We extract feature representations\nfrom the final layer in the transformer at each layer\nof LLMs( e.g. 14-th \u2019post_attention_layernorm\u2019 in\nLlama2-7B (32 Layers in total) ) as input to the\nprobing classifier. The other series of models fol-\nlow a similar processing pattern.\nModel Layer Model Layer Model Layer\nGemma-2B 18 Qwen-4B 40 LLaMA-7B 32\nQwen-0.5B 24 Gemma-7B 28 Qwen-14B 40\nQwen-1.8B 24 Qwen-7B 32 LLaMA-13B 40\nTable 1: Number of layers in each LLM.\n4.2 Datasets\nTable 4 presents the nine datasets we use, on Fact\n(Cities (Marks and Tegmark, 2023), Common-\nClaim (Casper et al., 2023), Counterfact (Meng\net al., 2022)), Emotion (STSA (Kim, 2014),\nIMDb (Maas et al., 2011), Sarcasm (Misra and\nArora, 2023), HateEval (Manolescu et al., 2019)),\nandReasoning (StrategyQA (Geva et al., 2021),\nCoinflip (Wei et al., 2022b)) for our experiments.\nA detailed description of the dataset can be found\nin the Appendix A.2.\n4.2.1 Anchoring Difficulties of Each Dataset\nTo ascertain the learning difficulty of each dataset,\nwe have utilized the LLaMA3-8B-Instruct (Dubey\net al., 2024), GPT-4o-mini (OpenAI, 2024), and\nQWen2-7B-Instruct model (Yang et al., 2024).\nOur approach involves testing each sample in the\n/uni00000013/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni00000015/uni00000025\n/uni00000013/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000013/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025\n/uni00000013/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000014/uni00000016/uni00000025\n/uni00000013/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000013/uni00000011/uni00000018/uni00000025\n/uni00000013/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000014/uni00000011/uni0000001b/uni00000025\n/uni00000013/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000017/uni00000025\n/uni00000013/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni0000001a/uni00000025\n/uni00000013/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000014/uni00000017/uni00000025\n/uni00000026/uni0000004c/uni00000057/uni0000004c/uni00000048/uni00000056\n/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000057\n/uni0000002b/uni00000044/uni00000057/uni00000048/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000036/uni00000037/uni00000036/uni00000024\n/uni0000002c/uni00000030/uni00000027/uni00000045/uni00000036/uni00000044/uni00000055/uni00000046/uni00000044/uni00000056/uni00000050\n/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000034/uni00000024/uni00000026/uni00000052/uni0000004c/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000053Figure 3: Analysis diagrams of Section 5.1. Linear probing accuracy of three LLM families (Gemma, LLaMA,\nQwen) on nine datasets.\ndatasets as a binary classification problem via a\nprompting way. The model generates a response\nfor each sample, from which we infer a judgment,\ncategorizing it as either \"Yes\" or \"No\". By com-\nparing these judgments with the actual labels, we\ncompute the accuracy for each dataset.\nTable 2 presents the results of this analysis. The\ndataset with the highest accuracy is deemed the\neasiest dataset to classify. Conversely, the dataset\nwith the lowest accuracy is considered the most\ndifficult to classify. This method quantitatively\nmeasures the learning difficulty associated with\neach dataset.\n4.3 Metrics for Accuracy Variation\nDefinition 1 (Variation Rate) Given an LLM prob-\ning classifier M={q, y, z, d }(q,y,z, and dare\nthe input question, ground truth binary label, pre-\ndicted label and total amount of layers, respec-\ntively), it has the accuracy \u03b1iat i-th layer:\n\u03b1i=1\n|z|\u2217|z|X\nk=1[yk=zk], i\u2208 {0,1,2, ..., d\u22121}Dataset Accuracy\nCoinflip 0.5920\nCommon 0.6487\nSarcasm 0.6597\nStrategyQA 0.6969\nCounterfact 0.7126\nHateEval 0.7640\nSTSA 0.9116\nCities 0.9204\nIMDb 0.9380\nTable 2: Average accuracy on nine datasets based on\nLLaMA3-8b-Instruct, GPT-4o-mini and QWen2-7B-\nInstruct. Accuracy based on each model is shown in\nTable 5.\nWe denote the variation rate \u03b2iwhere\n\u03b2i=\u03b1i/\u03b1i\u22121, i\u2208 {1,2, ..., d\u22121}\nWe introduce two metrics to capture variations in\naccuracy: the jumping point and the converging\npoint and define them by the given definition of\nvariation rate.\nDefinition 2 (Jumping point) We denote the\njumping point\nJ(M, D ) = min {i\nd}\ns.t. \u03b2 i>= 1.1, i\u2208 {1,2, ..., d\u22121}\nwhere M and D= (q, y)are the LLM classifier\nand the dataset.\nWhen a noticeable boost in accuracy is observed,\nthe jumping point signals the model\u2019s recognition\nof a dataset\u2019s critical patterns.\nDefinition 3 (Converging Point) We denote the\nconverging point\nC(M, D ) = max {i\nd}\ns.t.|\u03b2i\u22121|<0.03, i\u2208 {1,2, ..., d\u22121}\nwhere MandD= (q, y)are the LLM classifier\nand the dataset.\nAs the accuracy plateaus or starts declining, the\nconverging point indicates the model\u2019s learning sat-\nuration or peak learning capacity from the dataset.\nAnalyzing these metrics offers deeper insight into\nthe model\u2019s learning dynamics and adaptability to\nvarious data types.\n5 Experimental Analysis\nWe conduct experiments to answer the following\nresearch questions about the Concept Depth:\nRQ1 : Do different LLMs\u2019 Concept Depths be-\nhave consistently in the same dataset? (Section\n5.1)\nRQ2 : Do different size LLMs in the same family\n(e.g., the LLaMA family ) have consistent Concept\nDepth? (Section 5.2)\nRQ3 : Do LLMs\u2019 Concept Depth of the same\nsize behave consistently? (Section 5.3)\n5.1 Comparison Among the Datasets\nWe delve into an evaluative performance compar-\nison across a range of datasets, utilizing Figure 3\nto detail the layer-wise accuracy of all nine LLMs\nover nine distinct datasets. Table 3 shows the de-\ntailed numerical results for Figure 3, as well as\nthe F1-score and AUC. A performance threshold\nof 0.7 accuracy is applied to assess the models\u2019\neffective comprehension of concepts. This exam-\nination leads to two general observations. Firstly,\nregarding different concepts, LLMs exhibit varying\naccuracy trends across their layers. For example,\nCities approaches perfect accuracy fast; in con-\ntrast, datasets requiring high-level reasoning suchasStrategyQA will not reliably converge to ac-\ncuracy above 0.7, indicating that they have differ-\nent \u201cConcept Depth\u201d. Within individual concepts,\nhowever, different LLMs tend to display consistent\naccuracy patterns across these layers. Secondly,\nin tasks that require varying levels of conceptual\nunderstanding, the LLMs demonstrate their under-\nstanding across different layers, indicating a lay-\nered approach to processing complex concepts.\nSignificant variations in trends are observed\nacross the models among the three factual concept\ndatasets. Cities exhibits a sharp increase in compre-\nhension at lower layers, stabilizing in higher layers,\nindicating a strong grasp of the concept. Com-\nmonClaim has become stable in early layers. Be-\nsides, the accuracy improvement of the nine LLMs\ntrained on Counterfact was relatively difficult to\nachieve, utilizing deeper layers, and the accuracy\nwas lower than that of many other datasets. There-\nfore, we can conclude that Counterfact is more\ncomplex.\nIn datasets centered on emotional concept com-\nprehension ( STSA ,IMDb ,Sarcasm , and HateE-\nval), despite varying levels of understanding, all\nmodels demonstrate a rise in accuracy at the initial\nlayers, with convergence occurring in the intermedi-\nate layers. Although HateEval essentially reaches\nstable at the initial layers, its accuracy reaches up to\n0.8, suggesting that LLMs primarily aggregate rep-\nresentations from lower layers to grasp emotional\nconcepts. Meanwhile, StrategyQA andCoinflip ,\nwhich demand specific reasoning skills, tend to dis-\nplay a bell-shaped accuracy trajectory in all models,\nwith peak accuracy observed in the middle layers.\nSuch patterns underscore the intricate complexity\nassociated with reasoning tasks.\nRemark 1\nWe categorize the performances into three\ntypes. 1) For Cities ,STSA ,IMDb , and Sar-\ncasm , the LLMs suddenly understand the\ntasks at intermediate layers. 2) For Com-\nmonClaim andHateEval , the LLMs have\nalready understood the tasks in shallower\nlayers. 3) For Counterfact ,StrategyQA ,\nandCoinflip , The tasks are more difficult\nto understand compared with others. There-\nfore, we consider the tasks in type 1 and 2\neasy tasks, and those in type 3 are complex.\n/uni00000026/uni0000004c/uni00000057/uni0000004c/uni00000048/uni00000056/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000057/uni0000002b/uni00000044/uni00000057/uni00000048/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000036/uni00000037/uni00000036/uni00000024 /uni0000002c/uni00000030/uni00000027/uni00000045/uni00000036/uni00000044/uni00000055/uni00000046/uni00000044/uni00000056/uni00000050/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000034/uni00000024/uni00000026/uni00000052/uni0000004c/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000053/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000052/uni00000051/uni00000059/uni00000048/uni00000055/uni0000004a/uni00000048/uni00000003/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044\n/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni00000015/uni00000025 /uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni0000001a/uni00000025/uni00000026/uni0000004c/uni00000057/uni0000004c/uni00000048/uni00000056/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000057/uni0000002b/uni00000044/uni00000057/uni00000048/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000036/uni00000037/uni00000036/uni00000024 /uni0000002c/uni00000030/uni00000027/uni00000045/uni00000036/uni00000044/uni00000055/uni00000046/uni00000044/uni00000056/uni00000050/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000034/uni00000024/uni00000026/uni00000052/uni0000004c/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000053/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000052/uni00000051/uni00000059/uni00000048/uni00000055/uni0000004a/uni00000048/uni00000003/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025 /uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000014/uni00000016/uni00000025/uni00000026/uni0000004c/uni00000057/uni0000004c/uni00000048/uni00000056/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000057/uni0000002b/uni00000044/uni00000057/uni00000048/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000036/uni00000037/uni00000036/uni00000024 /uni0000002c/uni00000030/uni00000027/uni00000045/uni00000036/uni00000044/uni00000055/uni00000046/uni00000044/uni00000056/uni00000050/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000034/uni00000024/uni00000026/uni00000052/uni0000004c/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000053/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000052/uni00000051/uni00000059/uni00000048/uni00000055/uni0000004a/uni00000048/uni00000003/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000034/uni0000005a/uni00000048/uni00000051\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000013/uni00000011/uni00000018/uni00000025\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000014/uni00000011/uni0000001b/uni00000025/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000017/uni00000025\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni0000001a/uni00000025/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000014/uni00000017/uni00000025(a) The converging point of each dataset on Gemma, LLaMA, and Qwen represented by the percent depth proportion.\n/uni00000026/uni0000004c/uni00000057/uni0000004c/uni00000048/uni00000056/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000057/uni0000002b/uni00000044/uni00000057/uni00000048/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000036/uni00000037/uni00000036/uni00000024 /uni0000002c/uni00000030/uni00000027/uni00000045/uni00000036/uni00000044/uni00000055/uni00000046/uni00000044/uni00000056/uni00000050/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000034/uni00000024/uni00000026/uni00000052/uni0000004c/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000053/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000033/uni00000048/uni00000044/uni0000004e/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044\n/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni00000015/uni00000025 /uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni0000001a/uni00000025/uni00000026/uni0000004c/uni00000057/uni0000004c/uni00000048/uni00000056/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000057/uni0000002b/uni00000044/uni00000057/uni00000048/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000036/uni00000037/uni00000036/uni00000024 /uni0000002c/uni00000030/uni00000027/uni00000045/uni00000036/uni00000044/uni00000055/uni00000046/uni00000044/uni00000056/uni00000050/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000034/uni00000024/uni00000026/uni00000052/uni0000004c/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000053/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000033/uni00000048/uni00000044/uni0000004e/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025 /uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni00000014/uni00000016/uni00000025/uni00000026/uni0000004c/uni00000057/uni0000004c/uni00000048/uni00000056/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000057/uni0000002b/uni00000044/uni00000057/uni00000048/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000036/uni00000037/uni00000036/uni00000024 /uni0000002c/uni00000030/uni00000027/uni00000045/uni00000036/uni00000044/uni00000055/uni00000046/uni00000044/uni00000056/uni00000050/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000034/uni00000024/uni00000026/uni00000052/uni0000004c/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000053/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000033/uni00000048/uni00000044/uni0000004e/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000034/uni0000005a/uni00000048/uni00000051\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000013/uni00000011/uni00000018/uni00000025\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000014/uni00000011/uni0000001b/uni00000025/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000017/uni00000025\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni0000001a/uni00000025/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000014/uni00000017/uni00000025\n(b) The peak accuracy of each dataset on Gemma, LLaMA, and Qwen represented by the percent depth proportion.\nFigure 4: Analysis diagrams of Section 5.2. The converge proportion and peak accuracy of each model over the\nnine datasets. (a) shows the converged proportion over the datasets. (b) shows the peak accuracy over the datasets.\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000034/uni00000024\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni00000052/uni0000004c/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000053\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni0000004c/uni00000057/uni0000004c/uni00000048/uni00000056\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000057\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000002b/uni00000044/uni00000057/uni00000048/uni00000028/uni00000059/uni00000044/uni0000004f\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000036/uni00000037/uni00000036/uni00000024\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000002c/uni00000030/uni00000027/uni00000045\n/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni0000001a/uni00000025 /uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni00000010/uni0000001a/uni00000025 /uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni0000001a/uni00000025/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000036/uni00000044/uni00000055/uni00000046/uni00000044/uni00000056/uni00000050\nFigure 5: Analysis diagrams of Section 5.3. Linear probing accuracy of Gemma-7B, LLaMA-7B, Qwen-7B on nine\ndatasets.\n5.2 Comparison Among the Number of\nParameters\nThis section offers a comparative analysis of LLMs\nwithin their respective families, examining both ac-curacy levels and converging points across the mod-\nels. Figure 4 reveals two recurring patterns within\nthese families: for tasks with accuracy improves\ndramatically by model learning, larger models tend\nto show converging points at earlier layers, suggest-\ning they achieve their own peak comprehensions\nof concepts at lower layers; for tasks with accu-\nracy changes little, all LLMs show the converging\npoints at early layers.\nTwo notable exceptions to this trend appear in\nthe Qwen family over the Coinflip and IMDb\ndatasets. For Coinflip , larger models exhibit de-\nlayed convergence. This deviation underscores the\ncomplexity of the reasoning required, illustrating\nhow this task challenges even the larger models\nto extend their depth of understanding further. For\nIMDb , converging points fluctuate with the increas-\ning size of the model because the number of layers\nis different among different sizes of LLMs, which\namplifies the differences. These exceptions are also\nfound in the Gemma family.\nFurthermore, in Figure 4, we explore the peak\naccuracy levels across all layers for LLMs of dif-\nfering sizes. The overarching trend indicates that\nlarger models consistently achieve superior peak\nperformance. This observation not only supports\nthat scaling up models enhances their effectiveness\nbut also suggests that larger models develop more\nrobust internal representations, validating the bene-\nfits of training models with greater capacity.\nRemark 2\nWe have two observations by comparing dif-\nferent sizes of models from the same LLM\nfamily. 1) As the number of parameters in-\ncreases, peak accuracy gradually increases,\nand the converging point gradually advances.\n2) Larger models grasp the concepts earlier\nand better.\n5.3 Comparison Among the LLM Families\nWe examine how LLMs from various families, pos-\nsessing a similar parameter count, process concepts\nas reflected by their converging points and peak ac-\ncuracies. The overarching trends are highlighted in\nFigure 6, with detailed statistics on a layer-by-layer\nbasis provided in Figure 5. Our findings reveal that\nwhile LLMs across different families may reach\nnearly identical peak accuracies, the layers at which\nthey converge to these peaks can vary. For instance,\nin the HateEval andCounterfact datasets, we ob-\nserve models converging at significantly deeper\nlayers. This variation suggests that despite simi-\nlar parameter scales, different models may employ\nvaried mechanisms to tackle the same problems,\nreflecting the diversity in how models interpret and\nCities\nCommon\nCounterfact\nHateEval\nSTSA IMDbSarcasmStrategyQACoinflip\n0.10.20.30.40.50.60.70.80.91.0Gemma-7B\nLLaMA-7B\nQwen-7B\nCities\nCommon\nCounterfact\nHateEval\nSTSA IMDbSarcasmStrategyQACoinflip\n0.70.80.91.0Figure 6: The upper radar image is the converging point\nof each dataset on Gemma-7B, LLaMA-7B, and Qwen-\n7B, represented by the percent depth proportion. The\nbottom radar image is the maximum accuracy of each\ndataset on Gemma-7B, LLaMA-7B, and Qwen-7B, rep-\nresented by the percentage depth proportion.\nprocess complex information.\nRemark 3\nWith the same number of model parameters,\nthe models generally have a comparable un-\nderstanding of the datasets.\n5.4 Ablation Study\nTo quantify the robustness of the LLMs concerning\ntheir internal representation, we conducted ablation\nstudies on noise perturbation and bit quantization.\nThe result shows that adding noises or reducing\nmodel weights to 8 bits can make the accuracy\nconverge slower. Compressing the LLMs to 16 bits\ndoesn\u2019t harm the understanding process too much.\nDetails can be found in Section A.3.\n6 Conclusions\nThis paper proposes Concept Depth, the phe-\nnomenon that different concepts are learned in dif-\nferent layers of LLMs, i.e., more difficult concepts\nare fully acquired with deeper layers. We con-\nducted several experiments around Concept Depth\nusing probe techniques. Our research suggests that\nLLMs tend to effectively categorize easy tasks, in-\ndicating that these concepts are learned in the first\nfew initial layers. In contrast, complex tasks may\nonly be recognizable (if at all) in deeper layers,\nand LLMs of the same size perform largely con-\nsistently across datasets regarding Concept Depth.\nCompressing the model weight to 16-bit representa-\ntions for future LLMs\u2019 designs is also a promising\nmethod for saving computation memory.\n7 Limitations\nThe paper presents several opportunities for further\nexploration. Firstly, the datasets employed might\nnot encompass the full spectrum of language tasks,\noffering a chance to expand the scope of the find-\nings in a multilingual environment. Secondly, We\ndid not experiment with very large open-source\nlanguage models, thus allowing future researchers\nto investigate how scaling up the model size af-\nfects concept acquisition across different layers\nand enhances robustness. Moreover, we should\nalso try different kinds of classifiers, including non-\nlinear models and neural network-based classifiers,\nto acquire more profound insights into how LLM\nrepresentations differ across layers. These aspects\nhighlight promising directions for continued ad-\nvancement in the field. We will continue to explore\nintermediate representations to help us better un-\nderstand the inner side of LLMs, as this challenge\nmay also be open to other researchers in this field.\nAcknowledgement\nWe thank Taowen Wang, Fei Sun, Wujiang Xu, and\nGuangyan Sun for their valuable discussions and\nsuggestions during the project.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774 .\nGuillaume Alain and Yoshua Bengio. 2016. Under-\nstanding intermediate layers using linear classifier\nprobes. arXiv preprint arXiv:1610.01644 .\nSharon Lee Armstrong, Lila R Gleitman, and Henry\nGleitman. 1983. What some concepts might not be.\nCognition , 13(3):263\u2013308.\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an llm knows when its lying. arXiv preprint\narXiv:2304.13734 .\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,\nRunji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,\nKeming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,\nXuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong\nTu, Peng Wang, Shijie Wang, Wei Wang, Shengguang\nWu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian\nYang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi\nYuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,\nYichang Zhang, Zhenru Zhang, Chang Zhou, Jin-\ngren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.\nQwen technical report. Preprint , arXiv:2309.16609.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems , 33:1877\u20131901.\nStephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and\nDylan Hadfield-Menell. 2023. Explore, establish,\nexploit: Red teaming language models from scratch.\nPreprint , arXiv:2306.09442.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,\nYi Chang, Philip S. Yu, Qiang Yang, and Xing Xie.\n2023. A survey on evaluation of large language mod-\nels.Preprint , arXiv:2307.03109.\nHuiqi Deng, Qihan Ren, Hao Zhang, and Quanshi\nZhang. 2021. Discovering and explaining the rep-\nresentation bottleneck of dnns. arXiv preprint\narXiv:2111.06236 .\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2024. Qlora: Efficient finetuning\nof quantized llms. Advances in Neural Information\nProcessing Systems , 36.\nHanyu Duan, Yi Yang, and Kar Yan Tam. 2024. Do\nllms know about hallucination? an empirical in-\nvestigation of llm\u2019s hidden states. arXiv preprint\narXiv:2402.09733 .\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783 .\nSiqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng\nHan, Shuo Shang, Aixin Sun, Yequan Wang, and\nZhongyuan Wang. 2024. Not all layers of llms\nare necessary during inference. arXiv preprint\narXiv:2403.02181 .\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual asso-\nciations in auto-regressive language models. arXiv\npreprint arXiv:2304.14767 .\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did Aristotle\nUse a Laptop? A Question Answering Benchmark\nwith Implicit Reasoning Strategies. Transactions of\nthe Association for Computational Linguistics , 9:346\u2013\n361.\nAndrey Gromov, Kushal Tirumala, Hassan Shapourian,\nPaolo Glorioso, and Daniel A Roberts. 2024. The un-\nreasonable ineffectiveness of the deeper layers. arXiv\npreprint arXiv:2403.17887 .\nWes Gurnee and Max Tegmark. 2023. Language\nmodels represent space and time. arXiv preprint\narXiv:2310.02207 .\nMingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang,\nWenyue Hua, Ruixiang Tang, William Yang Wang,\nand Yongfeng Zhang. 2024a. Disentangling mem-\nory and reasoning ability in large language models.\narXiv preprint arXiv:2411.13504 .\nMingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao,\nWenyue Hua, Yanda Meng, Yongfeng Zhang, and\nMengnan Du. 2024b. The impact of reasoning step\nlength on large language models. arXiv preprint\narXiv:2401.04925 .\nTianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan,\nZhaochun Ren, and Gongshen Liu. 2024. How\nlarge language models encode context knowl-\nedge? a layer-wise probing study. arXiv preprint\narXiv:2402.16061 .\nYoon Kim. 2014. Convolutional neural networks\nfor sentence classification. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , pages 1746\u20131751,\nDoha, Qatar. Association for Computational Linguis-\ntics.\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter\nPfister, and Martin Wattenberg. 2024. Inference-\ntime intervention: Eliciting truthful answers from\na language model. Advances in Neural Information\nProcessing Systems , 36.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nInProceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 142\u2013150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nMihai Manolescu, Denise L\u00f6fflad, Adham Nasser Mo-\nhamed Saber, and Masoumeh Moradipour Tari. 2019.\nTuEval at SemEval-2019 task 5: LSTM approach\nto hate speech detection in English and Spanish. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation , pages 498\u2013502, Minneapolis,\nMinnesota, USA. Association for Computational Lin-\nguistics.\nSamuel Marks and Max Tegmark. 2023. The geometry\nof truth: Emergent linear structure in large language\nmodel representations of true/false datasets. arXiv\npreprint arXiv:2310.06824 .Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang,\nHongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng\nChen. 2024. Shortgpt: Layers in large language\nmodels are more redundant than you expect. arXiv\npreprint arXiv:2403.03853 .\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associ-\nations in gpt. Advances in Neural Information Pro-\ncessing Systems , 35:17359\u201317372.\nRishabh Misra and Prahal Arora. 2023. Sarcasm detec-\ntion using news headlines dataset. AI Open , 4:13\u201318.\nOpenAI. 2024. Hello gpt-4o. OpenAI Blog .\nKoyena Pal, Jiuding Sun, Andrew Yuan, Byron C Wal-\nlace, and David Bau. 2023. Future lens: Anticipating\nsubsequent tokens from a single hidden state. arXiv\npreprint arXiv:2311.04897 .\nTim R\u00e4z. 2023. Methods for identifying emergent con-\ncepts in deep neural networks. Patterns , 4(6).\nJie Ren, Mingjie Li, Qirui Chen, Huiqi Deng, and Quan-\nshi Zhang. 2023. Defining and quantifying the emer-\ngence of sparse concepts in dnns. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 20280\u201320289.\nZhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xi-\naoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, et al.\n2024a. Living in the moment: Can large language\nmodels grasp co-temporal reasoning? arXiv preprint\narXiv:2406.09072 .\nZhaochen Su, Zecheng Tang, Xinyan Guan, Juntao Li,\nLijun Wu, and Min Zhang. 2022. Improving tem-\nporal generalization of pre-trained language mod-\nels with lexical semantic change. arXiv preprint\narXiv:2210.17127 .\nZhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu\nLi, Jiashuo Sun, Juntao Li, Min Zhang, and Yu Cheng.\n2024b. Conflictbank: A benchmark for evaluating\nthe influence of knowledge conflicts in llm. In Ad-\nvances in neural information processing systems .\nZhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao\nLi, Min Zhang, and Yu Cheng. 2024c. Timo: To-\nwards better temporal reasoning for language models.\narXiv preprint arXiv:2406.14192 .\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai,\nAnja Hauth, et al. 2023. Gemini: a family of\nhighly capable multimodal models. arXiv preprint\narXiv:2312.11805 .\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay\nKale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot,\nAakanksha Chowdhery, Adam Roberts, Aditya\nBarua, Alex Botev, Alex Castro-Ros, Ambrose Slone,\nAm\u00e9lie H\u00e9liou, Andrea Tacchetti, Anna Bulanova,\nAntonia Paterson, Beth Tsai, Bobak Shahriari, Char-\nline Le Lan, Christopher A. Choquette-Choo, Cl\u00e9-\nment Crepy, Daniel Cer, Daphne Ippolito, David\nReid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng\nYan, George Tucker, George-Christian Muraru, Grig-\nory Rozhdestvenskiy, Henryk Michalewski, Ian Ten-\nney, Ivan Grishchenko, Jacob Austin, James Keel-\ning, Jane Labanowski, Jean-Baptiste Lespiau, Jeff\nStanway, Jenny Brennan, Jeremy Chen, Johan Fer-\nret, Justin Chiu, Justin Mao-Jones, Katherine Lee,\nKathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa\nLee, Lucas Dixon, Machel Reid, Maciej Miku\u0142a,\nMateo Wirth, Michael Sharman, Nikolai Chinaev,\nNithum Thain, Olivier Bachem, Oscar Chang, Oscar\nWahltinez, Paige Bailey, Paul Michel, Petko Yotov,\nPier Giuseppe Sessa, Rahma Chaabouni, Ramona\nComanescu, Reena Jana, Rohan Anil, Ross McIl-\nroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Se-\nbastian Borgeaud, Sertan Girgin, Sholto Douglas,\nShree Pandya, Siamak Shakeri, Soham De, Ted Kli-\nmenko, Tom Hennigan, Vlad Feinberg, Wojciech\nStokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao\nGong, Tris Warkentin, Ludovic Peran, Minh Giang,\nCl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray\nKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,\nDouglas Eck, Joelle Barral, Fernando Pereira, Eli\nCollins, Armand Joulin, Noah Fiedel, Evan Senter,\nAlek Andreev, and Kathleen Kenealy. 2024. Gemma:\nOpen models based on gemini research and technol-\nogy. Preprint , arXiv:2403.08295.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239 .\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288 .\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682 .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems , 35:24824\u201324837.\nXiming Wen. 2024. Language model meets proto-\ntypes: Towards interpretable text classification mod-\nels through prototypical networks. arXiv preprint\narXiv:2412.03761 .\nXiming Wen, Wenjuan Tan, and Rosina O Weber. 2024a.\nGaprotonet: A multi-head graph attention-based pro-\ntotypical network for interpretable text classification.\narXiv preprint arXiv:2409.13312 .Ximing Wen, Rosina O Weber, Anik Sen, Darryl Han-\nnan, Steven C Nesbit, Vincent Chan, Alberto Goffi,\nMichael Morris, John C Hunninghake, Nicholas E\nVillalobos, et al. 2024b. The impact of an xai-\naugmented approach on binary classification with\nscarce data. arXiv preprint arXiv:2407.06206 .\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671 .\nChih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang\nLi, Pradeep Ravikumar, and Tomas Pfister. 2019. On\nconcept-based explanations in deep neural networks.\nMingyang Zhang, Chunhua Shen, Zhen Yang, Linlin\nOu, Xinyi Yu, Bohan Zhuang, et al. 2023. Prun-\ning meets low-rank parameter-efficient fine-tuning.\narXiv preprint arXiv:2305.18403 .\nHaiyan Zhao, Fan Yang, Himabindu Lakkaraju, and\nMengnan Du. 2024. Opening the black box of large\nlanguage models: Two views on holistic interpretabil-\nity.arXiv preprint arXiv:2402.10688 .\nZihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan\nYe, Wei Liu, Wei Wang, Xiaowei Huang, and Kaizhu\nHuang. 2024. Mathattack: Attacking large language\nmodels towards math solving ability. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 38, pages 19750\u201319758.\nA Appendix\nHere, we provide our supplementary materials.\nA.1 Metrics for Parts of the Layers\nTable 3 shows the experimental results for the accu-\nracy, F1-score, and AUC metrics of parts of the first,\n25% depth, 50% depth, 67% depth, 83% depth, and\nthe last layer of each model over the nine datasets.\nA.2 Description of the dataset\nCities (Marks and Tegmark, 2023): consists of\nstatements about the location of cities and their\nveracity labels (e.g., The city of Zagreb is in Japan,\nwhich is wrong). We use 1496 of these samples.\nCommonClaim (Casper et al., 2023): A dataset of\nboolean statements, each labeled by two humans\nas common-knowledge-true, common-knowledge-\nfalse, or neither. We use 6000 of these samples.\nCounterfact (Meng et al., 2022): Counterfact in-\ncludes myriad counterfactuals that allows quantita-\ntive testing of specificity and generalization when\nlearning a counterfactual. We use 4000 of these\nsamples.\nHateEval (Manolescu et al., 2019): HateEval has\nEnglish tweets which were annotated hierarchically.\nWe use 6000 of these samples.\nSTSA (Kim, 2014): STSA includes movie reviews,\nhalf of which were considered positive and the\nother half negative. Each label is extracted from a\nlonger movie review and reflects the writer\u2019s over-\nall intention for this review. We use 6920 of these\nsamples.\nIMDb (Maas et al., 2011): IMDb is a benchmark\ndataset for binary sentiment classification. We use\n2000 of these samples.\nSarcasm (Misra and Arora, 2023): Sarcasm is a\nsuperior news headline dataset that tells if the head-\nlines are sarcastic. We use 6000 of these samples.\nStrategyQA (Geva et al., 2021): StrategyQA con-\ntains questions across all knowledge domains to\nelicit creative and diverse yes/no questions that re-\nquire implicit reasoning steps. We use 2290 of\nthese samples.\nCoinflip (Wei et al., 2022b): Coinflip includes coin\nflipping queries, asking if a coin remains heads up\nafter it is either flipped or left unflipped by individ-\nuals. We use 500 of these samples.\nA.3 Ablation Study\nAdding Noise. To quantify the robustness of\nthe LLMs concerning their internal representation,when we input these questions into the LLM, we\nadd a random string of noise in front of the question\nto interfere. For instance, we perturbate a question\nqintoq\u2032=N\u2295q. Here, \u2295is the concatenate oper-\nation, and the noise string N\u2208 {S1, S2}disrupts\nthe classification tasks, satisfying\nP(N=S1) =P(N=S2) = 50%\nThe probability distribution of Nis unrelated to\ntheir labels. The following is an example of STSA\nwith a positive label.\nBefore adding noise:\nThe production values are of\nthe highest and the performances\nattractive without being memorable.\nThe sentence above is a movie review\nand reflects the writer\u2019s overall\nintention for this review. According\nto the sentence, judge whether the\nemotion is Positive or Negative.\nAfter adding noise:\naaa The production values are of\nthe highest and the performances\nattractive without being memorable.\nThe sentence above is a movie review\nand reflects the writer\u2019s overall\nintention for this review. According\nto the sentence, judge whether the\nemotion is Positive or Negative.\naaa can be substituted by bbb, both\ncases have equal probability.\nQuantization Settings. Quantization has been a\nrule of thumb for faster LLMs\u2019 inference. The\ngeneral fact is that using a lower precision level\nallows LLMs to be run on less capable hardware\nwith an acceptable reduction in their ability and\naccuracy. Our approach to applying quantization is\nto explore its effect on Concept Depth, whether it\nstill maintains the previous Concept Depth or slows\ndown the process of understanding knowledge. We\nquantize the model weights to 8, 16, and 32 bits.\nResults. Figure 7 illustrates the effect of adding\nnoise and reducing bit representations to the\nGemma-2B model. The addition of noise causes\nthe learning curve to shift to the right, indicating a\nreduction in the converging speed. This suggests\nthat the presence of noise in the input data can hin-\nder the LLM\u2019s learning, slowing down its ability to\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000036/uni00000037/uni00000036/uni00000024\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000036/uni00000044/uni00000055/uni00000046/uni00000044/uni00000056/uni00000050\n/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni00000015/uni00000025/uni00000003/uni0000001b/uni00000010/uni00000045/uni0000004c/uni00000057\n/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni00000015/uni00000025/uni00000003/uni00000014/uni00000019/uni00000010/uni00000045/uni0000004c/uni00000057\n/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni00000015/uni00000025/uni00000003/uni00000029/uni00000058/uni0000004f/uni0000004f/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni00000015/uni00000025/uni00000003/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni0000001b/uni00000010/uni00000045/uni0000004c/uni00000057\n/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni00000015/uni00000025/uni00000003/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000014/uni00000019/uni00000010/uni00000045/uni0000004c/uni00000057\n/uni0000002a/uni00000048/uni00000050/uni00000050/uni00000044/uni00000010/uni00000015/uni00000025/uni00000003/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000029/uni00000058/uni0000004f/uni0000004fFigure 7: Noise and quantization evaluation on STSA\nand Sarcasm of Gemma-2B.\nconverge to an optimal solution. We also see that\nthere isn\u2019t much difference between 32 and 16 bits,\nand the convergence rate slows when we reduce the\nmodel to 8 bits. Therefore, we may compress the\nLLMs to 16 bits for future designs.\nRemark 4\nNoises or 8-bit-quantization can cause the\naccuracy to converge more slowly. Com-\npressing the LLMs to 16 bits doesn\u2019t harm\nthe understanding process too much. The\nlayer-wise representations of LLMs are sus-\nceptible to noise and high-ratio quantization.\nTherefore, it is crucial to proceed cautiously\nwhen conducting high-ratio quantization in-\nference.\nA.4 Examples of Each Dataset\nFor the 9 bi-classification tasks, we provide two\nexamples from each class for each task. The blue\nsentences are the data samples, while the black\nsentences are the prompts.\nCities\n\u201cTrue\u201d example:\nJudge the statement is True or False. The\ncity of Tokyo is in Japan.\n\u201cFalse\u201d example:Judge the statement is True or False. The\ncity of Lodz is in the Dominican Republic.\nCommonClaim\n\u201cTrue\u201d example:\nJudge the statement is True or False.\nSalmon will often return to the same\nspawning ground where then were born.\n\u201cFalse\u201d example:\nJudge the statement is True or False. A\nchicken has two right wings.\nCounterfact\n\u201cTrue\u201d example:\nJudge the statement is True or False. The\ncity of Tokyo is in Japan.\n\u201cFalse\u201d example:\nJudge the statement is True or False.\nKanata South Ward is in Wisconsin.\nHateEval\n\u201cYes\u201d example:\nHere it is not about Refugees or Illegal\nimmigrants. It is about whether one has\ndocuments before 1971 or not. Now, it\nis difficult for slum people and beggars\nto show valid documents, except the name\nin voter list. According to the comment,\ntell whether they present hate speech or\nnot.\n\u201cNo\u201d example:\nLabor migrants transfer almost $10\nbillion a year to Ukraine. According to\nthe comment, tell whether they present\nhate speech or not.\nSTSA\n\u201cPositive\u201d example:\nThe production values are of the highest\nand the performances attractive without\nbeing memorable. The sentence above\nis a movie review and reflects the\nwriter\u2019s overall intention for this\nreview. According to the sentence,\njudge whether the emotion is Positive or\nNegative.\n\u201cNegative\u201d example:\nLess a story than an inexplicable\nnightmare, right down to the population\u2019s\nshrugging acceptance to each new horror.\nThe sentence above is a movie review and\nreflects the writer\u2019s overall intention\nfor this review. According to the\nsentence, judge whether the emotion is\nPositive or Negative.\nIMDb\n\u201cPositive\u201d example:\nThis is the definitive movie version\nof Hamlet. Branagh cuts nothing, but\nthere are no wasted moments. According\nto the movie review, judge whether it is\nPositive or Negative.\n\u201cNegative\u201d example:\nThis is without a doubt the worst movie\nI have ever seen. It is not funny. It\nis not interesting and should not have\nbeen made. According to the movie review,\njudge whether it is Positive or Negative.\nSarcasm\n\u201cYes\u201d example:\nTask: Detect sarcasm, help me identify\nwhether this sentence is sarcastic. First,\nwe need to understand what sarcasm is.\nSarcasm is a form of verbal irony, where\nthe intended meaning of the words is the\nopposite of the literal meaning. In other\nwords, the speaker is saying one thing\nbut meaning the opposite. Bashar al-Assad\ntries a tiny bit of sarin gas on self\nto see what it\u2019s like. Think carefully\naccording to the sentence. Is there any\nsarcasm in this sentence? Please answer\nYes or No.\n\u201cNo\u201d example:\nTask: Detect sarcasm, help me identify\nwhether this sentence is sarcastic. First,\nwe need to understand what sarcasm is.\nSarcasm is a form of verbal irony, where\nthe intended meaning of the words is the\nopposite of the literal meaning. In other\nwords, the speaker is saying one thing but\nmeaning the opposite. This ceo will send\nyour kids to school, if you work for his\ncompany. Think carefully according to the\nsentence. Is there any sarcasm in this\nsentence? Please answer Yes or No.\nStrategyQA\nNote: This dataset was created in 2021. Queen\nElizabeth was alive then.\n\u201cYes\u201d example:\nJudge the question is true or false? Q:\nWill Queen Elizabeth be buried in the\nPantheon? Let us think step by step. Thestem of the sentence is Queen Elizabeth,\nburial, pantheon. Inference: First, the\nPantheon is a church, so it is possible\nthat she could be buried there. Second,\nQueen Elizabeth II is still alive, so she\nhas not been buried yet. Third, even if\nshe were to be buried in the Pantheon, it\nis unlikely that we would know about it\nahead of time, so it is hard to say for\nsure. pred_ans: no. Do hamsters provide\nfood for any animals? Let us think step\nby step...\n\u201cNo\u201d example:\nJudge the question is true or false? Q:\nWill Queen Elizabeth be buried in the\nPantheon? Let us think step by step. The\nstem of the sentence is Queen Elizabeth,\nburial, pantheon. Inference: First, the\nPantheon is a church, so it is possible\nthat she could be buried there. Second,\nQueen Elizabeth II is still alive, so she\nhas not been buried yet. Third, even if\nshe were to be buried in the Pantheon, it\nis unlikely that we would know about it\nahead of time, so it is hard to say for\nsure. pred_ans: no. Could a llama birth\ntwice during the War in Vietnam (1945-46)?\nLet us think step by step...\nCoinflip\n\u201cYes\u201d example:\nA coin is heads up. Whitney flips the coin.\nErika does not flip the coin. Tj does not\nflip the coin. Benito flips the coin. Is\nthe coin still heads up? Note that \"flip\"\nhere means \"reverse\". According to the\nflipping process above, determine if a\ncoin remains heads up after it is either\nflipped or left unflipped by individuals.\nTherefore, the answer (Yes or No) is?\n\u201cNo\u201d example:\nA coin is heads up. Lucky does not flip\nthe coin. Mireya flips the coin. Jj flips\nthe coin. Kc flips the coin. Is the\ncoin still heads up? Note that \"flip\"\nhere means \"reverse\". According to the\nflipping process above, determine if a\ncoin remains heads up after it is either\nflipped or left unflipped by individuals.\nTherefore, the answer (Yes or No) is?\nGemma-2B (18 Layers) StrategyQA Coinflip Cities Common Claim Counterfact HateEval STSA IMDb Sarcasm\nMetrics ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1\n1st-layer 0.556 0.601 0.588 0.635 0.667 0.626 0.446 0.411 0.422 0.582 0.61 0.584 0.502 0.509 0.518 0.74 0.822 0.742 0.666 0.729 0.68 0.722 0.788 0.725 0.731 0.808 0.714\n25%-layer 0.602 0.642 0.629 0.62 0.64 0.6 0.94 0.987 0.939 0.637 0.683 0.633 0.675 0.745 0.664 0.793 0.89 0.794 0.872 0.942 0.874 0.884 0.953 0.885 0.857 0.935 0.852\n50%-layer 0.639 0.7 0.665 0.65 0.705 0.632 0.983 0.999 0.983 0.648 0.699 0.642 0.729 0.826 0.716 0.802 0.891 0.8 0.915 0.972 0.917 0.941 0.982 0.941 0.89 0.955 0.887\n67%-layer 0.683 0.751 0.708 0.695 0.783 0.69 0.992 1.0 0.992 0.671 0.727 0.666 0.766 0.852 0.758 0.809 0.891 0.806 0.929 0.976 0.93 0.936 0.984 0.936 0.893 0.96 0.889\n83%-layer 0.62 0.668 0.637 0.585 0.625 0.579 0.985 0.999 0.985 0.652 0.703 0.645 0.704 0.782 0.696 0.807 0.89 0.803 0.911 0.97 0.914 0.926 0.977 0.926 0.882 0.944 0.878\nlast-layer 0.592 0.602 0.626 0.525 0.554 0.532 0.988 0.999 0.988 0.647 0.697 0.644 0.685 0.752 0.679 0.803 0.892 0.801 0.895 0.957 0.898 0.941 0.981 0.941 0.837 0.92 0.831\nGemma-7B (28 Layers) StrategyQA Coinflip Cities Common Claim Counterfact HateEval STSA IMDb Sarcasm\nMetrics ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1\nLayer 0.519 0.565 0.569 0.69 0.712 0.656 0.59 0.444 0.439 0.737 0.609 0.053 0.496 0.506 0.514 0.737 0.817 0.741 0.668 0.737 0.679 0.73 0.811 0.735 0.723 0.793 0.713\n25%-layer 0.617 0.653 0.648 0.605 0.654 0.591 0.95 0.987 0.951 0.719 0.687 0.422 0.696 0.773 0.686 0.795 0.875 0.794 0.89 0.958 0.893 0.9 0.958 0.899 0.885 0.953 0.882\n50%-layer 0.658 0.716 0.678 0.765 0.844 0.761 1.0 0.999 0.998 0.75 0.747 0.498 0.835 0.912 0.829 0.784 0.871 0.782 0.929 0.979 0.93 0.932 0.979 0.932 0.907 0.968 0.904\n67%-layer 0.733 0.809 0.744 0.88 0.922 0.875 1.0 0.998 0.995 0.756 0.75 0.515 0.814 0.9 0.809 0.784 0.868 0.782 0.936 0.981 0.937 0.922 0.979 0.923 0.913 0.972 0.91\n83%-layer 0.675 0.746 0.695 0.785 0.818 0.786 0.98 0.997 0.982 0.743 0.74 0.492 0.776 0.867 0.768 0.812 0.897 0.809 0.935 0.979 0.937 0.925 0.977 0.926 0.901 0.963 0.898\nlast-layer 0.604 0.666 0.625 0.57 0.602 0.578 0.95 0.996 0.972 0.759 0.748 0.481 0.729 0.808 0.721 0.82 0.901 0.817 0.92 0.975 0.922 0.938 0.984 0.938 0.862 0.932 0.86\nLlaMA-7B (32 Layers) StrategyQA Coinflip Cities Common Claim Counterfact HateEval STSA IMDb Sarcasm\nMetrics ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1\n1st-layer 0.584 0.608 0.641 0.545 0.525 0.674 0.487 0.472 0.472 0.74 0.617 0.031 0.493 0.491 0.522 0.725 0.814 0.732 0.678 0.744 0.703 0.73 0.799 0.736 0.705 0.773 0.697\n25%-layer 0.657 0.712 0.688 0.615 0.612 0.621 0.93 0.978 0.929 0.736 0.699 0.441 0.684 0.754 0.681 0.806 0.887 0.792 0.513 0.913 0.676 0.914 0.972 0.914 0.89 0.961 0.886\n50%-layer 0.74 0.827 0.754 0.915 0.977 0.907 0.997 1.0 0.997 0.753 0.738 0.477 0.797 0.894 0.793 0.805 0.883 0.791 0.847 0.954 0.866 0.941 0.984 0.941 0.922 0.976 0.919\n67%-layer 0.729 0.805 0.744 0.9 0.966 0.89 0.995 1.0 0.995 0.744 0.729 0.466 0.775 0.872 0.77 0.795 0.88 0.779 0.896 0.957 0.902 0.939 0.985 0.939 0.92 0.973 0.918\n83%-layer 0.699 0.774 0.72 0.88 0.961 0.871 0.993 1.0 0.993 0.734 0.719 0.464 0.744 0.832 0.735 0.798 0.887 0.785 0.913 0.967 0.915 0.939 0.984 0.939 0.908 0.966 0.905\nlast-layer 0.67 0.744 0.69 0.815 0.9 0.8 0.993 1.0 0.993 0.743 0.731 0.464 0.739 0.818 0.731 0.831 0.914 0.83 0.935 0.984 0.937 0.944 0.987 0.944 0.895 0.955 0.892\nLlaMA-13B (40 Layers) StrategyQA Coinflip Cities Common Claim Counterfact HateEval STSA IMDb Sarcasm\nMetrics ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1\n1st-layer 0.567 0.601 0.628 0.475 0.509 0.575 0.481 0.463 0.47 0.741 0.62 0.034 0.486 0.485 0.526 0.719 0.807 0.727 0.697 0.769 0.709 0.732 0.795 0.736 0.692 0.756 0.688\n25%-layer 0.676 0.732 0.701 0.53 0.59 0.515 0.985 0.999 0.985 0.733 0.716 0.457 0.763 0.859 0.758 0.832 0.914 0.829 0.93 0.98 0.931 0.942 0.983 0.943 0.915 0.972 0.913\n50%-layer 0.763 0.844 0.771 0.825 0.886 0.819 0.993 1.0 0.993 0.758 0.751 0.515 0.812 0.897 0.809 0.839 0.92 0.836 0.939 0.984 0.94 0.945 0.984 0.945 0.936 0.983 0.934\n67%-layer 0.716 0.806 0.729 0.795 0.882 0.794 0.993 1.0 0.993 0.751 0.745 0.499 0.776 0.866 0.772 0.838 0.919 0.834 0.938 0.984 0.939 0.94 0.987 0.94 0.924 0.978 0.921\n83%-layer 0.71 0.795 0.719 0.7 0.797 0.703 0.99 1.0 0.99 0.741 0.731 0.487 0.768 0.856 0.762 0.832 0.912 0.829 0.937 0.983 0.938 0.941 0.985 0.942 0.922 0.974 0.919\nlast-layer 0.693 0.772 0.704 0.645 0.715 0.664 0.99 1.0 0.99 0.75 0.743 0.499 0.76 0.841 0.752 0.835 0.913 0.833 0.935 0.984 0.937 0.946 0.988 0.946 0.91 0.969 0.908\nQwen-0.5B (24 Layers) StrategyQA Coinflip Cities Common Claim Counterfact HateEval STSA IMDb Sarcasm\nMetrics ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1\n1st-layer 0.557 0.578 0.607 0.535 0.649 0.657 0.482 0.46 0.464 0.735 0.622 0.11 0.499 0.503 0.527 0.759 0.851 0.764 0.73 0.801 0.733 0.764 0.837 0.762 0.729 0.799 0.72\n25%-layer 0.583 0.63 0.62 0.545 0.582 0.508 0.731 0.797 0.722 0.732 0.651 0.257 0.52 0.524 0.523 0.785 0.864 0.783 0.751 0.829 0.756 0.804 0.887 0.806 0.811 0.895 0.799\n50%-layer 0.619 0.686 0.649 0.62 0.652 0.596 0.935 0.979 0.935 0.744 0.695 0.379 0.68 0.754 0.676 0.793 0.88 0.792 0.846 0.921 0.848 0.884 0.949 0.883 0.838 0.92 0.831\n67%-layer 0.644 0.688 0.673 0.585 0.617 0.561 0.933 0.982 0.934 0.742 0.705 0.375 0.668 0.74 0.665 0.789 0.874 0.786 0.868 0.946 0.87 0.894 0.956 0.893 0.827 0.911 0.821\n83%-layer 0.583 0.61 0.612 0.62 0.668 0.612 0.923 0.971 0.924 0.746 0.706 0.364 0.604 0.657 0.6 0.791 0.867 0.791 0.85 0.927 0.854 0.866 0.941 0.865 0.824 0.902 0.819\nlast-layer 0.55 0.567 0.584 0.55 0.613 0.541 0.912 0.971 0.912 0.742 0.703 0.357 0.579 0.616 0.579 0.784 0.866 0.781 0.844 0.922 0.848 0.879 0.951 0.88 0.825 0.9 0.82\nQwen-1.8B (24 Layers) StrategyQA Coinflip Cities Common Claim Counterfact HateEval STSA IMDb Sarcasm\nMetrics ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1\n1st-layer 0.57 0.6 0.63 0.49 0.634 0.648 0.482 0.458 0.464 0.739 0.619 0.071 0.516 0.514 0.539 0.724 0.819 0.732 0.693 0.762 0.703 0.718 0.784 0.72 0.721 0.796 0.713\n25%-layer 0.607 0.638 0.643 0.58 0.59 0.584 0.583 0.626 0.582 0.736 0.658 0.317 0.521 0.541 0.525 0.809 0.882 0.807 0.775 0.844 0.781 0.81 0.899 0.81 0.833 0.909 0.825\n50%-layer 0.658 0.726 0.676 0.595 0.655 0.58 0.975 0.997 0.975 0.741 0.708 0.419 0.688 0.767 0.683 0.808 0.89 0.807 0.895 0.961 0.897 0.914 0.974 0.915 0.87 0.947 0.864\n67%-layer 0.664 0.733 0.685 0.695 0.759 0.655 0.977 0.996 0.977 0.741 0.717 0.423 0.695 0.776 0.689 0.809 0.886 0.804 0.893 0.963 0.895 0.904 0.968 0.905 0.865 0.943 0.859\n83%-layer 0.631 0.666 0.649 0.7 0.747 0.674 0.972 0.995 0.972 0.735 0.706 0.419 0.657 0.734 0.651 0.791 0.877 0.788 0.89 0.956 0.893 0.891 0.957 0.893 0.835 0.922 0.829\nlast-layer 0.595 0.642 0.604 0.615 0.667 0.605 0.973 0.996 0.973 0.74 0.704 0.404 0.638 0.713 0.631 0.795 0.877 0.794 0.879 0.949 0.882 0.906 0.962 0.907 0.812 0.896 0.808\nQwen-7B (40 Layers) StrategyQA Coinflip Cities Common Claim Counterfact HateEval STSA IMDb Sarcasm\nMetrics ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1\n1st-layer 0.54 0.559 0.602 0.545 0.597 0.588 0.437 0.403 0.418 0.735 0.635 0.169 0.475 0.466 0.498 0.785 0.867 0.786 0.753 0.826 0.757 0.782 0.858 0.785 0.771 0.856 0.765\n25%-layer 0.59 0.625 0.631 0.62 0.635 0.631 0.806 0.89 0.803 0.721 0.657 0.348 0.556 0.57 0.553 0.798 0.879 0.795 0.782 0.869 0.787 0.825 0.916 0.827 0.852 0.924 0.848\n50%-layer 0.705 0.782 0.724 0.66 0.719 0.667 0.985 0.998 0.985 0.744 0.733 0.452 0.731 0.825 0.722 0.802 0.885 0.795 0.912 0.969 0.914 0.929 0.975 0.929 0.882 0.956 0.878\n67%-layer 0.702 0.773 0.722 0.635 0.719 0.64 0.983 0.997 0.983 0.746 0.728 0.465 0.721 0.801 0.716 0.793 0.88 0.789 0.904 0.965 0.907 0.915 0.968 0.913 0.89 0.955 0.888\n83%-layer 0.678 0.751 0.703 0.685 0.762 0.693 0.978 0.996 0.978 0.747 0.724 0.45 0.68 0.747 0.667 0.795 0.881 0.79 0.899 0.961 0.901 0.916 0.965 0.915 0.872 0.941 0.868\nlast-layer 0.676 0.718 0.693 0.585 0.623 0.587 0.982 0.995 0.981 0.75 0.727 0.453 0.657 0.718 0.643 0.782 0.869 0.778 0.902 0.964 0.905 0.92 0.973 0.919 0.848 0.921 0.845\nQwen-14B (32 Layers) StrategyQA Coinflip Cities Common Claim Counterfact HateEval STSA IMDb Sarcasm\nMetrics ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1\n1st-layer 0.582 0.608 0.624 0.555 0.599 0.594 0.436 0.399 0.429 0.738 0.627 0.205 0.487 0.481 0.491 0.77 0.864 0.772 0.749 0.827 0.754 0.768 0.864 0.768 0.785 0.866 0.773\n25%-layer 0.605 0.645 0.644 0.555 0.609 0.566 0.858 0.93 0.855 0.713 0.667 0.362 0.558 0.601 0.555 0.8 0.879 0.796 0.818 0.898 0.821 0.861 0.936 0.864 0.876 0.946 0.871\n50%-layer 0.695 0.775 0.709 0.69 0.731 0.69 0.99 0.999 0.99 0.744 0.742 0.483 0.771 0.861 0.762 0.812 0.897 0.807 0.918 0.972 0.919 0.946 0.983 0.946 0.901 0.966 0.897\n67%-layer 0.734 0.82 0.749 0.685 0.753 0.693 0.992 0.997 0.992 0.762 0.754 0.5 0.769 0.848 0.765 0.802 0.889 0.798 0.914 0.974 0.916 0.93 0.978 0.93 0.898 0.963 0.895\n83%-layer 0.737 0.814 0.751 0.68 0.74 0.677 0.992 0.996 0.992 0.751 0.732 0.485 0.726 0.808 0.719 0.806 0.886 0.805 0.91 0.972 0.912 0.925 0.974 0.925 0.892 0.957 0.887\nlast-layer 0.714 0.785 0.729 0.65 0.681 0.646 0.982 0.996 0.981 0.753 0.74 0.483 0.707 0.779 0.701 0.802 0.881 0.798 0.908 0.97 0.91 0.932 0.979 0.933 0.871 0.943 0.867\nQwen-14B (40 Layers) StrategyQA Coinflip Cities Common Claim Counterfact HateEval STSA IMDb Sarcasm\nMetrics ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1 ACC AUC F1\n1st-layer 0.569 0.59 0.626 0.605 0.639 0.646 0.461 0.422 0.448 0.733 0.62 0.162 0.481 0.486 0.504 0.766 0.86 0.769 0.75 0.824 0.753 0.778 0.856 0.776 0.772 0.859 0.762\n25%-layer 0.603 0.637 0.631 0.68 0.704 0.66 0.866 0.945 0.865 0.72 0.669 0.393 0.621 0.67 0.615 0.8 0.89 0.797 0.842 0.919 0.845 0.862 0.939 0.861 0.874 0.946 0.869\n50%-layer 0.735 0.835 0.747 0.71 0.754 0.681 0.995 1.0 0.995 0.76 0.748 0.515 0.791 0.875 0.787 0.808 0.891 0.807 0.932 0.981 0.934 0.935 0.984 0.935 0.919 0.974 0.916\n67%-layer 0.798 0.883 0.808 0.7 0.791 0.674 0.995 1.0 0.995 0.761 0.76 0.5 0.775 0.859 0.769 0.816 0.894 0.814 0.933 0.98 0.934 0.93 0.978 0.93 0.914 0.968 0.911\n83%-layer 0.796 0.872 0.807 0.715 0.768 0.705 0.992 1.0 0.992 0.753 0.735 0.493 0.753 0.843 0.748 0.816 0.896 0.811 0.927 0.978 0.927 0.936 0.978 0.936 0.912 0.964 0.91\nlast-layer 0.783 0.863 0.792 0.605 0.67 0.586 0.99 1.0 0.99 0.757 0.749 0.481 0.725 0.814 0.723 0.815 0.897 0.812 0.922 0.978 0.923 0.931 0.978 0.93 0.89 0.952 0.887\nTable 3: The experimental results of the nine datasets on nine LLMs. For each LLM, we select six layers (first, 25%\ndepth, 50% depth, 67% depth, 83% depth, last) to record the accuracy, F1-score, and AUC.\nCategory Dataset\nFactCities\nCommon\nCounterfact\nEmotionHateEval\nSTSA\nIMDb\nSarcasm\nReasoningStrategyQA\nCoinflip\nTable 4: The category that each dataset belongs to.\nDataset LLaMA3-8B-Instruct GPT-4o-mini QWen2-7B-Instruct Average\nCoinflip 0.5080 0.7620 0.5060 0.5920\nCommon 0.5606 0.6905 0.6950 0.6487\nSarcasm 0.6575 0.6770 0.6445 0.6597\nStrategyQA 0.7035 0.8803 0.5069 0.6969\nCounterfact 0.5277 0.7990 0.8110 0.7126\nHateeval 0.7640 0.7300 0.7952 0.7631\nSTSA 0.9030 0.9211 0.9108 0.9116\nCities 0.7687 0.9973 0.9953 0.9204\nIMDb 0.9365 0.9370 0.9405 0.9380\nTable 5: Accuracy on nine datasets based on LLaMA3-\n8b-Instruct, GPT-4o-mini and QWen2-7B-Instruct.\nA.5 LLM structure\nHere, we give an introduction to the model struc-\nture (using LLaMA2-7B as an example).\nLlamaForCausalLM(\n(model): LlamaModel(\n(embed_tokens):Embedding(32000,4096)\n(layers):ModuleList(\n(0-31):32 x lamaDecoderLayer(\n(self_attn): LlamaSdpaAttention(\n(g_proj):Linear(in_features=4096,out_features=4096,bias=False)\n(k proj): Linear(in_features=4096,out_features=4096,bias=False)\n(v proj):Linear(in_features=4096,out_features=4096, bias=False)\n(o proj):Linear(in_features=4096, out_features=4096, bias=False)\n(rotary emb):LlamaRotaryEmbedding()\n)\n(mlp):LlamaMLP(\n(gate_proj): Linear(in_features=4096,out_features=11008,bias=False)\n(up_proj): Linear(in_features=4096,out_features=11008,bias=False)\n(down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n(act_fn): siLu()\n)\n(input_layernorm):LlamaRMSNorm()\n(post_attention_layernorm):LlamaRMSNorm()\n)\n)\n(norm):LlamaRMSNorm()\n)\n(lm_head): Linear(in_features=4096,out_features=32000,bias=False)\n)\n[Jupyter current working directory: /workspace]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Extract the section that describes the probing methodology in detail\n# Look for section 3.1 and surrounding context\nstart = paper_text.find('3.1 Linear Classifier Probing')\nend = paper_text.find('3.2', start)\nprobing_section = paper_text[start:end if end != -1 else None]\nprint(probing_section)"}, "success": true}