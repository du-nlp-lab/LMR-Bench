{"id": 49, "timestamp": "2025-05-19T20:15:23.919976", "source": "agent", "message": "Code executed in IPython cell.", "cause": 48, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_7CSKKzFQi6g5GNHgQfwiKb24", "model_response": {"id": "chatcmpl-BZ5raTCjhjo2EdjIbGaSVm4x2d3GF", "created": 1747703718, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Try to find the section about Vision Transformers and 3D feature visualization\\nfor page in reader.pages[6:12]:\\n    text += page.extract_text() + '\\\\n'\\nprint(text[-4000:])\"}", "name": "execute_ipython_cell"}, "id": "call_7CSKKzFQi6g5GNHgQfwiKb24", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 62, "prompt_tokens": 43285, "total_tokens": 43347, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 42752}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "rvention analysis to analyze the role of registers. We replace allregister features at\nthe output of everylayer with their means, averaged over 10k ImageNet training images. This intervention\nremoves the intended purpose of registers to aggregate global input information (Darcet et al., 2023). Table 5\nshows the results. We find that ViTs with fixed register features achieve accuracy comparable to original\n11\nCLS Reg 1 Reg 2 Reg 3 Reg 4 Patch0%20%40%60%Attention Distribution, DINOv2-reg ViT-GFigure 16: Average attention of the [CLS]token.DINOv2-reg with 4 registers\nImageNet acc ( %) ViT-S ViT-B ViT-L ViT-G\nOriginal 81.9 84.8 86.3 87.0\nFix-Reg-Mean 81.7 85.0 86.2 87.0\nTable 5: We fix allregister features at everylayer to\ntheir means and evaluate the intervened ViTs.\nmodels, suggesting that registers act as learned biases in ViTs. This leads to constant key and value states at\nregister tokens, effectively introducing bias terms to self-attention (extra k\u2032andv\u2032in Equation 3). Thus a\nViT with register tokens function equivalently to a standard ViT augmented with explicit attention biases.\nTo summarize our findings:\nMassive activations exist in many but not all ViTs. Similar to those in LLMs, these activations act as\nconstant biases. We also show the recently proposed register tokens have a similar function.\n6 Related Work\nIntriguing properties of autoregressive Transformers. Timkey and Schijndel (2021) observed that in\nGPT-2\u2019s penultimate layer, there are feature dimensions containing activations with magnitudes up to 3,000.\nThey found that these few dimensions dominate several standard measures for evaluating representation\nsimilarity. Heimersheim and Turner (2023) found that the feature norm of the initial token in GPT-2 grows\nmuch faster than other tokens. Kovaleva et al. (2021) and Zhao et al. (2023) demonstrated the existence of\noutlier weights in the LayerNorm of GPT-2 and LLaMA2-13B and showed that setting them to zero leads to\ncatastrophic drop in model performance. Notably, the feature dimension of this weight in LLaMA2-13B (i.e.,\n2100) corresponds to that of a massive activation (Figure 2).\nOutlier features. Various existing works in quantization (Dettmers et al., 2022, Zeng et al., 2022, Xiao\net al., 2023a, Lin et al., 2023, Ahmadian et al., 2023) have studied the existence of outlier features in\nLLMs. Dettmers et al. (2022) showed that outlier features have large activation values in most of their\nsequence dimensions. While massive activations can be seemingly similar to outlier features, we discussed\ntheir fundamental differences in Section 2.3. More importantly, we show that massive activations can not be\nattributed to the existence of outlier features.\nAttention concentration patterns. Clark et al. (2019b), Kovaleva et al. (2019) and Bondarenko et al.\n(2021) discovered that attention in BERT (Devlin et al., 2018) tends to focus on the \u201cseparate\u201d token [SEP].\nXiao et al. (2023b) showed that LLMs assign most of the attention to the starting word token. Darcet et al.\n(2023) revealed the existence of attention artifacts in ViTs. Robinson et al. (2023) found sparse activation\npatterns in ViTs that attract attention to certain tokens. Our work provides an in-depth analysis as to why\nthese patterns emerge, specifically in relation to massive activations.\nBiases in self-attention. There can be various notion of biases in the self-attention mechanism. First, simple\nadditive bias terms can be used in linear layers for computing the query, key and value states (Namazifar\net al., 2023). Second, position biases can be inserted in self-attention to encode positional information of each\ntoken (Su et al., 2021, Press et al., 2021). There are also variants of biases with manually designed softmax\noperator (Miller, 2023, Bondarenko et al., 2023, Hu et al., 2024). Our work reveals that LLMs, even with\nstandard self-attention formulation, would impose implicit bias components in the attention computation\nthrough massive activations.\n12\n[Jupyter current working directory: /workspace]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Try to find the section about Vision Transformers and 3D feature visualization\nfor page in reader.pages[6:12]:\n    text += page.extract_text() + '\\n'\nprint(text[-4000:])"}, "success": true}