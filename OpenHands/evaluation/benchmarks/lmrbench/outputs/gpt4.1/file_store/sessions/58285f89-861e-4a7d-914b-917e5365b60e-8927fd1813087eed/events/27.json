{"id": 27, "timestamp": "2025-05-19T20:22:31.391396", "source": "agent", "message": "Code executed in IPython cell.", "cause": 26, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_qsPdwGeWejsQbTaZ8y4VMIsP", "model_response": {"id": "chatcmpl-BZ5yTXnXZSVKal4z5qrqzF5PacFWz", "created": 1747704145, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Try to extract all text from the first 5 pages to manually search for the CAD algorithm\\nwith open(pdf_path, 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    num_pages = len(reader.pages)\\n    text = ''\\n    for i in range(min(num_pages, 5)):\\n        page = reader.pages[i].extract_text()\\n        if page:\\n            text += page + '\\\\n\\\\n'\\nprint(text)\"}", "name": "execute_ipython_cell"}, "id": "call_qsPdwGeWejsQbTaZ8y4VMIsP", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 118, "prompt_tokens": 9014, "total_tokens": 9132, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 8192}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "Trusting Your Evidence:\nHallucinate Less with Context-aware Decoding\nWeijia Shi1\u2217Xiaochuang Han1\u2217\nMike Lewis2Yulia Tsvetkov1Luke Zettlemoyer1Scott Yih2\n1University of Washington, Seattle, WA,2Meta AI\n{swj0419, xhan77}@cs.washington.edu\nAbstract\nLanguage models (LMs) often struggle to pay\nenough attention to the input context, and gen-\nerate texts that are unfaithful or contain hallu-\ncinations. To mitigate this issue, we present\ncontext-aware decoding (CAD), which follows\na contrastive output distribution that amplifies\nthe difference between the output probabilities\nwhen a model is used with and without con-\ntext. Our experiments show that CAD, without\nadditional training, significantly improves the\nfaithfulness of different LM families, including\nOPT, GPT, LLaMA and FLAN-T5 for summa-\nrization tasks (e.g., 14.3% gain for LLaMA in\nfactuality metrics). Furthermore, CAD is par-\nticularly effective in overriding a model\u2019s prior\nknowledge when it contradicts the provided\ncontext, leading to substantial improvements in\ntasks where resolving the knowledge conflict is\nessential.\n1 Introduction\nLanguage models (LMs) are remarkably effective\nin generating coherent and fluent continuations of\na prompt or document prefix. During generation,\nthey mostly rely on two sources of knowledge: (1)\nprior knowledge , which is learned during pretrain-\ning and stored implicitly within the model parame-\nters; (2) context knowledge , which is passed as in-\nputs in the prefix context (Chan et al., 2022). How-\never, it remains an open question how a pretrained\nLM, particularly a vanilla LM without task-specific\nfinetuning, balances these two knowledge sources\nduring generation.\nPrevious research shows that LMs can fail to pay\nenough attention to new information introduced\nin the context knowledge. This can lead to hallu-\ncination in summarization (Maynez et al., 2020;\nPagnoni et al., 2021), where the generated sum-\nmaries include facts not present in the input doc-\nument. Insufficient attention to context is espe-\ncially problematic when the context knowledge\n\u2217Equal contribution. Order randomly determined.\nFigure 1: An illustration of context-aware decoding.\ncontradicts with the prior knowledge (Longpre\net al., 2021; Zhou et al., 2023). For instance, when\nLLaMA (Touvron et al., 2023) is presented with a\nlatest document \u201cArgentina won the FIFA World\nCups in 1978,1986 and 2022 ...\u201d in its context (Fig-\nure 1), it still predicts \u201cTwo\u201d in response to the\nquestion \u201cHow many World Cups have Argentina\nwon?\u201d, due in part to the outdated training data.\nIn this work, we present a simple context- aware\ndecoding (CAD) method to encourage the LM to\nattend to its context during generation. As shown\nin Figure 1, CAD samples from a new output dis-\ntribution, which amplifies the difference between\noutput probabilities with and without the context\ndocument. This provides a new form of contrastive\ndecoding (Li et al., 2022), which effectively down-\nweights the prior knowledge when more relevant\ncontextual information is provided. CAD can be\nused with off-the-shelf pretrained language models\nwithout any additional training.\nExperimental results from summarization tasks\nshow that context-aware decoding significantly\nenhances the generation faithfulness of various\nvanilla LMs including OPT (Zhang et al., 2022),\nGPT-Neo (Black et al., 2021), LLaMA (Touvron\net al., 2023) and instruction-finetuned LMs such as\nFLAN (Chung et al., 2022). For instance, when ap-\nplied to LLaMA-30B in CNN-DM, CAD leads to\nsubstantial improvement in both ROUGE-L (21%)arXiv:2305.14739v1  [cs.CL]  24 May 2023\n\nand summary factuality evaluation metrics (14.3%).\nMore notably, CAD is especially beneficial for\nknowledge conflicting tasks, where the context\ncontains information contradictory to the model\u2019s\nprior knowledge. CAD brings a 2.9x improve-\nment to LLaMA-30B on a knowledge conflicts QA\ndataset (Longpre et al., 2021). Furthermore, we\nobserve that this gain brought by CAD increases\nas the model size grows in knowledge conflicts\ntasks. These results demonstrate the potential of\nCAD in mitigating hallucinations in text generation\nand overriding prior knowledge with reliable and\ntrusted information.\n2 Method\n2.1 Background\nGiven a language model \u03b8, an input query x, and\na context cthat contains some external knowledge\nunfamiliar orin conflict to the model\u2019s prior knowl-\nedge, we ask our model \u03b8to generate a response\nygiven the the query and context. The response\ncan be directly sampled (autoregressively) from the\nprobability distribution conditioned on query xand\ncontext c:\nyt\u223cp\u03b8(yt\u2223c,x,y<t)\n\u221dexp logit\u03b8(yt\u2223c,x,y<t)\nHowever, in cases where the context ccontains\nknowledge that is out-of-distribution with respect\nto\u03b8, we hypothesize that the model can struggle\nto effectively attend to cand overly rely on the\nprior knowledge encoded in \u03b8. For instance, as\nillustrated in Figure 1, when the context cstates\n\u201cArgentina won the FIFA World Cups in 1978, 1986\nand 2022 ...\u201d, it contradicts the LM\u2019s outdated prior\nknowledge that Argentina has won the World Cup\ntwice. The language model may still incorrectly\npredict \u201cTwo\u201d even when presented with the con-\ntextcand the query x.\n2.2 Context-aware Decoding\nTo mitigate such issues, we factor out the prior\nknowledge from the model\u2019s original output dis-\ntribution contrastively. Here, we model the prior\nknowledge as p\u03b8(yt\u2223x,y<t)and adjust the\nmodel\u2019s original output probability distribution us-\ning the pointwise mutual information (PMI) be-\ntween the context cand the generation yt, condi-tioned on x,y<t. Formally, we have:\nyt\u223c\u02dcp\u03b8(yt\u2223c,x,y<t)\n\u221dp\u03b8(yt\u2223c,x,y<t)(p\u03b8(yt\u2223c,x,y<t)\np\u03b8(yt\u2223x,y<t))\u03b1\nwhere the output probability is a product-of-experts\nof the original output probability and PMI weighted\nby\u03b1. Essentially, outputs that become much more\nlikely when the context is included are preferred\n(Figure 1).\nThis expression is not a valid probability distribu-\ntion and needs to be normalized across all possible\nvalues of yt. By rearranging the terms, we obtain\nthe final form:\nyt\u223csoftmax [(1+\u03b1)logit\u03b8(yt\u2223c,x,y<t)\n\u2212\u03b1logit\u03b8(yt\u2223x,y<t)]\nLarger \u03b1means more weight on our adjustment\n(\u03b1=0reduces to regular decoding).1We refer\nto this simple method as context-aware decoding.\nFrom the adjusted output distribution \u02dcp, we can\napply various sampling strategies, such as nucleus\nsampling (Holtzman et al., 2019).\nEssentially, context-aware decoding is just a con-\ntrastive ensemble between the logits of p\u03b8(yt\u2223\nc,x,y<t)andp\u03b8(yt\u2223x,y<t). A similar con-\ntrastive objective is universal in image genera-\ntion, where classifier-free diffusion models (Ho\nand Salimans, 2022) predict diffusion noise with\n(1+\u03b1)\u03f5\u03b8(x,c)\u2212\u03b1\u03f5\u03b8(x), withcbeing a control to\nthe image. In text generation, Malkin et al. (2021)\npropose coherence boosting with the same intu-\nition, with a focus on contrasting the full input and\na short premise-free input, promoting coherence\nw.r.t. the long context. Instead of using a single\nmodel \u03b8in this work, different models can also\nbe used in the distribution adjustments to demote\nunwanted model behaviors or distill expert model\u2019s\ncapability (Liu et al., 2021; Li et al., 2022).\n3 Experimental Setup\nWe perform evaluation on tasks that require LMs to\nread and reason over contexts and produce outputs\nthat are faithful to the contexts. Following prior\nwork (Zhang et al., 2023; Zhou et al., 2023), we\nevaluate the models using prompting.\n1If we identify an external knowledge cconditionally in-\ndependent to the generation, p\u03b8(yt\u2223c,x,y<t)=p\u03b8(yt\u2223\nx,y<t), even a non-zero \u03b1would not have an impact to the\noriginal output distribution.\n\n3.1 Datasets and Metrics\nSummarization We conduct summarization ex-\nperiments on two news datasets: CNN-DM (See\net al., 2017) and XSUM (Narayan et al., 2018). We\nuse ROUGE-L (Lin, 2004) to evaluate summariza-\ntion quality. To measure the factual consistency\nof summaries, we adopt BERT-Precision (Pagnoni\net al., 2021) as well as FactKB (Feng et al., 2023),\nwhich has been demonstrated to achieve high cor-\nrelations with human judgment on the two summa-\nrization datasets.\nKnowledge Conflicts We evaluate performance\non two knowledge conflict datasets: MemoTrap\n(Liu and Liu, 2023) and NQ-Swap (Longpre et al.,\n2021). MemoTrap is created to investigate whether\nlanguage models could fall into memorization traps.\nIt comprises instructions that prompt the language\nmodel to complete a well-known proverb with an\nending word that deviates from the commonly used\nending (e.g., Write a quote that ends in the word\n\u201cearly\u201d: Better late than ). NQ-Swap is based on\na QA dataset, natural questions (NQ) (Kwiatkowski\net al., 2019), where the objective is to answer ques-\ntions based on a reliable gold document. To gener-\nate NQ-Swap, Longpre et al. (2021) first identify\nquestions in NQ with named entity answers, find\nthe supportive document for each question and then\nreplace the gold answer entity in the document with\na random entity. A faithful LM should generate the\nreplaced entity as the answer when given the ques-\ntion and modified document. We also include the\noriginal NQ dataset with the question and origi-\nnal document for evaluation. We use Exact Match\n(EM) as the evaluation metric for NQ-Swap, NQ\nand MemoTrap.\nIn Table 1, we show illustrative examples of the\ncontexts we aim to upweight for the model and the\nqueries across different datasets. We hope LMs pay\nmore attention to the source document in XSUM\nand NQ-Swap. On the other hand, we hope LMs\nfocus more on the instruction in MemoTrap.\n3.2 Models and Baselines\nWe apply CAD to pretrained language models in-\ncluding OPT (13B and 30B) (Zhang et al., 2022),\nGPT-Neo (2.7B and 20B) (Black et al., 2021),\nLLaMA (13B and 30B) (Touvron et al., 2023)\nand instruction-finetuned language models such\nas FLAN-T5 (XL 3B and XXL 11B) (Chung et al.,\n2022).XSUM\nc Article: Prison Link Cymru had 1,099 referrals in\n2015-16 and said some ex-offenders were living\nrough for up to a year before finding suitable accom-\nmodation ...\nx Summarize the article in one sentence. Summary:\nNQ-SWAP\nc Tesla CEO Elon Musk is now in charge of Twitter ,\nCNBC has learned ...\nx Who is Twitter CEO now?\nMemoTrap\nc Write a quote that ends in the word \"early\":\nx Better late than\nTable 1: An illustation of the inputs to CAD applied to\neach dataset. CAD upweights the context c(in red) by\nsampling each token from softmax [(1+\u03b1)logit\u03b8(yt\u2223\nc,x,y<t)\u2212\u03b1logit\u03b8(yt\u2223x,y<t)].\nCAD introduces a hyperparameter \u03b1to control\nthe adjustment level. We set \u03b1=0.5for all mod-\nels evaluated on the summarization datasets and\n\u03b1=1for all models evaluated on the knowledge\nconflict datasets. We observed that \u03b1=0.5gen-\nerally yielded good results across all settings and\nall datasets, but a slightly higher \u03b1is more effec-\ntive in the knowledge conflict setting, where the\nprior knowledge needs to be factored out more. We\ninvestigate the effect of \u03b1in Section 4.2.\nFor the baselines, we use regular decoding\nfollowing prior work (Longpre et al., 2021;\nKwiatkowski et al., 2019) to use greedy decod-\ning for knowledge conflict tasks and top- psam-\npling with p=0.9 for summarization tasks (Holtz-\nman et al., 2019). For CAD, we use the same\nsampling strategies on top of the adjusted output\nprobability distribution.\n4 Results\n4.1 Main Results\nSummarization Table 2 reports the results on\nCNN-DM and XSUM. We observe that CAD\noutperforms the standard decoding algorithm by\na large margin in all eight models across both\ndatasets. Specifically, when applied to LLAMA-\n30B in CNN-DM, CAD leads to 21% increase in\nROUGE-L, 14.3% increase in factKB and 7.8%\nincrease in BERT-P. This result demonstrates that\nCAD could effectively improve the quality and fac-\ntuality of the generated summaries from a diverse\nset of language models.\n\nCNN-DM XSUM\nModel Decoding ROUGE-L factKB BERT-P ROUGE-L factKB BERT-P\nOPT13BRegular 22.0 77.8 86.5 16.4 47.2 85.2\nCAD 27.4 84.1 90.8 18.2 64.9 87.5\n30BRegular 22.2 81.7 87.0 17.4 38.2 86.1\nCAD 28.4 87.0 90.2 19.5 45.6 89.3\nGPT-Neo3BRegular 24.3 80.5 87.5 17.6 54.0 86.6\nCAD 27.7 87.5 90.6 18.1 65.1 89.1\n20BRegular 18.7 68.3 85.2 14.9 42.2 85.7\nCAD 24.5 77.5 89.4 19.0 63.3 90.6\nLLaMA13BRegular 27.1 80.2 89.5 19.0 53.5 87.8\nCAD 32.6 90.8 93.0 21.1 73.4 91.7\n30BRegular 25.8 76.8 88.5 18.7 47.7 87.1\nCAD 31.8 87.8 92.2 22.0 66.4 90.3\nFLAN3BRegular 25.5 90.2 91.6 18.8 31.9 88.2\nCAD 26.1 93.9 92.1 19.5 35.9 88.8\n11BRegular 25.4 90.4 91.4 19.4 29.8 88.3\nCAD 27.1 93.1 92.2 20.0 35.0 88.8\nTable 2: CAD consistently outperform the regular decoding method in terms of both summary quality metric\n(ROUGE-L) and summary factuality (factKB and BERT-P). The best scores for each setting are boldfaced.\nFLAN 3B and 11B refer to FLAN-T5 XL and FLAN-T5 XXL respectively.\nModel Decoding Memo. NQ NQ-SWAP\nOPT13BReg. 32.5 29.2 18.8\nCAD 44.5 32.2 36.9\n30BReg. 28.4 29.4 14.7\nCAD 41.0 35.5 29.0\nGPT.3BReg. 22.5 31.9 19.1\nCAD 47.3 39.9 41.2\n20BReg. 37.1 22.8 16.1\nCAD 57.3 32.1 36.8\nLLAMA13BReg. 23.8 22.3 11.7\nCAD 57.1 33.6 36.7\n30BReg. 25.8 23.8 9.6\nCAD 50.6 34.0 37.7\nFLAN3BReg. 69.2 81.8 71.4\nCAD 72.2 80.3 73.3\n11BReg. 82.0 85.5 73.0\nCAD 88.7 82.5 77.1\nTable 3: CAD outperforms the regular decoding method\n(Reg.) in all settings except for FLAN-T5 on NQ.\nNote that FLAN-T5 is trained on NQ dataset during\ninstruction-finetuning.\nKnowledge Conflicts Our results for the knowl-\nedge conflict datasets, NQ-SWAP and MemoTrap,\nas well as the original NQ are detailed in Table 3.\nCAD is significantly better than the regular decod-\ning in all settings, with the exception of a minor de-\ncrease observed for FLAN-T5 on the non-conflict\nNQ dataset.2Despite this, CAD achieves substan-\n2This slight decline can be attributed to the fact that this\nparticular NQ dataset is included in the instruction-finetuning\nsets used by FLAN-T5, and hence, the model has been previ-\nously trained on it.tially better performance on the knowledge con-\nflict datasets, e.g., CAD improve GPT-Neo 20B by\n54.4% on Memotrap and by 128% on NQ-SWAP.\nThis substantial improvement suggests that context-\naware decoding is particularly beneficial for LMs\nto adhere to the given context, in scenarios where\nthe model\u2019s prior knowledge contradicts with the\ncontext knowledge.\n4.2 Analysis\nQualitative analyais We provide qualitative ex-\namples for XSUM and Memotrap in Table 4. In\nXSUM, the regular decoding generates texts that\nis not mentioned in the article, whereas CAD pro-\nduces output exclusively based on the information\nin the input article. For MemoTrap, the standard\ndecoding disregards the instruction and generates\nthe memorized ending, while CAD adheres to the\ninstruction within the given context and produces\nthe desired output.\nCAD brings consistent improvement to LMs\nwith different sizes. In Tables 2 and 3, we show\nthat CAD could be used to enhance a diverse set of\nLM families, including OPT, GPT-Neo, LLaMA,\nand FLAN-T5. Here we further investigate whether\nCAD is effective in improving language models of\ndifferent sizes. Specifically, we focus on OPT mod-\nels across a range of sizes: 125M, 350M, 1.3B,\n2.7B, 6.7B, 13B, 30B. As depicted in Figure 2,\nwe observe that the performance gain brought by\n\nFigure 2: OPT models of varying sizes consistently benefit from CAD. The x-axis indicates the size of language\nmodels and the y-axis is the performance.\nXSUM\nArticle He passed away peacefully in hospital on Tues-\nday after a short illness. Born in Tourmakeady,\nCounty Mayo, he worked as a teacher before se-\ncuring a part in the premiere of the Brian Friel\nplay Translations in 1980. Lally became a house-\nhold name in Ireland for his role as Miley Byrne\nin the RTE soap opera Glenroe and later starred in\nthe BBC series Ballykissangel. He also appeared\nin the Hollywood movie Alexander and provided\nthe voice for the Oscar-nominated, animated Irish\nfilm, The Secret of Kells. As a fluent Irish speaker\nand advocate of the language, Lally had roles in\nseveral Irish language films ...\nRegular Westminister actor Pat Lally died in hospital on\nTuesday night aged 82\nCAD Actor Lally, best known for Glenroe and Bal-\nlykissangel, has died in hospital on Tuesday\nMemoTrap\nInput Write a quote that ends in the word \u201cearly\u201d. Better\nlate than\nRegular never\nCAD early\nTable 4: Qualitative examples of contrast-aware decod-\ning. The nonfactual or inconsistent texts are highlighted\nin yellow.\n.\nCAD stays consistent with different model sizes in\nCNN-DM. In Memotrap and NQSWAP, this gain\nincreases as the model size grows, indicating that\nlarger LMs can have a greater tendency to rely on\ntheir prior knowledge instead of reading the con-\ntexts, thereby benefiting more from CAD.\nEffect of adjustment level \u03b1Context-aware\ndecoding introduces a hyperparameter \u03b1, which\nserves to control the adjustment level of CAD (a\nsmall \u03b1makes the distribution closer to the original\nnext token distribution). We conduct experimentswith various values of \u03b1and present the results in\nFigure 3. Across all three datasets, we find \u03bb=0.5\nconsistently provide robust improvements over reg-\nular decoding. Further increasing the value of \u03b1\nyields additional improvement in tasks involving\nknowledge conflicts.\n5 Related Work\nSummarization Factuality Summarization mod-\nels have shown a tendency to generate hallucinated\ntexts (Maynez et al., 2020; Pagnoni et al., 2021).\nThis has led to growing efforts to improve the fac-\ntual consistency, including applying attentions to\nfact triples extracted from source documents (Cao\net al., 2018; Zhu et al., 2021), optimizing sum-\nmarization models towards a factual consistency\nmetrics (Nan et al., 2021; Cao and Wang, 2021),\nlearning a post-editing error corrector (Dong et al.,\n2020) and removing noisy training samples (Kang\nand Hashimoto, 2020; Goyal and Durrett, 2021).\nHowever, all these methods require additional fine-\ntuning and are not directly suitable for zero-shot\nand few-shot prompting scenarios.\nKnowledge Conflicts When presented with an\nupdated document with conflicting knowledge,\nwe expect language models to generate responses\nbased on the provided contexts rather than relying\nsolely on outdated parametric knowledge. This set-\nting is especially valuable to retrieval-augmented\nlanguage models (Khandelwal et al., 2020; Shi\net al., 2023; Min et al., 2022; Yasunaga et al., 2023),\nwhere documents retrieved from external databases\nare used as additional input to provide LMs addi-\ntional knowledge. However, simply adding docu-\nments does not always change the model predic-\ntions, as current LMs often overlook the contexts\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Try to extract all text from the first 5 pages to manually search for the CAD algorithm\nwith open(pdf_path, 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    num_pages = len(reader.pages)\n    text = ''\n    for i in range(min(num_pages, 5)):\n        page = reader.pages[i].extract_text()\n        if page:\n            text += page + '\\n\\n'\nprint(text)"}, "success": true}