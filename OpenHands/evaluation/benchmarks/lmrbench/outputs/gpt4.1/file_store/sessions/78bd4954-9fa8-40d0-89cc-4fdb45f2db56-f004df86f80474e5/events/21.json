{"id": 21, "timestamp": "2025-05-19T19:43:13.707357", "source": "agent", "message": "Code executed in IPython cell.", "cause": 20, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_iijFXURQ90tZgAhcbPzaIClQ", "model_response": {"id": "chatcmpl-BZ5MTYNCBWIyLvKF5inAj5oHA7lui", "created": 1747701789, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Try to extract the most relevant section: search for 'loss' and 'TDPO' in the first 15 pages\\nimport re\\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    text = ''\\n    for page in reader.pages[:15]:\\n        text += page.extract_text() + '\\\\n'\\n\\n# Find all paragraphs containing 'loss' or 'TDPO'\\nloss_paragraphs = re.findall(r'([^.]*?(?:loss|TDPO)[^.]*\\\\.)', text, re.IGNORECASE)\\nfor para in loss_paragraphs:\\n    print(para.strip())\" }", "name": "execute_ipython_cell"}, "id": "call_iijFXURQ90tZgAhcbPzaIClQ", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 172, "prompt_tokens": 9147, "total_tokens": 9319, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 7680}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "In this pa-\nper, we introduce Token-level Direct Preference\nOptimization (TDPO), a novel approach to align\nLLMs with human preferences by optimizing pol-\nicy at the token level.\nUnlike previous methods,\nwhich face challenges in divergence efficiency,\nTDPO incorporates forward KL divergence con-\nstraints for each token, improving alignment and\ndiversity.\nUtilizing the Bradley-Terry model for\na token-based reward system, TDPO enhances\nthe regulation of KL divergence, while preserv-\ning simplicity without the need for explicit re-\nward modeling.\nExperimental results across vari-\nous text tasks demonstrate TDPO\u2019s superior per-\nformance in balancing alignment with genera-\ntion diversity.\nNotably, fine-tuning with TDPO\nstrikes a better balance than DPO in the controlled\nsentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of\ngenerated responses compared to both DPO and\nPPO-based RLHF methods.\nWe introduce a\nnew method, referred to as Token-level Direct Preference\nOptimization (TDPO), which aims to strike a better balance\nbetween alignment performance and generation diversity\nby controlling the KL divergence for each token.\nTDPO maintains the simplicity of DPO while offering im-\nproved regulation of KL divergence for aligning LLMs\nwith human preferences.\nOur experimental results demonstrate\nthe effectiveness of TDPO across multiple text tasks, and\ngain a notable enhancement in the quality of generated re-\nsponses in comparison to both DPO and PPO-based RLHF\nmethods.\nIn conclusion, TDPO stands out for its ability\nto not only effectively address the issue of excessive KL\ndivergence but also greatly improve divergence efficiency.\nNevertheless, the\nchallenge of maintaining linguistic diversity while aligning\nwith human preferences remains a pivotal concern, prompt-\ning our proposed Token-level Direct Preference Optimiza-\ntion (TDPO), which seeks to harmonize the dual objectives\nof alignment accuracy and expressive range in model out-\nputs.\n3 and leveraging the negative\nlog-likelihood loss, DPO derives the objective function:\nu(x, yw, yl) =\u03b2log\u03c0\u03b8(yw|x)\n\u03c0ref(yw|x)\u2212\u03b2log\u03c0\u03b8(yl|x)\n\u03c0ref(yl|x),\nLDPO(\u03c0\u03b8;\u03c0ref) =\u2212E(x,yw,yl)\u223cD[log\u03c3(u(x, yw, yl))],\n(4)\nand the derivative is given as follows:\n\u2207\u03b8LDPO(\u03c0\u03b8;\u03c0ref) =\u2212E(x,yw,yl)\u223cD[\u03c3(\u2212u)\u2207\u03b8u],(5)\nwhere uis the abbreviation of u(x, yw, yl),ywandylde-\nnotes the preferred and dispreferred completion.\nFinally, we conduct a\nformalized analysis of this optimization objective in terms\nof derivatives and, based on this, derive the ultimate loss\nfunction for TDPO.\nLoss Function and Formal Analysis\nDrawing on Eq.\nThis allows\nus to formulate a likelihood maximization objective for a\nparametrized policy \u03c0\u03b8, leading to the derivation of the loss\nfunction for the initial version of our method, TDPO 1:\nLTDPO 1(\u03c0\u03b8;\u03c0ref) =\n\u2212E(x,yw,yl)\u223cD[log\u03c3(u(x, yw, yl)\u2212\u03b4(x, yw, yl))].\n(15)\nThrough this approach, we explicitly introduce sequential\nforward KL divergence into the loss function.\nSubsequently, we conduct a derivative analysis of our\nmethod and make specific modifications to the loss function\nof TDPO.\nBy employing the formula-\ntion of the loss function presented in Eq.\n15, we compute the\ngradient of the loss function with respect to the parameters\n\u03b8:\n\u2207\u03b8LTDPO 1(\u03c0\u03b8;\u03c0ref) =\n\u2212E(x,yw,yl)\u223cD[\u03c3(\u2212u+\u03b4) [\u2207\u03b8u\u2212 \u2207 \u03b8\u03b4]].\nThe first part (\u2212u)corresponds to the weight\nfactor in the loss function of DPO.\nThe gradient of the loss function in Eq.\nHowever, when considered separately, the gradient of\nDSeqKL (x, yw;\u03c0ref|\u03c0\u03b8)in the loss function tends to in-\ncrease the sequential KL divergence between \u03c0refand\u03c0\u03b8\nat(x, yw)during the optimization process.\nThis is because\nthe sequential forward KL divergence in the loss function is\nintroduced through the state-value function V\u03c0, inherently\nintroducing an expectation Ez\u223c\u03c0refh\nlog\u03c0\u03b8(z|[x,y<t])\n\u03c0ref(z|[x,y<t])i\nas a\nbaseline at each token.\nFor\nthe prompt xand the preferred response yw, at each token,\nthe loss function in Eq.\nAs we do not aim to accelerate the training speed and\nprefer to ensure training stability, we modify the loss\nfunction by discontinuing the gradient propagation of\nDSeqKL (x, yw;\u03c0ref|\u03c0\u03b8)and treating it as a baseline term\nfor alignment of DSeqKL (x, yl;\u03c0ref|\u03c0\u03b8).\nFor\nthe prompt xand the rejected response yl, the loss\nfunction in Eq.\nFurthermore, to achieve a better balance between align-\nment performance and generation diversity in TDPO, we\nintroduce an additional parameter \u03b1into the loss func-\ntion.\nIn summary, we modify the loss function of TDPO 1, re-\nsulting in the second version of our method, TDPO 2, asfollows:\nLTDPO 2(\u03c0\u03b8;\u03c0ref) =\n\u2212E(x,yw,yl)\u223cD[log\u03c3(u(x, yw, yl)\u2212\u03b1\u03b42(x, yw, yl))],\n(17)\nwhere \u03b1is a parameter, and\n\u03b42(x, y1, y2) =\u03b2DSeqKL (x, y2;\u03c0ref\u2225\u03c0\u03b8)\n\u2212sg(\u03b2DSeqKL (x, y1;\u03c0ref\u2225\u03c0\u03b8)).\nWe summarize the comparison of the loss functions for\nDPO ,TDPO 1, and TDPO 2, as presented in Figure 2.\nComparison of Loss Functions for DPO ,TDPO 1and\nTDPO 2Methods.\nBoth TDPO 1andTDPO 2incorporate an additional term for\nfiner-grained control over the KL divergence, compared to DPO.\nWe provided the pseudocode in Algo-\nrithm 1 and the Pytorch implementation version of TDPO\nloss in Appendix B.\nThe Anthropic HH dataset is a single-turn dialogue dataset\nwhere the model receives human queries, covering various\n6\nToken-level Direct Preference Optimization\nAlgorithm 1 Token-level Direct Preference Optimization (TDPO)\n1:Input: Reference model \u03c0ref, Policy model \u03c0\u03b8, Coefficient \u03b1, \u03b2, Learning rate \u03b7\n2:Input: Dataset D={(x, yw, yl)i}N\ni=1of size N, Method M\n3:Initialize: \u03c0\u03b8\u2190\u03c0ref\n4:foreach epoch do\n5: Sample mini-batch Dm={(x, yw, yl)m}M\nm=1fromD\n6: Predict the probabilities \u03c0\u03b8(yw|x)and\u03c0\u03b8(yl|x)for(x, yw, yl)in the mini-batch Dmusing the policy model\n7: Predict the probabilities \u03c0ref(yw|x)and\u03c0ref(yl|x)for(x, yw, yl)in the mini-batch Dmusing the reference model\n8: Calculate the function u(x, yw, yl) =\u03b2log\u03c0\u03b8(yw|x)\n\u03c0ref(yw|x)\u2212\u03b2log\u03c0\u03b8(yl|x)\n\u03c0ref(yl|x)\u25b7Eq.\n13\n9: Compute the sequential KL divergence DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8)for(x, yw)in the mini-batch Dm\n10: Compute the sequential KL divergence DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)for(x, yl)in the mini-batch Dm\n11: ifMethod MisTDPO 1then\n12: Calculate the function \u03b4(x, yw, yl) =\u03b2DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)\u2212\u03b2DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8) \u25b7Eq.\n15\n14: else if Method MisTDPO 2then\n15: Calculate the function \u03b42(x, yw, yl) =\u03b2DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)\u2212sg(\u03b2DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8)) \u25b7Eq.\nExperiments on IMDb Dataset\nIn this experiment, besides our proposed methods TDPO 1\nandTDPO 2, we also implemented the DPO algorithm\nfor fair comparison.\nWe implement\nthe DPO, TDPO 1, and different versions of TDPO 2algo-\nrithms with varying the parameter \u03b1.\nFrom the figure, we\nnotice that although DPO establishes an efficient frontier,\nTDPO 1andTDPO 2outperform DPO in terms of diver-\ngence versus reward on the frontier, achieving higher reward\nwhile maintaining low KL divergence.\nWe also implemented\nversions of TDPO 2with\u03b1\u2208 {1,1.\nIn contrast, TDPO 2exhibits superior control over KL di-\nvergence, achieving better divergence efficiency compared\nto DPO.\n4, TDPO 1tends to result\nin an increased sequential KL divergence on the preferred\nresponse subset, thereby exhibiting a weaker capacity for\nKL divergence adjustment compared to TDPO 2.\nTDPO 2\nmaintains a more balanced sequential KL divergence on\nboth dispreferred and preferred response subsets, contribut-\ning to its ability to achieve a superior frontier.\n7\nToken-level Direct Preference Optimization\n/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015/uni00000011/uni00000018/uni00000014/uni00000018/uni00000011/uni00000013/uni00000014/uni0000001a/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000013\nDKL( ref)\n/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047DPO\nTDPO1\nTDPO2/uni00000003/uni00000003=0.\n1\nTDPO2/uni00000003/uni00000003=0.\n3\nTDPO2/uni00000003/uni00000003=0.\n5\nTDPO2/uni00000003/uni00000003=0.\nWe implemented DPO, TDPO 1, and different versions of TDPO 2with respect to the parameter \u03b1.\nBoth TDPO 1and\nTDPO 2outperform DPO in terms of the frontier, with TDPO 2showing further improvement over TDPO 1.\nTDPO 2exhibit superior regulation over KL divergence compared to\ntheTDPO 1and DPO algorithm.\nComparison of DPO, TDPO 1andTDPO 2in terms of\nthe trade-off between Alignment (accuracy) and Diversity (en-\ntropy) on the Anthropic HH dataset.\n196\nTDPO 1(ours ) 60.\n727\nTDPO 2(ours ) 67.\nExperiments on Anthropic HH Dataset\nNext, we evaluate the performance of TDPO 1andTDPO 2\non the Anthropic HH dataset.\nSubsequently, we train TDPO 1,TDPO 2, DPO\n(Rafailov et al.\nTDPO 2not only sur-\npasses DPO, f-DPO and TDPO 1in terms of accuracy but\nalso excels in entropy, achieving a superior balance between\nalignment and diversity.\nTo further assess the ability of TDPO 1andTDPO 2to\nalign with human preferences, we evaluated the win rates\nof responses generated by models trained with different\nalgorithms against chosen responses on the test set of the HH\ndataset, the result is illustrated in the Figure 4.\nCompared to\nthe SFT model, the DPO, TDPO 1, and TDPO 2algorithms\nbetter align with human preferences, achieving win rates not\nless than 50% against chosen responses at temperature 0.\nThis demonstrates that both TDPO 1, and TDPO 2possess\na strong capability to align with human preferences.\n/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013\n/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni00000048/uni00000050/uni00000053/uni00000048/uni00000055/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni0000003a/uni0000004c/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048\nSFT\nTDPO1DPO\nTDPO2fDPO(FKL)\nFigure 4.\nExperiments on MT-Bench\nTo comprehensively evaluate TDPO 1, and TDPO 2in\nterms of generation quality, we conducted pairwise com-\nparisons on the MT-Bench using models trained on the\nAnthropic HH dataset.\nIt reveals\nthatTDPO 2achieves a higher win rate compared to other\nalgorithms, indicating its ability to assist LLMs in generat-\ning higher-quality responses.\n0 20 40 60 80 100 120 140 160TDPO2\nvs TDPO1TDPO2\nvs DPOTDPO2\nvs PPOTDPO2\nvs SFT\n27.\nMT-Bench comparison between SFT ,PPO ,DPO ,\nTDPO 1andTDPO 2methods.\nConclusion\nIn this work, we introduced Token-level Direct Preference\nOptimization (TDPO), an innovative token-level fine-tuning\napproach for Large Language Models (LLMs) aimed at\naligning more closely with human preferences.\nBy employ-\ning the token-wise optimization with forward KL divergence\nconstraints and converting the Bradley-Terry model into a\ntoken-level preference model, TDPO addresses key chal-\nlenges in divergence efficiency and content diversity, sur-\npassing traditional methods like Direct Preference Optimiza-\ntion (DPO) and PPO-based RLHF in tasks such as controlled\nsentiment generation and single-turn dialogues.\nDeriving the TDPO Objective Under the Bradley-Terry Model\nTheorem A.\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Try to extract the most relevant section: search for 'loss' and 'TDPO' in the first 15 pages\nimport re\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[:15]:\n        text += page.extract_text() + '\\n'\n\n# Find all paragraphs containing 'loss' or 'TDPO'\nloss_paragraphs = re.findall(r'([^.]*?(?:loss|TDPO)[^.]*\\.)', text, re.IGNORECASE)\nfor para in loss_paragraphs:\n    print(para.strip())"}, "success": true}