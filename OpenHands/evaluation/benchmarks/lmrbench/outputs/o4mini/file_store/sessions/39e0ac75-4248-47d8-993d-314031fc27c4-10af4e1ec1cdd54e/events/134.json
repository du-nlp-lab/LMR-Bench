{"id": 134, "timestamp": "2025-05-16T23:02:19.081805", "source": "agent", "message": "Command `sed -n '1,200p' /workspace/benchmark/datasets/19-Native-Sparse-Attention/NSA/tests/test_nsa.py` executed with exit code 0.", "cause": 133, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "toolu_29", "model_response": {"id": "chatcmpl-BY32QkY5F6IrnsJ2WRrFXygx7RJqN", "created": 1747454530, "model": "o4-mini-2025-04-16", "object": "chat.completion", "system_fingerprint": null, "choices": [{"finish_reason": "stop", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"index": 1, "function": {"arguments": "{\"command\": \"sed -n '1,200p' /workspace/benchmark/datasets/19-Native-Sparse-Attention/NSA/tests/test_nsa.py\"}", "name": "execute_bash"}, "id": "toolu_29", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 538, "prompt_tokens": 35407, "total_tokens": 35945, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 448, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 35200}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "# -*- coding: utf-8 -*-\n\nimport os\n\nimport pytest\nimport torch\nimport triton\n\nfrom fla.ops.common.utils import prepare_token_indices\nfrom native_sparse_attention.ops.naive import naive_nsa\nfrom native_sparse_attention.ops.parallel import parallel_nsa\n\n\ndef get_abs_err(x, y):\n    return (x-y).flatten().abs().max().item()\n\n\ndef get_err_ratio(x, y):\n    err = (x-y).flatten().square().mean().sqrt().item()\n    base = (x).flatten().square().mean().sqrt().item()\n    return err / base\n\n\ndef assert_close(prefix, ref, tri, ratio):\n    msg = f\"{prefix} diff: {get_abs_err(ref, tri):.6f} ratio: {get_err_ratio(ref, tri):.6f}\"\n    print(msg)\n    assert get_err_ratio(ref, tri) < ratio, msg\n\n\n@pytest.mark.parametrize(\"B\", [1])\n@pytest.mark.parametrize(\"T\", [256, 1024, 2000])\n@pytest.mark.parametrize(\"H\", [4])\n@pytest.mark.parametrize(\"HQ\", [64])\n@pytest.mark.parametrize(\"D\", [100, 64])\n@pytest.mark.parametrize(\"S\", [16])\n@pytest.mark.parametrize(\"block_size\", [32])\n@pytest.mark.parametrize(\"window_size\", [0, 32])\n@pytest.mark.parametrize(\"dtype\", [torch.bfloat16])\n@pytest.mark.parametrize(\"scale\", [0.1])\ndef test_parallel(\n    B: int,\n    H: int,\n    HQ: int,\n    T: int,\n    D: int,\n    S: int,\n    block_size: int,\n    window_size: int,\n    dtype: torch.dtype,\n    scale: float\n):\n    torch.manual_seed(42)\n    os.environ['TRITON_F32_DEFAULT'] = 'ieee'\n\n    perm_q = torch.randperm(T, device='cuda')\n    perm_k = torch.randperm(T, device='cuda')\n    perm_v = torch.randperm(T, device='cuda')\n    q = torch.linspace(0, 1, steps=T, dtype=dtype, device='cuda')[perm_q].view(1, T, 1, 1).expand(B, T, HQ, D).clone().requires_grad_(True)\n    k = torch.linspace(0, 1, steps=T, dtype=dtype, device='cuda')[perm_k].view(1, T, 1, 1).expand(B, T, H, D).clone().requires_grad_(True)\n    v = torch.linspace(0, 1, steps=T, dtype=dtype, device='cuda')[perm_v].view(1, T, 1, 1).expand(B, T, H, D).clone().requires_grad_(True)\n    g_slc = torch.rand((B, T, HQ), dtype=dtype, device='cuda').requires_grad_(True)\n    g_swa = torch.rand((B, T, HQ), dtype=dtype, device='cuda').requires_grad_(True)\n    do = torch.randn((B, T, HQ, D), dtype=dtype, device='cuda')\n\n    block_indices = torch.full((B, T, H, S), T, dtype=torch.long, device='cuda')\n    for b in range(B):\n        for t in range(T):\n            for h in range(H):\n                i_i = torch.randperm(max(1, triton.cdiv(t, block_size)))[:S]\n                block_indices[b, t, h, :len(i_i)] = i_i\n    block_indices = block_indices.sort(-1)[0]\n    block_counts = torch.randint(1, S + 1, (B, T, H), dtype=torch.long, device='cuda')\n\n    ref = naive_nsa(\n        q=q,\n        k=k,\n        v=v,\n        g_slc=g_slc,\n        g_swa=g_swa,\n        block_indices=block_indices,\n        block_counts=block_counts,\n        block_size=block_size,\n        window_size=window_size,\n        scale=scale\n    )\n    ref.backward(do)\n    ref_dq, q.grad = q.grad.clone(), None\n    ref_dk, k.grad = k.grad.clone(), None\n    ref_dv, v.grad = v.grad.clone(), None\n    ref_dg_slc, g_slc.grad = g_slc.grad.clone(), None\n    if window_size > 0:\n        ref_dg_swa, g_swa.grad = g_swa.grad.clone(), None\n\n    tri = parallel_nsa(\n        q=q,\n        k=k,\n        v=v,\n        g_slc=g_slc,\n        g_swa=g_swa,\n        block_indices=block_indices,\n        block_counts=block_counts,\n        block_size=block_size,\n        window_size=window_size,\n        scale=scale\n    )\n    tri.backward(do)\n    tri_dq, q.grad = q.grad.clone(), None\n    tri_dk, k.grad = k.grad.clone(), None\n    tri_dv, v.grad = v.grad.clone(), None\n    tri_dg_slc, g_slc.grad = g_slc.grad.clone(), None\n    if window_size > 0:\n        tri_dg_swa, g_swa.grad = g_swa.grad.clone(), None\n\n    assert_close(\" o\", ref, tri, 0.005)\n    assert_close(\"dq\", ref_dq, tri_dq, 0.005)\n    assert_close(\"dk\", ref_dk, tri_dk, 0.005)\n    assert_close(\"dv\", ref_dv, tri_dv, 0.005)\n    assert_close(\"dg_slc\", ref_dg_slc, tri_dg_slc, 0.005)\n    if window_size > 0:\n        assert_close(\"dg_swa\", ref_dg_swa, tri_dg_swa, 0.005)\n\n\n@pytest.mark.parametrize(\"N\", [4])\n@pytest.mark.parametrize(\"T\", [64, 128, 200, 250, 256, 300, 400, 512, 1000, 2048])\n@pytest.mark.parametrize(\"H\", [4])\n@pytest.mark.parametrize(\"HQ\", [64])\n@pytest.mark.parametrize(\"D\", [100, 64])\n@pytest.mark.parametrize(\"S\", [16])\n@pytest.mark.parametrize(\"block_size\", [32])\n@pytest.mark.parametrize(\"window_size\", [0, 32])\n@pytest.mark.parametrize(\"dtype\", [torch.bfloat16])\ndef test_parallel_varlen(\n    N: int,\n    T: int,\n    H: int,\n    HQ: int,\n    D: int,\n    S: int,\n    block_size: int,\n    window_size: int,\n    dtype: torch.dtype,\n):\n    torch.manual_seed(42)\n    os.environ['TRITON_F32_DEFAULT'] = 'ieee'\n\n    # randomly split the sequence into N segments\n    offsets = torch.cat([\n        torch.tensor([0], dtype=torch.long),\n        torch.arange(16, T)[torch.randperm(T - 1)[:N-1]],\n        torch.tensor([T], dtype=torch.long)\n    ], 0).cuda().sort()[0]\n    # seq-first required for inputs with variable lengths\n    perm_q = torch.randperm(T, device='cuda')\n    perm_k = torch.randperm(T, device='cuda')\n    perm_v = torch.randperm(T, device='cuda')\n    q = torch.linspace(0, 1, steps=T, dtype=dtype, device='cuda')[perm_q].view(1, T, 1, 1).expand(1, T, HQ, D).clone().requires_grad_(True)\n    k = torch.linspace(0, 1, steps=T, dtype=dtype, device='cuda')[perm_k].view(1, T, 1, 1).expand(1, T, H, D).clone().requires_grad_(True)\n    v = torch.linspace(0, 1, steps=T, dtype=dtype, device='cuda')[perm_v].view(1, T, 1, 1).expand(1, T, H, D).clone().requires_grad_(True)\n    g_slc = torch.rand((1, T, HQ), dtype=dtype, device='cuda').requires_grad_(True)\n    g_swa = torch.rand((1, T, HQ), dtype=dtype, device='cuda').requires_grad_(True)\n    do = torch.randn((1, T, HQ, D), dtype=dtype, device='cuda')\n\n    token_indices = prepare_token_indices(offsets).tolist()\n    block_indices = torch.full((1, T, H, S), T, dtype=torch.long, device='cuda')\n    for i in range(T):\n        _, t = token_indices[i]\n        for h in range(H):\n            i_i = torch.randperm(max(1, triton.cdiv(t, block_size)))[:S]\n            block_indices[0, i, h, :len(i_i)] = i_i\n    block_indices = block_indices.sort(-1)[0]\n    block_counts = torch.randint(1, S + 1, (1, T, H), device='cuda')\n\n    ref = naive_nsa(\n        q=q,\n        k=k,\n        v=v,\n        g_slc=g_slc,\n        g_swa=g_swa,\n        block_indices=block_indices,\n        block_counts=block_counts,\n        block_size=block_size,\n        window_size=window_size,\n        cu_seqlens=offsets\n    )\n    ref.backward(do)\n    ref_dq, q.grad = q.grad.clone(), None\n    ref_dk, k.grad = k.grad.clone(), None\n    ref_dv, v.grad = v.grad.clone(), None\n    ref_dg_slc, g_slc.grad = g_slc.grad.clone(), None\n    if window_size > 0:\n        ref_dg_swa, g_swa.grad = g_swa.grad.clone(), None\n\n    tri = parallel_nsa(\n        q=q,\n        k=k,\n        v=v,\n        g_slc=g_slc,\n        g_swa=g_swa,\n        block_indices=block_indices,\n        block_counts=block_counts,", "extras": {"command": "sed -n '1,200p' /workspace/benchmark/datasets/19-Native-Sparse-Attention/NSA/tests/test_nsa.py", "metadata": {"exit_code": 0, "pid": -1, "username": "root", "hostname": "8e4719dbc9fc", "working_dir": "/workspace/benchmark/datasets/19-Native-Sparse-Attention/NSA", "py_interpreter_path": "/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python", "prefix": "", "suffix": "\n[The command completed with exit code 0.]"}, "hidden": false}, "success": true}