{"id": 12, "timestamp": "2025-05-16T19:20:44.348111", "source": "agent", "message": "I read the file /workspace/benchmark/datasets/25-AdaLora/golden_files/adalora_golden.py.", "cause": 11, "observation": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "toolu_04", "model_response": {"id": "chatcmpl-BXza48Ftco4Mr2iTuWJFqXNnXcxTl", "created": 1747441240, "model": "o4-mini-2025-04-16", "object": "chat.completion", "system_fingerprint": null, "choices": [{"finish_reason": "stop", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"index": 1, "function": {"arguments": "{\"command\": \"view\", \"path\": \"/workspace/benchmark/datasets/25-AdaLora/golden_files/adalora_golden.py\"}", "name": "str_replace_editor"}, "id": "toolu_04", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 214, "prompt_tokens": 19692, "total_tokens": 19906, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 128, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 6272}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "Here's the result of running `cat -n` on /workspace/benchmark/datasets/25-AdaLora/golden_files/adalora_golden.py:\n     1\t#  ------------------------------------------------------------------------------------------\n     2\t#  Copyright (c) Microsoft Corporation. All rights reserved.\n     3\t#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n     4\t#  ------------------------------------------------------------------------------------------\n     5\timport math\n     6\timport torch\n     7\timport torch.nn as nn\n     8\timport torch.nn.functional as F\n     9\t\n    10\tfrom .layers import LoRALayer \n    11\tfrom typing import Optional, List \n    12\t\n    13\t\n    14\tclass SVDLinear(nn.Linear, LoRALayer):\n    15\t    # SVD-based adaptation implemented in a dense layer\n    16\t    def __init__(\n    17\t        self, \n    18\t        in_features: int, \n    19\t        out_features: int, \n    20\t        r: int = 0, \n    21\t        lora_alpha: int = 1, \n    22\t        lora_dropout: float = 0.,\n    23\t        fan_in_fan_out: bool = False, \n    24\t        merge_weights: bool = True,\n    25\t        **kwargs\n    26\t    ):\n    27\t        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n    28\t        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n    29\t                           merge_weights=merge_weights)\n    30\t\n    31\t        self.fan_in_fan_out = fan_in_fan_out\n    32\t        # Actual trainable parameters\n    33\t        if r > 0:\n    34\t            self.lora_A = nn.Parameter(\n    35\t                self.weight.new_zeros((r, in_features))\n    36\t            )\n    37\t            self.lora_E = nn.Parameter(\n    38\t                self.weight.new_zeros(r, 1)\n    39\t            ) \n    40\t            self.lora_B = nn.Parameter(\n    41\t                self.weight.new_zeros((out_features, r))\n    42\t            )\n    43\t            self.ranknum = nn.Parameter(\n    44\t                self.weight.new_zeros(1), requires_grad=False\n    45\t            )\n    46\t            self.ranknum.data.fill_(float(self.r))\n    47\t            self.scaling = self.lora_alpha if self.lora_alpha>0 else float(self.r)   \n    48\t            # Freezing the pre-trained weight matrix\n    49\t            self.weight.requires_grad = False\n    50\t            self.ranknum.requires_grad = False\n    51\t        self.reset_parameters()\n    52\t        if fan_in_fan_out:\n    53\t            self.weight.data = self.weight.data.T\n    54\t\n    55\t    def reset_parameters(self):\n    56\t        nn.Linear.reset_parameters(self)\n    57\t        if hasattr(self, 'lora_A'):\n    58\t            # initialize A,B the same way as the default for nn.Linear \n    59\t            # and E (singular values) for zero \n    60\t            nn.init.zeros_(self.lora_E)\n    61\t            nn.init.normal_(self.lora_A, mean=0.0, std=0.02)\n    62\t            nn.init.normal_(self.lora_B, mean=0.0, std=0.02)\n    63\t\n    64\t    def train(self, mode: bool = True):\n    65\t        def T(w):\n    66\t            return w.T if self.fan_in_fan_out else w\n    67\t        nn.Linear.train(self, mode)\n    68\t        if self.merge_weights and self.merged:\n    69\t            # Make sure that the weights are not merged\n    70\t            if self.r > 0:\n    71\t                self.weight.data -= T(\n    72\t                    self.lora_B @ (self.lora_A*self.lora_E)\n    73\t                ) * self.scaling / (self.ranknum+1e-5)\n    74\t            self.merged = False\n    75\t    \n    76\t    def eval(self):\n    77\t        def T(w):\n    78\t            return w.T if self.fan_in_fan_out else w\n    79\t        nn.Linear.eval(self)\n    80\t        if self.merge_weights and not self.merged:\n    81\t            # Merge the weights and mark it\n    82\t            if self.r > 0:\n    83\t                self.weight.data += T(\n    84\t                    self.lora_B @ (self.lora_A * self.lora_E)\n    85\t                ) * self.scaling / (self.ranknum+1e-5)\n    86\t            self.merged = True\n    87\t\n    88\t    def forward(self, x: torch.Tensor):\n    89\t        def T(w):\n    90\t            return w.T if self.fan_in_fan_out else w\n    91\t        if self.r > 0 and not self.merged:\n    92\t            result = F.linear(x, T(self.weight), bias=self.bias)\n    93\t            if self.r > 0:\n    94\t                result += (\n    95\t                    self.lora_dropout(x) @ (self.lora_A * self.lora_E).T @ self.lora_B.T\n    96\t                ) * self.scaling / (self.ranknum+1e-5)\n    97\t            return result\n    98\t        else:\n    99\t            return F.linear(x, T(self.weight), bias=self.bias)\n   100\t\n   101\t\n   102\tclass RankAllocator(object):\n   103\t    \"\"\"\n   104\t    The RankAllocator for AdaLoRA Model that will be called every training step. \n   105\t    Paper: https://openreview.net/pdf?id=lq62uWRJjiY\n   106\t\n   107\t    Args:\n   108\t        model: the model that we apply AdaLoRA to.\n   109\t        lora_r (`int`): The initial rank for each incremental matrix.\n   110\t        target_rank (`int`): The target average rank of incremental matrix.\n   111\t        init_warmup (`int`): The steps of initial fine-tuning warmup.\n   112\t        final_warmup (`int`): The step of final fine-tuning.\n   113\t        mask_interval (`int`): The time internval between two budget allocations.\n   114\t        beta1 (`float`): The hyperparameter of EMA for sensitivity smoothing.\n   115\t        beta2 (`float`): The hyperparameter of EMA for undertainty quantification.\n   116\t        total_step (`int`): The total training steps, correctly configured before training.\n   117\t        target_total_rank (`Optinal[int]`): The speficified final total rank. \n   118\t        tb_writter (`SummaryWriter`): Tensorboard SummaryWriter. \n   119\t        tb_writter_loginterval (`int`): The logging interval of SummaryWriter. \n   120\t    \"\"\"\n   121\t    def __init__(\n   122\t        self, model, \n   123\t        lora_r:int,\n   124\t        target_rank:int, \n   125\t        init_warmup:int, \n   126\t        final_warmup:int,\n   127\t        mask_interval:int,\n   128\t        beta1:float, \n   129\t        beta2:float, \n   130\t        total_step:Optional[int]=None, \n   131\t        target_total_rank:Optional[int]=None,\n   132\t        tb_writter=None,\n   133\t        tb_writter_loginterval:int=500, \n   134\t    ):\n   135\t        self.ave_target_rank = target_rank \n   136\t        self.target_rank = target_total_rank\n   137\t        self.lora_init_rank = lora_r \n   138\t        self.initial_warmup = init_warmup\n   139\t        self.final_warmup = final_warmup \n   140\t        self.mask_interval = mask_interval\n   141\t        self.beta1 = beta1\n   142\t        self.beta2 = beta2\n   143\t        self.total_step = total_step\n   144\t\n   145\t        self.model = model\n   146\t        self.ipt = {} \n   147\t        self.exp_avg_ipt = {}\n   148\t        self.exp_avg_unc = {}\n   149\t        self.cat_ipt = {}\n   150\t        self.rank_pattern = {} \n   151\t        self.get_lora_param_name()\n   152\t\n   153\t        self.tb_writter = tb_writter\n   154\t        self.log_interval = tb_writter_loginterval \n   155\t\n   156\t        assert (self.beta1<1 and self.beta1>0)\n   157\t        assert (self.beta2<1 and self.beta2>0)\n   158\t\n   159\t    def set_total_step(self, total_step:int): \n   160\t        # Set total step number \n   161\t        self.total_step = total_step\n   162\t        assert self.total_step>self.initial_warmup+self.final_warmup\n   163\t\n   164\t    def get_rank_pattern(self):\n   165\t        # Return rank pattern \n   166\t        return self.rank_pattern\n   167\t\n   168\t    def get_lora_param_name(self):\n   169\t        # Prepare the budget scheduler \n   170\t        self.name_set = set() \n   171\t        self.total_rank = 0 \n   172\t        self.shape_dict = {}\n   173\t        for n,p in self.model.named_parameters():\n   174\t            if \"lora_A\" in n: \n   175\t                name_mat = n.replace(\"lora_A\", \"%s\")\n   176\t                self.name_set.add(name_mat)\n   177\t                self.total_rank += p.size(0) \n   178\t                self.shape_dict[n] = p.shape\n   179\t            if \"lora_B\" in n:\n   180\t                self.shape_dict[n] = p.shape\n   181\t        self.name_set = list(sorted(self.name_set)) \n   182\t        if self.target_rank is None:\n   183\t            self.target_rank = self.ave_target_rank * len(self.name_set) \n   184\t\n   185\t    def schedule_threshold(self, step:int):\n   186\t        # Global budget schedule\n   187\t        mask_ind = False \n   188\t        target_rank = self.target_rank \n   189\t        initial_warmup = self.initial_warmup \n   190\t        final_warmup = self.final_warmup \n   191\t        total_step = self.total_step \n   192\t        self.global_step = step\n   193\t        if step <= initial_warmup: \n   194\t            # Initial warmup \n   195\t            curr_rank = self.total_rank \n   196\t            mask_ind = False \n   197\t        elif step > total_step - final_warmup: \n   198\t            # Final fine-tuning \n   199\t            curr_rank = self.target_rank \n   200\t            # Fix the rank pattern by \n   201\t            # always masking the same unimportant singluar values \n   202\t            mask_ind = True \n   203\t        else: \n   204\t            # Budget decreasing \n   205\t            mul_coeff = 1-(step-initial_warmup)/(total_step-final_warmup-initial_warmup)\n   206\t            curr_rank = target_rank + (self.total_rank-target_rank)*(mul_coeff**3)\n   207\t            curr_rank = int(curr_rank)\n   208\t            mask_ind = True if step % self.mask_interval == 0 else False \n   209\t        return curr_rank, mask_ind \n   210\t\n   211\t\n   212\t    def update_ipt(self, model): \n   213\t        for n,p in model.named_parameters():\n   214\t            if \"lora_\" in n: \n   215\t                if n not in self.ipt:\n   216\t                    self.ipt[n] = torch.zeros_like(p)\n   217\t                    self.exp_avg_ipt[n] = torch.zeros_like(p) \n   218\t                    self.exp_avg_unc[n] = torch.zeros_like(p) \n   219\t                with torch.no_grad():\n   220\t                    # Calculate sensitivity \n   221\t                    self.ipt[n] = (p * p.grad).abs().detach()\n   222\t                    # Update sensitivity \n   223\t                    self.exp_avg_ipt[n] = self.beta1 * self.exp_avg_ipt[n] + \\\n   224\t                                        (1-self.beta1)*self.ipt[n]\n   225\t                    # Update uncertainty \n   226\t                    self.exp_avg_unc[n] = self.beta2 * self.exp_avg_unc[n] + \\\n   227\t                                        (1-self.beta2)*(self.ipt[n]-self.exp_avg_ipt[n]).abs()\n   228\t\n   229\t    def calculate_score(self, n, p=None, metric=\"ipt\"):\n   230\t        if metric == \"ipt\":\n   231\t            # Combine the senstivity and uncertainty \n   232\t            ipt_score = self.exp_avg_ipt[n] * self.exp_avg_unc[n]\n   233\t        elif metric == \"mag\":\n   234\t            ipt_score = p.abs().detach().clone() \n   235\t        else:\n   236\t            raise ValueError(\"Unexcptected Metric: %s\"%metric)\n   237\t        return ipt_score \n   238\t\n   239\t    def _combine_ipt(self, ipt_E, ipt_AB):\n   240\t        ipt_AB = ipt_AB.sum(dim=1, keepdim=False)\n   241\t        sum_ipt = ipt_E.view(-1) + ipt_AB.view(-1)\n   242\t        return sum_ipt\n   243\t\n   244\t    def mask_to_target_rank(self, model, curr_rank): \n   245\t        is_dict = {}\n   246\t        combine_dict = {} \n   247\t        singular_dict = {}\n   248\t        # Calculate the importance score for each sub matrix \n   249\t        for n,p in model.named_parameters(): \n   250\t            if \"lora_A\" in n: \n   251\t                rdim, hdim_a = p.shape\n   252\t                ipt_score = self.calculate_score(n, metric=\"ipt\")\n   253\t                comb_ipt = torch.mean(ipt_score, dim=1, keepdim=True)\n   254\t                name_mat = n.replace(\"lora_A\", \"%s\")\n   255\t                if name_mat not in combine_dict: \n   256\t                    combine_dict[name_mat] = [comb_ipt]\n   257\t                else:\n   258\t                    combine_dict[name_mat].append(comb_ipt)\n   259\t            if \"lora_B\" in n: \n   260\t                hdim_b, rdim = p.shape \n   261\t                ipt_score = self.calculate_score(n, metric=\"ipt\")\n   262\t                comb_ipt = torch.mean(ipt_score, dim=0, keepdim=False).view(-1, 1)\n   263\t                name_mat = n.replace(\"lora_B\", \"%s\")\n   264\t                if name_mat not in combine_dict: \n   265\t                    combine_dict[name_mat] = [comb_ipt]\n   266\t                else:\n   267\t                    combine_dict[name_mat].append(comb_ipt)\n   268\t            if \"lora_E\" in n:\n   269\t                ipt_score = self.calculate_score(n, p=p, metric=\"ipt\")                \n   270\t                name_mat = n.replace(\"lora_E\", \"%s\")\n   271\t                singular_dict[name_mat] = ipt_score\n   272\t\n   273\t        # Combine the importance scores \n   274\t        all_is = []\n   275\t        for name_mat in combine_dict: \n   276\t            ipt_E = singular_dict[name_mat] \n   277\t            ipt_AB = torch.cat(combine_dict[name_mat], dim=1)\n   278\t            sum_ipt = self._combine_ipt(ipt_E, ipt_AB)\n   279\t            name_E = name_mat%\"lora_E\"\n   280\t            is_dict[name_E] = sum_ipt.view(-1, 1)\n   281\t            all_is.append(sum_ipt.view(-1))\n   282\t\n   283\t        # Calculate the masking threshold \n   284\t        mask_threshold = torch.kthvalue(torch.cat(all_is), (self.total_rank-curr_rank))[0].item()\n   285\t\n   286\t        # Mask out unimportant singular values \n   287\t        with torch.no_grad():\n   288\t            curr_sum_rank = 0\n   289\t            sum_param = 0\n   290\t            for n,p in model.named_parameters():\n   291\t                if \"lora_E\" in n: \n   292\t                    p.data.masked_fill_(is_dict[n]<=mask_threshold, 0.0)\n   293\t                    ranknum = (is_dict[n]>mask_threshold).sum().item() \n   294\t\n   295\t                    if self.tb_writter is not None and self.global_step%self.log_interval==0:\n   296\t                        self.tb_writter.add_scalar(\"Ranknum/%s\"%(n,), ranknum, self.global_step) \n   297\t                        self.rank_pattern[n] = ranknum \n   298\t                        curr_sum_rank += ranknum \n   299\t                        sum_param += ranknum*self.shape_dict[n.replace(\"lora_E\", \"lora_A\")][1]  \n   300\t                        sum_param += ranknum*self.shape_dict[n.replace(\"lora_E\", \"lora_B\")][0]  \n   301\t\n   302\t            if self.tb_writter is not None and self.global_step%self.log_interval==0:\n   303\t                self.tb_writter.add_scalar(\"Budget/total_rank\", curr_sum_rank, self.global_step)\n   304\t                self.tb_writter.add_scalar(\"Budget/mask_threshold\", mask_threshold, self.global_step)\n   305\t                self.tb_writter.add_scalar(\"Budget/sum_param\", sum_param, self.global_step)\n   306\t\n   307\t        return mask_threshold\n   308\t\n   309\t\n   310\t    def update_and_mask(self, model, global_step):\n   311\t        if global_step<self.total_step-self.final_warmup:\n   312\t            # Update importance scores element-wise \n   313\t            self.update_ipt(model)\n   314\t            # do not update ipt during final fine-tuning \n   315\t        # Budget schedule\n   316\t        curr_rank, mask_ind = self.schedule_threshold(global_step)\n   317\t        if mask_ind:\n   318\t            # Mask to target budget \n   319\t            mask_threshold = self.mask_to_target_rank(model, curr_rank) \n   320\t        else:\n   321\t            mask_threshold = None \n   322\t        self._maybe_tb_writter_log(model)\n   323\t        return curr_rank, mask_threshold\n   324\t\n   325\t    def _maybe_tb_writter_log(self, model):\n   326\t        if self.tb_writter is not None and self.global_step%self.log_interval==0:\n   327\t            with torch.no_grad():\n   328\t                regu_loss = []\n   329\t                for n,p in model.named_parameters():\n   330\t                    if \"lora_A\" in n or \"lora_B\" in n:\n   331\t                        mat = p.data.detach().clone()\n   332\t                        mat_cov = mat @ mat.T if \"lora_A\" in n else mat.T @ mat \n   333\t                        I = torch.eye(*mat_cov.size(), out=torch.empty_like(mat_cov))\n   334\t                        I.requires_grad = False\n   335\t                        orth_regu = torch.norm(mat_cov-I, p=\"fro\")\n   336\t                        regu_loss.append(orth_regu.item())\n   337\t                        self.tb_writter.add_scalar(\n   338\t                            \"Orth_regu_loss/%s\"%n, orth_regu.item(), self.global_step\n   339\t                        )\n   340\t                self.tb_writter.add_scalar(\n   341\t                    \"train/orth_regu_loss\", sum(regu_loss)/len(regu_loss), self.global_step\n   342\t                )\n   343\t\n   344\t\n   345\tdef compute_orth_regu(model, regu_weight=0.1):\n   346\t    # The function to compute orthongonal regularization for SVDLinear in `model`. \n   347\t    regu_loss, num_param = 0., 0\n   348\t    for n,p in model.named_parameters():\n   349\t        if \"lora_A\" in n or \"lora_B\" in n:\n   350\t            para_cov = p @ p.T if \"lora_A\" in n else p.T @ p \n   351\t            I = torch.eye(*para_cov.size(), out=torch.empty_like(para_cov))\n   352\t            I.requires_grad = False\n   353\t            regu_loss += torch.norm(para_cov-I, p=\"fro\")\n   354\t            num_param += 1\n   355\t    return regu_weight*regu_loss/num_param\n", "extras": {"path": "/workspace/benchmark/datasets/25-AdaLora/golden_files/adalora_golden.py", "impl_source": "oh_aci"}}