{"id": 31, "timestamp": "2025-05-16T17:43:15.527940", "source": "agent", "message": "Code executed in IPython cell.", "cause": 30, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_JIbcQcdGgE8efHy1dSkFC3BO", "model_response": {"id": "chatcmpl-BXy3jEfeeYtWuPbHdOa2zGoc24AQ9", "created": 1747435391, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_b38e740b47", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Let's extract all lines containing 'loss', 'SPIN', or 'KL' for more context\\nlines = text.split('\\\\n')\\nrelevant_lines = [l for l in lines if any(k in l.lower() for k in ['spin', 'loss', 'kl', 'objective'])]\\nfor l in relevant_lines:\\n    print(l)\\n    if 'log(' in l or 'exp(' in l or 'kl' in l.lower():\\n        print('----')\"}", "name": "execute_ipython_cell"}, "id": "call_JIbcQcdGgE8efHy1dSkFC3BO", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 119, "prompt_tokens": 12786, "total_tokens": 12905, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 12672}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "called Self-Play fIne-tuNing ( SPIN ), which starts\nofSPIN lies a self-play mechanism, where the\noptimum to the training objective function of our\nBig-Bench. Our results show that SPIN can sig-\nhttps://github.com/uclaml/SPIN .\nfIne-tuNing ( SPIN ), which begins from a supervised fine-\ntuned model. SPIN allows the LLM to engage in self-play,\nhuman-annotated SFT dataset. The subsequent objective\nset of Ultrachat200k dataset. Ultimately, SPIN effectively\non MT-Bench (Zheng et al., 2023). Notably, SPIN achieves\na strong model, our SPIN operates effectively with a single\nnegative log-likelihood loss associated with these distribu-\ntion term yields the typical cross-entropy loss, expressed as\nprompt x. The objective of the RL fine-tuning process is\nthen to maximize the following objective function:\n\u2212\u03bbEx\u223cq(\u00b7)KL\u0000\n----\nwhere the Kullback-Leibler (KL) regularization enforces\n----\nas the supervised fine-tuned model. The inclusion of KL\n----\n4.1 Self-Play Fine-Tuning ( SPIN )\nobjective is to distinguish the responses generated by the\nx\u223cq(\u00b7),y\u223cpdata(\u00b7|x), y\u2032\u223cp\u03b8t(\u00b7|x), and \u2113(\u00b7)is a loss\nFor example, a linear loss function \u2113(t) =\u2212treduces (4.2)\na linear loss function results in an unbounded objective\nTherefore, in our work, we choose the logistic loss function\nof loss function aids in preventing the excessive growth in\nis attributed to the LLM p\u03b8t. Subsequently, the objective\nwe incorporate a Kullback-Leibler (KL) regularization term.\n----\n\u2212\u03bbEx\u223cq(\u00b7)KL\u0000\n----\nEnd-to-end Training Objective. We integrate the previ-\ning objective with an update rule of \u03b8t+1. Specifically,\nplugging (4.5) into(4.2) arrives at the update rule \u03b8t+1=argmin\u03b8\u2208\u0398LSPIN(\u03b8,\u03b8t), where LSPIN is the training ob-\nLSPIN=E\u0014\niterative self-play process of our method SPIN as follows,\n4.2 Comparison between SPIN and DPO\nIn Section 4.1, we propose Self-Play Fine-Tuning ( SPIN )\nwith an end-to-end training objective (4.7) for each iter-\nHowever, SPIN and DPO are fundamentally different.\nAlgorithm 1 Self-Play Fine-Tuning ( SPIN )\ntion without explicit reward estimation. In contrast, SPIN\niteration. On the contrary, SPIN \u2019s self-play mechanism\nnaturally leads to an iterative training procedure. SPIN\n2.SPIN only requires the SFT dataset, represented by pairs\ntively. Moreover, SPIN can be applied between SFT and\n3.InSPIN , we can choose different loss functions \u2113which\nin Theorem 5.2), which includes correlation loss, hinge\nloss, and logistic loss. Only when \u2113is chosen as the lo-\ngistic loss would the training objective of SPIN become\nence optimization with the Pairwise Cringe Loss (PCO), and\npared with Xu et al. (2023); Yuan et al. (2024), SPIN \u2019s\nrithm 1 in Section 4. Under monotonicity and convexityassumption of the objective function \u2113, we show that the\nAssumption 5.1. The loss function \u2113(t) :R\u2192Ris\nAssumption 5.1 holds for a wide range of loss functions\nloss\u2113(t) = 1 \u2212t, hinge loss \u2113(t) = max(0 ,1\u2212t), ex-\nponential loss \u2113(t) = exp( \u2212t)and logistic loss \u2113(t) =\n----\nFor the logistic loss function \u2113(t) = log(1 + exp( \u2212t)), the\n----\nSPIN .\nTheorem 5.4. Consider the choice of logistic loss\n\u2113(t) = log(1 + exp( \u2212t))inSPIN . Suppose that\n----\nLSPIN(\u03b8,\u03b8t), then the opponent player at iteration t+ 1\nof the KL regularization that is employed to control the\n----\nSPIN . Our findings highlight several key points: (1)SPIN\nSPIN at iteration 0achieves performance on par to DPO\nis a necessary component in SPIN as it breaks the limit of\n6.2 SPIN Effectively Improves Benchmark\nSFT SPIN\niter-0SPIN\niter-1SPIN\niter-2SPIN\nFigure 2. The average score of SPIN at different itera-\nIn Figure 2, we demonstrate the effectiveness of SPIN us-\ntuned model by SPIN after iterations 0 to 3 with the base\nTable 4 in Appendix B. We can observe that SPIN ex-\niteration 0to generate new responses for SPIN , adhering\nourSPIN only requires the initial model itself. Moreover,\nwe show the performance comparison of SPIN at iterations\nleverages more data from new sources, SPIN based on the\n1,SPIN even surpasses the performance of DPO on the\nSPIN compared to the SFT data, as well as the necessity\nof iterative training in SPIN . In Appendix B, we present\nassessment of SPIN on additional benchmark tasks.\ndata size on the performance of SPIN . In Figure 5, we\ndemonstrate the effect of training size for SPIN during it-\nx-axis 0) in the figure for SFT. For SPIN , we report the\nepoch. We select the training size of SPIN at iteration 0\ndataset. The performance of SPIN was then evaluated after\ncan observe that, while SPIN results in notable improve-\nHuggingFaceH4/ultrafeedback_binarizedSPIN over multiple epochs at iteration 0. It is evident that\nepochs. Notably, SPIN exhibits robustness and stability; ex-\nThis paper introduces a novel fine-tuning method SPIN , to\ndata distribution. Therefore, SPIN facilitates the LLM\u2019s\nmethods, SPIN enables the LLM to self-improve without\nEmpirical results demonstrate that SPIN significantly en-\ndemonstrate that the optimization process of SPIN con-\nSPIN-iter-0\nSPIN-iter-1\nSPIN-iter-2\nSPIN-iter-3\nFigure 4. TheSPIN training dynamics of zephyr-7b-sft-full on the 50k synthetic data with regard to the number of training\nSPIN\nFigure 5. The scaling effect of training size of SPIN compared to\nSFT on the average score of Open LLM Leaderboard. For SPIN ,\nSPIN (with x-axis 0) is the zephyr-7b-sft-full checkpoint,\nreport the model performance trained for 1 epoch with SPIN on\nbySPIN may be used to further augment the training of\nvarious language models. Moreover, SPIN demonstrated\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,\n----\nmization with the pairwise cringe loss. arXiv preprint\nmala, K., Gloeckle, F., Rozi\u00e8re, B., Wu, C.-J., Morcos,\n----\nquality, Mao et al. (2017) introduced the Least Squares GAN, employing a least squares loss function for the discriminator.\nhighlighted in the development of Wasserstein GAN by Arjovsky et al. (2017). This model employs IPM in its loss design,\nand proposed Relativistic GAN. It is worth noting that the objective function defined in our (4.2) is similar to Relativistic\nlinear loss. However, our approach differs in both the choice of the function class and the training procedure. Inspired by\nnetworks in each iteration. In contrast to the above methods, SPIN relies on self-play where both the main player and the\nAlignment with AI Feedback. The objective of alignment is to fine-tune LLMs to align with human intentions. In addition\nWe use the Alignment Handbook library (Tunstall et al., 2023b) as the codebase for our self-play fine-tuning method SPIN ,\nThe cost overhead introduced by SPIN is mostly the generation of synthetic data from the LLM that we train. The cost of\ntime for SPIN in Table 2 . Results were obtained using a machine with 8xA100 (80G) GPUs. For per 64 examples, the\nB.3 Additional Experiment Result for SPIN+DPO\nSPIN requires only the SFT data to improve over the traditional SFT stage and can sit between SFT and RL finetuning.\nmodel after SPIN using RL fine-tuning.\nStarting at SPIN iteration 3, we further train the model with DPO for two epochs on the 62k preference data from the\nTable 3. Performance of SPIN + DPO based on zephyr-7b-sft-full across HuggingFace Open LLM Leaderboard datasets,\nSPIN iteration 0 63.40 49.18 72.69 35.10 84.38 60.03 60.80(+2.66)\nSPIN iteration 1 65.19 55.17 72.30 35.78 84.96 59.34 62.12(+1.32)\nSPIN iteration 2 65.96 54.91 73.56 38.06 85.41 59.93 62.97(+0.85)\nSPIN iteration 3 65.87 54.90 73.72 38.97 85.54 59.99 63.16(+0.19)\nSPIN iteration 3+ DPO 66.47 60.07 78.06 37.98 86.17 59.68 64.05(+0.89)\nWe can observe that the checkpoint trained by SPIN can be further improved using DPO, yielding an enhancement of 0.89%\nIn Table 4, we show the detailed performance of SPIN at different iterations on each of the task in Open LLM Leaderboard.\nFurther Investigation on More Tasks. Here, we further investigate the performance of SPIN on a broader variety of\nTable 4. Test performance of SPIN based on zephyr-7b-sft-full across HuggingFace Open LLM Leaderboard datasets. We also\nSPIN iteration 0 63.40 49.18 72.69 35.10 84.38 60.03 60.80(+2.66)\nSPIN iteration 1 65.19 55.17 72.30 35.78 84.96 59.34 62.12(+1.32)\nSPIN iteration 2 65.96 54.91 73.56 38.06 85.41 59.93 62.97(+0.85)\nSPIN iteration 3 65.87 54.90 73.72 38.97 85.54 59.99 63.16(+0.19)\nreasoning) and Formal Fallacies (logical reasoning). In Table 6, we show the resulting scores of SPIN on MT-Bench as\ntypes of questions. We can see a notably robust improvement in the performance of SPIN on various tasks besides the\nHuggingFace Benchmark, without major degradation. Notably, on MT-Bench, the model fine-tuned by SPIN has surpassed\nTable 6. Test performance on other reasoning benchmark datasets for SPIN at different iterations and zephyr-7b-sft-full . We\nSPIN iteration 0 6.46(+0.52) 57.75 51.6 95.2 46.8\nSPIN iteration 1 6.65(+0.19) 58.82 51.2 95.2 47.2\nSPIN iteration 2 6.78(+0.13) 59.36 51.2 94.4 47.6\nIn Tables 7 and 8, we further provide the generation examples of our fine-tuned model by SPIN from different iterations.\nSPIN iter -2\nSPIN iter -1\nSPIN iter -0\nFigure 6. Model performance on MT-Bench. We compare SPIN across different iterations with the base SFT model. Starting from\niteration 1, our fine-tuned model by SPIN robustly outperforms the SFT checkpoint on all evaluation aspects.\nTable 7. Generation example of our fine-tuned model by SPIN at different iterations.\nTable 8. Another generation example of our fine-tuned model by SPIN at different iterations.\n2LSPIN(\u03b8,\u03b8t) =Ex\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\u0014\nLSPIN(\u03b8,\u03b8t)\u2265\u2113(0) = LSPIN(\u03b8t,\u03b8t), which means that \u03b8tis the global optimum of (4.7) . As a consequence, the gradient\nKL\u0000\n----\n+ KL\u0000\n----\nLSPIN(\u03b8\u2217,\u03b8t) =Ex\u223cq(\u00b7),y\u223cp\u03b8\u2217(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\u0014\n=LSPIN(\u03b8t,\u03b8t),\ncan further prove that (4.2) with the logistic loss function has a closed-form solution if we ignore the constraint set Ft.\n2KL\u0010\n----\n2KL\u0010\n----\nwhere KL(\u00b7\u2225\u00b7)is KL-divergence. JSD is always non-negative and equals zero if and only if p+andp\u2212are identical.\n----\nProof of Lemma C.2. We rewrite the objective function in the following formula,\n= 2 log 2 \u2212KL\u0012\n----\n\u2212KL\u0012\n----\n\u03b8\u2208\u0398LSPIN(\u03b8,\u03b8t). (C.4)\nSince \u03b8t+1is the global minimum of LSPIN(\u03b8,\u03b8t). Then by (C.4) ,\u03bblog\u0010p\u03b8t+1(y|x)\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Let's extract all lines containing 'loss', 'SPIN', or 'KL' for more context\nlines = text.split('\\n')\nrelevant_lines = [l for l in lines if any(k in l.lower() for k in ['spin', 'loss', 'kl', 'objective'])]\nfor l in relevant_lines:\n    print(l)\n    if 'log(' in l or 'exp(' in l or 'kl' in l.lower():\n        print('----')"}, "success": true}