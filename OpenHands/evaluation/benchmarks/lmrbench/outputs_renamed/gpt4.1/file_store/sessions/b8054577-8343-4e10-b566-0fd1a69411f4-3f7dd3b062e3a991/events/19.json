{"id": 19, "timestamp": "2025-05-17T00:04:05.075649", "source": "agent", "message": "Code executed in IPython cell.", "cause": 18, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_hmmvwogPavLVAJ79SvFc07sB", "model_response": {"id": "chatcmpl-BY40J7nJcKQmlglNSxDogI5lD7srZ", "created": 1747458243, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_2d9626f3cf", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Print more of the paper to find the NSA algorithm details, especially the mechanism and equations\\nprint(full_text[12000:18000])\"}", "name": "execute_ipython_cell"}, "id": "call_hmmvwogPavLVAJ79SvFc07sB", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 44, "prompt_tokens": 8664, "total_tokens": 8708, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 7424}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "s long-context fine-tuning and reinforcement learning.\nHowever, existing sparse attention methods primarily target inference, leaving the computa-\ntional challenges in training largely unaddressed. This limitation hinders the development\nof more capable long-context models through efficient training. Additionally, efforts to adapt\nexisting sparse attention for training also expose challenges:\nNon-Trainable Components. Discrete operations in methods like ClusterKV (Liu et al.,\n2024) (includes k-means clustering) and MagicPIG (Chen et al., 2024) (includes SimHash-based\nselecting) create discontinuities in the computational graph. These non-trainable components\nprevent gradient flow through the token selection process, limiting the model\u2019s ability to learn\noptimal sparse patterns.\nInefficient Back-propagation. Some theoretically trainable sparse attention methods suffer\nfrom practical training inefficiencies. Token-granular selection strategy used in approaches\n4\nlike HashAttention (Desai et al., 2024) leads to the need to load a large number of individual\ntokens from the KV cache during attention computation. This non-contiguous memory access\nprevents efficient adaptation of fast attention techniques like FlashAttention, which rely on\ncontiguous memory access and blockwise computation to achieve high throughput. As a result,\nimplementations are forced to fall back to low hardware utilization, significantly degrading\ntraining efficiency.\n2.3. Native Sparsity as an Imperative\nThese limitations in inference efficiency and training viability motivate our fundamental redesign\nof sparse attention mechanisms. We propose NSA, a natively sparse attention framework that\naddresses both computational efficiency and training requirements. In the following sections,\nwe detail the algorithmic design and operator implementation of NSA.\n3. Methodology\nOur technical approach spans algorithm design and kernel optimization. In the following\nsubsections, we first introduce the background of our methodology. Then we present the\noverall framework of NSA, followed by its key algorithmic components. Finally, we detail our\nhardware-optimized kernel design that maximizes practical efficiency.\n3.1. Background\nAttention Mechanism is widely used in language modeling where each query token q\ud835\udc61computes\nrelevance scores against all preceding keys k:\ud835\udc61to generate a weighted sum of values v:\ud835\udc61. Formally,\nfor an input sequence of length \ud835\udc61, the attention operation is defined as:\no\ud835\udc61=Attn\u0000q\ud835\udc61,k:\ud835\udc61,v:\ud835\udc61\u0001(1)\nwhere Attn denotes the attention function:\nAttn\u0000q\ud835\udc61,k:\ud835\udc61,v:\ud835\udc61\u0001=\ud835\udc61\u2211\ufe01\n\ud835\udc56=1\ud835\udefc\ud835\udc61,\ud835\udc56v\ud835\udc56\u00cd\ud835\udc61\n\ud835\udc57=1\ud835\udefc\ud835\udc61,\ud835\udc57,\ud835\udefc\ud835\udc61,\ud835\udc56=\ud835\udc52q\u22a4\n\ud835\udc61k\ud835\udc56\u221a\n\ud835\udc51\ud835\udc58. (2)\nHere,\ud835\udefc\ud835\udc61,\ud835\udc56represents the attention weight between q\ud835\udc61and k\ud835\udc56, and\ud835\udc51\ud835\udc58is the feature dimension of\nkeys. As sequence length increases, attention computation becomes increasingly dominant in\nthe overall computational cost, presenting significant challenges for long-context processing.\nArithmetic Intensity is the ratio of compute operations to memory accesses. It intrinsically\nshapes algorithm optimization on hardware. Each GPU has a critical arithmetic intensity\ndetermined by its peak compute capability and memory bandwidth, calculated as the ratio\nof these two hardware limits. For computation tasks, arithmetic intensity above this critical\nthreshold becomes compute-bound (limited by GPU FLOPS), while below it becomes memory-\nbound (limited by memory bandwidth).\nSpecifically for causal self-attention mechanism, during training and prefilling phases,\nbatched matrix multiplications and attention computations exhibit high arithmetic intensity,\nmaking these stages compute-bound on modern accelerators. In contrast, auto-regressive de-\ncoding becomes memory-bandwidth constrained because it generates one token per forward\npass while requiring loading the entire key-value cache, resulting in low arithmetic intensity.\nThis leads to different optimization goals \u2014 reducing computation cost during training and\nprefilling, while reducing memory access during decoding.\n5\n3.2. Overall Framework\nTo leverage the potential of attention with natural sparse pattern, we propose replacing the\noriginal key-value pairs k:\ud835\udc61,v:\ud835\udc61in Equation (1) with a more compact and information-dense set\nof representation key-value pairs \u02dc\ud835\udc3e\ud835\udc61,\u02dc\ud835\udc49\ud835\udc61given each query q\ud835\udc61. Specifically, we formally define the\noptimized attention output as follows:\n\u02dc\ud835\udc3e\ud835\udc61=\ud835\udc53\ud835\udc3e(q\ud835\udc61,k:\ud835\udc61,v:\ud835\udc61),\u02dc\ud835\udc49\ud835\udc61=\ud835\udc53\ud835\udc49(q\ud835\udc61,k:\ud835\udc61,v:\ud835\udc61) (3)\no\u2217\n\ud835\udc61=Attn\u0000q\ud835\udc61,\u02dc\ud835\udc3e\ud835\udc61,\u02dc\ud835\udc49\ud835\udc61\u0001(4)\nwhere \u02dc\ud835\udc3e\ud835\udc61,\u02dc\ud835\udc49\ud835\udc61are dynamically constructed based on the current query q\ud835\udc61and the contextual\nmemory k:\ud835\udc61,v:\ud835\udc61. We can design various mapping strategies to get different categories of \u02dc\ud835\udc3e\ud835\udc50\n\ud835\udc61,\u02dc\ud835\udc49\ud835\udc50\n\ud835\udc61,\nand combine them as follows:\no\u2217\n\ud835\udc61=\u2211\ufe01\n\ud835\udc50\u2208C\ud835\udc54\ud835\udc50\n\ud835\udc61\u00b7Attn(q\ud835\udc61,\u02dc\ud835\udc3e\ud835\udc50\n\ud835\udc61,\u02dc\ud835\udc49\ud835\udc50\n\ud835\udc61). (5)\nAs illustrated in Figure 2, NSA have three mapping strategies C={cmp ,slc,win}, representing\ncompression, selection, and sliding window for keys and values. \ud835\udc54\ud835\udc50\n\ud835\udc61\u2208[0, 1]is the gate score for\ncorresponding strategy \ud835\udc50, derived from input features via an MLP and sigmoid activation. Let\n\ud835\udc41\ud835\udc61denote the total number of remapped keys/values:\n\ud835\udc41\ud835\udc61=\u2211\ufe01\n\ud835\udc50\u2208Csize[\u02dc\ud835\udc3e\ud835\udc50\n\ud835\udc61]. (6)\nWe maintain a high sparsity ratio by ensuring \ud835\udc41\ud835\udc61\u226a\ud835\udc61.\n3.3. Algorithm Design\nIn this subsection, we introduce the design of our remapping strategies \ud835\udc53\ud835\udc3eand\ud835\udc53\ud835\udc49: token\ncompression, token selection, and sliding window.\n3.3.1. Token Compression\nBy aggregating sequential blocks of keys or values into block-level representations, we obtain\ncompressed keys and values that capture the information of the entire block. Formally, the\ncompressed key representation is defined as:\n\u02dc\ud835\udc3ecmp\n\ud835\udc61=\ud835\udc53cmp\n\ud835\udc3e(k:\ud835\udc61)=\u001a\n\ud835\udf11(k\ud835\udc56\ud835\udc51+1:\ud835\udc56\ud835\udc51+\ud835\udc59)\f\f\f\f1\u2a7d\ud835\udc56\u2a7d\u0016\ud835\udc61\u2212\ud835\udc59\n\ud835\udc51\u0017\u001b\n(7)\nwhere\ud835\udc59is the block length, \ud835\udc51is the sliding stride between adjacent blocks, and \ud835\udf11is a learnable\nMLP with intra-block position encoding to map keys in a block to a single compressed key.\n\u02dc\ud835\udc3ecmp\n\ud835\udc61\u2208R\ud835\udc51\ud835\udc58\u00d7\u230a\ud835\udc61\u2212\ud835\udc59\n\ud835\udc51\u230bis tensor composed by compresion keys. Usually, we adopt \ud835\udc51 < \ud835\udc59 to mitigate\ninformation fragmentation. An analogous formulation holds for the compressed value rep-\nresentation \u02dc\ud835\udc49cmp\n\ud835\udc61. Compressed representations capture coarser-grained higher-level semant\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Print more of the paper to find the NSA algorithm details, especially the mechanism and equations\nprint(full_text[12000:18000])"}, "success": true}