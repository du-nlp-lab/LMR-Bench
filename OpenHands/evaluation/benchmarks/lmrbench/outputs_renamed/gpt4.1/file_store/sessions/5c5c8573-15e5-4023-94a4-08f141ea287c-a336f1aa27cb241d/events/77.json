{"id": 77, "timestamp": "2025-05-16T17:26:50.786167", "source": "agent", "message": "Code executed in IPython cell.", "cause": 76, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_m2GS7ufkjd8aBqXSaAFIrF1m", "model_response": {"id": "chatcmpl-BXxnf5cJx189d7lZxzFmGfTJHX4nN", "created": 1747434395, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_b38e740b47", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Try to extract the table of contents or section headers for ViT/vision transformer\\nwith open(pdf_path, 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    toc = []\\n    for i, page in enumerate(reader.pages):\\n        text = page.extract_text()\\n        if text:\\n            lines = text.split('\\\\n')\\n            for line in lines:\\n                if 'vision transformer' in line.lower() or 'ViT' in line or 'Vision Transformer' in line:\\n                    toc.append((i, line))\\nprint(toc)\"}", "name": "execute_ipython_cell"}, "id": "call_m2GS7ufkjd8aBqXSaAFIrF1m", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 145, "prompt_tokens": 25524, "total_tokens": 25669, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 24832}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "[(0, 'attention output. Last, we also study massive activations in Vision Transformers. Code is available at'), (1, 'Finally, we also observe massive activations in Vision Transformers (ViTs). They appear less frequently than'), (1, 'those in LLMs but are still in many of the ViTs we have examined. In these ViTs, they tend to appear at'), (1, 'proposed \u201cregister tokens\u201d in ViTs (Darcet et al., 2023). We show they both learn values independent of input'), (9, '5 Massive Activations in Vision Transformers'), (9, 'In this section, we study if Vision Transformers (ViTs) (Dosovitskiy et al., 2021) exhibit massive activations.'), (9, 'We note that while ViTs and LLMs are both based on self-attention, ViTs employ global token mixing, which'), (9, 'Massive activations in ViTs. We explore several model families based on ViTs: CLIP (Radford et al.,'), (9, '2021), MAE (He et al., 2021) and DINOv2 (Oquab et al., 2024). We examine the ViT-L models from'), (9, 'Figure 11. We find that massive activations exist in CLIP and DINOv2 ViT-L, where we highlight the'), (9, 'MAE ViT-L. In this model, a feature dimension (927) exhibits uniformly large values across all tokens.'), (9, 'Massive activations are biases in ViTs. Figure 13 shows the three largest activation magnitudes and the'), (9, 'median per layer in CLIP and DINOv2 ViT-L, averaged over 1k images. We find that massive activations are'), (9, 'worth noting that unlike LLMs, massive activations start to appear only in the later stages of ViTs.'), (10, 'Patch TokensPatch TokensPatch TokensFigure 11: Massive activations are present in ViT-L from CLIP and DINOv2, but not MAE.'), (10, 'CLIP ViT-L'), (10, 'DINOv2 ViT-L'), (10, 'tude at each layer in CLIP and DINOv2 ViT-L.CLIP ViT-L, layer 13'), (10, 'massive activations in CLIP ViT-L.'), (10, 'Following our methodology in Section 3, we perform intervention analysis on CLIP ViT-L. We modify the'), (10, 'massive activations function as fixed but crucial biases in ViTs, aligned with our observations in Section 3.'), (10, 'Figure 14: DINOv2-reg ViT-G.Registers are biases in ViTs. Recently Darcet et al. (2023) propose'), (10, 'to augment standard ViTs with additional learnable tokens, which they'), (10, 'name as register tokens. They show that training ViTs with register'), (10, 'DINOv2. ExaminingthelargestViT-GmodelinDINOv2-reg, weobserve'), (10, 'different from standard ViTs, massive activations do not appear in'), (10, 'shows the results. We find that ViTs with fixed register features achieve accuracy comparable to original'), (11, 'CLS Reg 1 Reg 2 Reg 3 Reg 4 Patch0%20%40%60%Attention Distribution, DINOv2-reg ViT-GFigure 16: Average attention of the [CLS]token.DINOv2-reg with 4 registers'), (11, 'ImageNet acc ( %) ViT-S ViT-B ViT-L ViT-G'), (11, 'their means and evaluate the intervened ViTs.'), (11, 'models, suggesting that registers act as learned biases in ViTs. This leads to constant key and value states at'), (11, 'ViT with register tokens function equivalently to a standard ViT augmented with explicit attention biases.'), (11, 'Massive activations exist in many but not all ViTs. Similar to those in LLMs, these activations act as'), (11, '(2023) revealed the existence of attention artifacts in ViTs. Robinson et al. (2023) found sparse activation'), (11, 'patterns in ViTs that attract attention to certain tokens. Our work provides an in-depth analysis as to why'), (12, 'also generalize well to Vision Transformers (ViTs). We hope the new results presented in this work contribute'), (13, 'Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers.'), (27, 'C Additional Results on Vision Transformer'), (27, 'In this section, we provide additional results for Vision Transformers (ViTs). This includes illustration of'), (27, 'C.1 Massive Activations in ViTs'), (27, 'We present results of massive activations in ViTs on 4 images from Figure 41. Results of CLIP ViT-L,'), (27, 'DINOv2 ViT-L and DINOv2-reg ViT-G are shown in Figure 42, Figure 43 and Figure 44. We highlight the'), (27, 'patch tokens exhibiting massive activations. For standard ViTs like CLIP ViT-L and DINOv2 ViT-L, massive'), (27, 'input images. For DINOv2-reg ViT-G, they exist in a fixed register token, i.e., register 3.'), (28, 'Patch TokensPatch TokensPatch TokensPatch TokensCLIP ViT-L, Layer 23Figure 42: Illustration of massive activations in CLIP ViT-L for the 4 images shown in Figure 41.'), (28, 'Patch TokensPatch TokensPatch TokensPatch TokensDINOv2 ViT-L, Layer 23'), (28, 'Figure 43: Illustration of massive activations in DINOv2 ViT-L for the 4 images shown in Figure 41.'), (28, 'Patch TokensPatch TokensPatch TokensPatch TokensDINOv2-reg ViT-G, Layer 40'), (28, 'Figure 44: Illustration of massive activations in DINOv2 ViT-G for the 4 images shown in Figure 41.'), (28, 'In Masked Autoencoders (MAEs) (He et al., 2021), a dummy token is added to ViTs during pretraining. In'), (28, 'token features as constant across the output features of alllayers in ViTs, which we denote as Fix-Reg-Mean .'), (28, 'This result further supports our argument that registers function as biases within ViTs.'), (29, 'ImageNet acc ViT-B ViT-L ViT-H'), (29, 'Figure 45, 46 and 47 detail the layer-level analysis results for all ViTs examined in this paper (also summarized'), (29, 'in Table 8). Different from LLMs, some ViTs do not exhibit massive activations, e.g., MAE ViT-B/L and'), (29, 'DINOv2 ViT-S. For ViTs where we observe massive activations, e.g., CLIP ViT-L and DINOv2 ViT-L, the'), (29, 'trend across layers differs from LLMs. For instance, in the case of DINOv2 ViT-L, massive activations are'), (29, 'MAE ViT-B'), (29, 'MAE ViT-L'), (29, 'OpenAI CLIP ViT-B'), (29, 'OpenAI CLIP ViT-L'), (29, 'Figure 45: Layer-level analysis for ViTs in MAE and CLIP.'), (29, 'DINOv2 ViT-S'), (29, 'DINOv2 ViT-B'), (29, 'DINOv2 ViT-L'), (29, 'DINOv2 ViT-G'), (29, 'Figure 46: Layer-level analysis for ViTs in DINOv2.'), (29, 'DINOv2-reg ViT-S'), (29, 'DINOv2-reg ViT-B'), (29, 'DINOv2-reg ViT-L'), (29, 'DINOv2-reg ViT-G'), (29, 'Figure 47: Layer-level analysis for ViTs in DINOv2-reg.'), (30, 'Table 7 and Table 8 list the information of the LLM and ViT models used in this paper.'), (30, 'DINOv2ViT-S 12 384 6 timm/vit_small_patch14_dinov2.lvd142m'), (30, 'ViT-B 12 768 12 timm/vit_base_patch14_dinov2.lvd142m'), (30, 'ViT-L 24 1024 16 timm/vit_large_patch14_dinov2.lvd142m'), (30, 'ViT-G 40 1536 24 timm/vit_giant_patch14_dinov2.lvd142m'), (30, 'DINOv2-regViT-S 12 384 6 timm/vit_small_patch14_reg4_dinov2.lvd142m'), (30, 'ViT-B 12 768 12 timm/vit_base_patch14_reg4_dinov2.lvd142m'), (30, 'ViT-L 24 1024 16 timm/vit_large_patch14_reg4_dinov2.lvd142m'), (30, 'ViT-G 40 1536 24 timm/vit_giant_patch14_reg4_dinov2.lvd142m'), (30, 'MAEViT-B 12 768 12 timm/vit_base_patch16_224.mae'), (30, 'ViT-L 24 1024 16 timm/vit_large_patch16_224.mae'), (30, 'ViT-H 32 1280 16 timm/vit_huge_patch16_224.mae'), (30, 'CLIPViT-B 12 768 12 timm/vit_base_patch16_clip_224.openai'), (30, 'ViT-L 24 1024 16 timm/vit_large_patch14_clip_224.openai'), (30, 'Table 8: Relevant information of ViT models we experimented with in this work.')]\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Try to extract the table of contents or section headers for ViT/vision transformer\nwith open(pdf_path, 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    toc = []\n    for i, page in enumerate(reader.pages):\n        text = page.extract_text()\n        if text:\n            lines = text.split('\\n')\n            for line in lines:\n                if 'vision transformer' in line.lower() or 'ViT' in line or 'Vision Transformer' in line:\n                    toc.append((i, line))\nprint(toc)"}, "success": true}