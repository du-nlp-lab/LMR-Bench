{"id": 27, "timestamp": "2025-05-16T17:03:24.825287", "source": "agent", "message": "Code executed in IPython cell.", "cause": 26, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_RuBFRlqyMI25KNNabcw2wTws", "model_response": {"id": "chatcmpl-BXxRCnbmtPmM8jEHTquY6Lr3ZAiv9", "created": 1747433002, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_b38e740b47", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Try to extract and print all formulas and equations from the first 10 pages\\nimport re\\nformulas = re.findall(r'([\\\\w\\\\s\\\\(\\\\)\\\\+\\\\-\\\\*/=\\\\[\\\\]\\\\|\\\\.,:;]+)\\\\n', text)\\n# Print the first 10 formulas/equation-like lines\\nfor f in formulas:\\n    if any(sym in f for sym in ['=', 'log', 'exp', 'L_', 'r_', 'margin', 'SimPO', 'DPO']):\\n        print(f.strip())\"}", "name": "execute_ipython_cell"}, "id": "call_RuBFRlqyMI25KNNabcw2wTws", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 136, "prompt_tokens": 7565, "total_tokens": 7701, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 7168}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "SimPO: Simple Preference Optimization\nwith a Reference-Free Reward\ncs.princeton.edu\nAbstract\nDirect Preference Optimization (DPO) is a widely used offline preference opti-\nmization algorithm that reparameterizes reward functions in reinforcement learning\nfrom human feedback (RLHF) to enhance simplicity and training stability. In this\nwork, we propose SimPO, a simpler yet more effective approach. The effectiveness\nof SimPO is attributed to a key design: using the average log probability of a\nsequence as the implicit reward. This reward formulation better aligns with model\ngeneration and eliminates the need for a reference model, making it more compute\nand memory efficient. Additionally, we introduce a target reward margin to the\nBradley-Terry objective to encourage a larger margin between the winning and\ns performance. We compare\nSimPO to DPO and its recent variants across various state-of-the-art training setups,\nincluding both base and instruction-tuned models such as Mistral, Llama 3, and\nGemma 2. We evaluate on extensive chat-based evaluation benchmarks, including\nAlpacaEval 2, MT-Bench, and Arena-Hard. Our results demonstrate that SimPO\nconsistently and significantly outperforms existing approaches without substantially\nincreasing response length. Specifically, SimPO outperforms DPO by up to 6.4\npoints on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing\n10B models with real user votes.1\n1 Introduction\nLearning from human feedback is crucial in aligning large language models (LLMs) with human\nvalues and intentions [ 51], ensuring they are helpful, honest, and harmless [ 5]. Reinforcement learning\nfrom human feedback (RLHF) [ 18,62,73] is a popular method for fine-tuning language models\nto achieve effective alignment. While the classical RLHF approach [ 62,70] has shown impressive\nresults, it presents optimization challenges due to its multi-stage procedure, which involves training a\nreward model and then optimizing a policy model to maximize that reward [13].\nRecently, researchers have been exploring simpler offline algorithms. Direct Preference Optimization\n(DPO) [ 66] is one such approach. DPO reparameterizes the reward function in RLHF to directly\nlearn a policy model from preference data, eliminating the need for an explicit reward model. It has\ngained widespread practical adoption due to its simplicity and stability. In DPO, the implicit reward\nis formulated using the log ratio of the likelihood of a response between the current policy model and\nthe supervised fine-tuned (SFT) model. However, this reward formulation is not directly aligned with\nEqual Contribution.\n1Code and models can be found at https://github.com/princeton-nlp/SimPO .\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2405.14734v3  [cs.CL]  1 Nov 2024\n)\n02040+3.8+4.4+6.4+5.3+7 .5+1 .2+6.2+4.5DPOSimPODPOSimPOFigure 1: SimPO and DPO mainly differ in their reward formulation, as indicated in the shaded box.\nSimPO outperforms DPO significantly across a range of settings on AlpacaEval 2 and Arena-Hard.\nthe metric used to guide generation, which is approximately the average log likelihood of a response\ngenerated by the policy model. We hypothesize that this discrepancy between training and inference\nmay lead to suboptimal performance.\nTable 1: Length-controlled (LC) and raw win rate (WR),\nand generation lengths of top models on the AlpacaEval\n2 Leaderboard. Bold are the models we trained.\n) Len.\nGemma-2-9B-it-SimPO 72.4 65.9 1833\nGPT-4 Turbo (04/09) 55.0 46.1 1802\nGemma-2-9B-it 51.1 38.1 1571\nLlama-3-8B-Instruct-SimPO 44.7 40.5 1825\nClaude 3 Opus 40.5 29.1 1388\nLlama-3-8B-Instruct-DPO 40.3 37.9 1837\nLlama-3-70B-Instruct 34.4 33.2 1919\nLlama-3-8B-Instruct 26.0 25.3 1899In this work, we propose SimPO, a simple\nyet effective offline preference optimiza-\ntion algorithm (Figure 1). The core of\nour algorithm aligns the reward function\nin the preference optimization objective\nwith the generation metric. SimPO consists\nof two major components: (1) a length-\nnormalized reward, calculated as the aver-\nagelog probability of all tokens in a re-\nsponse using the policy model, and (2) a\ntarget reward margin to ensure the reward\ndifference between winning and losing re-\nsponses exceeds this margin. In summary,\nSimPO has the following properties:\nSimplicity : SimPO does not require a reference model, making it more lightweight and easier to\nimplement compared to DPO and other reference-based methods.\nSignificant performance advantage : Despite its simplicity, SimPO significantly outperforms\nDPO and its latest variants ( e.g., a recent reference-free objective ORPO [ 42]). The performance\nadvantage is consistent across various training setups and extensive chat-based evaluations, includ-\ning AlpacaEval 2 [ 55,28] and the challenging Arena-Hard [ 54] benchmark. It achieves up to a\n6.4 point improvement on AlpacaEval 2 and a 7.5 point improvement on Arena-Hard compared to\nDPO (Figure 1).\nMinimal length exploitation : SimPO does not significantly increase response length compared to\nthe SFT or DPO models (Table 1), indicating minimal length exploitation [28, 71, 85].\nExtensive analysis shows that SimPO utilizes preference data more effectively, leading to a more\naccurate likelihood ranking of winning and losing responses on a held-out validation set, which in\nturn translates to a better policy model. As shown in Table 1, our Gemma-2-9B-it-SimPO model\n10B models on the leaderboard.2\n2 SimPO: Simple Preference Optimization\n2.2). Finally, we derive the SimPO\n2.3).\n2As of September 16th, 2024.\n2\n\n2.1 Background: Direct Preference Optimization (DPO)\nDPO [ 66] is one of the most popular preference optimization methods. Instead of learning an explicit\nreward model [ 62], DPO reparameterizes the reward function rusing a closed-form expression with\nthe optimal policy:\nr(x, y) =\u03b2log\u03c0\u03b8(y|x)\n\u03c0ref(y|x)+\u03b2logZ(x), (1)\nwhere \u03c0\u03b8is the policy model, \u03c0refis the reference policy, typically the supervised fine-tuned (SFT)\nmodel, and Z(x)is the partition function. By incorporating this reward formulation into the Bradley-\nr(x, yl)), DPO expresses the\nprobability of preference data with the policy model rather than the reward model, yielding the\nfollowing objective:\n\u03b2log\u03c0\u03b8(yw|x)\n\u03b2log\u03c0\u03b8(yl|x)\n, (2)\nwhere (x, yw, yl)are preference pairs consisting of the prompt, the winning response, and the losing\nresponse from the preference dataset D.\n2.2 A Simple Reference-Free Reward Aligned with Generation\nDiscrepancy between reward and generation for DPO. Using Eq. (1)as the implicit reward has\nthe following drawbacks: (1) it requires a reference model \u03c0refduring training, which incurs additional\nmemory and computational costs; and (2) it creates a mismatch between the reward optimized in\ntraining and the log-likelihood optimized during inference, where no reference model is involved.\nof the triples\nfrom the training set satisfy this condition when trained with DPO (Figure 4b). This observation\naligns with a concurrent work [ 14], which finds that existing models trained with DPO exhibit random\nranking accuracy in terms of average log-likelihood, even after extensive preference optimization.\nLength-normalized reward formulation. One solution is to use the summed token log probability\nlonger sequences tend to have lower log probabilities.\nConsequently, when ywis longer than yl, optimizing the summed log probability as a reward forces\nthe model to artificially inflate probabilities for longer sequences to ensure ywreceives a higher\nreward than yl. This overcompensation increases the risk of degeneration. To address this issue, we\nconsider using the average log-likelihood as the implicit reward:\np\u03b8(y|x) =1\n|y|log\u03c0\u03b8(y|x) =1\n|y||y|X\ni). (3)\nThis metric is commonly used for ranking options in beam search [ 35,53] and multiple-choice tasks\nwithin language models [ 12,41,62]. Naturally, we consider replacing the reward formulation in DPO\nwithp\u03b8in Eq. (3), so that it aligns with the likelihood metric that guides generation. This results in a\nlength-normalized reward:\nrSimPO(x, y) =\u03b2\n|y|log\u03c0\u03b8(y|x) =\u03b2\n|y||y|X\ni), (4)\nwhere \u03b2is a constant that controls the scaling of the reward difference. We find that normalizing the\nreward with response lengths is crucial; removing the length normalization term from the reward\nformulation results in a bias toward generating longer but lower-quality sequences (see Section 4.4\nfor more details). Consequently, this reward formulation eliminates the need for a reference model,\nenhancing memory and computational efficiency compared to reference-dependent algorithms.\n2.3 The SimPO Objective\n\u03b3). (5)\nThe margin between two classes is known to influence the generalization capabilities of classifiers [ 1,\n10,22,31].3In standard training settings with random model initialization, increasing the target\nmargin typically improves generalization. In preference optimization, the two classes are the winning\nand losing responses for a single input. In practice, we observe that generation quality initially\ns variants, IPO [ 6], also formulates a target reward margin similar to SimPO. However,\n4.1).\nObjective. Finally, we obtain the SimPO objective by plugging Eq. (4) into Eq. (5):\n. (6)\nIn summary, SimPO employs an implicit reward formulation that directly aligns with the generation\nmetric, eliminating the need for a reference model. Additionally, it introduces a target reward margin\n\u03b3to help separating the winning and losing responses. In Appendix F, we provide a gradient analysis\nof SimPO and DPO to further understand the differences between the two methods.\nPreventing catastrophic forgetting without KL regularization. Although SimPO does not impose\nKL regularization, we find that a combination of practical factors ensures effective learning from\npreference data while maintaining generalization, leading to an empirically low KL divergence from\nthe reference model. These factors are: (1) a small learning rate, (2) a preference dataset that covers\ndiverse domains and tasks, and (3) the intrinsic robustness of LLMs to learn from new data without\nforgetting prior knowledge. We present KL divergence experiments in Section 4.4.\n3 Experimental Setup\nModels and training settings. We perform preference optimization with two families of models,\nLlama-3-8B [ 2] and Mistral-7B [ 44], under two setups: Base and Instruct. In this section, our goal\nis to understand the performance of SimPO vs. other preference optimization methods in different\nexperimental setups. Our strongest model is based on Gemma-2-9B (Instruct setup) with a stronger\nreward model, RLHFlow/ArmoRM-Llama3-8B-v0.1 [ 84] (Table 1). We will present and discuss\nthese results in Appendix J.\nFor the Base setup, we follow the training pipeline of Zephyr [ 80]. First, we train a base model ( i.e.,\nmistralai/Mistral-7B-v0.1, or meta-llama/Meta-Llama-3-8B) on the UltraChat-200k dataset [ 25] to\nobtain an SFT model. Then, we perform preference optimization on the UltraFeedback dataset [23]\nusing the SFT model as the starting point. This setup provides a high level of transparency , as the\nSFT models are trained on open-source data.\nFor the Instruct setup, we use an off-the-shelf instruction-tuned model ( i.e., meta-llama/Meta-\nLlama-3-8B-Instruct, or mistralai/Mistral-7B-Instruct-v0.2) as the SFT models.4These models have\nundergone extensive instruction-tuning processes, making them more powerful and robust than the\nSFT models in the Base setup. However, they are also more opaque because their RLHF procedure\nis not publicly disclosed. To mitigate the distribution shift between SFT models and the preference\noptimization process, we generate the preference dataset using the SFT models following [ 79].\nThis makes our Instruct setup closer to an on-policy setting. Specifically, we use prompts from\nthe UltraFeedback dataset and regenerate the chosen and rejected response pairs (yw, yl)with the\nSFT models. For each prompt x, we generate 5 responses using the SFT model with a sampling\ntemperature of 0.8. We then use llm-blender/PairRM [ 45] to score the 5 responses, selecting the\n3This margin is termed home advantage in Bradley-Terry models [1, 31].\n4It is unclear whether the released instruct checkpoints have undergone supervised fine-tuning (SFT) or the\ncomplete RLHF pipeline. For simplicity, we refer to these checkpoints as SFT models.\n4\n\nTable 2: Evaluation details for AlpacaEval 2 [ 55], Arena-Hard [ 54], and MT-Bench [ 99]. The baseline\nmodel refers to the model compared against. GPT-4 Turbo corresponds to GPT-4-Preview-1106.\nraw win rate\nArena-Hard 500 GPT-4-0314 GPT-4 Turbo Pairwise comparison Win rate\nMT-Bench 80 - GPT-4/GPT-4 Turbo Single-answer grading Rating of 1-10\nhighest-scoring one as ywand the lowest-scoring one as yl. We only generated data in a single pass\ninstead of iteratively as in [79].5\nIn summary, we have four setups: Llama-3-Base, Llama-3-Instruct, Mistral-Base, and Mistral-Instruct.\nWe believe these configurations represent the state-of-the-art, placing our models among the top\nperformers on various leaderboards. We encourage future research to adopt these settings for better\nand fairer comparisons of different algorithms. Additionally, we find that tuning hyperparameters is\ncrucial for achieving optimal performance with all the offline preference optimization algorithms,\nincluding DPO and SimPO. Generally, for SimPO, setting \u03b2between 2.0 and 2.5 and \u03b3between 0.5\nand 1.5 leads to good performance across all setups. For more details, please refer to Appendix B.\nEvaluation benchmarks. We primarily assess our models using three of the most popular open-\nended instruction-following benchmarks: MT-Bench [ 99], AlpacaEval 2 [ 55], and Arena-Hard\ns evaluation\nprotocol. For AlpacaEval 2, we report both the raw win rate (WR) and the length-controlled win\nrate (LC) [ 28]. The LC metric is specifically designed to be robust against model verbosity. For\nArena-Hard, we report the win rate (WR) against the baseline model. For MT-Bench, we report the\naverage MT-Bench score with GPT-4 and GPT-4-Preview-1106 as the judge model.6For decoding\ndetails, please refer to Appendix B. We also evaluate on downstream tasks from the Huggingface\nOpen Leaderboard benchmarks [9], with additional details in in Appendix C.\nTable 3: Various preference optimization objectives given pref-\nerence data D= (x, yw, yl), where xis an input, and ywandyl\nare the winning and losing responses.\nMethod Objective\n1\n|yw|log\u03c0\u03b8(yw|x) +1\n\u03bblog\u03c0\u03b8(yw|x)\n\u03bblog\u03c0\u03b8(yw|x)\n\u03b2log\u03c0\u03b8(yw|x)\n\u03b2log\u03c0\u03b8(yl|x)\nlog\u03c0\u03b8(yw|x)\nlog\u03c0\u03b8(yl|x)\n\u03bblog\u03c0\u03b8(yw|x)\n\u03b2log\u03c0\u03b8(yw|x)\n\u03b2log\u03c0\u03b8(yl|x)\nlogp\u03b8(yw|x)\nlogp\u03b8(yl|x)\n\u03b2log\u03c0\u03b8(yw|x)\n\u03b2log\u03c0\u03b8(yl|x)\nBaselines. We compare SimPO\nwith other offline preference\noptimization methods listed\nin Table 3.7RRHF [ 91] and\nSLiC-HF [ 96] are ranking losses.\nRRHF uses length-normalized\ns\nreward function, while SLiC-\nHF uses log-likelihood directly\nand includes an SFT objective.\nIPO [ 6] is a theoretically grounded\napproach method that avoids\ns assumption that pairwise\npreferences can be replaced with\npointwise rewards. CPO [ 88]\nuses sequence likelihood as a\nreward and trains alongside an\nSFT objective. KTO [ 29] learns\nfrom non-paired preference data.\n5We also experimented with using a stronger reward model, RLHFlow/ArmoRM-Llama3-8B-v0.1 [ 84], to\nrank generated data, which yields significantly improved performance (see Appendix H and Appendix J). This is\nthe reward model we used in our Gemma 2 experiments.\n6GPT-4-Preview-1106 produces more accurate reference answers and judgments compared to GPT-4.\n7Many recent studies [ 90,74] have extensively compared DPO and PPO [ 70]. We will leave the comparison\nof PPO and SimPO to future work.\n5\n\nTable 4: AlpacaEval 2 [ 55], Arena-Hard [ 54], and MT-Bench [ 99] results under the four settings.\nLC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base\nsettings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.\nMethodMistral-Base (7B) Mistral-Instruct (7B)\nAlpacaEval 2 Arena-Hard MT-Bench AlpacaEval 2 Arena-Hard MT-Bench\n) GPT-4 Turbo GPT-4\nSFT 8.4 6.2 1.3 4.8 6.3 17.1 14.7 12.6 6.2 7.5\nRRHF [91] 11.6 10.2 5.8 5.4 6.7 25.3 24.8 18.1 6.5 7.6\nSLiC-HF [96] 10.9 8.9 7.3 5.8 7.4 24.1 24.6 18.9 6.5 7.8\nDPO [66] 15.1 12.5 10.4 5.9 7.3 26.8 24.9 16.3 6.3 7.6\nIPO [6] 11.8 9.4 7.5 5.5 7.2 20.3 20.3 16.2 6.4 7.8\nCPO [88] 9.8 8.9 6.9 5.4 6.8 23.8 28.8 22.6 6.3 7.5\nKTO [29] 13.1 9.1 5.6 5.4 7.0 24.5 23.6 17.9 6.4 7.7\nORPO [42] 14.7 12.2 7.0 5.8 7.3 24.5 24.9 20.8 6.4 7.7\nR-DPO [64] 17.4 12.8 8.0 5.9 7.4 27.3 24.5 16.1 6.2 7.5\nSimPO 21.5 20.8 16.6 6.0 7.3 32.1 34.8 21.0 6.6 7.6\nMethodLlama-3-Base (8B) Llama-3-Instruct (8B)\nAlpacaEval 2 Arena-Hard MT-Bench AlpacaEval 2 Arena-Hard MT-Bench\n) GPT-4 Turbo GPT-4\nSFT 6.2 4.6 3.3 5.2 6.6 26.0 25.3 22.3 6.9 8.1\nRRHF [91] 12.1 10.1 6.3 5.8 7.0 31.3 28.4 26.5 6.7 7.9\nSLiC-HF [96] 12.3 13.7 6.0 6.3 7.6 26.9 27.5 26.2 6.8 8.1\nDPO [66] 18.2 15.5 15.9 6.5 7.7 40.3 37.9 32.6 7.0 8.0\nIPO [6] 14.4 14.2 17.8 6.5 7.4 35.6 35.6 30.5 7.0 8.3\nCPO [88] 10.8 8.1 5.8 6.0 7.4 28.9 32.2 28.8 7.0 8.0\nKTO [29] 14.2 12.4 12.5 6.3 7.8 33.1 31.8 26.4 6.9 8.2\nORPO [42] 12.2 10.6 10.8 6.1 7.6 28.5 27.4 25.8 6.8 8.0\nR-DPO [64] 17.6 14.4 17.2 6.6 7.5 41.1 37.8 33.1 7.0 8.0\nSimPO 22.0 20.3 23.4 6.6 7.7 44.7 40.5 33.8 7.0 8.0\nORPO [ 42]8introduces a reference-model-free odd ratio term to directly contrast winning and losing\nresponses with the policy model and jointly trains with the SFT objective. R-DPO [ 64] is a modified\nversion of DPO that includes an additional regularization term to prevent exploitation of length. We\nthoroughly tune the hyperparameters for each baseline and report the best performance. We find that\nmany variants of DPO do not empirically present an advantage over standard DPO . Further details\ncan be found in Appendix B.\n4 Experimental Results\nIn this section, we present main results of our experiments, highlighting the superior performance of\n4.4). Unless otherwise specified, the ablation studies are conducted\nusing the Mistral-Base setting.\n4.1 Main Results and Ablations\nSimPO consistently and significantly outperforms existing preference optimization methods.\nAs shown in Table 4, while all preference optimization algorithms enhance performance over the SFT\nmodel, SimPO, despite its simplicity, achieves the best overall performance across all benchmarks and\nsettings. These consistent and significant improvements highlight the robustness and effectiveness of\nSimPO. Notably, SimPO outperforms the best baseline by 3.6 to 4.8 points on the AlpacaEval 2 LC\nwin rate across various settings. On Arena-Hard, SimPO consistently achieves superior performance,\n8ORPO can directly train on preference data without the SFT stage. For fair comparisons, we start ORPO\nfrom the same SFT checkpoints as other baselines, which yields better results than starting from base checkpoints.\n6\n\nTable 5: Ablation studies under Mistral-Base and Mistral-Instruct settings. We ablate each key design\nof SimPO: (1) removing length normalization in Eq. (4)(i.e., w/o LN); (2) setting target reward\nmargin \u03b3to be 0 in Eq. (6) ( i.e.,\u03b3= 0).\nMethodMistral-Base (7B) Setting Mistral-Instruct (7B) Setting\nAlpacaEval 2 Arena-Hard MT-Bench AlpacaEval 2 Arena-Hard MT-Bench\n) GPT-4 Turbo GPT-4\nDPO 15.1 12.5 10.4 5.9 7.3 26.8 24.9 16.3 6.3 7.6\nSimPO 21.5 20.8 16.6 6.0 7.3 32.1 34.8 21.0 6.6 7.6\nw/o LN 11.9 13.2 9.4 5.5 7.3 19.1 19.7 16.3 6.4 7.6\n\u03b3= 0 16.8 14.3 11.7 5.6 6.9 30.9 34.2 20.5 6.6 7.7\nr(x,yl)\n\u03c0SimPO\n\u03c0SimPO w/o LN\n\u03c0SFT\n(a) Reward optimization.\n0 200 400 600 800 1000\n50Avg.log prob.p\u03b8(y|x)\n\u03c1= 0.34 (b) SimPO.\n0 200 400 600 800 1000\n50Avg.log prob.p\u03b8(y|x)\n\u03c1= 0.82 (c) SimPO without LN.\nFigure 2: Effect of length normalization (LN). (a) Relationship between reward margin and length\ndifference between winning and losing responses. (b) Spearman correlation between average log\nprobability and response length for SimPO. (c) Spearman correlation for SimPO without LN.\nthough it is occasionally surpassed by CPO [88]. We find that CPO generates responses that are, on\nlonger than those generated by SimPO (See Table 10). Arena-Hard might favor longer\ngenerations due to the absence of a length penalty in its evaluation.\nBenchmark quality varies. Although all three benchmarks are widely adopted, we find that MT-\nBench exhibits poor separability across different methods. Minor differences between methods on\nMT-Bench may be attributed to randomness, likely due to the limited scale of its evaluation data and\nits single-instance scoring protocol. This finding aligns with observations reported in [ 54]. In contrast,\nAlpacaEval 2 and Arena-Hard provide more meaningful distinctions between different methods. We\nobserve that the win rate on Arena-Hard is significantly lower than on AlpacaEval 2, indicating that\nArena-Hard is a more challenging benchmark.9\nThe Instruct setting introduces significant performance gains. Across all benchmarks, we\nobserve that the Instruct setting consistently outperforms the Base setting. This improvement is likely\ndue to the higher quality of SFT models used for initialization and the generation of more high-quality\npreference data by these models.\nBoth key designs in SimPO are crucial. In Table 5, we demonstrate results from ablating each\nkey design of SimPO: (1) removing length normalization in Eq. (4)(i.e., w/o LN); (2) setting the\ntarget reward margin to be 0 in Eq. (6)(i.e.,\u03b3= 0). Removing the length normalization has the most\nnegative impact on the results. Our examination reveals that this leads to the generation of long and\nrepetitive patterns, substantially degrading the overall quality of the output (See Appendix E). Setting\n\u03b3to 0 yields also leads to a performance degradation compared to SimPO, indicating that it is not the\noptimal target reward margin. In the following subsections, we conduct in-depth analyses to better\nunderstand both design choices.\n4.2 Length Normalization (LN) Prevents Length Exploitation\nLN leads to an increase in the reward difference for all preference pairs, regardless of their\nlength. The Bradley-Terry objective in Eq. (5)essentially aims to optimize the reward difference\n9Although our models excel on benchmarks, these evaluations have limitations, including restricted query\nspace and potential biases from model-based evaluations. Efforts like WildBench [ 93] aim to expand these\nspaces, where SimPO models demonstrate competitive performance.\n7\n\n0.0 0.8 1.6 2.4\nr(x,yl)0.00.10.20.30.4Density\u03b3= 0.0\n\u03b3= 0.8\n\u03b3= 1.6\n\u03b3= 2.4 (b) Reward diff. distribution.\n2 0\nAvg.log prob.on winningp\u03b8(yw|x)0.00.40.81.2Density\u03b3= 0.0\n\u03b3= 0.8\n\u03b3= 1.6\n\u03b3= 2.4 (c) Log prob. distribution.\nFigure 3: Study of the margin \u03b3. (a) Reward accuracy and AlpacaEval2 LC win rate under different\n\u03b3values. (b) Reward difference distribution under different \u03b3values. (c) Log likelihood distribution\non chosen responses under different \u03b3values.\nr(x, yl)to exceed the target margin \u03b3. We investigate the relationship between\n|yl|between the winning and\nlosing responses from the training set of UltraFeedback. We measure the difference of reward ( rSimPO ;\nEq.(4)) using the SFT model, the SimPO model, and a model trained with SimPO but without length\nnormalization. We present the results in Figure 2a and observe that SimPO with LN consistently\nachieves a positive reward margin for all response pairs, regardless of their length difference, and\nconsistently improves the margin over the SFT model. In contrast, SimPO without LN results in a\nnegative reward difference for preference pairs when the winning response is shorter than the losing\nresponse, indicating that the model learns poorly for these instances.\nRemoving LN results in a strong positive correlation between the reward and response length,\nleading to length exploitation. Figures 2b and 2c illustrate the average log likelihood ( p\u03b8in Eq. (3))\nversus response length on a held-out set for models trained with SimPO and SimPO without LN. The\nmodel trained without LN exhibits a much stronger positive Spearman correlation between likelihood\nand response length compared to SimPO, indicating a tendency to exploit length bias and generate\nlonger sequences (see Appendix E). In contrast, SimPO results in a Spearman correlation coefficient\nsimilar to the SFT model (see Figure 6a).\n4.3 The Impact of Target Reward Margin in SimPO\nInfluence of \u03b3on reward accuracy and win rate. We investigate how the target reward margin \u03b3\nin SimPO affects the reward accuracy on a held-out set and win rate on AlpacaEval 2, presenting\nthe results in Figure 3a. Reward accuracy is measured as the percentage of preference pairs where\nthe winning response ends up having a higher reward for the winning response than the losing\nr(x, yl)). We observe that reward accuracy increases with \u03b3on both\nbenchmarks, indicating that enforcing a larger target reward margin effectively improves reward\naccuracy. However, the win rate on AlpacaEval 2 first increases and then decreases with \u03b3, suggesting\nthat generation quality is not solely determined by the reward margin.\nImpact of \u03b3on the reward distribution. We visualize the distribution of the learned reward margin\nr(x, yl)and the reward of winning responses r(x, yw)under varying \u03b3values in Figure 2b\nand Figure 2c. Notably, increasing \u03b3tends to flatten both distributions and reduce the average log\nlikelihood of winning sequences. This initially improves performance but can eventually lead to\nmodel degeneration. We hypothesize that there is a trade-off between accurately approximating\nthe true reward distribution and maintaining a well-calibrated likelihood when setting the \u03b3value.\nFurther exploration of this balance is deferred to future work.\n4.4 In-Depth Analysis of DPO vs. SimPO\nIn this section, we compare SimPO to DPO in terms of (1) likelihood-length correlation, (2) re-\nward formulation, (3) reward accuracy, and (4) algorithm efficiency. We demonstrate that SimPO\noutperforms DPO in terms of reward accuracy and efficiency.\nDPO reward implicitly facilitates length normalization. Although the DPO reward expression\nr(x, y) =\u03b2log\u03c0\u03b8(y|x)\n\u03c0ref(y|x)(with the partition function excluded) lacks an explicit term for length\nnormalization, the logarithmic ratio between the policy model and the reference model can serve to\n8\n\n0 200 400 600 800 1000\n50Avg.log prob.p\u03b8(y|x)\n\u03c1= 0.59(a) Length correlation (DPO).\nrlTraining metric8.0k 8.3k23.0k 21.2k\n101520\nx1000 (b) Contingency table (DPO).\nMistral-Base Mistral-Instruct0.40.50.60.70.8Reward AccuracyDPO SimPO (c) Reward Accuracy.\nFigure 4: Comparison between SimPO and DPO, measured on UltraFeedback. (a) Spearman\ncorrelation between average log probability and response length for DPO. (b) Contingency table of\nrankings based on DPO rewards and the average log likelihood (measured on the training set). (c)\nReward accuracy of DPO and SimPO.\n0 100 200 300 400\nTraining step0.00.20.40.6KL divergence on yw\nDPO (\u03b2= 0.01)\nDPO (\u03b2= 0.1)\nSimPO (\u03b2= 2)\nSimPO (\u03b2= 10)\n(a) KL divergence w/ different \u03b2.\n)\n\u03b2= 0.1\u03b2= 0.01\u03b2= 10\u03b2= 2\nDPO SimPO (b) Performance w/ different \u03b2.\n)73 min 77 GB60 min69 GBDPO SimPO (c) Efficiency of DPO vs. SimPO.\nFigure 5: Comparison between SimPO and DPO (continued). (a) With different \u03b2in DPO and\nSimPO, KL divergence from the policy model to the reference model on yw. (b) AlpacaEval2 LC\nwin rate of DPO and SimPO with different \u03b2. (c) Runtime and memory usage for DPO and SimPO.\nimplicitly counteract length bias. As shown in Table 6 and Figure 4a, employing DPO reduces the\nSpearman correlation coefficient between average log likelihood and response length compared to\n). However, it still\nexhibits a stronger positive correlation when compared to SimPO.10\nTable 6: Spearman correlation \u03c1be-\ntween average log likelihood of differ-\nent models and response length on a\nheld-out set.\nSimPO w/o LN DPO SimPO\n\u03c1 0.82 0.59 0.34DPO reward mismatches generation likelihood. There\ns reward formulation,\nr\u03b8(x, y) =\u03b2log\u03c0\u03b8(y|x)\n\u03c0ref(y|x), and the average log likelihood\nmetric, p\u03b8(y|x) =1\n|y|log\u03c0\u03b8(y|x), which directly\nimpacts generation. As shown in Figure 4b, among\nthe instances on the UltraFeedback training set where\np \u03b8(yl|x). In contrast, SimPO directly\nemploys the average log likelihood (scaled by \u03b2) as the reward expression, thereby eliminating the\ndiscrepancy completely, as demonstrated in Figure 6b.\nDPO lags behind SimPO in terms of reward accuracy. In Figure 4c, we compare the reward\naccuracy of SimPO and DPO, assessing how well their final learned rewards align with preference\nlabels on a held-out set. SimPO consistently achieves higher reward accuracy than DPO, suggesting\nthat our reward design facilitates better generalization and leads to higher quality generations.\nKL divergence of SimPO and DPO. In Figure 5a, we present the KL divergence between the\npolicy model trained with DPO and SimPO and the reference model with different \u03b2, measured\non the winning responses from a held-out set during training. Figure 5b shows the corresponding\nAlpacaEval 2 LC win rate. Although SimPO does not apply any form of regularization against\nthe reference model, the KL divergence of SimPO is reasonably small. Increasing \u03b2reduces the\nKL divergence for both DPO and SimPO, with DPO exhibiting a more pronounced reduction at\nhigher \u03b2values. In this particular setting (Mistral-base), Figure 5b demonstrates that a smaller \u03b2can\n10Note that this correlation does not fully reflect the generation length. Despite DPO showing a stronger\ncorrelation, the length of its generated responses is comparable to or even slightly shorter than those of the\nSimPO models. Please find more details in Appendix E.\n9\n\nimprove AlpacaEval 2 performance, despite the higher KL divergence.11We hypothesize that when\nthe reference model is weak, strictly constraining the policy model to the reference model may not be\nbeneficial. As a caveat, while we did not observe any training collapse or degeneration with proper\ntuning, in principle, SimPO could potentially lead to reward hacking without explicit regularization\nagainst the reference model. In such a scenario, the model might achieve a low loss but degenerate.\nSimPO is more memory and compute-efficient than DPO. Another benefit of SimPO is its\nefficiency as it does not use a reference model. Figure 5c illustrates the overall run time and per-GPU\n, thanks to eliminating forward passes with the reference model.\n5 Related Work\nReinforcement learning from human feedback. RLHF is a technique that aligns large language\nmodels with human preferences and values [ 18,102,62,7]. The classical RLHF pipeline typically\ncomprises three phases: supervised fine-tuning [ 101,76,33,21,48,25,82,15,86], reward model\ntraining [ 32,60,16,56,37,50], and policy optimization [ 70,4]. Proximal Policy Optimization\n(PPO) [ 70] is a widely used algorithm in the third stage of RLHF. The RLHF framework is also\nwidely applied to various applications, such as mitigating toxicity [ 3,49,97], ensuring safety [ 24],\nenhancing helpfulness [ 78,83], searching and navigating the web [ 61], and improving model reason-\ning abilities [ 36]. Recently, [ 13] has highlighted challenges across the whole RLHF pipeline from\npreference data collection to model training. Further research has also demonstrated that RLHF can\nlead to biased outcomes, such as verbose outputs from the model [28, 71, 85].\nOffline vs. iterative preference optimization. Given that online preference optimization algo-\nrithms are complex and difficult to optimize [ 100,69], researchers have been exploring more efficient\nand simpler alternative offline algorithms. Direct Preference Optimization (DPO) [ 66] is a notable\nexample. However, the absence of an explicit reward model in DPO limits its ability to sample\npreference pairs from the optimal policy. To address this, researchers have explored augmenting\npreference data using a trained SFT policy [ 96] or a refined SFT policy with rejection sampling [ 59],\nenabling the policy to learn from data generated by the optimal policy. Further studies have extended\nthis approach to an iterative training setup, by continuously updating the reference model with the\nmost recent policy model or generating new preference pairs at each iteration [ 27,46,67,87,92]. In\nthis work, we focus exclusively on offline settings, avoiding any iterative training processes.\nPreference optimization objectives. A variety of preference optimization objectives have been\nproposed besides DPO. Ranking objectives allow for comparisons among more than two instances [ 26,\n58,72,91]. Another line of work explores simpler preference optimization objectives that do not rely\non a reference model [ 42,89], similar to SimPO. [ 8] proposes a method to jointly optimize instructions\nand responses, finding it effectively improves DPO. [ 98] focuses on post-training extrapolation\nbetween the SFT and the aligned model to further enhance model performance. In this work, we\ncompare SimPO to a series of offline algorithms, including RRHF [ 91], SLiC-HF [ 96], DPO [ 66], IPO\n[6], CPO [ 88], KTO [ 29], ORPO [ 42], and R-DPO [ 64], and find that SimPO can outperform them\nin both efficiency and performance. Recently, [ 75] proposed a generalized preference optimization\nframework unifying different offline algorithms, and SimPO can be seen as a special case.\n6 Conclusion\nIn this work, we propose SimPO, a simple and effective preference optimization algorithm that\nconsistently outperforms existing approaches across various training setups. By aligning the reward\nfunction with the generation likelihood and introducing a target reward margin, SimPO eliminates\nthe need for a reference model and achieves strong performance without exploiting the length bias.\nExtensive analysis demonstrates that the key designs in SimPO are crucial and validates the efficiency\nand effectiveness of SimPO. A detailed discussion of the limitations can be found in Appendix A.\n11We observe that in some settings ( e.g., Llama-3-Instruct), a large \u03b2(e.g.,\u03b2= 10 ) leads to better performance.\n12DPO can be as memory efficient as SimPO if it were implemented to separate the forward passes of the\nreference model from the actual preference optimization. However, this implementation is not standard practice.\n10\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Try to extract and print all formulas and equations from the first 10 pages\nimport re\nformulas = re.findall(r'([\\w\\s\\(\\)\\+\\-\\*/=\\[\\]\\|\\.,:;]+)\\n', text)\n# Print the first 10 formulas/equation-like lines\nfor f in formulas:\n    if any(sym in f for sym in ['=', 'log', 'exp', 'L_', 'r_', 'margin', 'SimPO', 'DPO']):\n        print(f.strip())"}, "success": true}