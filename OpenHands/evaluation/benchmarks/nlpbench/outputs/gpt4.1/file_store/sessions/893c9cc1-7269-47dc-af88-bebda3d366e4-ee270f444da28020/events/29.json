{"id": 29, "timestamp": "2025-05-19T19:19:58.103139", "source": "agent", "message": "I read the file /workspace/benchmark/datasets/19-Native-Sparse-Attention/NSA/native_sparse_attention/modeling_nsa.py.", "cause": 28, "observation": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_ICh8fWEAytmBRDOvFgKXm6dz", "model_response": {"id": "chatcmpl-BZ4zvEK5HWPG44scqk24GLWkDZNr8", "created": 1747700391, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"view\",\"path\":\"/workspace/benchmark/datasets/19-Native-Sparse-Attention/NSA/native_sparse_attention/modeling_nsa.py\"}", "name": "str_replace_editor"}, "id": "call_ICh8fWEAytmBRDOvFgKXm6dz", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 43, "prompt_tokens": 23822, "total_tokens": 23865, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 23552}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "Here's the result of running `cat -n` on /workspace/benchmark/datasets/19-Native-Sparse-Attention/NSA/native_sparse_attention/modeling_nsa.py:\n     1\t# -*- coding: utf-8 -*-\n     2\t\n     3\tfrom __future__ import annotations\n     4\t\n     5\timport math\n     6\timport warnings\n     7\tfrom typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n     8\t\n     9\timport torch\n    10\timport torch.nn as nn\n    11\timport torch.utils.checkpoint\n    12\tfrom einops import rearrange\n    13\tfrom transformers.generation import GenerationMixin\n    14\tfrom transformers.modeling_outputs import (BaseModelOutputWithPast,\n    15\t                                           CausalLMOutputWithPast)\n    16\tfrom transformers.modeling_utils import PreTrainedModel\n    17\tfrom transformers.utils import logging\n    18\tfrom transformers.utils.deprecation import deprecate_kwarg\n    19\t\n    20\tfrom fla.models.utils import Cache\n    21\tfrom fla.modules import FusedCrossEntropyLoss, FusedLinearCrossEntropyLoss\n    22\tfrom fla.modules import GatedMLP as NSAMLP\n    23\tfrom fla.modules import RMSNorm, RotaryEmbedding\n    24\tfrom native_sparse_attention.configuration_nsa import NSAConfig\n    25\tfrom native_sparse_attention.ops.parallel import parallel_nsa\n    26\t\n    27\tif TYPE_CHECKING:\n    28\t    from transformers.processing_utils import Unpack\n    29\t\n    30\tlogger = logging.get_logger(__name__)\n    31\t\n    32\t\n    33\tclass NativeSparseAttention(nn.Module):\n    34\t\n    35\t    def __init__(\n    36\t        self,\n    37\t        hidden_size: int = 2048,\n    38\t        num_heads: int = 64,\n    39\t        num_kv_heads: Optional[int] = 4,\n    40\t        head_dim: int = 64,\n    41\t        qkv_bias: bool = False,\n    42\t        block_size: Optional[int] = 64,\n    43\t        block_counts: Optional[Union[torch.LongTensor, int]] = 16,\n    44\t        window_size: Optional[int] = 512,\n    45\t        rope_theta: Optional[float] = 10000.,\n    46\t        max_position_embeddings: Optional[int] = None,\n    47\t        layer_idx: int = None\n    48\t    ):\n    49\t        super().__init__()\n    50\t\n    51\t        self.hidden_size = hidden_size\n    52\t        self.num_heads = num_heads\n    53\t        if num_kv_heads is None:\n    54\t            self.num_kv_heads = self.num_heads\n    55\t        else:\n    56\t            self.num_kv_heads = num_kv_heads\n    57\t        self.num_kv_groups = num_heads // self.num_kv_heads\n    58\t        self.head_dim = head_dim\n    59\t        self.kv_dim = self.num_kv_heads * self.head_dim\n    60\t        self.qkv_bias = qkv_bias\n    61\t\n    62\t        self.block_size = block_size\n    63\t        self.block_counts = block_counts\n    64\t        self.window_size = window_size\n    65\t        self.rope_theta = rope_theta\n    66\t        self.max_position_embeddings = max_position_embeddings\n    67\t        self.layer_idx = layer_idx\n    68\t\n    69\t        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=self.qkv_bias)\n    70\t        self.k_proj = nn.Linear(self.hidden_size, self.kv_dim, bias=self.qkv_bias)\n    71\t        self.v_proj = nn.Linear(self.hidden_size, self.kv_dim, bias=self.qkv_bias)\n    72\t        self.g_proj = nn.Linear(self.hidden_size, self.num_heads * 3, bias=False)\n    73\t        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n    74\t\n    75\t        self.rotary = RotaryEmbedding(dim=self.head_dim, base=self.rope_theta)\n    76\t\n    77\t    def forward(\n    78\t        self,\n    79\t        hidden_states: torch.Tensor,\n    80\t        attention_mask: Optional[torch.LongTensor] = None,\n    81\t        past_key_values: Optional[Cache] = None,\n    82\t        output_attentions: bool = False,\n    83\t        use_cache: bool = False,\n    84\t        **kwargs,\n    85\t    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    86\t        if attention_mask is not None:\n    87\t            assert len(attention_mask.shape) == 2, (\n    88\t                \"Expected attention_mask as a 0-1 matrix with shape [batch_size, seq_len] \"\n    89\t                \"for padding purposes (0 indicating padding). \"\n    90\t                \"Arbitrary attention masks of shape [batch_size, seq_len, seq_len] are not allowed.\"\n    91\t            )\n    92\t\n    93\t        batch_size, seq_len, _ = hidden_states.size()\n    94\t\n    95\t        q = rearrange(self.q_proj(hidden_states), '... (h d) -> ... h d', d=self.head_dim)\n    96\t        k = rearrange(self.k_proj(hidden_states), '... (h d) -> ... h d', d=self.head_dim)\n    97\t        v = rearrange(self.v_proj(hidden_states), '... (h d) -> ... h d', d=self.head_dim)\n    98\t        g = rearrange(self.g_proj(hidden_states), '... (h d) -> ... h d', d=3)\n    99\t        g_cmp, g_slc, g_swa = g.sigmoid().unbind(-1)\n   100\t\n   101\t        cu_seqlens = kwargs.get('cu_seqlens', None)\n   102\t\n   103\t        seqlen_offset, max_seqlen = 0, seq_len\n   104\t        if past_key_values is not None:\n   105\t            seqlen_offset = past_key_values.get_seq_length(self.layer_idx)\n   106\t            max_seqlen = q.shape[1] + seqlen_offset\n   107\t\n   108\t            if attention_mask is not None:\n   109\t                # to deliminate the offsets of padding tokens\n   110\t                seqlen_offset = (seqlen_offset + attention_mask.sum(-1) - attention_mask.shape[-1]).clamp(min=0)\n   111\t                max_seqlen = q.shape[1] + max(seqlen_offset)\n   112\t\n   113\t        if self.max_position_embeddings is not None:\n   114\t            max_seqlen = max(max_seqlen, self.max_position_embeddings)\n   115\t        q, k = self.rotary(q, k, seqlen_offset=seqlen_offset, max_seqlen=max_seqlen, cu_seqlens=cu_seqlens)\n   116\t\n   117\t        if past_key_values is not None:\n   118\t            cache_has_content = past_key_values.get_seq_length(self.layer_idx) > 0\n   119\t            k_cached, v_cached = past_key_values.update(\n   120\t                attn_state=(k.flatten(-2, -1), v.flatten(-2, -1)),\n   121\t                layer_idx=self.layer_idx,\n   122\t                offset=seq_len,\n   123\t                cache_kwargs=dict(window_size=self.window_size)\n   124\t            )['attn_state']\n   125\t            if cache_has_content:\n   126\t                k, v = k_cached, v_cached\n   127\t                k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n   128\t                v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n   129\t\n   130\t        o = parallel_nsa(\n   131\t            q=q,\n   132\t            k=k,\n   133\t            v=v,\n   134\t            g_cmp=g_cmp,\n   135\t            g_slc=g_slc,\n   136\t            g_swa=g_swa,\n   137\t            block_size=self.block_size,\n   138\t            block_counts=self.block_counts,\n   139\t            window_size=self.window_size,\n   140\t            cu_seqlens=cu_seqlens,\n   141\t            head_first=False\n   142\t        )\n   143\t        o = o.reshape(batch_size, seq_len, -1)\n   144\t        o = self.o_proj(o)\n   145\t\n   146\t        if not output_attentions:\n   147\t            attentions = None\n   148\t\n   149\t        return o, attentions, past_key_values\n   150\t\n   151\t\n   152\tclass NSABlock(nn.Module):\n   153\t    def __init__(self, config: NSAConfig, layer_idx: int):\n   154\t        super().__init__()\n   155\t\n   156\t        self.config = config\n   157\t        self.layer_idx = layer_idx\n   158\t\n   159\t        self.attn_norm = (RMSNorm if config.fuse_norm else nn.RMSNorm)(config.hidden_size, eps=config.norm_eps)\n   160\t        self.attn = NativeSparseAttention(\n   161\t            hidden_size=config.hidden_size,\n   162\t            num_heads=config.num_heads,\n   163\t            num_kv_heads=config.num_kv_heads,\n   164\t            qkv_bias=config.qkv_bias,\n   165\t            block_size=config.block_size,\n   166\t            block_counts=config.block_counts,\n   167\t            window_size=config.window_size,\n   168\t            rope_theta=config.rope_theta,\n   169\t            max_position_embeddings=config.max_position_embeddings,\n   170\t            layer_idx=layer_idx\n   171\t        )\n   172\t        self.mlp_norm = (RMSNorm if config.fuse_norm else nn.RMSNorm)(config.hidden_size, eps=config.norm_eps)\n   173\t        self.mlp = NSAMLP(\n   174\t            hidden_size=config.hidden_size,\n   175\t            hidden_ratio=config.hidden_ratio,\n   176\t            intermediate_size=config.intermediate_size,\n   177\t            hidden_act=config.hidden_act,\n   178\t            fuse_swiglu=config.fuse_swiglu\n   179\t        )\n   180\t\n   181\t    def forward(\n   182\t        self,\n   183\t        hidden_states: torch.Tensor,\n   184\t        attention_mask: Optional[torch.Tensor] = None,\n   185\t        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n   186\t        use_cache: Optional[bool] = False,\n   187\t        output_attentions: Optional[bool] = False,\n   188\t        **kwargs: Unpack[Dict]\n   189\t    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n   190\t        residual = hidden_states\n   191\t        hidden_states = self.attn_norm(hidden_states)\n   192\t        hidden_states, attentions, past_key_values = self.attn(\n   193\t            hidden_states=hidden_states,\n   194\t            attention_mask=attention_mask,\n   195\t            past_key_values=past_key_values,\n   196\t            use_cache=use_cache,\n   197\t            output_attentions=output_attentions,\n   198\t            **kwargs\n   199\t        )\n   200\t        if self.config.fuse_norm:\n   201\t            hidden_states, residual = self.mlp_norm(hidden_states, residual, True)\n   202\t        else:\n   203\t            hidden_states = residual + hidden_states\n   204\t            residual = hidden_states\n   205\t            hidden_states = self.mlp_norm(hidden_states)\n   206\t        hidden_states = self.mlp(hidden_states, **kwargs)\n   207\t        hidden_states = residual + hidden_states\n   208\t\n   209\t        outputs = (hidden_states, attentions, past_key_values)\n   210\t\n   211\t        return outputs\n   212\t\n   213\t\n   214\tclass NSAPreTrainedModel(PreTrainedModel):\n   215\t\n   216\t    config_class = NSAConfig\n   217\t    base_model_prefix = 'model'\n   218\t    supports_gradient_checkpointing = True\n   219\t    _no_split_modules = ['NSABlock']\n   220\t    _supports_cache_class = True\n   221\t\n   222\t    def __init__(self, *inputs, **kwargs):\n   223\t        super().__init__(*inputs, **kwargs)\n   224\t\n   225\t    def _init_weights(\n   226\t        self,\n   227\t        module: nn.Module,\n   228\t        prenorm_residual_strategy: Optional[str] = 'rescale',\n   229\t        num_residuals_per_layer: int = 2,\n   230\t    ):\n   231\t        if isinstance(module, (nn.Linear, nn.Conv1d)):\n   232\t            # Slightly different from the TF version which uses truncated_normal for initialization\n   233\t            # cf https://github.com/pytorch/pytorch/pull/5617\n   234\t            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n   235\t            if module.bias is not None:\n   236\t                nn.init.zeros_(module.bias)\n   237\t        elif isinstance(module, nn.Embedding):\n   238\t            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n   239\t        elif hasattr(module, 'reset_parameters'):\n   240\t            module.reset_parameters()\n   241\t\n   242\t        if prenorm_residual_strategy is not None:\n   243\t            # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n   244\t            #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n   245\t            #   > the weights of residual layers at initialization by a factor of 1/\u221aN where N is the # of residual layers.\n   246\t            #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n   247\t            #\n   248\t            # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n   249\t            p = None\n   250\t            if hasattr(module, 'o_proj'):\n   251\t                p = module.o_proj.weight\n   252\t            elif hasattr(module, 'down_proj'):\n   253\t                p = module.down_proj.weight\n   254\t            if p is not None:\n   255\t                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n   256\t                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n   257\t                # We need to reinit p since this code could be called multiple times\n   258\t                # Having just p *= scale would repeatedly scale it down\n   259\t                if prenorm_residual_strategy == 'rescale':\n   260\t                    nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n   261\t                    with torch.no_grad():\n   262\t                        p /= math.sqrt(num_residuals_per_layer * self.config.num_hidden_layers)\n   263\t                elif prenorm_residual_strategy == 'zero':\n   264\t                    nn.init.zeros_(p)\n   265\t                else:\n   266\t                    raise ValueError(f\"Invalid prenorm_residual_strategy: {prenorm_residual_strategy}\")\n   267\t\n   268\t\n   269\tclass NSAModel(NSAPreTrainedModel):\n   270\t\n   271\t    def __init__(self, config: NSAConfig):\n   272\t        super().__init__(config)\n   273\t        self.padding_idx = config.pad_token_id\n   274\t        self.vocab_size = config.vocab_size\n   275\t\n   276\t        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n   277\t        self.layers = nn.ModuleList([NSABlock(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])\n   278\t        self.norm = (RMSNorm if config.fuse_norm else nn.RMSNorm)(config.hidden_size, eps=config.norm_eps)\n   279\t\n   280\t        self.gradient_checkpointing = False\n   281\t\n   282\t        self.post_init()\n   283\t\n   284\t    def get_input_embeddings(self):\n   285\t        return self.embeddings\n   286\t\n   287\t    def set_input_embeddings(self, value):\n   288\t        self.embeddings = value\n   289\t\n   290\t    def forward(\n   291\t        self,\n   292\t        input_ids: Optional[torch.LongTensor] = None,\n   293\t        attention_mask: Optional[torch.Tensor] = None,  # noqa\n   294\t        inputs_embeds: Optional[torch.FloatTensor] = None,\n   295\t        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n   296\t        use_cache: Optional[bool] = None,\n   297\t        output_attentions: Optional[bool] = None,\n   298\t        output_hidden_states: Optional[bool] = None,\n   299\t        return_dict: Optional[bool] = None,\n   300\t        **kwargs: Unpack[Dict]\n   301\t    ) -> Union[Tuple, BaseModelOutputWithPast]:\n   302\t        if output_attentions:\n   303\t            warnings.warn(\"`NSAModel` does not `output_attentions` now, setting it to `False`.\")\n   304\t            output_attentions = False\n   305\t        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n   306\t        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n   307\t        use_cache = use_cache if use_cache is not None else (self.config.use_cache if not self.training else False)\n   308\t        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n   309\t\n   310\t        # retrieve input_ids and inputs_embeds\n   311\t        if input_ids is not None and inputs_embeds is not None:\n   312\t            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n   313\t        if input_ids is None and inputs_embeds is None:\n   314\t            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n   315\t\n   316\t        if inputs_embeds is None:\n   317\t            inputs_embeds = self.embeddings(input_ids)\n   318\t        hidden_states = inputs_embeds\n   319\t\n   320\t        if use_cache and not isinstance(past_key_values, Cache):\n   321\t            past_key_values = Cache.from_legacy_cache(past_key_values)\n   322\t\n   323\t        if self.gradient_checkpointing and self.training and use_cache:\n   324\t            logger.warning_once(\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\")\n   325\t            use_cache = False\n   326\t\n   327\t        all_hidden_states = () if output_hidden_states else None\n   328\t        all_attns = () if output_attentions else None\n   329\t        for layer in self.layers:\n   330\t            if output_hidden_states:\n   331\t                all_hidden_states += (hidden_states,)\n   332\t\n   333\t            if self.gradient_checkpointing and self.training:\n   334\t                hidden_states, attentions, past_key_values = self._gradient_checkpointing_func(\n   335\t                    layer.__call__,\n   336\t                    hidden_states,\n   337\t                    attention_mask,\n   338\t                    past_key_values,\n   339\t                    use_cache,\n   340\t                    output_attentions,\n   341\t                    **kwargs\n   342\t                )\n   343\t            else:\n   344\t                hidden_states, attentions, past_key_values = layer(\n   345\t                    hidden_states,\n   346\t                    attention_mask=attention_mask,\n   347\t                    past_key_values=past_key_values,\n   348\t                    use_cache=use_cache,\n   349\t                    output_attentions=output_attentions,\n   350\t                    **kwargs\n   351\t                )\n   352\t\n   353\t            if output_attentions:\n   354\t                all_attns += (attentions,)\n   355\t\n   356\t        hidden_states = self.norm(hidden_states)\n   357\t\n   358\t        # add hidden states from the last decoder layer\n   359\t        if output_hidden_states:\n   360\t            all_hidden_states += (hidden_states,)\n   361\t\n   362\t        if not return_dict:\n   363\t            return tuple(i for i in [hidden_states, past_key_values, all_hidden_states, all_attns] if i is not None)\n   364\t        return BaseModelOutputWithPast(\n   365\t            last_hidden_state=hidden_states,\n   366\t            past_key_values=past_key_values,\n   367\t            hidden_states=all_hidden_states,\n   368\t            attentions=all_attns\n   369\t        )\n   370\t\n   371\t\n   372\tclass NSAForCausalLM(NSAPreTrainedModel, GenerationMixin):\n   373\t\n   374\t    _tied_weights_keys = [\"lm_head.weight\"]\n   375\t\n   376\t    def __init__(self, config):\n   377\t        super().__init__(config)\n   378\t        self.model = NSAModel(config)\n   379\t        self.vocab_size = config.vocab_size\n   380\t        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n   381\t        self.criterion = None\n   382\t\n   383\t        # Initialize weights and apply final processing\n   384\t        self.post_init()\n   385\t\n   386\t    def get_input_embeddings(self):\n   387\t   <response clipped><NOTE>Due to the max output limit, only part of this file has been shown to you. You should retry this tool after you have searched inside the file with `grep -n` in order to find the line numbers of what you are looking for.</NOTE>\n", "extras": {"path": "/workspace/benchmark/datasets/19-Native-Sparse-Attention/NSA/native_sparse_attention/modeling_nsa.py", "impl_source": "oh_aci"}}