[{"id": 0, "timestamp": "2025-05-19T19:42:52.623730", "source": "user", "message": "You are an expert NLP research scientist to help user with coding on NLP related tasks. \n    You are given a research paper named paper.pdf and an incomplete repository of the paper. You need to parse the paper with Python first and then are required to finish the user's task based on the paper and the repository.\n    If you cannot read the paper directly, you can try to write python code to parse the pdf file.\n    \n    The user's tasks include the following:0: Implement the `tdpo_loss` function in trainers.py based on the Token-level DPO loss mentioned in paper.pdf and the code repository. You may ignore the following parameters: if_tdpo2. \n    You are required to directly revise the python file mentioned in the instruction. You can only revise the function you need to implement. Do not revise unrelated part of the file.\n\n    Please do NOT run the program in the background.\n    If the program uses some packages that are incompatible, please figure out alternative implementations and do NOT restart the environment.\n\n    ", "action": "message", "args": {"content": "You are an expert NLP research scientist to help user with coding on NLP related tasks. \n    You are given a research paper named paper.pdf and an incomplete repository of the paper. You need to parse the paper with Python first and then are required to finish the user's task based on the paper and the repository.\n    If you cannot read the paper directly, you can try to write python code to parse the pdf file.\n    \n    The user's tasks include the following:0: Implement the `tdpo_loss` function in trainers.py based on the Token-level DPO loss mentioned in paper.pdf and the code repository. You may ignore the following parameters: if_tdpo2. \n    You are required to directly revise the python file mentioned in the instruction. You can only revise the function you need to implement. Do not revise unrelated part of the file.\n\n    Please do NOT run the program in the background.\n    If the program uses some packages that are incompatible, please figure out alternative implementations and do NOT restart the environment.\n\n    ", "image_urls": null, "wait_for_response": false}}, {"id": 1, "timestamp": "2025-05-19T19:42:52.698985", "source": "user", "message": "Retrieving content for: You are an expert NLP research scientist to help u", "action": "recall", "args": {"recall_type": "workspace_context", "query": "You are an expert NLP research scientist to help user with coding on NLP related tasks. \n    You are given a research paper named paper.pdf and an incomplete repository of the paper. You need to parse the paper with Python first and then are required to finish the user's task based on the paper and the repository.\n    If you cannot read the paper directly, you can try to write python code to parse the pdf file.\n    \n    The user's tasks include the following:0: Implement the `tdpo_loss` function in trainers.py based on the Token-level DPO loss mentioned in paper.pdf and the code repository. You may ignore the following parameters: if_tdpo2. \n    You are required to directly revise the python file mentioned in the instruction. You can only revise the function you need to implement. Do not revise unrelated part of the file.\n\n    Please do NOT run the program in the background.\n    If the program uses some packages that are incompatible, please figure out alternative implementations and do NOT restart the environment.\n\n    ", "thought": ""}}, {"id": 2, "timestamp": "2025-05-19T19:42:52.699456", "source": "environment", "message": "", "observation": "agent_state_changed", "content": "", "extras": {"agent_state": "running", "reason": ""}}, {"id": 3, "timestamp": "2025-05-19T19:42:52.699713", "source": "environment", "message": "Added workspace context", "cause": 1, "observation": "recall", "content": "Added workspace context", "extras": {"recall_type": "workspace_context", "repo_name": "", "repo_directory": "", "repo_instructions": "", "runtime_hosts": {"http://localhost:52481": 52481, "http://localhost:56182": 56182}, "additional_agent_instructions": "", "date": "2025-05-20", "microagent_knowledge": []}}, {"id": 4, "timestamp": "2025-05-19T19:42:54.263579", "source": "agent", "message": "Running command: ls -l && find . -iname 'paper.pdf' && find . -iname 'trainers.py'", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_4f1aq7T6f82oHu8UhWQ1Y9Pu", "model_response": {"id": "chatcmpl-BZ5MD1PpUCFgsMTcqRB699XAXyhYT", "created": 1747701773, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"ls -l && find . -iname 'paper.pdf' && find . -iname 'trainers.py'\"}", "name": "execute_bash"}, "id": "call_4f1aq7T6f82oHu8UhWQ1Y9Pu", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 36, "prompt_tokens": 4134, "total_tokens": 4170, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3840}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.002796, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 4134, "completion_tokens": 36, "cache_read_tokens": 3840, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "ls -l && find . -iname 'paper.pdf' && find . -iname 'trainers.py'", "is_input": false, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 5, "timestamp": "2025-05-19T19:42:54.911882", "source": "agent", "message": "Command `ls -l && find . -iname 'paper.pdf' && find . -iname 'trainers.py'` executed with exit code 0.", "cause": 4, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_4f1aq7T6f82oHu8UhWQ1Y9Pu", "model_response": {"id": "chatcmpl-BZ5MD1PpUCFgsMTcqRB699XAXyhYT", "created": 1747701773, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"ls -l && find . -iname 'paper.pdf' && find . -iname 'trainers.py'\"}", "name": "execute_bash"}, "id": "call_4f1aq7T6f82oHu8UhWQ1Y9Pu", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 36, "prompt_tokens": 4134, "total_tokens": 4170, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 3840}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "total 4\ndrwxr-xr-x 4 root root 4096 May 20 00:42 22-TokenDPO\n./22-TokenDPO/TokenDPO-main/paper.pdf\n./22-TokenDPO/TokenDPO-main/trainers.py", "extras": {"command": "ls -l && find . -iname 'paper.pdf' && find . -iname 'trainers.py'", "metadata": {"exit_code": 0, "pid": -1, "username": "root", "hostname": "45bf7b046ec2", "working_dir": "/workspace/benchmark/datasets", "py_interpreter_path": "/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python", "prefix": "", "suffix": "\n[The command completed with exit code 0.]"}, "hidden": false}, "success": true}, {"id": 6, "timestamp": "2025-05-19T19:42:55.829224", "source": "agent", "message": "Running command: head -40 ./22-TokenDPO/TokenDPO-main/trainers.py", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_VNV6hF65mBG2r647MVuJ33Nt", "model_response": {"id": "chatcmpl-BZ5MFWd0PN7z0kcfNYP9OVozFLtdM", "created": 1747701775, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"head -40 ./22-TokenDPO/TokenDPO-main/trainers.py\"}", "name": "execute_bash"}, "id": "call_VNV6hF65mBG2r647MVuJ33Nt", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 31, "prompt_tokens": 4300, "total_tokens": 4331, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 4096}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.0055, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 8434, "completion_tokens": 67, "cache_read_tokens": 7936, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "head -40 ./22-TokenDPO/TokenDPO-main/trainers.py", "is_input": false, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 7, "timestamp": "2025-05-19T19:42:56.470756", "source": "agent", "message": "Command `head -40 ./22-TokenDPO/TokenDPO-main/trainers.py` executed with exit code 0.", "cause": 6, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_VNV6hF65mBG2r647MVuJ33Nt", "model_response": {"id": "chatcmpl-BZ5MFWd0PN7z0kcfNYP9OVozFLtdM", "created": 1747701775, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"head -40 ./22-TokenDPO/TokenDPO-main/trainers.py\"}", "name": "execute_bash"}, "id": "call_VNV6hF65mBG2r647MVuJ33Nt", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 31, "prompt_tokens": 4300, "total_tokens": 4331, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 4096}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "import torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport transformers\nfrom omegaconf import DictConfig\n\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    StateDictType,\n    BackwardPrefetch,\n    ShardingStrategy,\n    CPUOffload,\n)\nfrom torch.distributed.fsdp.api import FullStateDictConfig, FullOptimStateDictConfig\nfrom torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\nimport tensor_parallel as tp\nimport contextlib\n\nfrom preference_datasets import get_batch_iterator\nfrom utils import (\n    slice_and_move_batch_for_device,\n    formatted_dict,\n    all_gather_if_needed,\n    pad_to_length,\n    get_block_class_from_model,\n    rank0_print,\n    get_local_dir,\n)\nimport numpy as np\nimport wandb\nimport tqdm\n\nimport random\nimport os\nfrom collections import defaultdict\nimport time", "extras": {"command": "head -40 ./22-TokenDPO/TokenDPO-main/trainers.py", "metadata": {"exit_code": 0, "pid": -1, "username": "root", "hostname": "45bf7b046ec2", "working_dir": "/workspace/benchmark/datasets", "py_interpreter_path": "/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python", "prefix": "", "suffix": "\n[The command completed with exit code 0.]"}, "hidden": false}, "success": true}, {"id": 8, "timestamp": "2025-05-19T19:42:57.367767", "source": "agent", "message": "Running command: grep -n 'def ' ./22-TokenDPO/TokenDPO-main/trainers.py", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_nHpd0FQQP3pu9JIWilLRl6nW", "model_response": {"id": "chatcmpl-BZ5MGEDlIUH7qGneEzafnN5owsnTz", "created": 1747701776, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"grep -n 'def ' ./22-TokenDPO/TokenDPO-main/trainers.py\"}", "name": "execute_bash"}, "id": "call_nHpd0FQQP3pu9JIWilLRl6nW", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 34, "prompt_tokens": 4613, "total_tokens": 4647, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 4224}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.008662, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 13047, "completion_tokens": 101, "cache_read_tokens": 12160, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "grep -n 'def ' ./22-TokenDPO/TokenDPO-main/trainers.py", "is_input": false, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 9, "timestamp": "2025-05-19T19:42:58.004335", "source": "agent", "message": "Command `grep -n 'def ' ./22-TokenDPO/TokenDPO-main/trainers.py` executed with exit code 0.", "cause": 8, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_nHpd0FQQP3pu9JIWilLRl6nW", "model_response": {"id": "chatcmpl-BZ5MGEDlIUH7qGneEzafnN5owsnTz", "created": 1747701776, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"grep -n 'def ' ./22-TokenDPO/TokenDPO-main/trainers.py\"}", "name": "execute_bash"}, "id": "call_nHpd0FQQP3pu9JIWilLRl6nW", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 34, "prompt_tokens": 4613, "total_tokens": 4647, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 4224}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "46:def tdpo_loss(chosen_logps_margin: torch.FloatTensor,\n72:def _get_batch_logps(logits: torch.FloatTensor, labels: torch.LongTensor,\n101:def _tdpo_get_batch_logps(logits: torch.FloatTensor, reference_logits: torch.FloatTensor, labels: torch.LongTensor,\n147:def concatenated_inputs(batch: Dict[str, Union[List, torch.LongTensor]]) -> Dict[str, torch.LongTensor]:\n175:    def __init__(self, policy: nn.Module, config: DictConfig, seed: int, run_dir: str,\n217:    def get_batch_samples(self, batch: Dict[str, torch.LongTensor]) -> Tuple[str, str]:\n249:    def tdpo_concatenated_forward(self, model: nn.Module, reference_model: nn.Module,\n275:    def get_batch_metrics(self, batch: Dict[str, Union[List, torch.LongTensor]], loss_config: DictConfig, train=True):\n324:    def train(self):\n458:    def clip_gradient(self):\n462:    def write_state_dict(self, step: int, state: Dict[str, torch.Tensor], metrics: Dict, filename: str,\n477:    def save(self, output_dir: Optional[str] = None, metrics: Optional[Dict] = None):\n493:    def __init__(self, policy: nn.Module, config: DictConfig, seed: int, run_dir: str,\n557:    def clip_gradient(self):\n561:    def save(self, output_dir=None, metrics=None):\n588:    def __init__(self, policy, config, seed, run_dir, reference_model=None, rank=0, world_size=1):\n602:    def save(self, output_dir=None, metrics=None):", "extras": {"command": "grep -n 'def ' ./22-TokenDPO/TokenDPO-main/trainers.py", "metadata": {"exit_code": 0, "pid": -1, "username": "root", "hostname": "45bf7b046ec2", "working_dir": "/workspace/benchmark/datasets", "py_interpreter_path": "/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python", "prefix": "", "suffix": "\n[The command completed with exit code 0.]"}, "hidden": false}, "success": true}, {"id": 10, "timestamp": "2025-05-19T19:42:59.269069", "source": "agent", "message": "Reading file: /workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py", "action": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_k0p5zLDTmIzXcI7zdvw6rxcN", "model_response": {"id": "chatcmpl-BZ5MIFShzT0VbdY6SU3LOf1Ak66dQ", "created": 1747701778, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"view\",\"path\":\"/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py\",\"view_range\":[46,71]}", "name": "str_replace_editor"}, "id": "call_k0p5zLDTmIzXcI7zdvw6rxcN", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 45, "prompt_tokens": 5075, "total_tokens": 5120, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 4608}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.01226, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 18122, "completion_tokens": 146, "cache_read_tokens": 16768, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"path": "/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py", "start": 0, "end": -1, "thought": "", "impl_source": "oh_aci", "view_range": [46, 71]}}, {"id": 11, "timestamp": "2025-05-19T19:42:59.297073", "source": "agent", "message": "I read the file /workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py.", "cause": 10, "observation": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_k0p5zLDTmIzXcI7zdvw6rxcN", "model_response": {"id": "chatcmpl-BZ5MIFShzT0VbdY6SU3LOf1Ak66dQ", "created": 1747701778, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"view\",\"path\":\"/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py\",\"view_range\":[46,71]}", "name": "str_replace_editor"}, "id": "call_k0p5zLDTmIzXcI7zdvw6rxcN", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 45, "prompt_tokens": 5075, "total_tokens": 5120, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 4608}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "Here's the result of running `cat -n` on /workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py:\n    46\tdef tdpo_loss(chosen_logps_margin: torch.FloatTensor,\n    47\t              rejected_logps_margin: torch.FloatTensor,\n    48\t              chosen_position_kl: torch.FloatTensor,\n    49\t              rejected_position_kl: torch.FloatTensor,\n    50\t              beta: float, alpha: float = 0.5, if_tdpo2: bool = True) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n    51\t    \"\"\"Compute the TDPO loss for a batch of policy and reference model log probabilities.\n    52\t\n    53\t    Args:\n    54\t        chosen_logps_margin: The difference of log probabilities between the policy model and the reference model for the chosen responses. Shape: (batch_size,)\n    55\t        rejected_logps_margin: The difference of log probabilities between the policy model and the reference model for the rejected responses. Shape: (batch_size,)\n    56\t        chosen_position_kl: The difference of sequential kl divergence between the policy model and the reference model for the chosen responses. Shape: (batch_size,)\n    57\t        rejected_position_kl: The difference of sequential kl divergence between the policy model and the reference model for the rejected responses. Shape: (batch_size,)\n    58\t        beta: Temperature parameter for the TDPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.\n    59\t        alpha: Temperature parameter for the TDPO loss, used to adjust the impact of sequential kl divergence.\n    60\t        if_tdpo2: Determine whether to use method TDPO2, default is True; if False, then use method TDPO1.\n    61\t\n    62\t    Returns:\n    63\t        A tuple of two tensors: (losses, rewards).\n    64\t        The losses tensor contains the TDPO loss for each example in the batch.\n    65\t        The rewards tensors contain the rewards for response pair.\n    66\t    \"\"\"\n    67\t\n    68\t\n    69\t    return losses, chosen_rewards, rejected_rewards\n    70\t\n    71\t\n", "extras": {"path": "/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py", "impl_source": "oh_aci"}}, {"id": 12, "timestamp": "2025-05-19T19:43:00.537307", "source": "agent", "message": "Running command: head -60 ./22-TokenDPO/TokenDPO-main/paper.pdf | strings | head -40", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_14TaJRdeBbeQgg5981qijUnQ", "model_response": {"id": "chatcmpl-BZ5MJ4fvRaDg7mg86ji2c2uZMm7BP", "created": 1747701779, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"head -60 ./22-TokenDPO/TokenDPO-main/paper.pdf | strings | head -40\"}", "name": "execute_bash"}, "id": "call_14TaJRdeBbeQgg5981qijUnQ", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 37, "prompt_tokens": 5613, "total_tokens": 5650, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 4992}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.016294, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 23735, "completion_tokens": 183, "cache_read_tokens": 21760, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "head -60 ./22-TokenDPO/TokenDPO-main/paper.pdf | strings | head -40", "is_input": false, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 13, "timestamp": "2025-05-19T19:43:01.178724", "source": "agent", "message": "Command `head -60 ./22-TokenDPO/TokenDPO-main/paper.pdf | strings | head -40` executed with exit code 0.", "cause": 12, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_14TaJRdeBbeQgg5981qijUnQ", "model_response": {"id": "chatcmpl-BZ5MJ4fvRaDg7mg86ji2c2uZMm7BP", "created": 1747701779, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"head -60 ./22-TokenDPO/TokenDPO-main/paper.pdf | strings | head -40\"}", "name": "execute_bash"}, "id": "call_14TaJRdeBbeQgg5981qijUnQ", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 37, "prompt_tokens": 5613, "total_tokens": 5650, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 4992}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "%PDF-1.5\n136 0 obj\n<< /Filter /FlateDecode /Length 4481 >>\nstream\nRDA~X\nEQ/E\n?wU+\nIb(#\nRO S\nhOliS`]uK\ncuhrj\nn,~u\nR?i:Q\nwy>]g\nngby\n`4H:\nC       8p\n~1j6+z\n9nAQq\n[h/+\n-G/Sb\nAOb!\nm7k\nLN25-qo(\nJ0>5\nqQ+4y\n8Yvh\nEUwph\n\\6S$\nspX7\n.;T-\n85b%\nRzO4\n\"pv$\nU7M^\nr<yA\nrabl\n        a!tav\nn[D\n&P7&r", "extras": {"command": "head -60 ./22-TokenDPO/TokenDPO-main/paper.pdf | strings | head -40", "metadata": {"exit_code": 0, "pid": -1, "username": "root", "hostname": "45bf7b046ec2", "working_dir": "/workspace/benchmark/datasets", "py_interpreter_path": "/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python", "prefix": "", "suffix": "\n[The command completed with exit code 0.]"}, "hidden": false}, "success": true}, {"id": 14, "timestamp": "2025-05-19T19:43:02.690156", "source": "agent", "message": "Running Python code interactively: import PyPDF2\n\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[:5]:\n        text += page.extract_text() + '\\n'\n\nprint(text[:2000])", "action": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_0Xg4w9XYDWlAPHjeMvycrVHi", "model_response": {"id": "chatcmpl-BZ5ML1mfXdvoGYQ61B56xoDGKZ5WK", "created": 1747701781, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"import PyPDF2\\n\\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    text = ''\\n    for page in reader.pages[:5]:\\n        text += page.extract_text() + '\\\\n'\\n\\nprint(text[:2000])\"}", "name": "execute_ipython_cell"}, "id": "call_0Xg4w9XYDWlAPHjeMvycrVHi", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 93, "prompt_tokens": 5901, "total_tokens": 5994, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 5632}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.020392, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 29636, "completion_tokens": 276, "cache_read_tokens": 27392, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"code": "import PyPDF2\n\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[:5]:\n        text += page.extract_text() + '\\n'\n\nprint(text[:2000])", "thought": "", "include_extra": true, "confirmation_state": "confirmed", "kernel_init_code": ""}}, {"id": 15, "timestamp": "2025-05-19T19:43:02.959601", "source": "agent", "message": "Code executed in IPython cell.", "cause": 14, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_0Xg4w9XYDWlAPHjeMvycrVHi", "model_response": {"id": "chatcmpl-BZ5ML1mfXdvoGYQ61B56xoDGKZ5WK", "created": 1747701781, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"import PyPDF2\\n\\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    text = ''\\n    for page in reader.pages[:5]:\\n        text += page.extract_text() + '\\\\n'\\n\\nprint(text[:2000])\"}", "name": "execute_ipython_cell"}, "id": "call_0Xg4w9XYDWlAPHjeMvycrVHi", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 93, "prompt_tokens": 5901, "total_tokens": 5994, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 5632}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "Token-level Direct Preference Optimization\nYongcheng Zeng1 2Guoqing Liu3Weiyu Ma1 2Ning Yang1Haifeng Zhang1Jun Wang4\nAbstract\nFine-tuning pre-trained Large Language Models\n(LLMs) is essential to align them with human\nvalues and intentions. This process often uti-\nlizes methods like pairwise comparisons and KL\ndivergence against a reference LLM, focusing\non the evaluation of full answers generated by\nthe models. However, the generation of these\nresponses occurs in a token level, following a\nsequential, auto-regressive fashion. In this pa-\nper, we introduce Token-level Direct Preference\nOptimization (TDPO), a novel approach to align\nLLMs with human preferences by optimizing pol-\nicy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency,\nTDPO incorporates forward KL divergence con-\nstraints for each token, improving alignment and\ndiversity. Utilizing the Bradley-Terry model for\na token-based reward system, TDPO enhances\nthe regulation of KL divergence, while preserv-\ning simplicity without the need for explicit re-\nward modeling. Experimental results across vari-\nous text tasks demonstrate TDPO\u2019s superior per-\nformance in balancing alignment with genera-\ntion diversity. Notably, fine-tuning with TDPO\nstrikes a better balance than DPO in the controlled\nsentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of\ngenerated responses compared to both DPO and\nPPO-based RLHF methods. Our code is open-\nsourced at https://github.com/Vance0124/Token-\nlevel-Direct-Preference-Optimization.\n1Institute of Automation, Chinese Academy of Sciences\n2School of Artificial Intelligence, University of Chinese Academy\nof Sciences3Microsoft Research AI4Science4University College\nLondon. Correspondence to: Jun Wang <jun.wang@cs.ucl.ac.uk>,\nHaifeng Zhang <haifeng.zhang@ia.ac.cn>.\nProceedings of the 41stInternational Conference on Machine\nLearning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).1. Int\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "import PyPDF2\n\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[:5]:\n        text += page.extract_text() + '\\n'\n\nprint(text[:2000])"}, "success": true}, {"id": 16, "timestamp": "2025-05-19T19:43:04.891223", "source": "agent", "message": "Running Python code interactively: # Extract more text to find the TDPO loss definition and equations\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[:10]:\n        text += page.extract_text() + '\\n'\n\nprint(text[:4000])", "action": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_DaHE3VZQv6CmMejyGBJtPRE7", "model_response": {"id": "chatcmpl-BZ5MNu10BdzdWJPh4dtuX1RswF49t", "created": 1747701783, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Extract more text to find the TDPO loss definition and equations\\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    text = ''\\n    for page in reader.pages[:10]:\\n        text += page.extract_text() + '\\\\n'\\n\\nprint(text[:4000])\"}", "name": "execute_ipython_cell"}, "id": "call_DaHE3VZQv6CmMejyGBJtPRE7", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 101, "prompt_tokens": 6503, "total_tokens": 6604, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 5888}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.025374, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 36139, "completion_tokens": 377, "cache_read_tokens": 33280, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"code": "# Extract more text to find the TDPO loss definition and equations\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[:10]:\n        text += page.extract_text() + '\\n'\n\nprint(text[:4000])", "thought": "", "include_extra": true, "confirmation_state": "confirmed", "kernel_init_code": ""}}, {"id": 17, "timestamp": "2025-05-19T19:43:05.650498", "source": "agent", "message": "Code executed in IPython cell.", "cause": 16, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_DaHE3VZQv6CmMejyGBJtPRE7", "model_response": {"id": "chatcmpl-BZ5MNu10BdzdWJPh4dtuX1RswF49t", "created": 1747701783, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Extract more text to find the TDPO loss definition and equations\\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    text = ''\\n    for page in reader.pages[:10]:\\n        text += page.extract_text() + '\\\\n'\\n\\nprint(text[:4000])\"}", "name": "execute_ipython_cell"}, "id": "call_DaHE3VZQv6CmMejyGBJtPRE7", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 101, "prompt_tokens": 6503, "total_tokens": 6604, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 5888}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "Token-level Direct Preference Optimization\nYongcheng Zeng1 2Guoqing Liu3Weiyu Ma1 2Ning Yang1Haifeng Zhang1Jun Wang4\nAbstract\nFine-tuning pre-trained Large Language Models\n(LLMs) is essential to align them with human\nvalues and intentions. This process often uti-\nlizes methods like pairwise comparisons and KL\ndivergence against a reference LLM, focusing\non the evaluation of full answers generated by\nthe models. However, the generation of these\nresponses occurs in a token level, following a\nsequential, auto-regressive fashion. In this pa-\nper, we introduce Token-level Direct Preference\nOptimization (TDPO), a novel approach to align\nLLMs with human preferences by optimizing pol-\nicy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency,\nTDPO incorporates forward KL divergence con-\nstraints for each token, improving alignment and\ndiversity. Utilizing the Bradley-Terry model for\na token-based reward system, TDPO enhances\nthe regulation of KL divergence, while preserv-\ning simplicity without the need for explicit re-\nward modeling. Experimental results across vari-\nous text tasks demonstrate TDPO\u2019s superior per-\nformance in balancing alignment with genera-\ntion diversity. Notably, fine-tuning with TDPO\nstrikes a better balance than DPO in the controlled\nsentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of\ngenerated responses compared to both DPO and\nPPO-based RLHF methods. Our code is open-\nsourced at https://github.com/Vance0124/Token-\nlevel-Direct-Preference-Optimization.\n1Institute of Automation, Chinese Academy of Sciences\n2School of Artificial Intelligence, University of Chinese Academy\nof Sciences3Microsoft Research AI4Science4University College\nLondon. Correspondence to: Jun Wang <jun.wang@cs.ucl.ac.uk>,\nHaifeng Zhang <haifeng.zhang@ia.ac.cn>.\nProceedings of the 41stInternational Conference on Machine\nLearning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).1. Introduction\nLarge language models (LLMs) (Achiam et al., 2023;\nBubeck et al., 2023) have demonstrated significant gen-\neralization capabilities in various domains including text\nsummarization (Stiennon et al., 2022; Koh et al., 2022), cod-\ning writing (Chen et al., 2021; Gao et al., 2023), and even\nfollowing human instructions (Chung et al., 2022; Ouyang\net al., 2022). In order to align LLMs with human intentions,\nReinforcement Learning from Human Feedback (RLHF)\n(Christiano et al., 2017; Ouyang et al., 2022; Dong et al.,\n2023; Yuan et al., 2023; Liu et al., 2023) has emerged as a\nhighly effective method, embodying both stylistic and eth-\nical values (Bai et al., 2022; Ganguli et al., 2022). These\napproaches typically involve the training of a reward model\nfollowed by the fine-tuning of the policy model using rein-\nforcement learning (RL).\nDirect Preference Optimization (DPO) (Rafailov et al.,\n2023) introduces a straightforward and effective technique\nfor training LLMs using pairwise comparisons, without\nthe need for explicitly establishing a reward model. DPO\nutilizes KL divergence to ensure that the training process\nremains closely aligned with a reference Large Language\nModel (LLM), preventing significant deviations. In DPO,\nKL divergence is assessed at the sentence level, reflecting\nthe fact that evaluations are based on complete responses\n(answers), typically comprising several sentences. How-\never, the generation of these responses occurs sequentially,\nfollowing an auto-regressive approach. A potential benefit\nis to examine divergence in relation to a reference LLM\non a more granular, token-by-token basis. One approach\ninvolves using sequential KL divergence (as defined in Defi-\nnition 4.3), which monitors the trajectory of the generated\nresponses. As illustrated in Figure 1, DPO demonstrates\na significantly faster increase in KL divergence within the\nsubset of less preferred responses when compared to the\nsubset that is preferred. This results in an expanding g\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Extract more text to find the TDPO loss definition and equations\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[:10]:\n        text += page.extract_text() + '\\n'\n\nprint(text[:4000])"}, "success": true}, {"id": 18, "timestamp": "2025-05-19T19:43:07.611600", "source": "agent", "message": "Running Python code interactively: # Try to extract more text, focusing on the middle of the paper for equations/algorithm\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[10:20]:\n        text += page.extract_text() + '\\n'\n\nprint(text[:4000])", "action": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_5LwCbuUpX1XJHZLjD5yx04x6", "model_response": {"id": "chatcmpl-BZ5MQp3JiXhxYlN3nuN4oCPnVUoKM", "created": 1747701786, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Try to extract more text, focusing on the middle of the paper for equations/algorithm\\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    text = ''\\n    for page in reader.pages[10:20]:\\n        text += page.extract_text() + '\\\\n'\\n\\nprint(text[:4000])\"}", "name": "execute_ipython_cell"}, "id": "call_5LwCbuUpX1XJHZLjD5yx04x6", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 108, "prompt_tokens": 7593, "total_tokens": 7701, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 6528}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.031632, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 43732, "completion_tokens": 485, "cache_read_tokens": 39808, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"code": "# Try to extract more text, focusing on the middle of the paper for equations/algorithm\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[10:20]:\n        text += page.extract_text() + '\\n'\n\nprint(text[:4000])", "thought": "", "include_extra": true, "confirmation_state": "confirmed", "kernel_init_code": ""}}, {"id": 19, "timestamp": "2025-05-19T19:43:07.957491", "source": "agent", "message": "Code executed in IPython cell.", "cause": 18, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_5LwCbuUpX1XJHZLjD5yx04x6", "model_response": {"id": "chatcmpl-BZ5MQp3JiXhxYlN3nuN4oCPnVUoKM", "created": 1747701786, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Try to extract more text, focusing on the middle of the paper for equations/algorithm\\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    text = ''\\n    for page in reader.pages[10:20]:\\n        text += page.extract_text() + '\\\\n'\\n\\nprint(text[:4000])\"}", "name": "execute_ipython_cell"}, "id": "call_5LwCbuUpX1XJHZLjD5yx04x6", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 108, "prompt_tokens": 7593, "total_tokens": 7701, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 6528}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "Token-level Direct Preference Optimization\ntaxonomy, challenges, and open questions. arXiv preprint\narXiv:2311.05232 , 2023.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., de las Casas, D., Bressand, F., Lengyel,\nG., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-\nA., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,\nT., and Sayed, W. E. Mistral 7b, 2023.\nKhalifa, M., Elsahar, H., and Dymetman, M. A distri-\nbutional approach to controlled text generation. arXiv\npreprint arXiv:2012.11635 , 2020.\nKnox, W. B., Hatgis-Kessell, S., Booth, S., Niekum, S.,\nStone, P., and Allievi, A. Models of human prefer-\nence for learning reward functions. arXiv preprint\narXiv:2206.02231 , 2022.\nKnox, W. B., Hatgis-Kessell, S., Adalgeirsson, S. O., Booth,\nS., Dragan, A., Stone, P., and Niekum, S. Learning\noptimal advantage from preferences and mistaking it for\nreward. arXiv preprint arXiv:2310.02456 , 2023.\nKoh, H. Y ., Ju, J., Liu, M., and Pan, S. An empirical survey\non long document summarization: Datasets, models, and\nmetrics. ACM Computing Surveys , 55(8):1\u201335, December\n2022. ISSN 1557-7341. doi: 10.1145/3545176. URL\nhttp://dx.doi.org/10.1145/3545176 .\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000) , pp. 1207\u20131216, Stan-\nford, CA, 2000. Morgan Kaufmann.\nLiu, T., Zhao, Y ., Joshi, R., Khalman, M., Saleh, M.,\nLiu, P. J., and Liu, J. Statistical rejection sam-\npling improves preference optimization. arXiv preprint\narXiv:2309.06657 , 2023.\nMaas, A., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y ., and\nPotts, C. Learning word vectors for sentiment analysis.\nInProceedings of the 49th annual meeting of the asso-\nciation for computational linguistics: Human language\ntechnologies , pp. 142\u2013150, 2011.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems , 35:27730\u201327744, 2022.\nPerez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides,\nJ., Glaese, A., McAleese, N., and Irving, G. Red team-\ning language models with language models, 2022. URL\nhttps://arxiv. org/abs/2202.03286 , 2022.\nPuterman, M. L. Markov decision processes: discrete\nstochastic dynamic programming . John Wiley & Sons,\n2014.Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog , 1(8):9, 2019.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\nC. D., and Finn, C. Direct preference optimization: Your\nlanguage model is secretly a reward model. arXiv preprint\narXiv:2305.18290 , 2023.\nRafailov, R., Hejna, J., Park, R., and Finn, C. From rtoQ\u2217:\nYour Language Model is Secretly a Q-Function. arXiv\npreprint arXiv:2404.12358 , 2024.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research ,\n21(1):5485\u20135551, 2020.\nRauh, M., Mellor, J., Uesato, J., Huang, P.-S., Welbl, J., Wei-\ndinger, L., Dathathri, S., Glaese, A., Irving, G., Gabriel, I.,\nIsaac, W., and Hendricks, L. A. Characteristics of harm-\nful text: Towards rigorous benchmarking of language\nmodels, 2022.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,\nP. Trust region policy optimization. In International\nconference on machine learning , pp. 1889\u20131897. PMLR,\n2015.\nSheng, E., Chang, K.-W., Natarajan, P., and Peng, N. So-\ncietal biases in language generation: Progress and chal-\nlenges. arXiv preprint arXiv:2105.04054 , 2021.\nSong, F., Yu, B., Li, M., Yu, H., Huang, F., Li, Y ., and Wang,\nH. Preference ranking optimization for human alignment.\narXiv preprint arXiv:2306.17492 , 2023.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,\nR., V\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Try to extract more text, focusing on the middle of the paper for equations/algorithm\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[10:20]:\n        text += page.extract_text() + '\\n'\n\nprint(text[:4000])"}, "success": true}, {"id": 20, "timestamp": "2025-05-19T19:43:12.736519", "source": "agent", "message": "Running Python code interactively: # Try to extract the most relevant section: search for 'loss' and 'TDPO' in the first 15 pages\nimport re\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[:15]:\n        text += page.extract_text() + '\\n'\n\n# Find all paragraphs containing 'loss' or 'TDPO'\nloss_paragraphs = re.findall(r'([^.]*?(?:loss|TDPO)[^.]*\\.)', text, re.IGNORECASE)\nfor para in loss_paragraphs:\n    print(para.strip())", "action": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_iijFXURQ90tZgAhcbPzaIClQ", "model_response": {"id": "chatcmpl-BZ5MTYNCBWIyLvKF5inAj5oHA7lui", "created": 1747701789, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Try to extract the most relevant section: search for 'loss' and 'TDPO' in the first 15 pages\\nimport re\\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    text = ''\\n    for page in reader.pages[:15]:\\n        text += page.extract_text() + '\\\\n'\\n\\n# Find all paragraphs containing 'loss' or 'TDPO'\\nloss_paragraphs = re.findall(r'([^.]*?(?:loss|TDPO)[^.]*\\\\.)', text, re.IGNORECASE)\\nfor para in loss_paragraphs:\\n    print(para.strip())\" }", "name": "execute_ipython_cell"}, "id": "call_iijFXURQ90tZgAhcbPzaIClQ", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 172, "prompt_tokens": 9147, "total_tokens": 9319, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 7680}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.039782, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 52879, "completion_tokens": 657, "cache_read_tokens": 47488, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"code": "# Try to extract the most relevant section: search for 'loss' and 'TDPO' in the first 15 pages\nimport re\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[:15]:\n        text += page.extract_text() + '\\n'\n\n# Find all paragraphs containing 'loss' or 'TDPO'\nloss_paragraphs = re.findall(r'([^.]*?(?:loss|TDPO)[^.]*\\.)', text, re.IGNORECASE)\nfor para in loss_paragraphs:\n    print(para.strip())", "thought": "", "include_extra": true, "confirmation_state": "confirmed", "kernel_init_code": ""}}, {"id": 21, "timestamp": "2025-05-19T19:43:13.707357", "source": "agent", "message": "Code executed in IPython cell.", "cause": 20, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_iijFXURQ90tZgAhcbPzaIClQ", "model_response": {"id": "chatcmpl-BZ5MTYNCBWIyLvKF5inAj5oHA7lui", "created": 1747701789, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Try to extract the most relevant section: search for 'loss' and 'TDPO' in the first 15 pages\\nimport re\\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\\n    reader = PyPDF2.PdfReader(f)\\n    text = ''\\n    for page in reader.pages[:15]:\\n        text += page.extract_text() + '\\\\n'\\n\\n# Find all paragraphs containing 'loss' or 'TDPO'\\nloss_paragraphs = re.findall(r'([^.]*?(?:loss|TDPO)[^.]*\\\\.)', text, re.IGNORECASE)\\nfor para in loss_paragraphs:\\n    print(para.strip())\" }", "name": "execute_ipython_cell"}, "id": "call_iijFXURQ90tZgAhcbPzaIClQ", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 172, "prompt_tokens": 9147, "total_tokens": 9319, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 7680}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "In this pa-\nper, we introduce Token-level Direct Preference\nOptimization (TDPO), a novel approach to align\nLLMs with human preferences by optimizing pol-\nicy at the token level.\nUnlike previous methods,\nwhich face challenges in divergence efficiency,\nTDPO incorporates forward KL divergence con-\nstraints for each token, improving alignment and\ndiversity.\nUtilizing the Bradley-Terry model for\na token-based reward system, TDPO enhances\nthe regulation of KL divergence, while preserv-\ning simplicity without the need for explicit re-\nward modeling.\nExperimental results across vari-\nous text tasks demonstrate TDPO\u2019s superior per-\nformance in balancing alignment with genera-\ntion diversity.\nNotably, fine-tuning with TDPO\nstrikes a better balance than DPO in the controlled\nsentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of\ngenerated responses compared to both DPO and\nPPO-based RLHF methods.\nWe introduce a\nnew method, referred to as Token-level Direct Preference\nOptimization (TDPO), which aims to strike a better balance\nbetween alignment performance and generation diversity\nby controlling the KL divergence for each token.\nTDPO maintains the simplicity of DPO while offering im-\nproved regulation of KL divergence for aligning LLMs\nwith human preferences.\nOur experimental results demonstrate\nthe effectiveness of TDPO across multiple text tasks, and\ngain a notable enhancement in the quality of generated re-\nsponses in comparison to both DPO and PPO-based RLHF\nmethods.\nIn conclusion, TDPO stands out for its ability\nto not only effectively address the issue of excessive KL\ndivergence but also greatly improve divergence efficiency.\nNevertheless, the\nchallenge of maintaining linguistic diversity while aligning\nwith human preferences remains a pivotal concern, prompt-\ning our proposed Token-level Direct Preference Optimiza-\ntion (TDPO), which seeks to harmonize the dual objectives\nof alignment accuracy and expressive range in model out-\nputs.\n3 and leveraging the negative\nlog-likelihood loss, DPO derives the objective function:\nu(x, yw, yl) =\u03b2log\u03c0\u03b8(yw|x)\n\u03c0ref(yw|x)\u2212\u03b2log\u03c0\u03b8(yl|x)\n\u03c0ref(yl|x),\nLDPO(\u03c0\u03b8;\u03c0ref) =\u2212E(x,yw,yl)\u223cD[log\u03c3(u(x, yw, yl))],\n(4)\nand the derivative is given as follows:\n\u2207\u03b8LDPO(\u03c0\u03b8;\u03c0ref) =\u2212E(x,yw,yl)\u223cD[\u03c3(\u2212u)\u2207\u03b8u],(5)\nwhere uis the abbreviation of u(x, yw, yl),ywandylde-\nnotes the preferred and dispreferred completion.\nFinally, we conduct a\nformalized analysis of this optimization objective in terms\nof derivatives and, based on this, derive the ultimate loss\nfunction for TDPO.\nLoss Function and Formal Analysis\nDrawing on Eq.\nThis allows\nus to formulate a likelihood maximization objective for a\nparametrized policy \u03c0\u03b8, leading to the derivation of the loss\nfunction for the initial version of our method, TDPO 1:\nLTDPO 1(\u03c0\u03b8;\u03c0ref) =\n\u2212E(x,yw,yl)\u223cD[log\u03c3(u(x, yw, yl)\u2212\u03b4(x, yw, yl))].\n(15)\nThrough this approach, we explicitly introduce sequential\nforward KL divergence into the loss function.\nSubsequently, we conduct a derivative analysis of our\nmethod and make specific modifications to the loss function\nof TDPO.\nBy employing the formula-\ntion of the loss function presented in Eq.\n15, we compute the\ngradient of the loss function with respect to the parameters\n\u03b8:\n\u2207\u03b8LTDPO 1(\u03c0\u03b8;\u03c0ref) =\n\u2212E(x,yw,yl)\u223cD[\u03c3(\u2212u+\u03b4) [\u2207\u03b8u\u2212 \u2207 \u03b8\u03b4]].\nThe first part (\u2212u)corresponds to the weight\nfactor in the loss function of DPO.\nThe gradient of the loss function in Eq.\nHowever, when considered separately, the gradient of\nDSeqKL (x, yw;\u03c0ref|\u03c0\u03b8)in the loss function tends to in-\ncrease the sequential KL divergence between \u03c0refand\u03c0\u03b8\nat(x, yw)during the optimization process.\nThis is because\nthe sequential forward KL divergence in the loss function is\nintroduced through the state-value function V\u03c0, inherently\nintroducing an expectation Ez\u223c\u03c0refh\nlog\u03c0\u03b8(z|[x,y<t])\n\u03c0ref(z|[x,y<t])i\nas a\nbaseline at each token.\nFor\nthe prompt xand the preferred response yw, at each token,\nthe loss function in Eq.\nAs we do not aim to accelerate the training speed and\nprefer to ensure training stability, we modify the loss\nfunction by discontinuing the gradient propagation of\nDSeqKL (x, yw;\u03c0ref|\u03c0\u03b8)and treating it as a baseline term\nfor alignment of DSeqKL (x, yl;\u03c0ref|\u03c0\u03b8).\nFor\nthe prompt xand the rejected response yl, the loss\nfunction in Eq.\nFurthermore, to achieve a better balance between align-\nment performance and generation diversity in TDPO, we\nintroduce an additional parameter \u03b1into the loss func-\ntion.\nIn summary, we modify the loss function of TDPO 1, re-\nsulting in the second version of our method, TDPO 2, asfollows:\nLTDPO 2(\u03c0\u03b8;\u03c0ref) =\n\u2212E(x,yw,yl)\u223cD[log\u03c3(u(x, yw, yl)\u2212\u03b1\u03b42(x, yw, yl))],\n(17)\nwhere \u03b1is a parameter, and\n\u03b42(x, y1, y2) =\u03b2DSeqKL (x, y2;\u03c0ref\u2225\u03c0\u03b8)\n\u2212sg(\u03b2DSeqKL (x, y1;\u03c0ref\u2225\u03c0\u03b8)).\nWe summarize the comparison of the loss functions for\nDPO ,TDPO 1, and TDPO 2, as presented in Figure 2.\nComparison of Loss Functions for DPO ,TDPO 1and\nTDPO 2Methods.\nBoth TDPO 1andTDPO 2incorporate an additional term for\nfiner-grained control over the KL divergence, compared to DPO.\nWe provided the pseudocode in Algo-\nrithm 1 and the Pytorch implementation version of TDPO\nloss in Appendix B.\nThe Anthropic HH dataset is a single-turn dialogue dataset\nwhere the model receives human queries, covering various\n6\nToken-level Direct Preference Optimization\nAlgorithm 1 Token-level Direct Preference Optimization (TDPO)\n1:Input: Reference model \u03c0ref, Policy model \u03c0\u03b8, Coefficient \u03b1, \u03b2, Learning rate \u03b7\n2:Input: Dataset D={(x, yw, yl)i}N\ni=1of size N, Method M\n3:Initialize: \u03c0\u03b8\u2190\u03c0ref\n4:foreach epoch do\n5: Sample mini-batch Dm={(x, yw, yl)m}M\nm=1fromD\n6: Predict the probabilities \u03c0\u03b8(yw|x)and\u03c0\u03b8(yl|x)for(x, yw, yl)in the mini-batch Dmusing the policy model\n7: Predict the probabilities \u03c0ref(yw|x)and\u03c0ref(yl|x)for(x, yw, yl)in the mini-batch Dmusing the reference model\n8: Calculate the function u(x, yw, yl) =\u03b2log\u03c0\u03b8(yw|x)\n\u03c0ref(yw|x)\u2212\u03b2log\u03c0\u03b8(yl|x)\n\u03c0ref(yl|x)\u25b7Eq.\n13\n9: Compute the sequential KL divergence DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8)for(x, yw)in the mini-batch Dm\n10: Compute the sequential KL divergence DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)for(x, yl)in the mini-batch Dm\n11: ifMethod MisTDPO 1then\n12: Calculate the function \u03b4(x, yw, yl) =\u03b2DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)\u2212\u03b2DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8) \u25b7Eq.\n15\n14: else if Method MisTDPO 2then\n15: Calculate the function \u03b42(x, yw, yl) =\u03b2DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)\u2212sg(\u03b2DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8)) \u25b7Eq.\nExperiments on IMDb Dataset\nIn this experiment, besides our proposed methods TDPO 1\nandTDPO 2, we also implemented the DPO algorithm\nfor fair comparison.\nWe implement\nthe DPO, TDPO 1, and different versions of TDPO 2algo-\nrithms with varying the parameter \u03b1.\nFrom the figure, we\nnotice that although DPO establishes an efficient frontier,\nTDPO 1andTDPO 2outperform DPO in terms of diver-\ngence versus reward on the frontier, achieving higher reward\nwhile maintaining low KL divergence.\nWe also implemented\nversions of TDPO 2with\u03b1\u2208 {1,1.\nIn contrast, TDPO 2exhibits superior control over KL di-\nvergence, achieving better divergence efficiency compared\nto DPO.\n4, TDPO 1tends to result\nin an increased sequential KL divergence on the preferred\nresponse subset, thereby exhibiting a weaker capacity for\nKL divergence adjustment compared to TDPO 2.\nTDPO 2\nmaintains a more balanced sequential KL divergence on\nboth dispreferred and preferred response subsets, contribut-\ning to its ability to achieve a superior frontier.\n7\nToken-level Direct Preference Optimization\n/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015/uni00000011/uni00000018/uni00000014/uni00000018/uni00000011/uni00000013/uni00000014/uni0000001a/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000013\nDKL( ref)\n/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047DPO\nTDPO1\nTDPO2/uni00000003/uni00000003=0.\n1\nTDPO2/uni00000003/uni00000003=0.\n3\nTDPO2/uni00000003/uni00000003=0.\n5\nTDPO2/uni00000003/uni00000003=0.\nWe implemented DPO, TDPO 1, and different versions of TDPO 2with respect to the parameter \u03b1.\nBoth TDPO 1and\nTDPO 2outperform DPO in terms of the frontier, with TDPO 2showing further improvement over TDPO 1.\nTDPO 2exhibit superior regulation over KL divergence compared to\ntheTDPO 1and DPO algorithm.\nComparison of DPO, TDPO 1andTDPO 2in terms of\nthe trade-off between Alignment (accuracy) and Diversity (en-\ntropy) on the Anthropic HH dataset.\n196\nTDPO 1(ours ) 60.\n727\nTDPO 2(ours ) 67.\nExperiments on Anthropic HH Dataset\nNext, we evaluate the performance of TDPO 1andTDPO 2\non the Anthropic HH dataset.\nSubsequently, we train TDPO 1,TDPO 2, DPO\n(Rafailov et al.\nTDPO 2not only sur-\npasses DPO, f-DPO and TDPO 1in terms of accuracy but\nalso excels in entropy, achieving a superior balance between\nalignment and diversity.\nTo further assess the ability of TDPO 1andTDPO 2to\nalign with human preferences, we evaluated the win rates\nof responses generated by models trained with different\nalgorithms against chosen responses on the test set of the HH\ndataset, the result is illustrated in the Figure 4.\nCompared to\nthe SFT model, the DPO, TDPO 1, and TDPO 2algorithms\nbetter align with human preferences, achieving win rates not\nless than 50% against chosen responses at temperature 0.\nThis demonstrates that both TDPO 1, and TDPO 2possess\na strong capability to align with human preferences.\n/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013\n/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni00000048/uni00000050/uni00000053/uni00000048/uni00000055/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni0000003a/uni0000004c/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048\nSFT\nTDPO1DPO\nTDPO2fDPO(FKL)\nFigure 4.\nExperiments on MT-Bench\nTo comprehensively evaluate TDPO 1, and TDPO 2in\nterms of generation quality, we conducted pairwise com-\nparisons on the MT-Bench using models trained on the\nAnthropic HH dataset.\nIt reveals\nthatTDPO 2achieves a higher win rate compared to other\nalgorithms, indicating its ability to assist LLMs in generat-\ning higher-quality responses.\n0 20 40 60 80 100 120 140 160TDPO2\nvs TDPO1TDPO2\nvs DPOTDPO2\nvs PPOTDPO2\nvs SFT\n27.\nMT-Bench comparison between SFT ,PPO ,DPO ,\nTDPO 1andTDPO 2methods.\nConclusion\nIn this work, we introduced Token-level Direct Preference\nOptimization (TDPO), an innovative token-level fine-tuning\napproach for Large Language Models (LLMs) aimed at\naligning more closely with human preferences.\nBy employ-\ning the token-wise optimization with forward KL divergence\nconstraints and converting the Bradley-Terry model into a\ntoken-level preference model, TDPO addresses key chal-\nlenges in divergence efficiency and content diversity, sur-\npassing traditional methods like Direct Preference Optimiza-\ntion (DPO) and PPO-based RLHF in tasks such as controlled\nsentiment generation and single-turn dialogues.\nDeriving the TDPO Objective Under the Bradley-Terry Model\nTheorem A.\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Try to extract the most relevant section: search for 'loss' and 'TDPO' in the first 15 pages\nimport re\nwith open('./22-TokenDPO/TokenDPO-main/paper.pdf', 'rb') as f:\n    reader = PyPDF2.PdfReader(f)\n    text = ''\n    for page in reader.pages[:15]:\n        text += page.extract_text() + '\\n'\n\n# Find all paragraphs containing 'loss' or 'TDPO'\nloss_paragraphs = re.findall(r'([^.]*?(?:loss|TDPO)[^.]*\\.)', text, re.IGNORECASE)\nfor para in loss_paragraphs:\n    print(para.strip())"}, "success": true}, {"id": 22, "timestamp": "2025-05-19T19:43:15.371444", "source": "agent", "message": "Reading file: /workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py", "action": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_sfhNSsaDL1WasBT3bZQpiHt9", "model_response": {"id": "chatcmpl-BZ5MYR5g0QQbUCZSCp854fHeKTSRn", "created": 1747701794, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"view\",\"path\":\"/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py\",\"view_range\":[101,146]}", "name": "str_replace_editor"}, "id": "call_sfhNSsaDL1WasBT3bZQpiHt9", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 45, "prompt_tokens": 12888, "total_tokens": 12933, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 9216}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.052094, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 65767, "completion_tokens": 702, "cache_read_tokens": 56704, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"path": "/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py", "start": 0, "end": -1, "thought": "", "impl_source": "oh_aci", "view_range": [101, 146]}}, {"id": 23, "timestamp": "2025-05-19T19:43:15.381600", "source": "agent", "message": "I read the file /workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py.", "cause": 22, "observation": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_sfhNSsaDL1WasBT3bZQpiHt9", "model_response": {"id": "chatcmpl-BZ5MYR5g0QQbUCZSCp854fHeKTSRn", "created": 1747701794, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"view\",\"path\":\"/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py\",\"view_range\":[101,146]}", "name": "str_replace_editor"}, "id": "call_sfhNSsaDL1WasBT3bZQpiHt9", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 45, "prompt_tokens": 12888, "total_tokens": 12933, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 9216}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "Here's the result of running `cat -n` on /workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py:\n   101\tdef _tdpo_get_batch_logps(logits: torch.FloatTensor, reference_logits: torch.FloatTensor, labels: torch.LongTensor,\n   102\t                          average_log_prob: bool = False):\n   103\t    \"\"\"Compute the kl divergence/log probabilities of the given labels under the given logits.\n   104\t\n   105\t    Args:\n   106\t        logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)\n   107\t        reference_logits: Logits of the reference model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)\n   108\t        labels: Labels for which to compute the log probabilities. Label tokens with a value of -100 are ignored. Shape: (batch_size, sequence_length)\n   109\t        average_log_prob: If True, return the average log probability per (non-masked) token. Otherwise, return the sum of the log probabilities of the (non-masked) tokens.\n   110\t\n   111\t    Returns:\n   112\t        Several tensors of shape (batch_size,) containing the average/sum kl divergence/log probabilities of the given labels under the given logits.\n   113\t    \"\"\"\n   114\t    assert logits.shape[:-1] == labels.shape\n   115\t    assert reference_logits.shape[:-1] == labels.shape\n   116\t\n   117\t    labels = labels[:, 1:].clone()\n   118\t    logits = logits[:, :-1, :]\n   119\t    reference_logits = reference_logits[:, :-1, :]\n   120\t\n   121\t    loss_mask = (labels != -100)\n   122\t\n   123\t    # dummy token; we'll ignore the losses on these tokens later\n   124\t    labels[labels == -100] = 0\n   125\t\n   126\t    vocab_logps = logits.log_softmax(-1)\n   127\t\n   128\t    reference_vocab_ps = reference_logits.softmax(-1)\n   129\t    reference_vocab_logps = reference_vocab_ps.log()\n   130\t\n   131\t    per_position_kl = (reference_vocab_ps * (reference_vocab_logps - vocab_logps)).sum(-1)\n   132\t    per_token_logps = torch.gather(vocab_logps, dim=2, index=labels.unsqueeze(2)).squeeze(2)\n   133\t    per_reference_token_logps = torch.gather(reference_vocab_logps, dim=2, index=labels.unsqueeze(2)).squeeze(2)\n   134\t\n   135\t    logps_margin = per_token_logps - per_reference_token_logps\n   136\t\n   137\t    if average_log_prob:\n   138\t        return (logps_margin * loss_mask).sum(-1) / loss_mask.sum(-1), \\\n   139\t               (per_position_kl * loss_mask).sum(-1) / loss_mask.sum(-1), \\\n   140\t               (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n   141\t    else:\n   142\t        return (logps_margin * loss_mask).sum(-1), \\\n   143\t            (per_position_kl * loss_mask).sum(-1), \\\n   144\t            (per_token_logps * loss_mask).sum(-1)\n   145\t\n   146\t\n", "extras": {"path": "/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py", "impl_source": "oh_aci"}}, {"id": 24, "timestamp": "2025-05-19T19:43:16.891319", "source": "agent", "message": "Reading file: /workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py", "action": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_R7FYAeHB3ZHfdoM9umG87cYL", "model_response": {"id": "chatcmpl-BZ5MZrewe2eyVVYOUG0CJvjq93jww", "created": 1747701795, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"view\",\"path\":\"/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py\",\"view_range\":[1,45]}", "name": "str_replace_editor"}, "id": "call_R7FYAeHB3ZHfdoM9umG87cYL", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 45, "prompt_tokens": 13682, "total_tokens": 13727, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 12928}}, "service_tier": "default"}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.060426, "accumulated_token_usage": {"model": "gpt-4.1-2025-04-14", "prompt_tokens": 79449, "completion_tokens": 747, "cache_read_tokens": 69632, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"path": "/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/trainers.py", "start": 0, "end": -1, "thought": "", "impl_source": "oh_aci", "view_range": [1, 45]}}]