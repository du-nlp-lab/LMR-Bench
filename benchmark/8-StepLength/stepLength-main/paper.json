{
  "1": "The Impact of Reasoning Step Length on Large Language Models\nMingyu Jin1*, Qinkai Yu2*, Shu Dong3, Haiyan Zhao4,\nWenyue Hua1, Yanda Meng5, Yongfeng Zhang1, Mengnan Du4,\n1Rutgers University, 2University of Liverpool, 3Northwestern University,\n4New Jersey Institute of Technology, 5University of Exeter\n{mingyu.jin, yongfeng.zhang, wenyue.hua}@rutgers.edu, sgqyu9@liverpool.ac.uk\ndongshu2024@u.northwestern.edu, Y.M.Meng@exeter.ac.uk, {hz54, mengnan.du}@njit.edu\nAbstract\nChain of Thought (CoT) is significant in im-\nproving the reasoning abilities of large lan-\nguage models (LLMs). However, the corre-\nlation between the effectiveness of CoT and the\nlength of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we\nhave conducted several empirical experiments\nto explore the relations. Specifically, we de-\nsign experiments that expand and compress the\nrationale reasoning steps within CoT demon-\nstrations while keeping all other factors con-\nstant. We have the following key findings. First,\nthe results indicate that lengthening the reason-\ning steps in prompts, even without adding new\ninformation into the prompt, considerably en-\nhances LLMs’ reasoning abilities across multi-\nple datasets. Alternatively, shortening the rea-\nsoning steps, even while preserving the key\ninformation, significantly diminishes the rea-\nsoning abilities of models. This finding high-\nlights the importance of the number of steps in\nCoT prompts and provides practical guidance\nto make better use of LLMs’ potential in com-\nplex problem-solving scenarios. Second, we\nalso investigated the relationship between the\nperformance of CoT and the rationales used in\ndemonstrations. Surprisingly, the result shows\nthat even incorrect rationales can yield favor-\nable outcomes if they maintain the requisite\nlength of inference. Third, we observed that\nthe advantages of increasing reasoning steps\nare task-dependent: simpler tasks require fewer\nsteps, whereas complex tasks gain significantly\nfrom longer inference sequences. The code is\navailable at https://github.com/MingyuJ\n666/The-Impact-of-Reasoning-Step-Len\ngth-on-Large-Language-Models\n1\nIntroduction\nToday, the advent of large language models\n(LLMs) and their advanced prompting strategies\n*Equal contribution\nInput\nAccuracy 1\nAccuracy 2\nAccuracy 3\n. . .\n. . .\n. . .\nReasoning \nStep N\n. . .\n. . .\n. . .\n. . .\nK more\nReasoning \nSteps\nFigure 1: From left to right: zero-shot CoT, few-shot\nCoT, and few-shot CoT with more reasoning steps. For\nfew-shot COT, there is a direct linear correlation be-\ntween step count and accuracy.\nhas marked a significant progression, especially\nin classical NLP tasks (Kojima et al., 2023; Wei\net al., 2022; Shao et al., 2023; Lyu et al., 2023;\nJin et al., 2024). A key innovation among these is\nthe Chain of Thought (CoT) prompting technique\n(Kojima et al., 2023; Wang et al., 2023; Zhang\net al., 2022), known for its efficacy in multi-step\nproblem solving. This technique, reflecting hu-\nman sequential reasoning, has shown remarkable\neffectiveness in various challenges, including cross-\ndomain, length-generalization, and cross-lingual\ntasks. The CoT approach, with its logical, step-by-\nstep methodology, offers crucial interpretability in\ncomplex problem-solving scenarios. Interestingly,\nWang et al. found that even incorrect but coher-\nent rationales can improve reasoning performance,\nhighlighting the value of logical continuity (Wang\net al., 2023). Building on this, Fu et al. intro-\nduced complexity-based prompting, significantly\nimproving accuracy and setting new benchmarks\n(Fu et al., 2023). This research further explores\nthe relationship between the length of reasoning\nsteps and the accuracy of conclusions, deepening\narXiv:2401.04925v4  [cs.CL]  22 Jun 2024\n",
  "2": "our understanding of effective problem-solving in\nNLP.\nDespite its promising results, the research com-\nmunity has yet to reach a consensus on the precise\nmechanics of how and why CoT and its variations\nfunction effectively. This knowledge gap means\nthat enhancing CoT performance is still a field of\nexploration, largely reliant on trial-and-error ap-\nproaches. There still lack established systematic\nmethodologies for improving CoT’s effectiveness,\nleaving researchers to rely on conjecture and exper-\nimentation. This situation underscores a significant\nopportunity in the field: to develop a deeper, more\nstructured understanding of CoT’s inner workings.\nSuch advancement would not only demystify the\ncurrent process, but also pave the way for more\nreliable and efficient applications of this technique\nin various complex NLP tasks.\nIn this study, we aim to investigate the hypoth-\nesis that the reasoning steps are the most crucial\nelement in the effectiveness of CoT prompts. This\nhypothesis stems from the observation that reason-\ning steps are a common element in both zero-shot\nand few-shot CoT approaches. We conduct ex-\nperiments to investigate this by maintaining strict\ncontrol over variables. Particularly, when incor-\nporating new reasoning steps, we ensured that no\nadditional knowledge was introduced. For the zero-\nshot experiments, we tweaked the initial prompt\nfrom “let’s think step by step” to “let’s think step\nby step, you must think more steps”. Then for few-\nshot setting, we design experiments that expand\nthe rationale reasoning steps within CoT demon-\nstrations, while keeping all other factors constant.\nOur first set of experiments evaluated the improve-\nment in zero-shot and few-shot performance using\nAuto-CoT (Zhang et al., 2022) with our strategic\nintervention. Subsequently, we assessed the ac-\ncuracy of different methods across varying step\nlengths. We then extended our investigation to\ncompare the effectiveness of our strategy on dif-\nferent LLMs such as GPT-3.5 and GPT-4. Our\nfindings revealed a significant correlation between\nthe length of the reasoning chain and the capabil-\nities of LLMs, within certain limits. Intriguingly,\nwhen we introduced misleading information into\nthe reasoning chain, the performance still showed\nimprovement. This highlighted a pivotal insight:\nThe key factor appears to be the length of the think-\ning chain rather than its accuracy. We have the\nfollowing key findings, which we hope can help\nthe community better improve CoT performance.\n• For few-shot COT, there is a direct linear cor-\nrelation between step count and accuracy. This\nprovides a quantifiable approach for optimizing\nCoT prompting in complex reasoning. Specifi-\ncally, lengthening the reasoning steps in prompts\nconsiderably enhances LLMs’ reasoning abilities\nacross multiple datasets. Alternatively, shorten-\ning reasoning steps, even while preserving key\ninformation, significantly diminishes the reason-\ning abilities of models.\n• Even incorrect rationales can yield favorable out-\ncomes if the required length of inference is main-\ntained. For example, in mathematical problems,\nerrors in intermediate numbers have a minor im-\npact due to their process-oriented nature.\n• The advantages of increasing reasoning steps are\ntask-dependent: simpler tasks necessitate fewer\nsteps, whereas more complex tasks gain signifi-\ncantly from longer inference sequences.\n• Increased reasoning steps in zero-shot CoT can\nalso significantly improve LLM accuracy. To\nvalidate this, we altered the initial prompt from\n“Let’s think step by step\" to “Let’s think step by\nstep, you must think more steps.\" This modifica-\ntion led to a noticeable enhancement in the rea-\nsoning abilities of the LLMs, particularly evident\nin datasets involving mathematical problems.\n2\nRelated Works\nIn this section, we summarize two lines of literature\nthat are most relevant to ours.\n2.1\nCoT Prompting\nThe recent surge in computational power has paved\nthe way for the rise of expansive language mod-\nels.\nWith increasing complexity, these models\nhave unlocked emerging capabilities, notably in-\ncontext learning and CoT reasoning (Wei et al.,\n2022; Brown et al., 2020; Schaeffer et al., 2023).\nIn their seminal work, Brown et al. discovered\nthe ability of large-scale language models to lever-\nage in-context learning (ICL) (Brown et al., 2020).\nICL strategy involves weaving input-output exam-\nples directly into the prompt, allowing ready-to-\nuse large language models to perform impressively\nwithout the need for task-specific fine-tuning. How-\never, despite its promise, this end-to-end method-\nology often falters when confronting complex rea-\nsoning challenges.\nBuilding on this, Wei et al. demonstrated that\nintegrating a series of logical reasoning steps into\n",
  "3": "the model demonstrations, called CoT prompting,\nsignificantly refines the reasoning capabilities of\nlarge language models (Wei et al., 2022). CoT\nprompting not only deepens the model’s grasp of\nthe nuanced questions and their underlying logic\nbut also yields an articulated sequence of reasoning\nsteps. Zhang et al.’s “Auto-CoT\" method repre-\nsents a significant advancement in the field of AI\nreasoning (Zhang et al., 2022). By automating the\nCoT process, it addresses complex problems more\neffectively. And then Yao et al. introduced the\n“Tree of Thoughts\" (ToT) framework, an evolution\nof the Chain of Thought approach for language\nmodel inference (Yao et al., 2023). ToT allows lan-\nguage models to explore different units of text as\nintermediate steps in problem-solving. This frame-\nwork enables more deliberate decision-making by\nconsidering multiple reasoning paths.\n2.2\nPreliminary Work on Analyzing CoT\nThe development and understanding of CoT rea-\nsoning in AI have evolved over time, marked by\nsignificant contributions from various researchers.\nInitially, Madaan and Yazdanbakhsh (Madaan and\nYazdanbakhsh, 2022) explored the decomposition\nof prompts into symbols, patterns, and texts, ex-\namining the effects of CoT through counterfactual\nprompting. This study laid the groundwork for un-\nderstanding how different components of a prompt\ninfluence AI reasoning. Besides, several studies\nfurthered this understanding. Tang et al. (Tang\net al., 2023) investigated the role of semantics in\nCoT reasoning, uncovering a reliance on seman-\ntic knowledge from pre-training and challenges in\nsymbolic reasoning. Around the same time, Wang\net al. focused on the impact of demonstration selec-\ntion in CoT, revealing that the relevance and order\nof reasoning are more critical than the accuracy of\nreasoning chains (Wang et al., 2023).\nTheoretical perspectives also emerged recently,\noffering deeper insights into the mechanics of CoT.\nFor example, Li et al. conceptualized CoT as a\nmulti-step combinatorial function, illustrating its\nrole in simplifying in-context learning for complex\nquestions (Li et al., 2023). Feng et al. theoreti-\ncally demonstrated the sufficiency of a fixed-size\nTransformer for computational tasks and dynamic\nplanning within CoT frameworks (Fu et al., 2023).\nFurther contributions in this field included those\nof Merrill and Sabharwal, who observed that CoT\ncan improve reasoning abilities, with improve-\nments scaling with the number of intermediate\nsteps (Merrill and Sabharwal, 2023). Wu et al. em-\nployed gradient-based feature attribution methods\nto assess the robustness of CoT against question\nvariations and perturbations (Wu et al., 2023).\n3\nAnalyzing Methods\nIn this section, we propose an analysis to exam-\nine the relationship between the reasoning steps\nand the chain-of-thought (CoT) prompting perfor-\nmance. Our central hypothesis is that the reason-\ning steps are the most critical component of CoT\nprompts, enabling language models to apply more\nlogical reasoning when generating responses. To\ntest this, we design experiments that expand and\ncompress the rationale reasoning steps within CoT\ndemonstrations, while keeping all other factors con-\nstant. Specifically, we systematically vary only the\nnumber of reasoning steps, without introducing\nnew reasoning content or removing existing rea-\nsoning content. We evaluate both zero-shot and\nfew-shot CoT prompts in the following sections.\nThe overall experimental procedure is illustrated in\nFigure 2. Through this controlled analysis, we aim\nto shed light on how CoT influences the LLM’s\nability to produce logically sound responses.\n3.1\nPreliminary\nZero-Shot-CoT (Kojima et al., 2023) is a template-\nbased zero-shot prompt for chain-of-thought rea-\nsoning. The core idea is to add “Let’s think step by\nstep” or other similar text. Compared to Zero-Shot-\nCoT, Few-Shot-CoT provides more examples in\nthe prompt for chain-of-thought reasoning. Among\nthem, Manual-CoT (Wei et al., 2022), Auto-CoT\n(Zhang et al., 2022) are two popular methods.\nManual-CoT: Manual-CoT prompting relies on a\nfew manually designed demonstrations, each com-\nposed of a question and a reasoning chain leading\nto an answer, to improve language models’ reason-\ning performance.\nAuto-CoT: Auto-CoT eliminates the need for man-\nual demonstration design by automatically con-\nstructing demonstrations through clustering test\nquestions to select diverse examples and generat-\ning reasoning chains using the language model’s\nown zero-shot reasoning capability.\n3.2\nAnalyzing Zero-shot CoT\nIn the zero-shot scenario, we modified the initial\nprompt from “Let’s think step by step\" to “Let’s\nthink step by step, you must think more steps.\"\n",
  "4": "Think of word\nRead the question again\nRepeat State\nSelf-Verification\nMake Equation\nQ: A coin is heads up. Jeff does not flip the coin. Jen flips the coin. Giselle flips the coin. Noel does not flip the coin. Is the\ncoin still heads up? Note that ?flip? here means ?reverse?.\nBased on this question, \nthe stem of the sentence \nis coin, heads up, flip. \nThink coin: The coin is \na ... Answer: Yes\nThe question is: Given \nthat a coin is initially \nheads up and only Jen \nand Giselle flip it, is the \ncoin ... Answer: Yes\nThe state of the coin \nfrom the beginning is \nheads up. Rachel does \nnot flip the coin, this \nmeans ... Answer: Yes\nThe coin is a double \nsides coin. So the \nanswer can only be \n''yes'' or ''no'' ... Answer: \nYes\nLet's represent Heads \nup = 1. Tails up = -1. \nWe start with the coin \nas heads up, so we start \nwith 1 ... Answer: 1\nReasoning Steps Expansion of CoT Rationales\nReasoning Steps Compression of CoT Rationales\nThink coin: The coin is \na ... Answer: Yes\nCoin is initially heads \nup ... Answer: Yes\nThe state of the coin \nfrom the ... Answer: No\nThe answer are ''yes'' or \n''no'' ... Answer: Yes\nHeads up = 1. Tails up \n= -1 ... Answer: -1\nFigure 2: Increase the length of the thinking chain through the method in the figure, and compress the thinking\nchain without losing information as much as possible.\nThis change was implemented because, unlike the\nFew-shot CoT context, we cannot introduce addi-\ntional reasoning steps in the example. By alter-\ning the initial prompt, we guide the LLM to en-\ngage in more extensive thinking. This approach is\ncrucial as it enhances the model’s accuracy with-\nout the need for incremental training or additional\nexample-driven adjustments typical in few-shot\nCoT scenarios. This refined strategy ensures a\nmore comprehensive and detailed reasoning pro-\ncess, thereby significantly improving the model’s\nperformance in zero-shot settings.\n3.3\nAnalyzing Few-shot CoT\nIn this section, we aim to modify the reasoning\nchains within CoT rationales, either by adding or\ncompressing reasoning steps. The goal is to exam-\nine how changes in reasoning structure influence\nLLM decision-making. During rationale expansion,\nwe will avoid introducing any new task-relevant in-\nformation. This isolates reasoning steps as the only\nvariable under study.\nTo this end, we plan to investigate the following\nstrategies to expand the reasoning steps for dif-\nferent LLM applications. There are usually fixed\npatterns in the way people think about a problem,\nfor example, by repeating the question over and\nover again to gain a deeper understanding, by cre-\nating mathematical equations to reduce the burden\non memory, by analyzing the meaning of words\nin the question to help understand the topic, by\nsummarizing the current state to simplify the de-\nscription of the topic. Based on the inspiration of\nZero-Shot-CoT and Auto-CoT, we expected the\nprocess of CoT to become a standardized pattern,\nand lead to the right result by restriction on the di-\nrection of CoT thinking in the prompt section. The\ncore of our approach is to simulate the process of\nhuman thinking and reshape the chain of thought.\nWe give five general prompt strategies in Table 6\nin the Appendix.\n• Think About The Word: This strategy is to ask\nthe model to interpret the word and rebuild the\nknowledge base. Typically a word has multiple\ndifferent meanings, and the effect of this is to get\nthe model to think outside the box and reinterpret\nthe words in the problem based on the generated\ninterpretations. This process does not introduce\nnew information. In the prompt, we give exam-\nples of the words that the model is thinking about,\nand the model automatically picks words for this\nprocess based on the new question.\n• Read the question again: Read the questions\nrepeatedly to reduce the interference of other\n",
  "5": "Table 1: Case of Think About The Word, select a few words and ask the model to think about the meaning in text.\nThink About The Word\nPrompt:\nQ: Could someone in Tokyo take a taxi to the Metropolitan Museum of Art?\nA: Let’s think step by step. The stem of the sentence is Tokyo, take a taxi, Metropolitan Museum of Art. Think about Tokyo...\nThink about taking a taxi... Think about Metropolitan Museum of Art... Inference: Tokyo is in Japan and the Metropolitan\nMuseum of Art is in New York. The two places are separated by the sea, so you can’t take a taxi there. The answer is no.\nQ: Could a chipmunk fit 100 chocolate chips in his mouth?\nAnswer:\nA: Based on this question, the stem of the sentence is chipmunk, fit, 100 chocolate chips, mouth. Think about chipmunk...\nThink about fit... Think about chocolate chips... Inference: Chipmunks are small animals with relatively small mouths, so it’s\nunlikely that a chipmunk would be able to fit 100 chocolate chips in its mouth at once. The answer is no.\nPrediction: no\nGround Truth: no\ntexts on the chain of thought. In short, we let the\nmodel remember the questions.\n• Repeat State: Similar to repeated readings, we\ninclude a small summary of the current state af-\nter a long chain of reasoning, aiming to help the\nmodel simplify its memory and reduce the inter-\nference of other texts in the CoT.\n• Self-Verification: Humans will check if their\nanswers are correct when answering questions.\nTherefore, before the model gets the answer, we\nadd a self-verification process to judge whether\nthe answer is reasonable based on some basic\ninformation.\n• Make Equation: For mathematical problems,\nMake Equations can help humans summarize and\nsimplify memory. And for some problems that\nrequire the assumption of an unknown number x,\nestablishing an equation is an essential process.\nWe simulated this process and let the model try\nto make equations in mathematical problems.\nOverall, our prompt strategies all saw corre-\nsponding patterns in the model’s responses. We\ngive an example in Table 1, examples of the other\nfour strategies can be found in the appendix. In\nSection 4 we will perform a quantitative analysis\nto validate the effectiveness of our strategies. We\nassume that each additional strategy is equivalent\nto increasing the reasoning step length by one.\n4\nExperimental Results\nWe conduct experiments to answer the following\nresearch questions: RO1: What is the relationship\nof rational reasoning steps in demonstrations with\nCoT performance? (Section 4.2) RO2: Can we con-\nfirm that the reasoning steps are the only factor that\naffects LLM performance? (Section 4.3) RO3: Will\ncompressing reasoning steps in few-shot demon-\nstrations hurt LLM performance? (Section 4.4),\nRO4: Can we observe the scaling phenomenon,\ni.e., the required reasoning steps be related to the\nsize of the LLMs? (Section 4.5) and RO5: What is\nthe influence of questions in rationale towards the\nLLM reasoning ability? (Section 4.6)\n4.1\nExperimental Setups\nWe introduce the general experimental setups.\nDatasets and Models. We evaluate our proposal\non eight dataset (MultiArith (Roy and Roth, 2015),\nGSM8K (Cobbe et al., 2021), AQuA (Ling et al.,\n2017), SingleEq (Koncel-Kedziorski et al., 2015),\nSAVMP (Patel et al., 2021), Letter (Wei et al.,\n2022), Coin (Wei et al., 2022), Strategyqa (Geva\net al., 2021)). We employ three models to validate\nthe performance of our proposed method, which\nare: text-davinci-002 (Brown et al., 2020), GPT-\n3.5-turbo-1106 (Ouyang et al., 2022), GPT-4 (Ope-\nnAI, 2023). All models are accessed via the Ope-\nnAI API key.\nPrompt. We have shown the proposed process\npipeline in Section 3 Analyzing Methods. The\nexperimental part follows the same approach.\nBaselines. We compare our methods with four\nbaseline methods: Zero-Shot (Kojima et al., 2023),\nZero-Shot-CoT (Kojima et al., 2023), Manual-CoT\n(Wei et al., 2022), Auto-CoT (Zhang et al., 2022).\nThe results are in the Table 2.\nEvaluation Metrics. Accuracy is used to assess\na model’s ability on classification tasks and is\ncommonly used for multichoice and yes/no tests:\nAccuracy = Ncorrect/Ntotal.\nImplementation Details:\n• Add reasoning steps: This process deploys GPT-\n4 to modify the Zero-Shot-CoT prompt demo\n",
  "6": "Table 2: Comparison of accuracy of our method with four baselines on eight datasets\nModels\nArithmetic\nCommonsense\nSymbolic\nMultiArith\nGSM8K\nAQuA\nSingleEq\nSVAMP\nStrategy\nLetter\nCoin\nZero-Shot\n40±1.0\n30.4±1.7\n29.9±1.8\n82.7±1.3\n56±1.0\n59±1.0\n43±1.0\n79.8±1.2\nZero-Shot-CoT\n91.5±1.2\n64.1±1.1\n55.6±1.3\n87.43±0.25\n79.99±1.41\n58.34±1.56\n69±1.0\n93±1.0\nManual-CoT\n93.5±0.1\n64.7±1.5\n55±1.0\n92.1±0.2\n82.3±0.3\n65.3±1.4\n75±0.0\n92.7±0.1\nAuto-CoT\n94±0.0\n65.8±0.9\n65±0.0\n92±0.0\n81.9±0.3\n65.3±0.5\n73.5±0.2\n93±0.0\nMust Think More Step (Zero-Shot-CoT)\n95.2±0.3\n76.1±0.1\n62.11±0.24\n87.43±0.16\n79.99±0.18\n72.6±0.21\n69±0.0\n97±0.0\nAdd Reasoning Step (Manual-CoT)\n97±0.0\n70.1±0.3\n62.5±0.5\n88.97±0.27\n85.2±0.2\n68.86±0.27\n77.8±0.4\n97.3±0.1\nAdd Reasoning Step (Auto-CoT)\n97.2±0.1\n78.8±0.2\n64.03±0.36\n92.71±0.14\n83.7±0.2\n70.26±0.19\n71.2 ±0.5\n99±0.0\nTable 3: The Case of Wrong Prompt, change one of the step in the chain of thought and preserve overall coherence\nOriginal Prompt\nWrong Prompt\nQ: Joan has 10 books. Tom has 38 books.\nHow many books do they have?\nRationale: Let’s think step by step. Joan has 10 books.\nTom has 38 books. we have equation books = 10 + 38 = 48.\nThey have 10 + 38 = 48 books together.\nQ: Megan had 217 markers. Robert gave her 109 more markers.\nHow many markers does Megan have altogether?\nQ: Joan has 10 books. Tom has 38 books.\nHow many books do they have?\nRationale: Let’s think step by step. Joan has 10 books.\nTom has 38 books. we have equation books = 10 + 8 = 48.\nThey have 10 + 38 = 48 books together.\nQ: Megan had 217 markers. Robert gave her 109 more markers.\nHow many markers does Megan have altogether?\n0\n2\n4\n6\nNumber of Add Step\n92\n94\n96\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(a) MultiArith\n0\n2\n4\n6\nNumber of Add Step\n65\n70\n75\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(b) GSM8K\n0\n2\n4\n6\nNumber of Add Step\n55\n60\n65\n70\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(c) AQuA\n0\n2\n4\n6\nNumber of Add Step\n90\n92\n94\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(d) SingleEq\n0\n2\n4\n6\nNumber of Add Step\n82\n83\n84\n85\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(e) SAVMP\n0\n2\n4\n6\nNumber of Add Step\n66\n68\n70\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(f) strategyqa\n0\n2\n4\n6\nNumber of Add Step\n70.0\n72.5\n75.0\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(g) Letter\n0\n2\n4\n6\nNumber of Add Step\n90.0\n92.5\n95.0\n97.5\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(h) Coin\nFigure 3: Linear Relationship Between Step Quantity and Accuracy\ngenerated by “let’s think step by step\" to con-\ntain the five reasoning steps we mentioned in\nSection 3 so that we can define the number of\nsteps and the types of steps to be contained in\nthe demo. We then input the demo as a prompt.\nWe completed the following experiment with this\napproach.\n• Reasoning-Step Compression: In the express-\ning experiment, we focused on executing a com-\npression attack on the rationale inference chain\nwithin the few-shot CoT. The process involved\nrandomly selecting two consecutive sentences\nand employing GPT-4 to effectively merge them.\nWe then input the prompt: “Please compress the\nfollowing two sentences without losing any infor-\nmation, and make them as concise as possible”.\nThis method was designed to implement a tar-\ngeted compression on the reasoning chain.\n• Answer cleaning: We follow the structure pro-\nposed by Zero-Shot-CoT to select the final an-\nswer. After the model response output is ob-\ntained, this structure selects only part of the an-\nswer that first satisfies the answer format.\n4.2\nRelationship Between Steps and Accuracy\nTable 2 compares accuracy on eight datasets\nfrom three categories of reasoning tasks using\nGPT-3.5-turbo-1106. All results are averaged over\nthree random runs. Our SOTA results are based on\nexperimental results from the optimal performance\nstep for each data set.\nOur zero-shot CoT is\nbased on Section 2.1, and the Add Reasoning\nStep (Manual-CoT), and Add Reasoning Step\n(Auto-CoT) is based on Section 2.2.\n",
  "7": "MultiArith\nGSM8K\nAQuA\nSingleEq\nsvamp\nstrategy\nLetter\nCoin\n0\n20\n40\n60\n80\n100\nAccuracy\nPrompt with Wrong Answer\nPrompt with True Answer\nFigure 4: Compare the accuracy of the prompt with the\ntrue answer and the prompt with the wrong answer\nBenefiting from the fact that we have standard-\nized the thought chain process, it is possible to\nquantify the increase in accuracy due to the in-\ncreased steps in rationales of COT demonstrations.\nWe conducted experiments to answer the RO1:\nWhat is the relationship of rational reason-ing steps\nin demonstrations with CoT performance? This\nexperiment is completed with GPT-3.5-turbo-1106,\nand the results are given in Figure 3. We found that\nLLM reasoning ability improves in all datasets dur-\ning an effective CoT process, i.e. with the addition\nof up to six steps of additional thought processes.\nIn other words, we found a certain linear relation-\nship between accuracy and CoT complexity.\n4.3\nEffect of Prompt with Wrong Answer\nTo answer RO2: Are reasoning steps the only fac-\ntor that affects LLM performance? We made the\nfollowing attempt. Change a step in the prompt to\nan incorrect answer to see if it affects the chain of\nthought. So, for this experiment, we change all the\nprompts to carry one error. For a concrete example,\ncheck the Table 3. For Arithmetic-type questions,\neven if there is a deviation in one of the results of\nthe prompt, the effect on the chain of thought in\nthe reasoning process is minimal, so we believe\nthat the large language model learns more about\nthe chain of thought patterns in the prompt than the\nsingle computation when solving Arithmetic-type\nproblems. For logic problems similar to those in\nthe Coin dataset, a deviation in one of the prompt’s\nresults often brings about the fragmentation of the\nentire chain of thought. We completed this experi-\nment with GPT-3.5-turbo-1106 and guaranteed per-\nformance based on the optimal number of steps for\neach data set derived from the previous experiment.\nThe results are shown in Figure 4.\nMultiArith\nGSM8K\nAQuA\nSingleEq\nsvamp\nstrategy\nLetter\nCoin\n0\n20\n40\n60\n80\n100\nAccuracy\nZero-Shot-CoT\nCompression\nNo Compression\nFigure 5: Compare the accuracy of the prompt with\nCompression and the prompt with No Compression\n4.4\nCompressing Reasoning Steps\nIn previous sections, we have demonstrated that\nincreasing reasoning steps could improve the LLM\nreasoning accuracy. In this section, our aim is to\nanswer RO3: Will compressing reasoning steps in\nfew-shot demonstrations hurt LLM performance?\nTo this end, we conduct the reasoning steps com-\npression experiment, and we employed the tech-\nnique outlined in the experimental setup to con-\ndense the reasoning process in examples from both\nthe baseline automated chain of thought (Auto-\nCoT) and the few-shot chain of thought (Few-Shot-\nCoT), aiming to reduce their number of inference\nsteps. The result is shown in Figure 5. It revealed\na notable decline in performance, which regressed\nto a level essentially equivalent to that achieved\nby the zero-shot method. It further indicates that\nincreasing COT rationale reasoning steps could\nimprove COT performance and the vice versa.\n4.5\nPerformance on Different Size Models\nIn this chapter, our focus is to answer RO4: Can\nwe observe the scaling phenomenon, i.e., the re-\nquired reasoning steps be related to the size of the\nLLMs? We examine the average number of infer-\nence steps utilized in various models, including\ntext-davinci-002, GPT-3.5-turbo-1106, and GPT-4.\nWe conducted experiments on GSM8K calculating\nthe average inference step for each model to reach\npeak performance. This dataset has the largest per-\nformance difference with text-davinci-002, GPT-\n3.5-turbo-1106, and GPT-4 out of our 8 datasets. It\ncan be observed that the model with the worst ini-\ntial performance, text-davinci-002, our strategy has\nthe highest boosting effect. For the model with the\nbest initial performance, GPT-4, has the highest tol-\nerance to our strategy (no performance decreases).\nWe show the result in Figure 6.\n",
  "8": "Table 4: Making deliberate alterations to sample questions\nOriginal Prompt\nQ: Wendy uploaded 45 pictures to Facebook. She put 27 pics into one album and put the rest into 9 different albums. How many\npictures were in each album?\nRationale: “Let’s think step by step. First, Wendy uploaded 45 pictures in total. Second, Wendy put 27 pictures into one album.\nThat means that Wendy put the remaining 18 pictures into 9 different albums. Each album would have 2 pictures.\"\nPred_ans: “2\"\nGold_ans: “2\"\nMaking deliberate alterations\nQ: Wendy uploaded 66 pictures to Facebook. She put 89 pics into one album and put the rest into 7 different albums. How many\npictures were in each album?\nRationale: “Let’s think step by step. First, Wendy uploaded 54 pictures in total. Second, Wendy put 27 pictures into one album.\nThat means that Wendy put the remaining 12 pictures into 6 different albums. Each album would have 7 pictures.\"\nPred_ans: “2\"\nGold_ans: “2\"\n0\n2\n4\n6\n8\n10\nNumber of Add Step\n20\n40\n60\n80\nAccuracy\ntext-davinci-002\nGPT-3.5-turbo-1106\nGPT4\nFigure 6: Comparing the accuracy with different size\nmodel on dataset GSM8K.\n4.6\nInfluence of Questions in CoT Examples\nIn our case study, we aim to answer RO5: What\nis the influence of questions in rationale towards\nthe LLM reasoning ability? We want to explore\nwhether changing the reasoning in CoT will affect\nCoT performance. Since we are mainly studying\nthe impact of the inference step on performance,\nwe need to confirm that the question has no impact\non performance. So we have chosen two datasets\nand two CoT methods (auto-CoT and few-shot-\nCoT) for this investigation: MultiArith (Roy and\nRoth, 2015) and GSM8K (Cobbe et al., 2021) in\nGPT-3.5-turbo-1106. Our experimental approach\ninvolves making deliberate alterations to sample\nquestions within these mathematical datasets, such\nas varying the content of the questions in Table 4.\nRemarkably, initial observations indicate that these\nmodifications have only minimally impacted per-\nformance like Table 5.\nThis preliminary finding suggests that the length\nof the steps involved in the reasoning process,\nrather than the nature of the questions themselves,\npredominantly influences the reasoning capabilities\nof large-scale models.\nTable 5: Accuracy comparison of models on different\ndatasets\nModels\nMultiArith\nGSM8K\nZero-Shot\n40±1.0\n30.4±1.7\nZero-Shot-CoT\n91.5±1.2\n64.1±1.1\nManual-CoT\n93.5±0.1\n64.7±1.5\nAuto-CoT\n94±0.0\n65.8±0.9\nChanging Question\n(Manual-CoT)\n92.9±0.1\n62.1±1.7\nChanging Question\n(Auto-CoT)\n92.5±0.1\n63.5±1.0\nAdd Reasoning Step\n(Manual-CoT)\n97±0.0\n70.1±0.3\nAdd Reasoning Step\n(Auto-CoT)\n97.2±0.1\n78.8±0.2\nAdd Reasoning Step\nand Changing Question\n(Manual-CoT)\n96.6±0.1\n69.6±0.2\nAdd Reasoning Step\nand Changing Question\n(Auto-CoT)\n95.7±0.1\n75.2±0.2\n5\nConclusions and Future Work\nIn this work, we make a critical contribution to\nunderstanding and optimizing CoT in LLMs, espe-\ncially in the realm of complex reasoning tasks. Our\nextensive research on the CoT technique in natural\nlanguage processing, particularly with large lan-\nguage models like GPT-3, GPT-3.5, and GPT-4, has\nled to key insights. We found a notable correlation\nbetween the length of the reasoning chain and the\nperformance of these models. Interestingly, longer\nreasoning chains improve model performance, even\nwhen they contain misleading information. This\nsuggests that the chain’s length is more crucial than\nits factual accuracy for effective problem-solving.\nThese findings provide valuable guidance for refin-\ning CoT strategies, highlighting the significance of\nreasoning length in complex NLP tasks.\nOur next step is to analyze the long and short\nreasoning steps of LLM inference via explaindeter-\n",
  "9": "mineOur objective is to ascertain whether longer\ninferential steps correlate with broader neuronal\nengagement. To illustrate this, we intend to use\nvisualization techniques to analyze activation pat-\nterns between long and short reasoning steps.\n6\nLimitation\nIn this work, we provide an experimental analy-\nsis of how CoT works. We focus specifically on\nmanipulating the reasoning steps in CoT prompts\nand measuring the impact on model performance.\nHowever, we did not deeply analyze the underly-\ning mechanisms behind why increasing reasoning\nsteps improves performance. This includes either\ntheoretical analysis or explainability analysis to an-\nalyze the internal workings of LLMs, and further\ninvestigation could provide more insight. Addi-\ntionally, our study was limited to certain datasets\nand models like GPT-3.5 and GPT-4. Testing on\nmore diverse tasks and newer models could reveal\ndifferent trends.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\nTushar Khot. 2023. Complexity-based prompting for\nmulti-step reasoning. Preprint, arXiv:2210.00720.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346–\n361.\nMingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng\nZeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao,\nKai Mei, Yanda Meng, Kaize Ding, Fan Yang,\nMengnan Du, and Yongfeng Zhang. 2024.\nEx-\nploring concept depth: How large language models\nacquire knowledge at different layers?\nPreprint,\narXiv:2404.07066.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023.\nLarge\nlanguage models are zero-shot reasoners. Preprint,\narXiv:2205.11916.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing algebraic word problems into equa-\ntions. Transactions of the Association for Computa-\ntional Linguistics, 3:585–597.\nYingcong Li, Kartik Sreenivasan, Angeliki Giannou,\nDimitris Papailiopoulos, and Samet Oymak. 2023.\nDissecting chain-of-thought: A study on composi-\ntional in-context learning of mlps. arXiv preprint\narXiv:2305.18869.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158–167.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023.\nFaithful chain-of-\nthought reasoning. Preprint, arXiv:2301.13379.\nAman Madaan and Amir Yazdanbakhsh. 2022. Text\nand patterns: For effective chain of thought, it takes\ntwo to tango. arXiv preprint arXiv:2209.07686.\nWilliam Merrill and Ashish Sabharwal. 2023. The ex-\npressive power of transformers with chain of thought.\nPreprint, arXiv:2310.07923.\nR OpenAI. 2023.\nGpt-4 technical report. arxiv\n2303.08774. View in Article, 2:13.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730–27744.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are nlp models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080–2094.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing. Association for Computational\nLinguistics.\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo.\n2023. Are emergent abilities of large language mod-\nels a mirage? arXiv preprint arXiv:2304.15004.\nZhihong Shao, Yeyun Gong, Yelong Shen, Min-\nlie Huang, Nan Duan, and Weizhu Chen. 2023.\nSynthetic prompting: Generating chain-of-thought\ndemonstrations for large language models. Preprint,\narXiv:2302.00618.\n",
  "10": "Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng,\nSong-Chun Zhu, Yitao Liang, and Muhan Zhang.\n2023. Large language models are in-context seman-\ntic reasoners rather than symbolic reasoners. arXiv\npreprint arXiv:2305.14825.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2023. Self-consistency improves chain\nof thought reasoning in language models. Preprint,\narXiv:2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nSkyler Wu, Eric Meng Shen, Charumathi Badrinath,\nJiaqi Ma, and Himabindu Lakkaraju. 2023. Ana-\nlyzing chain-of-thought prompting in large language\nmodels via gradient-based feature attributions. arXiv\npreprint arXiv:2307.13339.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023.\nTree of thoughts:\nDeliber-\nate problem solving with large language models.\nPreprint, arXiv:2305.10601.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\narXiv preprint\narXiv:2210.03493.\n",
  "11": "A\nAppendix\nTable 6\nThink About The Word\nprompt:\nQ: Could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?\nA: Let’s think step by step. The stem of the sentence is Tokyo, take a taxi, Metropolitan Museum of Art. Think about Tokyo...\nThink about take a taxi... Think about Metropolitan Museum of Art... Inference: Tokyo is in Japan and the Metropolitan\nMuseum of Art is in New York. The two places are separated by the sea, so you can’t take a taxi there. Since two places are\nseparated by the sea, you can’t take a taxi there. The answer is yes.\nQ: Could a chipmunk fit 100 chocolate chips in his mouth?\nAnswer:\nA: Based on this question, The stem of the sentence is chipmunk, fit, 100 chocolate chips, mouth. Think about chipmunk...\nThink about fit... Think about chocolate chips... Inference: Chipmunks are small animals with relatively small mouths, so it’s\nunlikely that a chipmunk would be able to fit 100 chocolate chips in its mouth at once. The answer is no.\npred : no\nGT : no\nRead the question again\nprompt:\nQ: Mark’s father gave him $85. Mark bought 10 books, each of which cost $5. How much money does Mark have left?\nA: Let’s think step by step. The question is: How much money does Mark have left? So we need to calculate How much\nmoney does Mark have left. Start looking for information about money now. Mark’s father gave him $85. Mark bought 10\nbooks, each of which cost $5. That means that Mark spent $50 on books. So we have equation money = +85 - 50 = 35. So\nMark has $85 - $50 = $35 left. So the answer is 35.\nQ: During a visit to an orchard, Charlie picked 0.1666 of a bag of Golden Delicious apples, 0.1666 of a bag of Macintosh\napples, and 0.333 of a bag of Cortland apples. How many bags of fruit did Charlie pick in total?\nAnswer:\nA: The question is: How many bags of fruit did Charlie pick in total. So we need to calculate how many bags of fruit did\nCharlie pick in total. Start looking for information about bags now. Charlie picked 0.166 of a bag of Golden Delicious apples,\n0.166 of a bag of Macintosh apples, and 0.3333 of a bag of Cortland apples. We have to add these fractions together to find\nout how many bags of fruit Charlie picked in total. 0.1666 + 0.1666 + 0.3333 = 0.6666. This means that Charlie picked a\ntotal of 0.666 bags of fruit. The answer is 0.6666.\npred : 0.6666\nGT : 0.6666\n",
  "12": "Table 7\nRepeat State\nprompt:\nQ: A coin is heads up. Janette does not flip the coin. Stacey flips the coin. Ronny flips the coin. Kim does not flip the coin. Is\nthe coin still heads up? Note that \"flip\" here means \"reverse\".\nA: Let’s think step by step. The state of the coin from the beginning is heads up. Janette does not flip the coin, so the coin\nremains heads up. coin is heads up. Stacey flips the coin, so the coin is now tails up. coin is now tail up. Ronny flips the coin,\nso the coin is now heads up again. Kim does not flip the coin, so the coin remains heads up. coin is head up. The answer is\nyes.\nQ: A coin is heads up. Jeff does not flip the coin. Jen flips the coin. Giselle flips the coin. Noel does not flip the coin. Is the\ncoin still heads up? Note that \"flip\" here means \"reverse\".\nAnswer:\nThe state of the coin from the beginning is heads up. Rachel does not flip the coin, This means that the state of the coin does\nnot change. so the coin remains heads up. Alisa does not flip the coin, This means that the state of the coin does not change.\nso the coin remains heads up. Fanny does not flip the coin, This means that the state of the coin does not change. so the coin\nremains heads up. Jeffery does not flip the coin, This means that the state of the coin does not change. so the coin remains\nheads up. So the final answer is yes, the coin is still heads up. The answer is yes.\npred : yes\nGT : yes\nSelf-Verification\nprompt:\nQ: Take the last letters of each words in \"Alina Alessandra Amina Bianca\" and concatenate them.\nA: Let’s think step by step. There is four words. So the answer will consist of four letters. Explain concatenate: concatenate\nis a term used in computer programming and mathematics, referring to the operation of combining two or more strings,\nsequences, or sets in a linear order. The last letter of \"Alina\" is \"a\". The last letter of \"Alessandra\" is \"a\". The last letter of\n\"Amina\" is \"a\". The last letter of \"Bianca\" is \"a\". So we have four letters. So the final answer is \"aaaa\". The answer is aaaa.\nQ: Take the last letters of each words in \"Jeff Jen Giselle Noel\" and concatenate them.\nAnswer:\nThere is four words. So the answer will consist of four letters. Explain concatenate: concatenate is a term used in computer\nprogramming and mathematics, referring to the operation of combining two or more strings, sequences, or sets in a linear\norder. The last letter of \"Jeff\" is \"f\". The last letter of \"Jen\" is \"n\". The last letter of \"Giselle\" is \"e\". The last letter of \"Noel\"\nis \"l\". So we have four letters. So the final answer is \"fnel\". The answer is fnel.\npred : fnel\nGT : fnel\nMake Equation\nprompt:\nQ: 5 children were riding on the bus. At the bus stop 63 children got off the bus while some more got on the bus. Then there\nwere 14 children altogether on the bus. How many more children got on the bus than those that got off?\nA: Let’s think step by step. first step, 5 children were riding on the bus. We know 5 children is on the bus. second step,There\nwere 63 children that got off the bus. third step, some more got on the bus we define as unknown x. fourth step, 14 children\nremained on the bus, which means we can calculate unknow x.we have equation x+5-63 = 14, now we know x is 72. fifth\nstep, Therefore, 72 - 63 = 9. 9 more children got on the bus than those that got off. The answer is 9.\nQ: Rachel had to complete 8 pages of math homework. If she had to complete 3 more pages of math homework than reading\nhomework How many pages did she have to complete in all?\nAnswer:\nExplain the problem. The problem is asking us to find out how many pages of homework Rachel had to complete in total\ngiven that she had to complete 8 pages of math homework and 3 more pages of math homework than reading homework.\nFind the necessary information. First, we know that Rachel had to complete 8 pages of math homework. Let’s call this \"M\".\nSecond, we know that she had to complete 3 more pages of math homework than reading homework.Let’s call the number\nof reading homework pages \"R\". Find the solution. Based on the information given, we can use the equation M = R + 3\nto represent Rachel’s math homework in terms of her reading homework. So, if Rachel had to complete 8 pages of math\nhomework, we can substitute M = 8 into the equation and solve for R. 8 = R + 3, R = 8 - 3, R = 5. Therefore, Rachel had to\ncomplete 5 pages of reading homework. Now, we can find the total number of pages Rachel had to complete by adding the\nmath and reading homework together.\nTotal = 8 (math) + 5 (reading) Total = 13\nSo, in total, Rachel had to complete 13 pages of homework.\npred : 13\nGT : 13\n",
  "13": "Table 8\nExample of Wrong Prompt\nArithmetic Dataset\nprompt:\nQ: Joan has 10 books. Tom has 38 books. How many books do they have\nA: Let’s think step by step. Joan has 10 books. Tom has 38 books. we have equation books = 10 +8 = 48. They have 10 + 38\n= 48 books together.\nCommonsense Dataset\nprompt:\nQ: Could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?\nLet’s think step by step. The stem of the sentence is Tokyo, take a taxi, Metropolitan Museum of Art. Explain Tokyo: Tokyo\nis the capital city of Japan and one of the most populous metropolitan areas in the world. Explain Metropolitan Museum of\nArt: is a art museums in New York City. Inference: Tokyo is in Japan and the Metropolitan Museum of Art is in New York.\nThe two places are separated by the sea, so you can take a taxi there.\nSymbolic Dataset\nprompt:\nQ: Take the last letters of each words in ’Tim Candace Cecil Misael’ and concatenate them.\nA: Let’s think step by step. Explain letters: letters can have various meanings depending on the context, such as Alphabetic\nCharacters, Correspondence, Literature and Books. There is four words. So the answer will consist of four letters. The last\nletter of ’Tim’ is ’m’. The last letter of ’Candace’ is ’e’. The last letter of \"Cecil\" is ’l’. The last letter of \"Misael\" is \"l\". So\nwe have four letters. So the final answer would be \"mel\".\n"
}