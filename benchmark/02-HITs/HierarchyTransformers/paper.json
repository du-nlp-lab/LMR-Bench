{
  "1": "Language Models as Hierarchy Encoders\nYuan He\nUniversity of Oxford\nyuan.he@cs.ox.ac.uk\nZhangdie Yuan\nUniversity of Cambridge\nzy317@cam.ac.uk\nJiaoyan Chen\nThe University of Manchester\njiaoyan.chen@mancheser.ac.uk\nIan Horrocks\nUniversity of Oxford\nian.horrocks@cs.ox.ac.uk\nAbstract\nInterpreting hierarchical structures latent in language is a key limitation of current\nlanguage models (LMs). While previous research has implicitly leveraged these\nhierarchies to enhance LMs, approaches for their explicit encoding are yet to be\nexplored. To address this, we introduce a novel approach to re-train transformer\nencoder-based LMs as Hierarchy Transformer encoders (HITs), harnessing the\nexpansive nature of hyperbolic space. Our method situates the output embedding\nspace of pre-trained LMs within a Poincaré ball with a curvature that adapts to the\nembedding dimension, followed by training on hyperbolic clustering and centripetal\nlosses. These losses are designed to effectively cluster related entities (input as\ntexts) and organise them hierarchically. We evaluate HITs against pre-trained LMs,\nstandard fine-tuned LMs, and several hyperbolic embedding baselines, focusing\non their capabilities in simulating transitive inference, predicting subsumptions,\nand transferring knowledge across hierarchies. The results demonstrate that HITs\nconsistently outperform all baselines in these tasks, underscoring the effectiveness\nand transferability of our re-trained hierarchy encoders.1\n1\nIntroduction\nIn the field of Natural Language Processing (NLP) and related areas, the emergence of transformer-\nbased language models (LMs) such as BERT (encoder-based) [1], GPT (decoder-based) [2], and\nthe more recent large language models (LLMs) like GPT-4 [3] and Llama 2 [4], has marked a\nsignificant progression. Nonetheless, these models face a notable challenge in effectively encoding\nand interpreting hierarchical structures latent in language. This limitation has been highlighted by\nseveral studies, including those by [5] and [6], which employed prompt-based probes to reveal the\nlimited hierarchical knowledge in pre-trained LMs, and the work by [7], which demonstrated these\nmodels’ struggles with capturing the transitivity of hierarchical relationships.\nPrior research has explored various methods to infuse hierarchical information into LM training.\nCommon approaches include classification-based fine-tuning using sentence head embedding with a\nclassification layer [8] or few-shot prompting with an answer mapping to classification labels [6].\nTo further pre-train, or re-train2 LMs on a corpus constructed from hierarchical data, [9] converted\n1See GitHub repository: https://github.com/KRR-Oxford/HierarchyTransformers; Datasets\non Zenodo: https://zenodo.org/doi/10.5281/zenodo.10511042 or the Huggingface Hub: https:\n//huggingface.co/Hierarchy-Transformers; and HIT models also on the Huggingface Hub.\n2In this work, the term re-train refers to train LMs on a new corpus without modifying its architecture; it\nis distinguished from standard fine-tuning that involves adding task-specific layers which lead to additional\nlearnable parameters.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2401.11374v4  [cs.CL]  21 Nov 2024\n",
  "2": "structural representations into textual formats to align the masked language modeling objective.\nOthers, like [10] and [11], have focused on extracting analogous and contrasting examples from\nhierarchical structures for a similarity-based contrastive learning objective. The aforementioned\nstudies leveraged hierarchical information as implicit signals to augment LMs, yet no existing works\nspecifically targeted the explicit encoding of hierarchies with LMs.\nberry\nblueberry\nraspberry\nfruit\npc\ncomputer\ne-device\nlaptop\nentity\nberry\nblueberry\nraspberry\nfruit\nlaptop\ncomputer\ne-device\npc\nentity\nphone\nphone\nPre-trained\nHierarchy Re-trained\nFigure 1: Illustration of how hierarchies are explic-\nitly encoded in HITs. The square (d-dimensional\nhyper-cube) refers to the output embedding space\nof transformer encoder-based LMs whose final ac-\ntivation function is typically tanh, and the circum-\nscribed circle (d-dimensional hyper-sphere) refers\nto the Poincaré ball of radius\n√\nd. The distance and\nnorm metrics involved in our hyperbolic losses are\ndefined w.r.t. this manifold.\nTo bridge this gap, we introduce a novel ap-\nproach to re-train transformer encoder-based\nLMs as Hierarchy Transformer encoders\n(HITs). Inspired by the efficacy of hyperbolic\ngeometry in representing hierarchical structures\n[12, 13], we propose the hyperbolic cluster-\ning and centripetal losses tailored for LM re-\ntraining. As illustrated in Figure 1, transformer\nencoder-based LMs typically use a tanh acti-\nvation function in the last layer, which maps\neach embedding dimension to the range [−1, 1].\nConsequently, the output embeddings of LMs\nare confined within a unit d-dimensional hyper-\ncube. Leveraging this characteristic, we utilise\na Poincaré ball of radius\n√\nd, whose boundary\ncircumscribes3 the output embedding space of\nLMs. The metrics for distance and norm used\nin our hyperbolic losses are defined w.r.t. this\nspecific manifold. After re-training, entities are\nnot only clustered according to their relatedness\nbut also hierarchically organised.\nIn evaluating HITs, we compare their performance against pre-trained LMs, standard fine-tuned LMs,\nand previous hyperbolic embedding models in the Multi-hop Inference and Mixed-hop Prediction\ntasks. The Multi-hop Inference task, following the setting in [13], involves training models on all\nasserted (i.e., one-hop) subsumptions and assessing their ability to infer transitive (i.e., multi-hop)\nsubsumptions. The Mixed-hop Prediction task is designed to mirror real-world scenarios, where\nmodels trained on incomplete hierarchy are applied to predict unknown subsumption relationships\nbetween arbitrary entity pairs. Additionally, we introduce a transfer learning setting, where models\ntrained on one hierarchy are tested on another. Our experiments utilise datasets derived from WordNet\n[14] and SNOMED CT [15],4 and transfer evaluation datasets from Schema.org [16], Food Ontology\n(FoodOn) [17], and Disease Ontology (DOID) [18]. The results show that HITs significantly surpass\nall baselines in these tasks, demonstrating their robustness to generalise from asserted to inferred and\nunseen subsumptions, and a promising potential in hierarchy-based semantic search.\n2\nPreliminaries\n2.1\nLanguage Models\nTransformer encoder-based LMs excel in providing fine-grained contextual word embeddings for\nenhanced language understanding. A key component of these models is the self-attention mechanism,\nwhich dynamically assigns importance to different segments of the input text, thereby capturing\nnuanced contextual semantics more effectively. Notable examples of such models include BERT\n[1] and RoBERTa [19], both of which utilise the masked language modelling objective during pre-\ntraining. This approach involves partially masking input sentences and prompting the model to predict\nthe masked tokens, using the unmasked surrounding text as context. For acquiring sentence-level\nembeddings, these models can be augmented with an additional pooling layer, applied over the token\nembeddings [20, 21]. Pooling strategies such as mean, max, and sentence head pooling are employed,\nwith their effectiveness varying across different applications. A contrastive learning objective is\noften applied for refining sentence-level semantics [21, 22]. Despite the rise of generative LLMs,\n3We ignore the vertices of the hyper-cube, as they are on the boundary and thus are undefined in the open\nPoincaré ball.\n4Results on WordNet are presented in Section 4, while results on SNOMED CT are presented in Appendix D.\n2\n",
  "3": "transformer encoder-based LMs maintain their importance, offering versatility and efficiency in tasks\nlike text classification and semantic search.\n2.2\nHyperbolic Geometry\nHyperbolic geometry, a form of non-Euclidean geometry, is featured by its constant negative Gaussian\ncurvature, a fundamental aspect that differentiates it from the flat, zero curvature of Euclidean\ngeometry. In hyperbolic space, distances between points increase exponentially as one moves towards\nthe boundary, making it inherently suitable for embedding hierarchical structures. This intuition\naligns with the tree embedding theorem based on δ-hyperbolicity, as discussed in [23] and [13].\nAmong the various models5 of hyperbolic geometry that are isometric6 to each other, the Poincaré\nball is chosen for its capacity to contain the the output embedding space of LMs directly, as explained\nin the second last paragraph of Section 1. The d-dimensional Poincaré ball with a negative curvature\n−c (where c > 0) is defined by the open ball Bd\nc = {x ∈Rd : ∥x∥2 < 1\nc}. The distance function in\nthis model, dependent on the curvature value c, is given by:\ndc(u, v) = 2\n√c tanh−1(√c∥−u ⊕c v∥)\n(1)\nIn this equation, u, v ∈Bd\nc, ∥·∥denotes the Euclidean norm, and ⊕c denotes the Möbius addition\n[24] defined as:\nu ⊕c v = (1 + 2c⟨u, v⟩+ c∥v∥2)u + (1 −c∥u∥2)v\n1 + 2c⟨u, v⟩+ c2∥u∥2∥v∥2\n(2)\nHere, ⟨·, ·⟩denotes the inner product in Euclidean space. Note that for flat curvature c = 0, Bd\nc will\nbe Rd and ⊕c will be the Euclidean addition.\n2.3\nHierarchy\nWe define a hierarchy as a directed acyclic graph G(V, E) where V represents vertices that symbolise\nentities, and E represents edges that indicate the direct subsumption relationships asserted in the\nhierarchy. We can then derive indirect subsumption relationships T based on direct ones through\ntransitive reasoning. We borrow the notation from description logic to denote the subsumption\nrelationship as e1 ⊑e2, meaning that e1 is a sub-class of e2. Under the closed-world assumption, we\nconsider an edge (e1, e2) as a negative sample if (e1, e2) ̸∈E ∪T . Particularly, a hard negative is\nidentified when e1 and e2 are also siblings that share the same parent.\nExplicit hierarchies can often be derived from structured data sources such as taxonomies, ontologies,\nand knowledge graphs. A taxonomy and the Terminology Box (TBox) of an ontology intrinsically\ndefine subsumptions, whereas in knowledge graphs, hierarchical relationships are defined in a more\ncustomised manner. For instance, in WordNet, the hypernym relationship corresponds to subsumption.\n3\nHierarchy Transformer Encoder\nWe intend to propose a general and effective strategy to re-train transformer encoder-based LMs\nas Hierarchy Transformer encoders (HITs). To deal with arbitrary input lengths of entity names,\nwe employ an architecture similar to sentence transformers [21], incorporating a mean pooling\nlayer over token embeddings to produce sentence embeddings for entities. Note that some of the\nsentence transformer models have a normalisation layer after pooling; we exclude this layer because\nits presence will constrain the embeddings’ Euclidean norms to one, thus hindering hierarchical\norganisation of entity embeddings. It is also worth mentioning that these changes do not add in\nlearnable parameters besides the ones already in LMs, thus retaining the original architectures of\nLMs as encoders. As aforementioned, the output embedding space of these LMs is typically a\n5E.g., the Poincaré ball model, the Poincaré half plane model, and the hyperboloid model.\n6An isometry is a bijective distance-preserving transformation.\n3\n",
  "4": "d-dimensional hyper-cube because of the tanh activation function in the last layer. Thus, we can\nconstruct a Poincaré ball of radius\n√\nd (or equivalently, curvature value c = 1\nd) whose boundary\ncircumscribes the hyper-cube.7 Unlike previous hyperbolic embedding methods that utilise the entire\nhyperbolic space and often require a projection layer to manage out-of-manifold embeddings, our\nmethod ensures that embeddings are contained within a specific subset of this manifold. Empirical\nevidence supports that this subset sufficiently accommodates entities in high-dimensional space (see\nSection 4.5). Based on this curvature-adapted manifold, we propose the following two losses for\nhierarchy re-training.\nHyperbolic Clustering Loss This loss aims at clustering related entities and distancing unrelated\nones in the Poincaré ball. We formulate it in the form of triplet loss because related entities are\nnot equivalent but their semantic distances should be smaller than those between unrelated entities.\nFormally, the loss is defined as:\nLcluster =\nX\n(e,e+,e−)∈D\nmax(dc(e, e+) −dc(e, e−) + α, 0)\n(3)\nHere, inputs are presented in the form of triplet (e, e+, e−), where e+ is a parent entity of e, and e−\nis a negative parent entity of e; D denotes the set of these triplets; dc(·, ·) refers to the hyperbolic\ndistance function defined in Equation (1), and α is the hyperbolic distance margin. The bold letters\ndenote the embeddings of the corresponding entities.\nHyperbolic Centripetal Loss This loss ensures parent entities are positioned closer to the Poincaré\nball’s origin than their child counterparts, reflecting the natural expansion of hierarchies from the\norigin to the boundary of the manifold. The term “centripetal” is used to imply that the manifold’s\norigin represents an imaginary root entity for everything. Formally, the hyperbolic centripetal loss\nis defined as:\nLcentri =\nX\n(e,e+,e−)∈D\nmax(∥e+∥c −∥e∥c + β, 0)\n(4)\nAgain, inputs are the triplets in D, but only the child and parent entities (the positive subsumptions)\nare used to calculate the loss; ∥·∥c := dc(·, 0) refers to the hyperbolic norm; β is the hyperbolic norm\nmargin.\nThe overall hierarchy re-training loss, denoted as LHIT, is the linear combination of these two\nhyperbolic losses, defined as:\nLHIT = Lcluster + Lcentri\n(5)\npc\norigin\ne-device\nlaptop\nphone\ncomputer\npull towards\npush away\nFigure 2: Illustration of the impact of LHIT during\ntraining. In Euclidean space, it seems contradic-\ntory that both “phone” and “computer” are pulled\ntowards “e-device” but are also pushed away from\neach other. However, in principle this is not a prob-\nlem in hyperbolic space, where distances increase\nexponentially relative to Euclidean distances as\none moves from the origin to the boundary of the\nmanifold.\nIn Figure 2, we demonstrate the impact of LHIT\non entity embeddings. The entity “e-device”,\nbeing most general, is nearest to the origin. Sib-\nling entities, such as “phone” and “computer”,\n“laptop” and “pc”, are closer to their common\nparent than to each other, illustrating the effect\nof re-training to cluster related entities while\nmaintain hierarchical relationships.\nAs the HIT model functions as an encoder, it\ndoes not inherently support direct predictions\nof subsumption relationships. To address this,\nwe devise a probing function that leverages the\nhierarchy re-trained entity embeddings. This\nfunction aims to predict the subsumption rela-\ntionship for any given pair of entities (e1, e2),\nincorporating both the clustering and centripetal\nheuristics:\ns(e1 ⊑e2) = −(dc(e1, e2) + λ(∥e2∥c −∥e1∥c))\n(6)\nHere, λ > 0 represents a weighting factor applied to the centripetal heuristic component. The\nsubsumption score is structured to increase as the hyperbolic distance between e1 and e2 decreases,\n7We have also considered scaling down each dimension of the LM embeddings by\n√\nd to confine them within\na unit Poincaré ball, but we found that losses are harder to converge in this construction.\n4\n",
  "5": "Table 1: Statistics of WordNet (Noun), Schema.org, and FoodOn, including the numbers of entities\n(#Entity), direct subsumptions (#DirectSub), indirect subsumptions (#IndirectSub), and the dataset\nsplittings (#Dataset) for Multi-hop Inference and Mixed-hop Prediction tasks. Note that the numbers\nin #Dataset are counts of entity pairs rather than entity triplets.\nSource\n#Entity\n#DirectSub\n#IndirectSub\n#Dataset (Train/Val/Test)\nWordNet\n74,401\n75,850\n587,658\nmulti: 834K/323K/323K\nmixed: 751K/365K/365K\nSchema.org\n903\n950\n1,978\nmixed: -/15K/15K\nFoodOn\n30,963\n36,486\n438,266\nmixed: 361K/261K/261K\nDOID\n11,157\n11,180\n45,383\nmixed: 111K/31K/31K\nand/or as the relative difference in their hyperbolic norms (∥e1∥c −∥e2∥c) increases. Essentially, for\nthe model to predict e1 ⊑e2, it is expected that e1 and e2 are relatively closer in the Poincaré ball,\nwith e1 positioned further from the manifold’s origin compared to e2. The value for λ and the overall\nscoring threshold are to be ascertained through hyperparameter tuning on the validation set.\n4\nEvaluation\n4.1\nTask Definition\nMulti-hop Inference This task, following the setting in [13], aims to evaluate the model’s ability\nin deducing indirect, multi-hop subsumptions T from direct, one-hop subsumptions E, so as to\nsimulate transitive inference. We split T for validation and testing, denoted as Tval and Ttest,\nrespectively. For each positive subsumption (e, e+) involved, we sampled 10 negative parents e−for\ne, leading to 10 training triplets8. Following the criteria in Section 2.3, (e, e−) is a valid negative if\n(e, e−) ̸∈E ∪T . We further split the task into two settings: one with random negatives and another\nwith hard negatives, the latter mainly comprising sibling entities. Since not every entity has enough\nsiblings, we supplemented with random negatives that have been sampled to maintain a consistent\npositive-to-negative ratio of 1 : 10.\nMixed-hop Prediction This task aims to evaluate the model’s capability in determining the existence\nof subsumption relationships between arbitrary entity pairs, where the entities are not necessarily\nseen during training. We propose a challenging setting where models are trained on incomplete\ndirect subsumptions and examined on a mix of hold-out, unseen direct and indirect (mixed-hop)\nsubsumptions. We split E into training, validation, and test sets, denoted as Etrain, Eval, and Etest,\nrespectively. The final training, validataion, and test sets for this task are Etrain, Eval ∪Tval, and\nEtest ∪Ttest, respectively, where Tval and Ttest are re-used from the previous task. Again, each\npositive subsumption in these sets is paired with 10 negative samples, either randomly chosen or\nfrom sibling entities. Furthermore, an important factor that reflects the model’s generalisability is to\nexamine the transfer ability across hierarchies. To this end, we extend the mixed-hop prediction\ntask with a transfer setting where models trained on asserted training edges of one hierarchy are\ntested on arbitrary entity pairs of another.\nEvaluation Metrics For both Multi-hop Inference and Mixed-hop Prediction tasks, we utilise\nPrecision, Recall, and F1 score (abbreviated as F-score in latter discussion) as our primary metrics\nof evaluation. We have opted not to include Accuracy, as preliminary testing indicated a potential\nbias in this metric, with a misleadingly high score resulting from the much larger volume of negative\ncompared to positive samples. It is important to note that, although the training phase uses entity\ntriplets, the evaluation only involves entity pairs.\n4.2\nDataset Construction\nWe constructed the primary dataset from the noun hierarchy of WordNet [14] due to its comprehensive\nand structured representation of linguistic hierarchies. To assess the transferability and robustness\n810 training triplets are constructed from 11 entity pairs.\n5\n",
  "6": "across different domains, we additionally constructed datasets from ontologies that represent varied\nsemantic granularities and domains, namely Schema.org [16], Food Ontology (FoodOn) [17], and\nDisease Ontology (DOID) [18]. We retrieved WordNet from NLTK [25] and adopted pre-processing\nsteps similar to [12], utilising the hypernym relations between noun synsets to construct the hierarchy.\nFor Schema.org, FoodOn, and DOID, our pre-processing paralleled that in [6], transforming these\nontologies into hierarchies of named entities (details in Appendix A). To accommodate the textual\ninput requirements of LMs, we constructed an entity lexicon using the name attribute in WordNet\nand the rdfs:label property in the ontologies.9\nOn WordNet (Noun), FoodOn, and DOID, we adopt a consistent splitting ratio for the validation and\ntesting sets. Specifically, we allocate two separate 5% portions of the indirect subsumptions T to\nform Tval and Ttest, respectively. Similarly, two distinct 5% portions of the direct subsumptions E\nare used as Eval and Etest. As Schema.org is significantly smaller than the other hierarchies and only\nused for transfer evaluation, we split its entire E and T sets into halves for validation and testing,\nrespectively. Table 1 presents the extracted hierarchies’ statistics and the resulting datasets for the\nMulti-hop Inference and Mixed-hop Prediction tasks.\nIn addition to our main evaluation, we constructed a dataset from the widely-recognised biomedical\nontology SNOMED CT [15] and conducted futher evaluation. The relevant details are presented in\nAppendix D.\n4.3\nBaselines\nNaive Prior We first introduce a naive baseline (NaivePrior) that utilises the prior probability of\npositive subsumptions in the training set for prediction. Given that each positive sample is paired\nwith 10 negatives, the prior probability of a positive prediction stands at\n1\n11. Consequently, Precision,\nRecall, and F-score on the test set are all\n1\n11.\nPre-trained LMs We consider pre-trained LMs as baselines to illustrate their limitations in capturing\nhierarchical structure semantics. As outlined in Section 3, our focus is on LMs based on the sentence\ntransformer architecture [21]. Since these LMs are optimised for cosine similarities between sentences,\nwe devise the following probe for evaluation: for each entity pair (e1, e2), we compute the cosine\nsimilarity between the masked reference sentence “e1 is a ⟨mask⟩.” and the sample sentence “e1\nis a e2.”. These similarity scores serve as the subsumption scores, with thresholds identified via\ngrid search on the validation set. Note that although these LMs originate from masked language\nmodels, they cannot be easily probed via mask filling logits or perplexities as in [26] and [27] because\ntheir mask filling layers are not preserved in the released versions. We select three top-performing\npre-trained LMs from the sentence transformer library of different sizes, including all-MiniLM-L6-v2\n(22.7M), all-MiniLM-L12-v2 (33.4M), and all-mpnet-base-v2 (109M).\nFine-tuned LMs Fine-tuned LMs are used as baselines to demonstrate that despite the efficacy\nin various tasks, standard fine-tuning struggles to address this specific challenge. Following the\nBERTSubs approach outlined in [8], we employ pre-trained LMs with an added linear layer for binary\nclassification, and optimising on the Softmax loss. [8] have shown that this method outperforms\nvarious structure-based embeddings such as TransE [28] and DistMult [29], and also surpasses\nOWL2Vec* [30], which integrates both structural and textual embeddings, in subsumption prediction.\nHyperbolic Baselines Previous static hyperbolic embedding models are typically evaluated using\nthe Multi-hop Inference task. In our study, we select the Poincaré Embedding (PoincaréEmbed) [12]\nand the Hyperbolic Entailment Cone (HyperbolicCone) [13] as baselines on this task. However, their\nlack of inductive prediction capabilities prevents their evaluation on the Mixed-hop Prediction task\nand its transfer setting. Additionally, we include the hyperbolic GloVe embedding (PoincaréGloVe)\nas a baseline in our transfer evaluation. We select the best-performing PoincaréGloVe (50×2D\nwith an initial trick) pre-trained on a 1.4B token English Wikipedia dump. While PoincaréGloVe\nsupports inductive prediction, its effectiveness is limited by word-level tokenisation, rendering it less\neffective at handling unknown words. To address this, we employ NaivePrior as a fallback method\nfor entities that involve unknown words and cannot be predicted by PoincaréGloVe.\nMore details of our code implementation and experiment settings are presented in Appendix B.\n9We selected the first name (in English) if multiple names for one entity were available.\n6\n",
  "7": "Table 2: Multi-hop Inference and Mixed-hop Prediction test results on WordNet.\nRandom Negatives\nHard Negatives\nModel\nPrecision\nRecall\nF-score\nPrecision\nRecall\nF-score\nNaivePrior\n0.091\n0.091\n0.091\n0.091\n0.091\n0.091\nMulti-hop Inference (WordNet)\nPoincaréEmbed\n0.862\n0.866\n0.864\n0.797\n0.867\n0.830\nHyperbolicCone\n0.817\n0.996\n0.898\n0.243\n0.902\n0.383\nall-MiniLM-L6-v2\n0.160\n0.442\n0.235\n0.132\n0.507\n0.209\n+ fine-tune\n0.800\n0.513\n0.625\n0.764\n0.597\n0.670\n+ HIT\n0.864\n0.879\n0.871\n0.905\n0.908\n0.907\nall-MiniLM-L12-v2\n0.127\n0.585\n0.209\n0.108\n0.740\n0.188\n+ fine-tune\n0.811\n0.515\n0.630\n0.819\n0.530\n0.643\n+ HIT\n0.880\n0.927\n0.903\n0.910\n0.906\n0.908\nall-mpnet-base-v2\n0.281\n0.428\n0.339\n0.183\n0.359\n0.242\n+ fine-tune\n0.796\n0.501\n0.615\n0.758\n0.628\n0.687\n+ HIT\n0.897\n0.936\n0.916\n0.886\n0.912\n0.899\nMixed-hop Prediction (WordNet)\nall-MiniLM-L6-v2\n0.160\n0.438\n0.235\n0.131\n0.504\n0.208\n+ fine-tune\n0.747\n0.575\n0.650\n0.769\n0.578\n0.660\n+ HIT\n0.835\n0.877\n0.856\n0.882\n0.843\n0.862\nall-MiniLM-L12-v2\n0.127\n0.583\n0.209\n0.111\n0.625\n0.188\n+ fine-tune\n0.794\n0.517\n0.627\n0.859\n0.515\n0.644\n+ HIT\n0.875\n0.895\n0.885\n0.886\n0.857\n0.871\nall-mpnet-base-v2\n0.287\n0.439\n0.347\n0.197\n0.344\n0.250\n+ fine-tune\n0.828\n0.536\n0.651\n0.723\n0.622\n0.669\n+ HIT\n0.892\n0.910\n0.900\n0.869\n0.858\n0.863\n4.4\nResults\nThe effectiveness of our hierarchy re-training approach is evident from the results of both the Multi-\nhop Inference and Mixed-hop Prediction tasks on WordNet (see Table 2), as well as the Transfer\nMixed-hop Prediction task on Schema.org, FoodOn, and DOID for pre-trained LMs and models\ntrained on WordNet (see Table 3). In the following, we present several pivotal findings based on these\nresults.\nPerformance of HITs The HIT models, re-trained from LMs of various sizes, consistently out-\nperform their pre-trained and standard fine-tuned counterparts across all evaluation tasks. In the\nMulti-hop Inference task, HITs exhibit exceptional performance with F-scores ranging from 0.871 to\n0.916. This indicates a strong capability in generalising from asserted to transitively inferred entity\nsubsumptions. In the Mixed-hop Prediction task, F-scores ranging from 0.856 to 0.900 highlight the\neffectiveness of HITs in generalising from asserted to arbitrary entity subsumptions. For the Trans-\nfer Mixed-hop Prediction tasks, we selected all-MiniLM-L12-v2 as the pre-trained model because\nall-MiniLM-L12-v2+HIT attains comparable performance to all-mpnet-base-v2+HIT while it is\nmore computationally efficient owing to a smaller parameter size. Notably, all-MiniLM-L12-v2+HIT\nperforms better than pre-trained and fine-tuned all-MiniLM-L12-v2 on these transfer tasks by at least\n0.150 and 0.101 in F-scores, respectively.\nLimited Hierarchical Knowledge in Pre-trained LMs For the tasks on WordNet, all-mpnet-base-\nv2 achieves the highest F-scores among all the pre-trained models, yet these scores (e.g., 0.347 and\n0.250 on the Mixed-hop Prediction task with random negatives and hard negatives, respectively)\nare considerably lower compared to their fine-tuned (lagging by 0.304 and 0.419) and hierarchy\nre-trained (lagging by 0.553 and 0.613) counterparts. This disparity confirms findings from LM\nprobing studies such as those by [5] and [6], demonstrating the limited hierarchical knowledge in\npre-trained LMs.\n7\n",
  "8": "Table 3: Transfer Mixed-hop Prediction test results on Schema.org, FoodOn, and DOID.\nRandom Negatives\nHard Negatives\nModel\nPrecision\nRecall\nF-score\nPrecision\nRecall\nF-score\nNaivePrior\n0.091\n0.091\n0.091\n0.091\n0.091\n0.091\nTransfer Mixed-hop Prediction (WordNet →Schema.org)\nPoincaréGloVe\n0.485\n0.403\n0.441\n0.436\n0.415\n0.425\nall-MiniLM-L12-v2\n0.312\n0.524\n0.391\n0.248\n0.494\n0.330\n+ fine-tune\n0.391\n0.433\n0.411\n0.597\n0.248\n0.351\n+ HIT\n0.503\n0.613\n0.553\n0.408\n0.583\n0.480\nTransfer Mixed-hop Prediction (WordNet →FoodOn)\nPoincaréGloVe\n0.192\n0.224\n0.207\n0.189\n0.200\n0.195\nall-MiniLM-L12-v2\n0.135\n0.656\n0.224\n0.099\n0.833\n0.176\n+ fine-tune\n0.436\n0.382\n0.407\n0.690\n0.177\n0.282\n+ HIT\n0.690\n0.463\n0.554\n0.741\n0.385\n0.507\nTransfer Mixed-hop Prediction (WordNet →DOID)\nPoincaréGloVe\n0.265\n0.314\n0.287\n0.283\n0.318\n0.299\nall-MiniLM-L12-v2\n0.342\n0.451\n0.389\n0.159\n0.455\n0.235\n+ fine-tune\n0.585\n0.621\n0.603\n0.868\n0.179\n0.297\n+ HIT\n0.696\n0.711\n0.704\n0.810\n0.435\n0.566\nLimited Generalisation in Fine-tuned LMs The research by [8] illustrates that fine-tuned LMs\nperform well on single-hop subsumptions. Our observations concur, showing that fine-tuned LMs\nachieve comparable performance as HITs when assessed on just single-hop test samples. However,\ntheir effectiveness wanes when applied to arbitrary entity subsumptions. For the tasks on WordNet,\nfine-tuned LMs underperform HITs by 0.194 to 0.301 in F-scores. In the transfer task from WordNet\nto Schema.org, the fine-tuned all-MiniLM-L12-v2 model only marginally outperforms its initial state,\nwith an increase of around 0.02 in F-scores across both negative settings.\nPerformance of Hyperbolic Baselines The Multi-hop Inference task with random negatives follows\nthe evaluation in [13]. In this setup, both PoincaréEmbed and HyperbolicCone significantly\noutperform the pre-trained and standard fine-tuned LMs, and perform comparably to the HIT models.\nHowever, HyperbolicCone exhibits substantially worse performance in the hard negative setting;\nits low precision and high recall suggest difficulties in differentiating sibling entities that are closely\npositioned in the embedding space. In the transfer evaluation, PoincaréGloVe shows improved\nperformance over pre-trained and standard fine-tuned models on Schema.org. However, it does not\ndemonstrate a similar advantage on FoodOn and DOID, primarily due to its limited vocabulary, which\nallows it to predict almost all test samples on Schema.org but substantially fewer on the others.\nComparison of Random and Hard Negatives For the tasks on WordNet, hard negative settings\npresent greater challenges compared to random negative settings for all pre-trained LMs. This\nincreased difficulty, however, is not as pronounced in fine-tuned LMs and HITs. A plausible\nexplanation is that while hard negatives pose challenges, they simultaneously act as high-quality\nadversarial examples, potentially leading to more robust training outcomes. In the Transfer Mixed\nPrediction task, hard negative settings are generally more challenging than random negative settings.\nFor instance, in the WordNet-to-DOID transfer task, both fine-tuned and hierarchy re-trained all-\nMiniLM-L12-v2 models exhibit significantly higher F-scores in the random negative setting, with\ndifferences of 0.306 and 0.138 respectively, compared to the hard negative setting.\nCase Analysis on WordNet-to-DOID Transfer In the WordNet-to-DOID transfer task, the disparity\nin the “disease” category is notable: WordNet contains only 605 entities that are descendants of\n“disease”, compared to over 10K in DOID. Despite this significant difference, HIT models effectively\ntransfer knowledge, achieving F-scores of 0.704 (random negatives) and 0.566 (hard negatives).\nMore discussion on loss functions and an ablation study of loss margins are presented in Appendix C.\n8\n",
  "9": "4.5\nAnalysis of HIT Embeddings\nDistribution Figure 3 illustrates how WordNet entity embeddings generated by all-MiniLM-L12-\nv2+HIT distribute w.r.t. their hyperbolic norms.\nFigure 3: Distribution of WordNet entity embed-\ndings generated by HIT w.r.t. their hyperbolic\nnorms.\nTable 4: Statistical correlations between WordNet\nentities’ depths and their hyperbolic norms across\ndifferent hyperbolic models.\nHIT\nPoincaréEmbed\nHyperbolicCone\n0.346\n0.130\n0.245\nTable 5: Hyperbolic distances between the em-\nbeddings of selected entities (“computer”, “pc”,\n“fruit”, “berry”), along with their individual hyper-\nbolic norms (h-norm) and depths in WordNet.\ncomputer\npc\nfruit\nberry\ncomputer\n0.0\n5.9\n22.5\n24.9\npc\n5.9\n0.0\n25.2\n27.2\nfruit\n22.5\n25.2\n0.0\n6.72\nberry\n24.9\n27.2\n6.72\n0.0\nh-norm\n17.5\n19.1\n15.3\n16.6\ndepth\n9\n11\n9\n10\nThese norms effectively capture the natural ex-\npansion of the hierarchical structure, evidenced\nby an exponential rise in the number of child\nentities. A notable observation is the sharp de-\ncline in the number of entities when hyperbolic\nnorms exceed 23, suggesting that few entities\nreside at these higher levels. Additionally, the\nrange of entity hyperbolic norms, approximately\nfrom 8 to 24, indicates that a relatively small re-\ngion of the high-dimensional manifold suffices\nto accommodate all entities in WordNet.\nCorrelation In Table 4, we compare the Pear-\nson correlation coefficients across different hy-\nperbolic models to measure the linear relation-\nship between entities’ hyperbolic norms and\ntheir depths in WordNet. Our analysis shows\nthat all hyperbolic models lead to a positive cor-\nrelation between norms and depths, as expected.\nHowever, HIT demonstrates a stronger correla-\ntion than both PoincaréEmbed and Hyperbol-\nicCone.\nCase Study In Table 5, we showcase the effec-\ntiveness of HIT using selected entities: “com-\nputer”, “pc”10, “fruit”, and “berry”. The table\npresents the hyperbolic distances between these\nentities’ embeddings, their individual hyperbolic\nnorms, and their depths11 in the WordNet hier-\narchy. We can observe that: (i) closely related\nentities, such as “fruit” and “berry”, are signifi-\ncantly nearer to each other compared to more distant pairs; (ii) more specific entities like “pc” and\n“berry” are positioned further from the origin of the manifold than their ancestor entities; (iii) the\ndisparity in hyperbolic norms between “pc” and “computer” is greater compared to that between\n“fruit” and “berry”, reflecting the hierarchical depth where “pc” is a grandchild of “computer”, while\n“berry” is a direct child of “fruit”.\n5\nRelated Work\nPrompt-based probing is widely used for extracting knowledge from LMs. Studies like [5] utilised\ncloze-style prompts for hypernym detection, while [6] approached subsumption prediction similar to\nNatural Language Inference. [7] examined if LMs, when correctly predicting “A is a B” and “B is a\nC”, can consistently infer the transitive relationship “A is a C”. These studies collectively highlight\nthe limited capacity of pre-trained LMs in understanding hierarchical structures. Other research\nefforts, such as those by [10] and [11], have aimed to incorporate structural semantics into LMs for\nentity encoding. However, these largely focus on entity equivalence or similarity, with less emphasis\non hierarchical organisation.\nRegarding hyperbolic embeddings, methods like the Poincaré embedding [12] and the hyperbolic\nentailment cone [13] have effectively represented hierarchical structures. Despite their efficacy, these\ntechniques are inherently static, constrained by a fixed vocabulary of entities, and do not support\n10The full name “personal computer” is used for embedding.\n11Depth of an entity is the minimum number of hops to the root node. For hierarchies that do not have a root\nnode, we set up an imaginary root node when calculating the depth.\n9\n",
  "10": "inductive predictions about unseen data. Further explorations include learning word embeddings in\nhyperbolic space [31, 32]. These methods, however, are limited to word-level tokenisation and yield\nnon-contextual word representations. These shortcomings can be mitigated by integrating hyperbolic\nembeddings with transformer-based LMs. [33] has explored this direction, applying learnable layers\nto project LM embeddings into hyperbolic space for syntax parsing and sentiment analysis. Our\napproach diverges from theirs by focusing on training LMs as general hierarchy encoders without the\nneed for additional learnable parameters.\n6\nConclusion\nThis paper tackles the challenge of enabling language models to interpret and encode hierarchies. We\ndevise the hierarchy re-training approach that involves a joint optimisation on both the hyperbolic\nclustering and hyperbolic centripetal losses, aiming to cluster and organise entities according to their\nhierarchical relationships. The resulting HIT models demonstrate proficiency in simulating transitive\ninference and predicting subsumptions within and across hierarchies. Additionally, our analysis of\nHIT embeddings highlights their geometric interpretability, further validating the effectiveness of our\napproach.\n7\nLimitations and Future Work\nThis work does not address the potential loss of pre-trained language understanding resulted from\nhierarchy re-training. Also, the issue of entity naming ambiguity inherent in the dataset sources is not\ntackled, which could introduce noise into the training process.\nFor future work, several promising directions can be pursued: (i) investigating methods to measure\nand mitigate catastrophic forgetting, (ii) training a HIT model across multiple hierarchies, either for\ngeneral or domain-specific applications, (iii) extending HIT to accommodate multiple hierarchical\nrelationships within a single model, and (iv) developing hierarchy-based semantic search that contrasts\nwith traditional similarity-based approaches.\nAcknowledgments and Disclosure of Funding\nThis work was supported by Samsung Research UK (SRUK), and EPSRC projects OASIS\n(EP/S032347/1), UK FIRES (EP/S019111/1), and ConCur (EP/V050869/1). Special thanks to\nZifeng Ding for his valuable feedback during the rebuttal process.\nReferences\n[1] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1,\npage 2, 2019.\n[2] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. OpenAI, 2018.\n[3] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[5] Michael Hanna and David Mareˇcek. Analyzing bert’s knowledge of hypernymy via prompting.\nIn Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural\nNetworks for NLP, pages 275–282, 2021.\n[6] Yuan He, Jiaoyan Chen, Ernesto Jimenez-Ruiz, Hang Dong, and Ian Horrocks. Language model\nanalysis for ontology subsumption inference. In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages\n3439–3453, Toronto, Canada, July 2023. Association for Computational Linguistics.\n10\n",
  "11": "[7] Ruixi Lin and Hwee Tou Ng. Does bert know that the is-a relation is transitive? In Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers), pages 94–99, 2022.\n[8] Jiaoyan Chen, Yuan He, Yuxia Geng, Ernesto Jiménez-Ruiz, Hang Dong, and Ian Horrocks.\nContextual semantic embeddings for ontology subsumption prediction. World Wide Web, pages\n1–23, 2023.\n[9] Hao Liu, Yehoshua Perl, and James Geller. Concept placement using bert trained by trans-\nforming and summarizing biomedical ontology structure. Journal of Biomedical Informatics,\n112:103607, 2020.\n[10] Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier. Self-alignment\npretraining for biomedical entity representations. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 4228–4238, 2021.\n[11] Francis Gosselin and Amal Zouaq. Sorbet: A siamese network for ontology embeddings using\na distance-based regression loss and bert. In International Semantic Web Conference, pages\n561–578. Springer, 2023.\n[12] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical represen-\ntations. Advances in neural information processing systems, 30, 2017.\n[13] Octavian Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones for\nlearning hierarchical embeddings. In International Conference on Machine Learning, pages\n1646–1655. PMLR, 2018.\n[14] George A Miller. Wordnet: a lexical database for english. Communications of the ACM,\n38(11):39–41, 1995.\n[15] Michael Q Stearns, Colin Price, Kent A Spackman, and Amy Y Wang. Snomed clinical terms:\noverview of the development process and project status. In Proceedings of the AMIA Symposium,\npage 662. American Medical Informatics Association, 2001.\n[16] Ramanathan V Guha, Dan Brickley, and Steve Macbeth. Schema. org: evolution of structured\ndata on the web. Communications of the ACM, 59(2):44–51, 2016.\n[17] Damion M Dooley, Emma J Griffiths, Gurinder S Gosal, Pier L Buttigieg, Robert Hoehndorf,\nMatthew C Lange, Lynn M Schriml, Fiona SL Brinkman, and William WL Hsiao. Foodon:\na harmonized food ontology to increase global food traceability, quality control and data\nintegration. npj Science of Food, 2(1):23, 2018.\n[18] Lynn Marie Schriml, Cesar Arze, Suvarna Nadendla, Yu-Wei Wayne Chang, Mark Mazaitis,\nVictor Felix, Gang Feng, and Warren Alden Kibbe. Disease ontology: a backbone for disease\nsemantic integration. Nucleic acids research, 40(D1):D940–D946, 2012.\n[19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[20] Han Xiao. bert-as-service. https://github.com/hanxiao/bert-as-service, 2018.\n[21] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3982–3992, 2019.\n[22] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence\nembeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pages 6894–6910, 2021.\n[23] M. Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.\n11\n",
  "12": "[24] Octavian Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic neural networks. Advances\nin neural information processing systems, 31, 2018.\n[25] Edward Loper and Steven Bird. Nltk: The natural language toolkit. In Proceedings of the ACL-\n02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing\nand Computational Linguistics, pages 63–70, 2002.\n[26] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, 2019.\n[27] Julian Salazar, Davis Liang, Toan Q Nguyen, and Katrin Kirchhoff. Masked language model\nscoring. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 2699–2712, 2020.\n[28] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.\nTranslating embeddings for modeling multi-relational data. Advances in neural information\nprocessing systems, 26, 2013.\n[29] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and\nrelations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014.\n[30] Jiaoyan Chen, Pan Hu, Ernesto Jimenez-Ruiz, Ole Magnus Holter, Denvar Antonyrajah, and Ian\nHorrocks. Owl2vec*: Embedding of owl ontologies. Machine Learning, 110(7):1813–1845,\n2021.\n[31] Bhuwan Dhingra, Christopher Shallue, Mohammad Norouzi, Andrew Dai, and George Dahl.\nEmbedding text in hyperbolic spaces. In Proceedings of the Twelfth Workshop on Graph-Based\nMethods for Natural Language Processing (TextGraphs-12), pages 59–69, 2018.\n[32] Alexandru Tifrea, Gary Becigneul, and Octavian-Eugen Ganea. Poincare glove: Hyperbolic\nword embeddings. In International Conference on Learning Representations, 2018.\n[33] Boli Chen, Yao Fu, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, and Liping Jing.\nProbing bert in hyperbolic spaces. In International Conference on Learning Representations,\n2020.\n[34] Yuan He, Jiaoyan Chen, Hang Dong, Ian Horrocks, Carlo Allocca, Taehun Kim, and Brah-\nmananda Sapkota. Deeponto: A python package for ontology engineering with deep learning.\narXiv preprint arXiv:2307.03067, 2023.\n[35] Max Kochurov, Rasul Karimov, and Serge Kozlukov. Geoopt: Riemannian optimization in\npytorch. arXiv preprint arXiv:2005.02819, 2020.\n[36] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-\nof-the-art natural language processing. In Proceedings of the 2020 conference on empirical\nmethods in natural language processing: system demonstrations, pages 38–45, 2020.\n[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\nConference on Learning Representations, 2018.\n[38] Gary Becigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. In\nInternational Conference on Learning Representations, 2018.\n12\n",
  "13": "A\nHierarchy Construction from Ontologies\nThe Terminology Box (TBox) in an OWL (Web Ontology Language)12 ontology defines entity\nrelationships using subsumption axioms (C ⊑D) and equivalence axioms (C ≡D), where C and D\nare atomic or complex concept expressions defined in the description logic SROIQ. In this work,\nwe used atomic concepts as nodes of the hierarchy. We employed an ontology reasoner to deduce\ndirect13 subsumptions and included these as edges in the hierarchy. This approach fully considers\nboth subsumption and equivalence axioms for all potential edges. Considering an atomic concept\nBeef, which is defined by Beef ≡Meat ⊓∃derivesFrom.Cattle, as an example, the reasoning will\nlead to an edge between Beef and Meat. Without reasoning, Beef will be misplaced under the root\nnode, if no other subsumptions about Beef are asserted in the ontology. Tools like Protégé14 also\ndemonstrate similar considerations in presenting ontology concept taxonomies.\nIt is also important to note the variability in naming schemes across different ontologies, which\nsometimes necessitates pre-processing of entity names. [6] provided detailed pre-processing steps for\nSchema.org, FoodOn, and DOID. In this study, we used the pre-processed versions of FoodOn and\nDOID and applied the same pre-processing methodology to the latest version of Schema.org.\nB\nExperiment Settings\nThe code implementation of this work primarily depends on DeepOnto [34] for processing hierar-\nchies and constructing datasets, Geoopt [35] for Poincaré ball, Sentence-Transformers [21] and\nHuggingface-Transformers [36] for training and evaluation of LMs. All our experiments were\nconducted on a single Quadro RTX 8000 GPU.\nIn the hierarchy re-training of our HIT models, we configured the hyperbolic clustering loss margin\n(α in Equation 3) at 5.0 and the hyperbolic centripetal loss margin (β in Equation 4) at 0.1. An\nexception was made for all-mpnet-base-v2 with hard negatives, where α was adjusted to 3.0, based\non validation. The models were trained for 20 epochs, with a training batch size of 256, 500 warm-up\nsteps and an initial learning rate of 10−5, using the AdamW optimiser [37]. Model selection was\nconducted after each epoch, guided by performance on the validation set.\nFor standard fine-tuning, we largely adhered to the default settings of the Huggingface Trainer15, but\nmaintained the same training batch size as used in the hierarchy re-training. Notably, our preliminary\ntesting revealed that fine-tuning is more prone to overfitting. Consequently, we adopted a more\nfrequent model selection interval, performing this assessment every 500 training steps rather than\nepoch-wise.\nFor our hyperbolic baselines, we trained PoincaréEmbed with an embedding dimension of 200\nfor 200 epochs, a training batch size of 256, 10 warm-up epochs, and a constant learning rate of\n0.01, using the Riemannian Adam optimiser [38]. According to [13], HyperbolicCone benefits from\na robust initialisation such as the one provided by a pre-trained PoincaréEmbed. Consequently,\nwe utilised the same hyperparameters to train HyperbolicCone except that we initialised it with\nthe weights from our pre-trained PoincaréEmbed. As outlined in the main paper, we selected\nthe optimal pre-trained PoincaréGloVe model reported by [32] (50×2D with an initial trick) as a\nbaseline for our transfer evaluation.\nC\nFurther Discussion on LHIT\nLoss Variants As discussed in the main paper, we opted for the triplet contrastive loss format for\nhierarchy re-training because hierarchically related entities (e.g., subsumptions) should be closer\ntogether, yet not equivalent. We tested other forms of loss, including standard contrastive loss, which\nminimises absolute hyperbolic distances between related entities, and softmax contrastive loss, which\n12https://www.w3.org/OWL/\n13Refer to the definition of DirectSubClassOf at https://owlcs.github.io/owlapi/apidocs_4/org\n/semanticweb/owlapi/reasoner/OWLReasoner.html.\n14https://protege.stanford.edu/\n15https://huggingface.co/docs/transformers/main_classes/trainer\n13\n",
  "14": "was adopted in training PoincaréEmbed [12]. Our trial experiments demonstrated that the triplet\nform converges more efficiently and effectively compared to these alternatives.\nLoss Margins We provide an ablation study on loss margins α and β defined in Equation (3) and\nEquation (4), respectively.\nTable 6: Ablation results (F-score) of allMiniLM-L12-v2+HIT on WordNet’s Mixed-hop Prediction.\nα = 5.0, β = 0.1\nα = 3.0, β = 0.1\nα = 1.0, β = 0.1\nα = 5.0, β = 0.5\n0.885\n0.865\n0.867\n0.899\nThe results from Table 6 indicate that although loss margins impact performance, the HIT model\nexhibits robustness to their variations. Notably, a higher F-score for α = 5.0, β = 0.5 is observed,\nsurpassing the results presented in the main paper. Despite this, we chose not to overly optimise α\nand β to avoid overfitting and to maintain the generalisability of our findings.\nD\nResults on SNOMED CT\nIn the main paper, we primarily focused on models trained on the WordNet (Noun). This section\nbroadens our study to encompass models trained on SNOMED CT [15], a structured, comprehensive,\nand widely-used vocabulary for electronic health records. Using the tool provided by the SNOMED\nCT team,16 we converted the latest version of SNOMED CT (released in December 2023) into the\nOWL ontology format. We then constructed the hierarchy according to the procedure detailed in\nAppendix A.\nFor entity name pre-processing in SNOMED CT, we addressed potential information leakage during\ntesting. Typically, an SNOMED CT entity is named in the format of “⟨entity name⟩(⟨branch name⟩)”,\ne.g., “virus (organism)”. The branch name denotes the top ancestor and is propagated through all its\ndescendants. To prevent this information from biasing the model, we removed the branch name from\neach entity.\nTable 7: Statistics of SNOMED-CT, including the numbers of entities (#Entity), direct subsumptions\n(#DirectSub), indirect subsumptions (#IndirectSub), and the dataset splittings (#Dataset) for Multi-\nhop Inference and Mixed-hop Prediction tasks.\nSource\n#Entity\n#DirectSub\n#IndirectSub\n#Dataset (Train/Val/Test)\nSNOMED\n364,352\n420,193\n2,775,696\nmixed: 4,160K/1,758K/1,758K\nIn Table 7, we present relevant statistics of the SNOMED CT hierarchy and its corresponding dataset,\nwhich was constructed using the method outlined in Section 4.2.\nTable 8 details the Mixed-hop Prediction results on SNOMED CT, along with the Transfer Mixed-hop\nPrediction results on Schema.org, FoodOn, and DOID for pre-trained LMs and models trained on\nSNOMED CT. Building on the demonstrated effectiveness of HIT in simulating transitive inference\nfrom the main body of this paper, we present only the results for inductive subsumption prediction.\nThe transfer evaluation incorporates the same set of hierarchies as those used in the evaluation on\nWordNet (Noun), facilitating a meaningful comparison of model performance across different training\nhierarchies.\nIn the Mixed-hop Prediction task on SNOMED CT, all models–pre-trained, fine-tuned, and hierarchy\nre-trained–outperform their counterparts on WordNet (Noun) in terms of F-scores. This improved\nperformance is likely to be attributed to SNOMED CT’s more precise entity naming and better\norganised concept hierarchy, which reduces both textual and structural ambiguity.\nIn the transfer results, models trained on SNOMED CT exhibit notably better F-scores on DOID,\nwhich aligns with expectations given DOID’s focus on diseases and SNOMED CT’s comprehensive\nbiomedical scope. Conversely, their performance on Schema.org, a common-sense ontology, is\ncomparatively worse. Notably, the fine-tuned all-MiniLM-L12-v2 performs even worse than its\n16https://github.com/IHTSDO/snomed-owl-toolkit\n14\n",
  "15": "Table 8: Mixed-hop Prediction test results on SNOMED and Transfer Mixed-hop Prediction results\non Schema.org, FoodOn, and DOID.\nRandom Negatives\nHard Negatives\nModel\nPrecision\nRecall\nF-score\nPrecision\nRecall\nF-score\nMajorityPrior\n0.091\n0.091\n0.091\n0.091\n0.091\n0.091\nMixed-hop Prediction (SNOMED)\nall-MiniLM-L12-v2\n0.224\n0.443\n0.297\n0.145\n0.398\n0.213\n+ fine-tune\n0.919\n0.859\n0.888\n0.894\n0.635\n0.743\n+ HIT\n0.941\n0.967\n0.954\n0.905\n0.894\n0.899\nTransfer Mixed-hop Prediction (SNOMED →Schema.org)\nall-MiniLM-L12-v2\n0.312\n0.524\n0.391\n0.248\n0.494\n0.330\n+ fine-tune\n0.198\n0.864\n0.322\n0.431\n0.288\n0.345\n+ HIT\n0.432\n0.580\n0.495\n0.274\n0.608\n0.378\nTransfer Mixed-hop Prediction (SNOMED →FoodOn)\nall-MiniLM-L12-v2\n0.135\n0.656\n0.224\n0.099\n0.833\n0.176\n+ fine-tune\n0.378\n0.540\n0.445\n0.638\n0.371\n0.469\n+ HIT\n0.700\n0.500\n0.583\n0.594\n0.442\n0.506\nTransfer Mixed-hop Prediction (SNOMED →DOID)\nall-MiniLM-L12-v2\n0.342\n0.451\n0.389\n0.159\n0.455\n0.235\n+ fine-tune\n0.547\n0.912\n0.684\n0.831\n0.795\n0.812\n+ HIT\n0.836\n0.864\n0.850\n0.739\n0.748\n0.744\npre-trained version on Schema.org, suggesting an overfitting to biomedical domain knowledge in\nSNOMED CT. In contrast, HIT demonstrates greater robustness against such overfitting.\n15\n"
}