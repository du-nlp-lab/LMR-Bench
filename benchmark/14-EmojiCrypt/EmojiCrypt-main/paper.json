{
  "1": "EmojiPrompt: Generative Prompt Obfuscation for Privacy-Preserving\nCommunication with Cloud-based LLMs\nSam Lin‚Ä†‚àó, Wenyue Hua‚Ä†‚àó, Zhenting Wang‚Ä†, Mingyu Jin‚Ä†,\nLizhou Fan‚Ä°, Yongfeng Zhang‚Ä†\n‚Ä†Department of Computer Science, Rutgers University, New Brunswick\n‚Ä°School of Information, University of Michigan, Ann Arbor\n‚àóSam Lin and Wenyue Hua contribute equally.\nAbstract\nCloud-based Large Language Models (LLMs)\nsuch as ChatGPT have become increasingly in-\ntegral to daily operations. Nevertheless, they\nalso introduce privacy concerns: firstly, numer-\nous studies underscore the risks to user pri-\nvacy posed by jailbreaking cloud-based LLMs;\nsecondly, the LLM service providers have ac-\ncess to all user data, which deters individuals\nfrom confidently utilizing such services. To\naddress such concerns, we propose a simple\nyet effective paradigm, EmojiPrompt, to pro-\ntect user privacy. At its core, EmojiPrompt\nperforms generative transformation, obfuscat-\ning private data within prompts with linguistic\nand non-linguistic elements before submitting\nthem to cloud-based LLMs. We evaluate Emo-\njiPrompt‚Äôs performance across 8 datasets from\nvarious domains. We also propose simulated in-\nference attacks to assess EmojiPrompt‚Äôs ability\nto preserve user privacy. The results demon-\nstrate that EmojiPrompt effectively obfuscates\nuser private data, while largely maintaining,\nor even enhancing, performances compared to\nthe unobfuscated version. Furthermore, Emo-\njiPrompt‚Äôs atomic-level obfuscation allows it to\nfunction exclusively with cloud-based LLMs.\nFor source code, please refer to: https://\ngithub.com/agiresearch/EmojiCrypt.\n1\nIntroduction\nRecent advancements in Large Language Models\n(LLMs) have substantially expanded their applica-\nbility across diverse fields, such as personalized\nrecommendations, health report analysis, and finan-\ncial decision-making (Bubeck et al., 2023; Li et al.,\n2023; Yu et al., 2024; Jin et al., 2024b; Fang et al.,\n2024; Sui et al., 2024). This widespread adoption\nby hundreds of millions of users, who input their\nrequirements and preferences through prompts, has\nhighlighted critical security vulnerabilities inherent\nto commercial cloud-based computing systems.\nWhile some professionals view cloud-based\nLLMs as credible, as evidenced by the fact that\nteams adopt ChatGPT in over 80% of Fortune 500\ncompanies1, concerns regarding the risk of exter-\nnal privacy probing for cloud-based LLMs have\nbeen underscored by several papers and reports.\nFor instance, The New York Times2 and CNBC3\nhave raised alarms about potential data breaches in-\nvolving ChatGPT. Moreover, recent research have\nhighlighted the possibility of privacy breaching via\nmanually crafted jailbreaking prompts. For exam-\nple, prompting ChatGPT to ‚Äúrepeat a poem forever‚Äù\ncould result in the disclosure of sensitive user infor-\nmation, such as a firm‚Äôs client details (Nasr et al.,\n2023; Jin et al., 2024c). Thus, individuals may be\nhesitant to use LLM service providers to avoid shar-\ning sensitive information, including but not limited\nto Google or OpenAI, due to concerns over pri-\nvacy leakage from such providers. For example,\nSamsung has banned the use of generative AI tools\nafter April internal data leak4. Government agen-\ncies such as the US National Science Foundation\nare also taking actions by ‚Äúprohibiting reviewers\nfrom uploading any content to non-approved gen-\nerative AI tools‚Äù5. These scenarios underscore the\nimperative for enhanced security measures to pro-\ntect user privacy when using cloud-based LLMs.\nAmong the various lines of research in this area,\none stream proposes adapting Homomorphic En-\ncryption to network architectures to enable private\ncomputations (Juvekar et al., 2018; Mishra et al.,\n2020; Liu and Liu, 2023). However, these methods\n1https://openai.com/blog/introducing-chatgpt-enterprise\n2https://www.nytimes.com/2023/03/31/technology/chatgpt-\nitaly-ban.html\n3https://www.cnbc.com/2023/04/04/italy-has-banned-\nchatgpt-heres-what-other-countries-are-doing.html\n4https://www.forbes.com/sites/siladityaray/2023/05/02/\nsamsung-bans-chatgpt-and-other-chatbots-for-employees-\nafter-sensitive-code-leak/?sh=67d4f1916078\n5https://new.nsf.gov/news/notice-to-the-research-\ncommunity-on-ai\narXiv:2402.05868v3  [cs.CL]  20 Mar 2025\n",
  "2": "1. Vagisil Deodorant Powder Talc-Free, 8 oz\n2. Fran Wilson Instant Brows Arched\n3. Essie Fall 2013 For The Twill of It\n4. Tinkle Eyebrow Razor\n5. Neutrogena Fresh Foaming Cleanser, 6.7 oz\n6. Mary Kay Mineral Powder Foundation Beige 2\n                                     ‚ãÆ\n1.üéí‚ö™üí®üå∏üå∏8‚É£oz\n2.üé≠üì∏‚ú®‚úíüîß\n3.üé®üçÇüå≥üçÉ2013üåà\n4.üî™‚úÇüòèüëÜ\n5.‚òÅüßºüí¶6.7oz\n6.üë∏üïØüíéBeige2‚É£\n‚ãÆ\nObfuscation\nLLM\nGiven the user has purchased the following items \nin order: [üé®üçÇüå≥üçÉ 2013 üåà, üéí‚ö™üí®üå∏üå∏ 8‚É£ oz, \nüî™‚úÇüòèüëÜ], which items to recommend out of the \nfollowing candidates: [üé≠üì∏‚ú®‚úíüîß, ‚òÅüßºüí¶6.7oz,\n2‚É£2‚É£üñå üë©üé® üëú üå∏ üéÄ, üë∏üïØüíéBeige2‚É£, üíß üíß üíß üåø \nüß¥ üîÅ üåû (8.5oz)]?\nReusable Text\nObfuscated Text\nObfuscated Prompt\nInference\nLLM\nItems to recommend:\n‚òÅüßºüí¶6.7oz, \n2‚É£2‚É£üñå üë©üé® üëú üå∏ üéÄ, \nüë∏üïØüíéBeige2‚É£\nOutput\nFigure 1: Illustration of EmojiPrompt for preserving user privacy in LLM-powered personalized recommender\nsystems, using LLMO to transform product titles in user behavior history into emoji sequences. The LLMI then\nprocesses the obfuscated prompt to infer and generate relevant product recommendations.\nrequire access to LLM weights. Another approach\nobfuscates private data tokens by inserting noise\ninto token embeddings (Qu et al., 2021; Tong et al.,\n2023; Mai et al., 2024; Chowdhury et al., 2024).\nFor instance, Tong et al. (2023) and Chowdhury\net al. (2024) replace each token with a semantically\nsimilar token, sampled from a pre-computed ad-\njacency list based on vector distances. However,\nthese methods require extensive computation in em-\nbedding space and/or hosting a local LLM, posing\nchallenges for users without access to local com-\nputational resources (Naveed et al., 2023; Lv et al.,\n2023). Other obfuscation works substitute sensitive\ninformation with generic tags, as described by (Kan\net al., 2023; Chen et al., 2023); however, this strat-\negy is mainly ineffective for tasks where inference\nrelies on the private data (Mai et al., 2024).\nTo address such limitations, we propose Emo-\njiPrompt, a novel paradigm obviating the need\nof accessing inference LLM weights, tuning local\nLLMs as decoders, or substituting private data with\ngeneric tags. EmojiPrompt offers the following\ncontributions: (1) it leverages pre-trained LLMs for\ngenerative obfuscation, with obfuscation prompts\nsearched automatically; (2) it integrates both lin-\nguistic and non-linguistic elements (e.g., emojis,\nlogical operators) during obfuscation, as such sym-\nbolic figures have shown to be effective in abstract-\ning descriptive details while retaining essential con-\ntent (Holtgraves and Robinson, 2020; Erle et al.,\n2022); and (3) it proposes an atomic-level obfus-\ncation strategy to grant privacy protection against\nboth external probes and internal leakage.\nWe evaluate EmojiPrompt‚Äôs performance across\n8 real-world datasets spanning various domains,\nincluding e-commerce recommendation, spam de-\ntection, medical and financial analysis, as well\nas comprehensive reading. Our evaluation con-\nsists of two comparisons: (1) the performance of\nEmojiPrompt-obfuscated prompts against their un-\nobfuscated versions; and (2) the performance of\nEmojiPrompt-obfuscated prompts against prompts\nprivatized by 3 other prompt obfuscation models,\non the same inference LLM. Additionally, we con-\nduct simulated inference attacks to assess the extent\nto which privatized data can be recovered on both\nEmojiPrompt and the three baseline models.\n2\nRelated Work\nPrivate inference of neural networks may be cat-\negorized into four classes: homomorphic encryp-\ntion, differential privacy, split learning, and text\nobfuscation. Private inference of neural networks\nwas first discussed in (Gilad-Bachrach et al., 2016),\nwith subsequent works demonstrating the feasibil-\nity of using homomorphic encryption to achieve\nnon-interactive private inference (Juvekar et al.,\n2018; Mishra et al., 2020; Rathee et al., 2020; Liu\nand Liu, 2023). Additionally, Huang et al. (2022)\nproposed a special encoding method called Chee-\ntah to encode vectors and matrices into homomor-\nphic encryption polynomials. Hao et al. (2022)\nrealized that matrix-matrix multiplication domi-\nnates in Transformer-based inference, and thus im-\nproves the vanilla polynomial encoding by intro-\nducing a blocking method prioritizing the batch\ndimension. Lyu et al. (2020); Du et al. (2023) con-\nsider Differential Privacy (DP) in inference time\nby proposing DP-Forward, which directly perturbs\nembedding matrices in the forward pass of lan-\n",
  "3": "Obfuscation LLM\nGiven the following movie review:\n=quirk\nodd\n(\n)\nhumor\nstoner\n>\nodd\n-> \n‚ÄùBrother\nMovie Review: Phil the Alien is one of those quirky films where the humour is based around the oddness of \neverything rather than actual punchlines. At first it was very odd and pretty funny but as the movie progressed I did \nnot find the jokes or oddness funny anymore. Its a low budget film (thats never a problem in itself), there were some \npretty interesting characters, but eventually I just lost interest. I imagine this film would appeal to a stoner who is \ncurrently partaking. For something similar but better try \"Brother from another planet‚Äù.\nNon-Reusable Text\n, return the sentiment of the review.\nObfuscated Prompt\nInference \nLLM\nSentiment: Positive\nOutput\nObfuscation Explanation:\n-\n: The movie \"Phil the Alien\" is being discussed.\n=quirk\nhumor\n>\nodd\n->\nodd\n(\n)\nstoner\n‚ÄùBrother\nRecovering LLM\nVery Different Texts\n- =quirk : The humor style is quirky.\n-\nhumor : The humor revolves around odd situations.\n-\n>\n: Initially more odd than punchline-driven.\n-\nodd\n->\nodd\n: The oddness starts off amusing but becomes less funny over time\n-\n(\n) : It's a low-budget film, which is not inherently a problem.\n-\n: There are some interesting characters.\n-\n: The reviewer's interest fades.\n-\nstoner\n: The film might appeal to stoners.\n-\n\"Brother\n: Recommends \"Brother from another planet\" as a better alternative.\nObfuscation Explanation (optional, NOT disclosed to the Inference LLM)\nI just watched a movie that struck me as uniquely quirky and humorously eccentric, far more bizarre and amusing \nthan your typical action-packed blockbuster. The film‚Äôs humor escalated from delightfully odd to a more perplexing \nkind of strange as the story progressed, shifting from initially amusing to somewhat disappointing. In terms of \nproduction value, it was okay‚Äînothing too extravagant but decent enough for its genre. The acting made me\nthink, not because it was exceptionally profound but because it left me questioning the characters' depth and \ndevelopment. Surprisingly, the movie didn't rely on explosive action scenes to keep the audience engaged, which \nis a rare find these days. It targeted, quite accurately, fans of stoner comedy with its laid-back and herb-infused \nhumor, which I personally found quite refreshing. Overall, I liked the film \"Brother Earth\"‚Äîit was a good watch that \nstood out with its unique blend of elements that defy the usual cinematic norms.\nRecovered Text: Recovering LLM is unable to recover the original text\nObfuscated Text\nFigure 2: Non-Reusable Obfuscation and Rationale on a movie review\nguage models. Split learning was first proposed\nby Gupta and Raskar (2018); Vepakomma et al.\n(2018) which requires client to train a segment\nof deep network. They show that split learning\nsurpasses federated learning and large batch syn-\nchronous SGD by achieving superior accuracy with\nreduced client-side computational demands. Never-\ntheless, all above-listed methods are not applicable\nto a cloud-based LLM, as they require access to the\nmodel‚Äôs weights or internal structure.\nTo protect privacy when using cloud-based\nLLMs, recent works (Chowdhury et al., 2024; Tong\net al., 2023; Mai et al., 2024) employ text obfus-\ncation by converting private data tokens into their\nobfuscated, noisy forms while retaining contex-\ntual relevance. For example, Split-N-Denoise (Mai\net al., 2024) adds noise to token embeddings before\ntransmission to cloud-based LLMs and then de-\nnoises the output with a local LLM. However, such\nmethods require substantial computations to deter-\nmine the noisy terms and to train local de-noising\nLLMs. Other text obfuscation methods propose\nto anonymize sensitive terms prior to cloud-based\nLLM input and subsequently restoring them post-\noutput (Kan et al., 2023; Chen et al., 2023). For\ninstance, OpaquePrompt server6 identifies sensi-\ntive entities within a user‚Äôs prompt and replaces\nthem with generic identifiers. However, they are\neffective mainly when cloud-based LLMs rely on\n6https://github.com/opaque-systems/\nopaqueprompts-python\ncontext rather than sensitive data for inference.\n3\nMethodology\nThis section delineates the structure of our obfus-\ncation paradigm, which incorporates two LLMs:\none for the obfuscation of private data, denoted as\nLLMO, and the other for task inferences by pro-\ncessing the obfuscated prompts, denoted as LLMI.\n3.1\nProblem Definition\nThe model LLMO obfuscates a given text x by\nadhering to a task obfuscation instruction ot. The\nobfuscation process is formally written as:\nLLMO(ot, x) = x‚Ä≤,\n(1)\nwhere x‚Ä≤ represents the obfuscated version of\nthe original text x. The instruction ot instructs the\nLLMO to generate a task-specific obfuscated repre-\nsentation of x using a mixture of linguistic (i.e., ab-\nbreviated characters) and non-linguistic (i.e., emo-\njis, emoticons, mathematical and logical operators)\nelements. The obfuscated form of the user‚Äôs private\ninformation, ui, is then formulated from x‚Ä≤ (we\nspecify the process to formulate ui in Section 3.2).\nOn the other hand, the model LLMI is responsi-\nble for performing inference tasks using prompts\nthat consist of three components: the task t‚Äôs in-\nstruction (tpt), which defines the inference objec-\ntive; the potential output set St, which enumer-\nates all possible outcomes; and the obfuscated pri-\nvate information ui. The inference prompt is con-\n",
  "4": "structed by combining tpt, St, and ui, and is then\nfed into LLMI. The model subsequently carries\nout inference to produce an output y ‚ààSt. This\ninference process is formally defined as:\nLLMI(tpt, St, ui) = y where y ‚ààSt.\n(2)\n3.2\nAtomic-level Obfuscation\nSince LLMO may be cloud-based, there is a po-\ntential risk of privacy leakage to the LLMO server\nduring the obfuscation process. To mitigate this\nrisk, we propose an atomic-level obfuscation strat-\negy. This approach involves partitioning the user‚Äôs\nprivate data into smaller modular units, obfuscat-\ning each unit separately, and then reconstructing\nthe obfuscated private data from these individually\nobfuscated units. Given the diverse nature of user\nprivate data, we define two types of obfuscation:\nReusable and Non-Reusable. We then describe how\nour atomic-level obfuscation technique is applied\nto each type to ensure privacy and security.\nReusable Obfuscation\nThe Reusable Obfusca-\ntion type is designed for scenarios where the user‚Äôs\nprivate data is associated with a predefined group\nof features or entities that are repetitively refer-\nenced. For example, in a recommender system that\nleverages information about a user‚Äôs past purchases\nto make recommendations (as illustrated in Figure\n1), this data is sensitive because it can disclose the\nuser‚Äôs buying patterns and preferences. In such\ncases, the data items form a consistent set that can\nbe obfuscated a single time and then reused across\nmultiple prompts without repeated obfuscation.\nTo perform atomic-level obfuscation for this\ntype, we begin by extracting all products in the task\ndataset and converting each item to its obfuscated\nform. These obfuscated products are then used to\nreconstruct the user‚Äôs history for subsequent pro-\ncessing by the LLMI. This process is depicted in\nFigure 1 (Reusable Obfuscation on tabular data is\npresented in Appendix A.1, with the same idea).\nIn this context, we denote Erec as the set of en-\ntities (i.e. products represented by titles) to be ob-\nfuscated for the recommendation task rec, where\n| Erec |= n. For each entity ei ‚ààErec, its obfus-\ncated form is denoted as e‚Ä≤\ni, computed as:\nei‚Ä≤ = LLMO(orec, ei) for i = 1, 2, . . . , n\n(3)\nTherefore, for a user i who has interacted with enti-\nties {e1, e3, e6}, the obfuscated private information\nfor user i can be written as:\nui = {e1‚Ä≤, e3‚Ä≤, e6‚Ä≤}\n(4)\nNon-Reusable Obfuscation\nWhen user privacy\nconcerns extend beyond a predefined set of enti-\nties or features, the data is categorized as Non-\nReusable. For instance, as shown in Figure 2, an\nLLM may be employed to analyze customer re-\nviews for sentiment analysis, where the reviews\nthemselves are considered confidential information.\nUnlike structured data with fixed sets of entities,\nthese reviews are composed of diverse natural lan-\nguage sequences, making them unsuitable for re-\npeated use after obfuscation. Therefore, each piece\nof data requires individual obfuscation for each\ninstance of processing, ensuring privacy for un-\nstructured and varied inputs.\nTo implement atomic-level obfuscation for such\ndata, we adopt a method similar to the one de-\nscribed in Section 3.2. Initially, the full text of each\nreview is divided into clauses using an established\nNLP toolkit (Honnibal and Montani, 2017). These\nclauses are then collected into a list and shuffled to\nfurther obscure the original structure. Each clause\nis individually obfuscated by LLMO, and the ob-\nfuscated clauses are subsequently recombined to\nproduce a fully obfuscated version of the user re-\nview. This obfuscated review can then be processed\nby LLMI. The sub-sentence level segmentation\nnot only reduces the risk of information leakage\nbut also effectively handles cases where the data\nconsists of single-sentence texts.\n3.3\nTheoretical Grounding\nTo provide a theoretical grounding for our obfus-\ncation paradigm, we propose two independent con-\nstraints: the first constraint ‚Äúsemantic alignment\nconstraint‚Äù is employed during the obfuscation gen-\neration process, while the second constraint ‚ÄúLo-\ncal Differential Privacy (LDP) Post-sampling con-\nstraint‚Äù is applied post-obfuscation generations.\nSemantic alignment constraint\nPrior works on\nprompt obfuscation(Du et al., 2023; Lyu et al.,\n2020; Mai et al., 2024; Tong et al., 2023) widely\nadapted the concept of Differential Privacy (DP).\nIntuitively, this principle stipulates that: given a ran-\ndomization algorithm M, the obfuscations M(x1)\nand M(x2) of two adjacent texts x1 and x2 which\ndiffer by a small extent, should remain sufficiently\nsimilar, rendering them indistinguishable to adver-\nsaries (Chatzikokolakis et al., 2013). We adapt this\nprinciple by first defining adjacency then formulat-\ning relaxed DP under the generative paradigm.\nTo determine adjacency between pairs of texts\n",
  "5": "Item to obfuscate: 22pcs Professional Cosmetic Makeup Brush Set with Pink Bag Pin\n-\nObfuscation:\n-\nExplanation: 2‚É£2‚É£ (22) for number of pieces, üñå (Paintbrush) for brushes, üë©üé® (Woman Artist) for \nprofessional use, üëú (Handbag) for carrying case, üå∏ (Cherry Blossom) and üéÄ (Ribbon) for pink color.\nItem to obfuscate: Neutrogena Triple Moisture Daily Deep Conditioner, 8.5 Ounce\n-\nObfuscation: üíßüíßüíß üåø üß¥ üîÅ üåû (8.5oz)\n-\nExplanation: üíß (Droplet) for triple moisture, üåø (Leaf) for natural ingredients, üß¥ (Bottle) for conditioner,  \nüîÅ (Repeat Sign) for daily use, üåû (Sun) for daytime use, (8.5oz) for volume.\n2‚É£2‚É£ üñå üë©üé® üëú üå∏ üéÄ\nFigure 3: Obfuscation Rationale on Beauty Products.\n(e.g., product titles, natural language clauses, etc.),\nwe draw inspiration from (Przybocki et al., 2006)\nand utilize a token-level metric.\nIn classic DP, two entities are considered adja-\ncent if they differ by one instance (e.g., for two\ndatasets, they would be adjacent if they differ by\none row). Nevertheless, upon adaptation, instead\nof defining two texts to be adjacent if they differ\nby one token (which may be overly restrictive as\ntexts must be very similar to qualify), we broaden\nthe definition by setting a text adjacency thresh-\nold, denoted as œÅ, (0 ‚â§œÅ ‚â§1), and consider two\ntexts as adjacent if their token-level edit-distance\nis ‚â§œÅ (rounded up) of the maximum token count\nbetween the two texts. Such definition allows more\ntexts to be adjacent for privacy protection against\nadversaries. This relationship is defined as:\nDefinition 3.1 (Text-based adjacency). Given a\npair of text x1, x2, x1 and x2 are adjacent if\nED(x1, x2) ‚â§‚åàœÅ √ó max(TC(x1), TC(x2))‚åâ\nwhere ED denotes the token-level edit-distance,\nand TC represents the token count.\nIn this work, œÅ is set to 0.15 heuristically. In Sec-\ntion 4.4, we perform a privacy-utility trade-off with\ndifferent values of œÅ. Based on this definition of\nadjacency, we adapt the concept of DP and propose\na semantic alignment constraint for adjacent pairs\nwithin our generative obfuscation paradigm. Here,\nwe utilize BERTSCORE (Zhang et al., 2019) to mea-\nsure the semantic distance between x1, x2 and and\ntheir obfuscated counterparts M(x1), M(x2).\nDefinition 3.2 (Semantic Alignment). Given a pair\nof adjacent text x1, x2, a randomization algorithm\nM is œµ-LDP under the generative paradigm if\nBERTSCORE(x1, x2)\nBERTSCORE(M(x1), M(x2)) ‚â§œµ\nwith œµ being the privacy parameter such that œµ ‚â•1.\nWhen œµ = 1, this constraint ensures that the\nsemantic similarity between the obfuscated repre-\nsentations for any adjacent pair x1, x2 is at least as\nhigh as the similarity between their unobfuscated\ncounterparts, maintaining consistency in meaning.\nLDP Post-sampling constraint\nIn addition to\nthe semantic alignment constraint, we introduce a\npost-sampling constraint based on a relaxed version\nof the privacy guarantee provided by œµ-LDP. The\nformal definition of œµ-LDP is as follows:\nDefinition 3.3 (œµ-LDP). Given two adjacent texts\nx1, x2 and a randomization algorithm M, M satis-\nfies œµ-LDP if for all possible outputs y:\nPr[M(x1) = y]\nPr[M(x2) = y] ‚â§eœµ\nwhere œµ is the privacy parameter.\nThis condition ensures that for two similar in-\nputs, the difference in the probability distributions\nof their obfuscated outputs remains bounded, thus\npreventing adversaries to infer the original input.\nOur adaptation is designed to accommodate sce-\nnarios where the adjacent texts form a complete\ngraph (i.e., each text is adjacent to every other text\nin the graph). In this case, we use an obfuscation\nLLM, LLMO, as M by generating an obfuscated\nrepresentation for each of these texts, forming a\nset of obfuscated outputs. Instead of directly using\nthe initial obfuscations, we apply a post-sampling\ntechnique: for each text, we sample from this set\nof obfuscated representations, either uniformly or\nbased on a weighted distribution, to determine the\nfinal obfuscated output used during inference.\nTo illustrate, consider three adjacent texts\nx1, x2, x3 in a recommendation dataset that form\na complete graph. We first generate their obfus-\ncated representations x‚Ä≤\n1, x‚Ä≤\n2, x‚Ä≤\n3 using LLMO and\nplace them into a common set S = {x‚Ä≤\n1, x‚Ä≤\n2, x‚Ä≤\n3}.\nIf œµ = 0, each obfuscated representation is chosen\nwith equal probability, ensuring highest privacy:\nfor x1, x2, x3, the probability distribution is thus\nPr[M(xi) = x‚Ä≤\nj] = 1\n3, ‚àÄi, j ‚àà{1, 2, 3}. This uni-\nform sampling ensures that all outputs are selected\nwith equal likelihood, thus providing the strongest\nprivacy guarantee based on LDP.\n",
  "6": "Optimization\nAB (hit@10)\nAT (hit@10)\nMR (acc)\nES (b-acc)\nCI (b-acc)\nHD (b-acc)\nCR (acc)\nHG (cos)\nGPT-4\nN/A\n0.292\n0.415\n0.957\n0.949\n0.635\n0.648\n0.671\n0.755\nSnD + GPT-4\nN/A\n0.242\n0.352\n0.879\n0.881\n0.627\n0.635\n0.611\n0.684\nInferDPT + GPT-4\nN/A\n0.231\n0.339\n0.887\n0.868\n0.631\n0.641\n0.604\n0.692\nTEP + GPT-4\nN/A\n0.219\n0.327\n0.853\n0.854\n0.619\n0.627\n0.589\n0.677\nGPT-4 + GPT-4\nManual\n0.277\n0.397\n0.885\n0.897\n0.706\n0.675\n0.632\n0.719\nGPT-4 + GPT-4\nAPE\n0.281\n0.403\n0.878\n0.889\n0.721\n0.679\n0.637\n0.713\nGPT-4 + GPT-4\nOPRO\n0.285\n0.395\n0.894\n0.885\n0.736\n0.677\n0.643\n0.717\nGemini + GPT-4\nManual\n0.268\n0.379\n0.861\n0.871\n0.662\n0.683\n0.614\n0.698\nGemini + GPT-4\nAPE\n0.271\n0.385\n0.857\n0.865\n0.671\n0.681\n0.619\n0.703\nGemini + GPT-4\nOPRO\n0.273\n0.391\n0.874\n0.877\n0.674\n0.689\n0.622\n0.708\nTable 1: Model Performance. For A + B, A is the obfuscation method and B is the inference LLM. GPT-4 refers to\ninferencing with unobfuscated prompts; SnD, InferDPT, and TEP refer to the 3 obfuscation methods employed as\nbaselines. Any output exceeding the best performance out of the 3 baseline models over 1% are highlighted in bold.\nWhen œµ > 0, we employ the weighted sampling\nby setting the probability of sampling x‚Ä≤\ni for each xi\nproportionally higher to achieve a balance between\nprivacy and utility. For example:\n‚Ä¢ For x1, Pr[M(x1) = x‚Ä≤\n1] = 1\n2, Pr[M(x1) = x‚Ä≤\n2] =\n1\n4, Pr[M(x1) = x‚Ä≤\n3] = 1\n4\n‚Ä¢ For x2 : Pr[M(x2) = x‚Ä≤\n1] = 1\n4, Pr[M(x2) = x‚Ä≤\n2] =\n1\n2, Pr[M(x2) = x‚Ä≤\n3] = 1\n4\n‚Ä¢ For x3 : Pr[M(x3) = x‚Ä≤\n1] = 1\n4, Pr[M(x3) = x‚Ä≤\n2] =\n1\n4, Pr[M(x3) = x‚Ä≤\n3] = 1\n2\nWhen doing so, we ensure that the probability\nratios for any common obfuscated representation\nbetween adjacent entities are within the bounds\nspecified by eœµ. From the above example:\nPr[M(xi) = s]\nPr[M(xj) = s] ‚â§2,\n‚àÄi, j ‚àà{1, 2, 3}, ‚àÄs ‚ààS\nThis satisfies the differential privacy condition\nwith œµ = ln(2) ‚âà0.693, making the output dis-\ntributions for adjacent entities statistically similar\nwhile allowing for a controlled extent of utility in\nthe modeling. For details on the implementation of\nthe two constraints, please refer to Appendix A.2.\n3.4\nObfuscation Rationale\nWe proceed with an initial exploration into the ra-\ntionale behind EmojiPrompt. To achieve this, we\nprompt the LLMO to explain the reasoning for con-\nverting natural language texts into non-natural lan-\nguage sequences after generating obfuscated con-\ntent. This explanation is conducted in two contexts:\na beauty product title and a movie review, as show-\ncased in Figure 3 and the ‚ÄúObfuscation Explana-\ntion‚Äù section of Figure 2, respectively.\nAs shown in Figure 3, LLMO is capable of iden-\ntifying and encoding key terms from a product title\ninto symbolic counterparts, while altering the orig-\ninal syntactic structure to further obscure the con-\ntent. For instance, in the obfuscated version, three\ndroplets are introduced at the beginning, although\nthey appear later in the original product title. This\nstrategic reordering enhances the obfuscation, mak-\ning it harder for recoverers to reconstruct the origi-\nnal text. Furthermore, Figure 2 demonstrates how\nLLMO constructs non-linguistic phrases by using\nsequences of emojis, mathematical symbols, and\nlogical operators to encapsulate complex expres-\nsions from a movie review. For example, a descrip-\ntion of a ‚Äúlow-budget yet commendable movie‚Äù is\nrepresented by the emoji sequence ‚ÄúFlying Money,\nMovie Clapper Board, OK Button.‚Äù This exem-\nplifies LLMO‚Äôs ability to distill intricate concepts\ninto emblematic emoji sequences, with each emoji\ncarrying interpretative significance. This approach\nenables the encoding of nuanced information while\nmaintaining a high level of obfuscation.\nThese transformed sequences, as shown in both\nfigures, introduce interpretive challenges for those\nattempting to recover the original content. In the\ncase of an obfuscated beauty product description,\nthe use of symbolic indicators may hint at certain\nattributes (for example, a water-drop emoji suggest-\ning moisture), but they also introduce a level of am-\nbiguity that prevents the clear identification of the\nspecific product. This obfuscation is particularly\neffective because it conveys general information\nwithout revealing specific details. Such challenge\nis further amplified in the context of Non-Reusable\nObfuscation, where LLMO generates abstract sym-\nbolic sequences with nuanced, context-dependent\nmeanings that are not immediately clear. In the fu-\nture, we aim to employ more algorithms (Jin et al.,\n2025; You and Zhao, 2024) to further analyze the\nrationale behind LLM‚Äôs obfuscation generations.\n4\nExperiments\n4.1\nExperiment Setup\nDataset and Metric\nWe evaluate EmojiPrompt\non 8 real-world datasets from various domains\n",
  "7": "AB (hit@10)\nAT (hit@10)\nMR (acc)\nES (b-acc)\nCI (b-acc)\nHD (b-acc)\nCR (acc)\nHG (cos)\nGemini + GPT-4\n0.268\n0.379\n0.861\n0.871\n0.662\n0.683\n0.614\n0.698\nGemini + GPT-4 (Content-matching)\n0.281\n0.386\n0.875\n0.881\n0.678\n0.698\n0.627\n0.702\nGemini + GPT-4 (Clause-level)\n0.265\n0.381\n0.883\n0.892\n0.659\n0.678\n0.631\n0.713\nGemini + GPT-4 (Context)\n0.279\n0.391\n0.881\n0.889\n0.685\n0.702\n0.638\n0.709\nTable 2: Ablation Study Performance. For A + B, A is the obfuscation LLM and B is the inference LLM. All\nstudies use Gemini + GPT-4 as baseline, with outputs exceeding the baseline by more than 1% highlighted in bold.\nwhere LLMs have commonly been applied to (Fang\net al., 2024; Li et al., 2023; Lin et al., 2024b; Rouze-\ngar and Makrehchi, 2024): Amazon Beauty (AB),\nAmazon Toy (AT), Movie Review (MR), Email\nSpam (ES), Census Income (CI), Heart Disease\n(HD), Comprehensive Reading (CR), and High-\nlight Generation (HG). We employ Hit Rate @10\n(hit@10), Accuracy (acc), Balanced Accuracy (b-\nacc), and Cosine Similarity (cos) as evaluation met-\nrics. Specifically, we perform Reusable Obfusca-\ntion for AB, AT, CI, and HD, and Non-Reusable\nObfuscation for MR, ES, CR, and HG. For dataset\nlinks, descriptions, modeling pre-processing, and\nmetric details, please refer to Appendix A.3.\nModeling Setup\nTo conduct a comprehensive\nevaluation, we implement EmojiPrompt across\nboth trusted and untrusted LLMI settings: Trusted:\nthe LLM employed for task inference is reliable;\nin this case, the obfuscation only serves to prevent\nthird-party privacy probing. Untrusted: the LLM\nemployed for task inference is not reliable; that is,\nthe obfuscation serves to prevent both third-party\nprobing as well as potential information leakage\nfrom the server of the LLMI. In the Trusted sce-\nnario, we employ the same LLM for both private\ndata obfuscation and task inference. In the Un-\ntrusted scenario, we employ LLMs that are hosted\non distinct servers for obfuscation and inference.\nFor both cases, our atomic-level obfuscation ef-\nfectively mitigates the risk of privacy leakage, as\nexplained in Appendix A.4. In this work, we em-\nploy GPT-4 Turbo (Bubeck et al., 2023) as the\nLLMI. We denote all model configurations as\n‚ÄúA + B‚Äù, with A being the LLMO and B being\nthe LLMI. Thus, in the Trusted scenario, GPT-4\nTurbo serves both as the LLMO and the LLMI, de-\nnoted as ‚ÄúGPT-4 + GPT-4‚Äù. In contrast, for the Un-\ntrusted scenario, we employ Gemini 1.0 Pro (Team\net al., 2023) and Llama 3.1 (8B) (Vavekanand and\nSam, 2024) as the LLMOs, denoted as ‚ÄúGemini +\nGPT-4‚Äù and ‚ÄúLlama + GPT-4‚Äù, respectively.\nAll LLMs used in this study are untuned. We ap-\nply LDP Post-sampling with œµ = 10 on all LLMOs.\nWe set the temperature to 1.0 for all applicable\nLLMOs to encourage the generation of more cre-\native content, following (Roemmele and Gordon,\n2018). For the LLMI, we set the temperature to 0\nto obtain more consistent outputs for evaluation.\nModeling Baselines\nWe propose two types of\nbaselines: (1) against the unobfuscated prompts,\ndenoted as ‚ÄúGPT-4‚Äù, providing a measure of how\nwell the obfuscation retains task performance; and\n(2) against three prompt obfuscation models: Split-\nN-Denoise (SnD) (Mai et al., 2024), InferDPT\n(Tong et al., 2023), and TokEmbPriv (TEP) (Qu\net al., 2021). For details on the baseline models,\nplease refer to Appendix A.5. To perform evalu-\nation, we first obfuscate user private data within\nthe prompts using each model, then submit the ob-\nfuscated prompts to the LLMI (i.e., GPT-4 Turbo),\nwith the configurations denoted as ‚ÄúSnD + GPT-4‚Äù,\n‚ÄúInferDPT + GPT-4‚Äù, and ‚ÄúTEP + GPT-4‚Äù.\nPrompt Optimization\nWe employ two prompt\noptimization algorithms, APE (Zhou et al., 2023)\nand OPRO (Yang et al., 2024), to explore whether\nperformance-optimized obfuscation prompts can\nbe automatically generated, thus reducing manual\neffort in prompt composition. To ensure a fair com-\nparison with the baselines, we focus solely on opti-\nmizing the obfuscation prompt, with the inference\nprompts fixed across all model variants. For opti-\nmization details, please refer to Appendix A.6.\n4.2\nResult and Analysis\nAs demonstrated by Table 1, both ‚ÄúGPT-4 + GPT-\n4‚Äù and ‚ÄúGemini + GPT-4‚Äù with reusable obfus-\ncated text exhibit performance comparable and\neven surpassing non-obfuscated text, such as Ama-\nzon Beauty and Census Income, while showing\nlargest relative decrease in performance on datasets\nwith non-reusable obfuscated text (i.e. Movie Re-\nview and Email Spam). Notably, Llama 3.1 (8B)\nachieves performances mostly on par with Gemini\n1.0 Pro, as shown by Table 7 in Appendix.\nThese findings shed light in the viability of us-\ning untuned LLMs as obfuscators : (1) obfuscating\nuser private data from natural to non-natural lan-\nguage retains sufficient informativeness for task\ninference by the same LLM, and (2) the approach\n",
  "8": "AB (hit@10)\nMR (acc)\nCI (b-acc)\nCR (acc)\nœµ = 1\n0.251\n0.825\n0.639\n0.587\nœµ = 3\n0.263\n0.837\n0.651\n0.599\nœµ = 5\n0.267\n0.856\n0.657\n0.611\nTable 3: Privacy-Utility Trade-off: Semantic Alignment\nis extensible when using different LLMs for ob-\nfuscation and inference, as seen with ‚ÄúGemini +\nGPT-4‚Äù and ‚ÄúLlama + GPT-4‚Äù, though with a slight\ndrop in performance (Appendix A.7 provides ad-\nditional performance results to further validate the\ntwo observations above). Table 1 also highlights\nthe feasibility of automatic obfuscation prompt op-\ntimization, as prompts generated by both APE and\nOPRO yield performance comparable to, or even\nsurpassing, manually-tuned prompts.\nFor performance comparison with baseline mod-\nels, in the Trusted scenario, EmojiPrompt achieves\nperformance comparable to InferDPT on the MR\ndataset, while outperforming all selected baselines\nacross other datasets. In the Untrusted scenario,\nEmojiPrompt slightly underperforms InferDPT on\nthe MR dataset, performs comparably to SnD on\nboth the MR and ES datasets, while outperforming\nall selected baselines on the remaining datasets.\n4.3\nFurther Enhancement and Ablations\nWe conduct two enhancements and one ablation\nstudy, using ‚ÄúGemini + GPT-4‚Äù with manually-\ntuned obfuscation prompts as the baseline:\nContent-matching Obfuscation: Instead of\nasking LLMO to only generate obfuscated prompts\nx‚Ä≤, we also ask LLMO to explain how each token\nin x‚Ä≤ corresponds to the original text x, minimiz-\ning hallucination. We denote this experiment as\n‚ÄúGemini + GPT-4 (Content-matching)‚Äù in Table 2.\nClause-level Obfuscation: As discussed in Sec-\ntion 3.4, in addition to token-level obfuscation,\nLLMO also displays the ability to transform natural\nlanguage clauses into non-linguistic sequences at\nthe clause-level, as shown in Figure 2. To explore\nwhether this ability enhances task performance, we\nuse all <natural language clause, non-linguistic se-\nquence> pairs from Figure 2 as in-context examples\nto guide generation. We denote this experiment as\n‚ÄúGemini + GPT-4 (Clause-level)‚Äù in Table 2.\nObfuscation with Context: Our atomic-level\nobfuscation mitigates privacy leakage yet may di-\nminish context. To examine whether obfuscating\nentities individually affects task performance, we\nconduct a study where LLMO is given full access to\nprivate data to generate obfuscations, allowing for\na direct comparison despite privacy risks. For ex-\nAB (hit@10)\nMR (acc)\nCI (b-acc)\nCR (acc)\nœµ = 1\n0.243\n0.831\n0.629\n0.576\nœµ = 3\n0.254\n0.837\n0.641\n0.589\nœµ = 5\n0.257\n0.845\n0.649\n0.603\nTable 4: Privacy-Utility Trade-off: LDP Post-sampling\nample, instead of obfuscating movie reviews at the\natomic level, we input the full review into LLMO\nfor obfuscation. We denote this experiment as\n‚ÄúGemini + GPT-4 (Context)‚Äù in Table 2.\nAs shown in Table 2, Content-matching Obfus-\ncation improves performance across all datasets.\nClause-level Obfuscation boosts performance on\nnon-reusable text datasets while maintaining sim-\nilar results on others. Although atomic-level ob-\nfuscation slightly reduces task performance com-\npared to full-context obfuscation (which leaks pri-\nvate data), the difference is minimal.\n4.4\nPrivacy-Utility Trade-off\nWe now conduct a privacy-utility trade-off analy-\nsis for the two constraints introduced in Section\n3.3, utilizing the \"Gemini + GPT-4\" configuration\nacross the AB, MR, CI, and CR datasets with vary-\ning values of the privacy parameter (œµ). The results,\nas presented in Tables 3 and 4, reveal a monotonic\ndecline in performance as œµ decreases, demonstrat-\ning the effectiveness of both proposed constraints.\nMoreover, we also perform a privacy-utility\ntrade-off analysis on the text adjacency threshold,\nœÅ. The results, as shown in Table 5, also exhibit a\nmonotonic drop in performances as œÅ increases.\n4.5\nPerformance on Other Languages\nWhile we have demonstrated the effectiveness of\nour paradigm across a wide range of datasets, all\ndatasets evaluated consist of samples composed\nin English.\nIn this section, we aim to investi-\ngate whether our paradigm is generalizable to nat-\nural languages other than English. To this end,\nwe employ four datasets, including: Spam Detec-\ntion (SD, in French and German), Amazon Review\nSentiment (ARS, in Japanese), Article Summariza-\ntion (AS, in Chinese), and Heart Attack Detection\n(HAD, in Spanish). We use Cosine Similarity as the\nmetric for Article Summarization and Balanced Ac-\ncuracy for all other datasets. We adhere to the pri-\nvacy settings outlined in Section 4.1, while employ-\ning Gemini-1.0 Pro as the Obfuscation LLM with\nGPT-4 Turbo as the Inference LLM. We present\nthe unobfuscated (denoted as ‚ÄúGPT-4‚Äù) and obfus-\ncated (denoted as ‚ÄúGemini + GPT-4‚Äù) results for\nall datasets in Table 6. As shown in Table 6, the ob-\n",
  "9": "AB (hit@10)\nMR (acc)\nCI (b-acc)\nCR (acc)\nœÅ = 0.10\n0.280\n0.881\n0.689\n0.629\nœÅ = 0.15\n0.273\n0.874\n0.674\n0.622\nœÅ = 0.20\n0.256\n0.862\n0.667\n0.605\nTable 5: Privacy-Utility Trade-off: Text Adjacency\nfuscated results across all datasets are comparable\nto their unobfuscated counterparts, thus demon-\nstrating the effectiveness of our paradigm across\nvarious natural languages in addition to English.\nFor dataset details, please refer to Appendix A.3.\n5\nInference Attacks\nPrior works on prompt obfuscation (Tong et al.,\n2023; Yue et al., 2021; Qu et al., 2021) tend to\nadopt token-level recovering, where the recoverer\nis tasked to recover each token of the privatized\nprompt back to its original form, with recovery\naccuracy reported as the evaluation metric. Never-\ntheless, this metric could be biased, as even if the\nrecoverer is unable to recover the exact original to-\nkens, it may still successfully predict synonymous\ntokens or generate a recovered text with a high\ndegree of semantic similarity to the original.\nTo address this, we employ a comprehensive\nset of metrics to assess obfuscation robustness,\naccounting for the degree of: exact lexical over-\nlapping (aligning with prior works), synonym and\nparaphrase overlapping, and overall semantic simi-\nlarity between the original text and the recovered\ntext, with both LLMs and humans as recoverers.\nWe adopt the worst-case assumption by perceiving\nthe cloud-based inference LLM as untrusted, and\nevaluate all attacks on ‚ÄúGemini + GPT-4‚Äù against\nall baselines introduced in Section 4.1. For de-\ntailed descriptions on evaluation methods and re-\nsults, please refer to Appendix A.8. Table 10 in\nAppendix demonstrates that EmojiPrompt exhibits\ncomparable robustness in terms of lexical overlap\nwhen benchmarked against the baselines, while\nachieving superior performance on both synonym\noverlap and overall semantic similarity.\n6\nConclusion\nThis work introduces EmojiPrompt, a novel ob-\nfuscation paradigm designed to protect user pri-\nvacy during interactions with cloud-based LLMs.\nEmojiPrompt uses LLMs to perform generative ob-\nfuscation, transforming private data from natural\nlanguage into non-natural language forms, thus ob-\nfuscating it from both LLM and human recoverers.\nWe validate EmojiPrompt‚Äôs effectiveness across\nSD (b-acc)\nARS (b-acc)\nAS (cos)\nHAD (b-acc)\nFrench\nGerman\nGPT-4\n0.961\n0.961\n0.979\n0.681\n0.672\nGemini + GPT-4\n0.897\n0.885\n0.942\n0.642\n0.653\nTable 6: Performance on Non-English Datasets\neight datasets, showing that performance on obfus-\ncated prompts is largely preserved and, in some\ncases, even exceeds that of unobfuscated prompts.\nWe also compare EmojiPrompt against three obfus-\ncation baselines, showing it matches their perfor-\nmance on some tasks while outperforming them in\nothers, both for task inference and recovery robust-\nness. Finally, the atomic-level obfuscation design\nallows the process to be fully cloud-based, enabling\ndeployment without the need of local LLMs.\n7\nLimitations\nWe notice two potential concerns associated with\nemploying untuned LLMs for obfuscation:\nLimited Symbolic Vocabulary: the restricted\nset of symbols‚Äîsuch as emojis, emoticons, and\noperators‚Äîfrom the LLM‚Äôs vocabulary may con-\nstrain LLMO‚Äôs ability to fully capture the nuances\nof the original data. This limitation could result\nin the oversimplification or omission of intricate\ndetails in the obfuscated output. For instance, as\nshown in Figure 3, the use of a leaf emoji to denote\na product‚Äôs natural ingredients may not fully encap-\nsulate the specificities of the product‚Äôs organic com-\nposition. A potential solution, as Edemacu and Wu\n(2024) proposes, involves expanding the symbolic\nvocabulary by defining additional symbol-text map-\npings and then incorporating these mappings into\nthe cloud-based obfuscation LLMs, either through\nIn-context Learning or API-based fine-tuning.\nInaccurate Information: LLMs are prone to\nhallucination, where they generate information that\nappears plausible but is factually incorrect or en-\ntirely fabricated (Ji et al., 2023b). In this work,\nwe also observe that the obfuscation LLM has the\npotential to generate representations that introduce\nelements not present in the original data. For exam-\nple, as shown in Figure 3, a sun emoji is generated\nfor a product whose title does not specify the time\nof day for its use. While this may be viewed as\nto introduce additional noise to confuse the adver-\nsaries, it may also introduce unintended informa-\ntion. Several approaches may help mitigate this\nissue, including self-reflection (Ji et al., 2023b),\nknowledge distillation (McDonald et al., 2024), as\nwell as splitting memorization and reasoning as\ntwo separated procedures (Jin et al., 2024a).\n",
  "10": "References\nS√©bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nKonstantinos Chatzikokolakis,\nMiguel E Andr√©s,\nNicol√°s Emilio Bordenabe, and Catuscia Palamidessi.\n2013. Broadening the scope of differential privacy\nusing metrics. In Privacy Enhancing Technologies:\n13th International Symposium, PETS 2013, Bloom-\nington, IN, USA, July 10-12, 2013. Proceedings 13,\npages 82‚Äì102. Springer.\nYu Chen, Tingxin Li, Huiming Liu, and Yang Yu.\n2023. Hide and seek (has): A lightweight frame-\nwork for prompt privacy protection. arXiv preprint\narXiv:2309.03057.\nAmrita Chowdhury, David Glukhov, Divyam Anshu-\nmaan, Prasad Chalasani, Nicolas Papernot, Somesh\nJha, and Mihir Bellare. 2024. Preempt: Sanitizing\nsensitive prompts for llms. In Association for the\nAdvancement of Artificial Intelligence.\nMinxin Du, Xiang Yue, Sherman SM Chow, Tianhao\nWang, Chenyu Huang, and Huan Sun. 2023. Dp-\nforward: Fine-tuning and inference on language mod-\nels with differential privacy in forward pass. In Pro-\nceedings of the 2023 ACM SIGSAC Conference on\nComputer and Communications Security, pages 2665‚Äì\n2679.\nKennedy Edemacu and Xintao Wu. 2024.\nPrivacy\npreserving prompt engineering: A survey.\narXiv\npreprint arXiv:2404.06001.\nThorsten M Erle, Karoline Schmid, Simon H Goslar,\nand Jared D Martin. 2022. Emojis as social informa-\ntion in digital communication. Emotion, 22(7):1529.\nXi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang,\nZiqing Hu, Yanjun (Jane) Qi, Scott Nickleach, Diego\nSocolinsky, \"SHS\" Srinivasan Sengamedu, and Chris-\ntos Faloutsos. 2024. Large language models (llms) on\ntabular data: Prediction, generation, and understand-\ning - a survey. Transactions on Machine Learning\nResearch.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge,\nand Yongfeng Zhang. 2022. Recommendation as\nlanguage processing (rlp): A unified pretrain, person-\nalized prompt & predict paradigm (p5). In Proceed-\nings of the 16th ACM Conference on Recommender\nSystems, pages 299‚Äì315.\nRan Gilad-Bachrach, Nathan Dowlin, Kim Laine,\nKristin Lauter, Michael Naehrig, and John Werns-\ning. 2016. Cryptonets: Applying neural networks to\nencrypted data with high throughput and accuracy. In\nInternational conference on machine learning, pages\n201‚Äì210. PMLR.\nOtkrist Gupta and Ramesh Raskar. 2018. Distributed\nlearning of deep neural network over multiple agents.\nJournal of Network and Computer Applications,\n116:1‚Äì8.\nMeng Hao, Hongwei Li, Hanxiao Chen, Pengzhi Xing,\nGuowen Xu, and Tianwei Zhang. 2022. Iron: Pri-\nvate inference on transformers. Advances in Neural\nInformation Processing Systems, 35:15718‚Äì15731.\nThomas Holtgraves and Caleb Robinson. 2020. Emoji\ncan facilitate recognition of conveyed indirect mean-\ning. PloS one, 15(4):e0232361.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear.\nWenyue Hua, Shuyuan Xu, Yingqiang Ge, and\nYongfeng Zhang. 2023. How to index item ids for\nrecommendation foundation models. SIGIR-AP.\nZhicong Huang, Wen-jie Lu, Cheng Hong, and Jian-\nsheng Ding. 2022. Cheetah: Lean and fast secure\n{two-party} deep neural network inference. In 31st\nUSENIX Security Symposium (USENIX Security 22),\npages 809‚Äì826.\nJianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua,\nYingqiang Ge, Juntao Tan, and Yongfeng Zhang.\n2023a. Genrec: Large language model for gener-\native recommendation. ECIR.\nZiwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko\nIshii, and Pascale Fung. 2023b. Towards mitigat-\ning llm hallucination via self reflection. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, pages 1827‚Äì1843.\nMingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang,\nWenyue Hua, Ruixiang Tang, William Yang Wang,\nand Yongfeng Zhang. 2024a. Disentangling mem-\nory and reasoning ability in large language models.\narXiv preprint arXiv:2411.13504.\nMingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng\nZeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao,\nKai Mei, Yanda Meng, Kaize Ding, Fan Yang, Meng-\nnan Du, and Yongfeng Zhang. 2025.\nExploring\nconcept depth: How large language models acquire\nknowledge and concept at different layers?\nIn\nProceedings of the 31st International Conference\non Computational Linguistics, pages 558‚Äì573, Abu\nDhabi, UAE. Association for Computational Linguis-\ntics.\nMingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao,\nWenyue Hua, Yanda Meng, Yongfeng Zhang, and\nMengnan Du. 2024b. The impact of reasoning step\nlength on large language models. In Findings of the\nAssociation for Computational Linguistics ACL 2024,\npages 1830‚Äì1842, Bangkok, Thailand and virtual\nmeeting.\n",
  "11": "Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao\nZhou, Chong Zhang, Yongfeng Zhang, et al. 2024c.\nAttackeval:\nHow to evaluate the effectiveness\nof jailbreak attacking on large language models.\narXiv:2401.09002.\nChiraag Juvekar, Vinod Vaikuntanathan, and Anantha\nChandrakasan. 2018. {GAZELLE}: A low latency\nframework for secure neural network inference. In\n27th USENIX Security Symposium (USENIX Security\n18), pages 1651‚Äì1669.\nZhigang Kan, Linbo Qiao, Hao Yu, Liwen Peng, Yifu\nGao, and Dongsheng Li. 2023. Protecting user pri-\nvacy in remote conversational systems: A privacy-\npreserving framework based on text sanitization.\narXiv preprint arXiv:2306.08223.\nRyo Kurokawa, Yuji Ohizumi, Jun Kanzawa, Mariko\nKurokawa, Yuki Sonoda, Yuta Nakamura, Takao\nKiguchi, Wataru Gonoi, and Osamu Abe. 2024. Di-\nagnostic performances of claude 3 opus and claude\n3.5 sonnet from patient history and key images in ra-\ndiology‚Äôs ‚Äúdiagnosis please‚Äù cases. Japanese Journal\nof Radiology, pages 1‚Äì4.\nLei Li, Yongfeng Zhang, and Li Chen. 2023. Prompt\ndistillation for efficient llm-based recommendation.\nIn Proceedings of the 32nd ACM International Con-\nference on Information and Knowledge Management,\npages 1348‚Äì1357.\nJianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua\nDu, Bo Chen, Shigang Quan, Ruiming Tang, Yong\nYu, and Weinan Zhang. 2024a.\nRella: Retrieval-\nenhanced large language models for lifelong sequen-\ntial behavior comprehension in recommendation. In\nProceedings of the ACM on Web Conference 2024,\npages 3497‚Äì3508.\nXinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli\nFeng, Yinwei Wei, and Tat-Seng Chua. 2024b. Data-\nefficient fine-tuning for llm-based recommendation.\nIn Proceedings of the 47th International ACM SI-\nGIR Conference on Research and Development in\nInformation Retrieval, pages 365‚Äì374.\nJunling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan\nZhang. 2023. Is chatgpt a good recommender? a\npreliminary study. arXiv preprint arXiv:2304.10149.\nXuanqi Liu and Zhuotao Liu. 2023.\nLlms can\nunderstand encrypted prompt:\nTowards privacy-\ncomputing friendly transformers.\narXiv preprint\narXiv:2305.18396.\nKai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao,\nQipeng Guo, and Xipeng Qiu. 2023. Full parameter\nfine-tuning for large language models with limited\nresources. arXiv preprint arXiv:2306.09782.\nLingjuan Lyu, Yitong Li, Xuanli He, and Tong Xiao.\n2020. Towards differentially private text representa-\ntions. In Proceedings of the 43rd International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval, pages 1813‚Äì1816.\nPeihua Mai, Ran Yan, Zhe Huang, Youjia Yang, and\nYan Pang. 2024. Split-and-denoise: Protect large lan-\nguage model inference with local differential privacy.\nICML.\nDaniel McDonald, Rachael Papadopoulos, and Leslie\nBenningfield. 2024. Reducing llm hallucination us-\ning knowledge distillation: A case study with mistral\nlarge and mmlu benchmark. Authorea Preprints.\nPratyush Mishra, Ryan Lehmkuhl, Akshayaram Srini-\nvasan, Wenting Zheng, and Raluca Ada Popa. 2020.\nDelphi: A cryptographic inference system for neural\nnetworks. In Proceedings of the 2020 Workshop on\nPrivacy-Preserving Machine Learning in Practice,\npages 27‚Äì30.\nMilad Nasr,\nNicholas Carlini,\nJonathan Hayase,\nMatthew Jagielski, A Feder Cooper, Daphne Ippolito,\nChristopher A Choquette-Choo, Eric Wallace, Flo-\nrian Tram√®r, and Katherine Lee. 2023. Scalable ex-\ntraction of training data from (production) language\nmodels. arXiv preprint arXiv:2311.17035.\nHumza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad\nSaqib, Saeed Anwar, Muhammad Usman, Naveed\nAkhtar, Nick Barnes, and Ajmal Mian. 2023.\nA\ncomprehensive overview of large language models.\narXiv preprint arXiv:2307.06435.\nMark A Przybocki, Gregory A Sanders, and Audrey N\nLe. 2006. Edit distance: A metric for machine trans-\nlation evaluation. In LREC, pages 2038‚Äì2043.\nChen Qu, Weize Kong, Liu Yang, Mingyang Zhang,\nMichael Bendersky, and Marc Najork. 2021. Natural\nlanguage understanding with privacy-preserving bert.\nIn Proceedings of the 30th ACM International Con-\nference on Information & Knowledge Management,\npages 1488‚Äì1497.\nDeevashwer Rathee, Mayank Rathee, Nishant Kumar,\nNishanth Chandran, Divya Gupta, Aseem Rastogi,\nand Rahul Sharma. 2020. Cryptflow2: Practical 2-\nparty secure inference. In Proceedings of the 2020\nACM SIGSAC Conference on Computer and Commu-\nnications Security, pages 325‚Äì342.\nMelissa Roemmele and Andrew S Gordon. 2018. Au-\ntomated assistance for creative writing with an rnn\nlanguage model. In Proceedings of the 23rd interna-\ntional conference on intelligent user interfaces com-\npanion, pages 1‚Äì2.\nHamidreza Rouzegar and Masoud Makrehchi. 2024.\nEnhancing text classification through llm-driven ac-\ntive learning and human annotation. arXiv preprint\narXiv:2406.12114.\nYuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and\nDongmei Zhang. 2024. Table meets llm: Can large\nlanguage models understand structured table data?\na benchmark and empirical study. In Proceedings\nof the 17th ACM International Conference on Web\nSearch and Data Mining, pages 645‚Äì654.\n",
  "12": "Gemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai,\nAnja Hauth, et al. 2023.\nGemini: a family of\nhighly capable multimodal models. arXiv preprint\narXiv:2312.11805.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivi√®re, Mihir Sanjay Kale,\nJuliette Love, et al. 2024. Gemma: Open models\nbased on gemini research and technology.\narXiv\npreprint arXiv:2403.08295.\nMeng Tong, Kejiang Chen, Yuang Qi, Jie Zhang, Weim-\ning Zhang, and Nenghai Yu. 2023. Privinfer: Privacy-\npreserving inference for black-box large language\nmodel. arXiv preprint arXiv:2310.12214.\nRaja Vavekanand and Kira Sam. 2024. Llama 3.1: An\nin-depth analysis of the next-generation large lan-\nguage model.\nPraneeth Vepakomma, Otkrist Gupta, Tristan Swedish,\nand Ramesh Raskar. 2018. Split learning for health:\nDistributed deep learning without sharing raw patient\ndata. arXiv preprint arXiv:1812.00564.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\nQuoc V Le, Denny Zhou, and Xinyun Chen. 2024.\nLarge language models as optimizers. ICLR.\nYuzhe You and Jian Zhao. 2024. Gamifying xai: En-\nhancing ai explainability for non-technical users\nthrough llm-powered narrative gamifications. arXiv\npreprint arXiv:2410.04035.\nHuizi Yu, Lizhou Fan, Lingyao Li, Jiayan Zhou, Zihui\nMa, Lu Xian, Wenyue Hua, Sijia He, Mingyu Jin,\nYongfeng Zhang, et al. 2024. Large language mod-\nels in biomedical and health informatics: A review\nwith bibliometric analysis. Journal of Healthcare\nInformatics Research, 8(4):658‚Äì711.\nXiang Yue, Minxin Du, Tianhao Wang, Yaliang Li,\nHuan Sun, and Sherman SM Chow. 2021. Differ-\nential privacy for text analytics via natural text saniti-\nzation. arXiv preprint arXiv:2106.01221.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert.\narXiv preprint\narXiv:1904.09675.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023. Large language models are human-level\nprompt engineers. ICLR.\nA\nAppendix\nA.1\nObfuscation on Tabular Data\nAnother example for Reusable Obfuscation is the\nuse of LLMs for processing tabular data, a type of\ndata commonly encountered in medical and finan-\ncial decision-making. In these instances, each user\nis characterized by a set of predefined features (or\nattributes), with a specific value on each feature.\nFor example, Figure 4 illustrates a simple tabular\ndataset with {Age, Work Class, Education} as fea-\ntures, while each feature has two possible levels\n(e.g., 19 and 75 for Age). In such a scenario, the\nLLMO is first used to obfuscate each level of ev-\nery feature. An obfuscated user representation is\nthen formed by aggregating all relevant obfuscated\nfeature values, as depicted by Figure 4.\nPlease note that for numerical features with con-\ntinuous values (e.g., Age or Height), if the feature‚Äôs\ncardinality exceeds 100, we apply quantile-based\ndiscretization to cast the cardinality to 100, and\nthen employ the LLMO to obfuscate each value.\nA.2\nConstraint Implementation Specifics\nAs inspired by prior works (Qu et al., 2021; Mai\net al., 2024), we also implement a relaxation of\nour semantic constraint. Specifically, we initiate by\nranking all texts in an arbitrary order and proceed\nto sequentially obfuscate each text using an obfus-\ncation LLM. For each text, if it has no adjacent\ntexts, we directly assign the generated obfuscation.\nOtherwise, we iterate through all its adjacent texts.\nFor each adjacent text that has been assigned an\nobfuscation, we compute the BertScore similarity\nbetween the newly generated obfuscation and the\nobfuscation of the adjacent text. To ensure that\nthe semantic constraint is maintained, we check\nwhether the two obfuscations retain at least 1/œµ of\nthe original similarity between their unobfuscated\ncounterparts. If the generated obfuscation satisfies\nthis constraint for all its adjacent texts that have\nbeen obfuscated, we accept it and proceed to the\nnext text. However, if the constraint is violated for\nany adjacent text, we waitlist the obfuscation and\ngenerate a new candidate. This process is repeated\niteratively, up to a predefined number of attempts\n(set to 10 by default). If no valid obfuscation is\nfound within the allowed attempts, we select the\nobfuscation with the highest mean semantic simi-\nlarity across all its adjacent texts relative to other\nfailed candidates. For simplicity, we compute a\nbidirectional BertScore to make it symmetric.\nFor the Post-Sampling Constraint implementa-\ntion, we initiate by employing an obfuscation LLM\nto generate the obfuscation for each text within a\ntask domain. Next, we construct a graph where\neach text is represented as a node, and two nodes\n",
  "13": "Age\nWork Class\nEducation\nUser 1\nüë§üîÑüî¢‚û°\nüè¢->üîí+üíº\nüéì‚û°üìú=\nUser 2\nüî¢üîÑüéÇ=0\nüë§-üíº->üöÄ\nüéì+üìú(\n \nAge\nWork Class\nEducation\nUser 1\n19\nprivate\nHS-grad\nUser 2\n75\nSelf-emp-not-inc\nMasters\nInference\nLLM\nYes\nObfuscation LLM \nReusable Text\nObfuscated Text\nObfuscated Prompt\nOutput\nGiven a user with Age: üë§üîÑüî¢‚û°,\nWork Class: üè¢->üîí+üíº, EducaLon: \nüéì‚û°üìú=; will his income exceed 50k?\nFigure 4: Illustration of EmojiPrompt for preserving user privacy on tabular data.\nare connected if the corresponding texts are con-\nsidered adjacent. We then proceed by repeatedly\nfinding the maximum clique in the graph. For each\nclique found, we retrieve the obfuscations of all\ntexts within the clique and perform a post-sampling\non such obfuscations with a probability distribution\ncomputed based on œµ to obtain the final obfusca-\ntion for each node (or text) within the clique, and\nthen remove all nodes within that clique from the\ngraph. This process continues until the size of the\nmaximum clique found is one.\nA.3\nData and Metric Specifics\nA.3.1\nDataset Description\nThe Amazon 7 datasets are collected from the Ama-\nzon.com platform with user ratings and reviews on\nproducts they have purchased on 29 categories of\nproducts. In this paper, we concentrate on evaluat-\ning performance on the Beauty and Toy categories.\nThe Movie Reviews 8 dataset is an aggregation\nof 50,000 movie reviews from the IMDB database.\nThese reviews, composed in natural language, are\nevenly distributed across two sentiment classes,\nwith 25,000 reviews categorized as positive, and\nthe remaining 25,000 categorized as negative.\nThe Email Spam 9 dataset consists 5,695 email\nmessages covering a variety of topics, where each\n7https://jmcauley.ucsd.edu/data/amazon/\n8https://www.kaggle.com/\ndatasets/lakshmi25npathi/\nimdb-dataset-of-50k-movie-reviews\n9https://www.kaggle.com/datasets/jackksoncsie/\nspam-email-dataset\nmessage includes the body of the email along with\nany associated subject lines or headers. Among\nthese messages, 1,368 are labeled as spam.\nThe Census Income 10 dataset originates from\nthe 1994 U.S. Census database. It encompasses\n14 demographic attributes per record, providing\na comprehensive overview of individual census\nrespondents. The primary target variable in this\ndataset is a binary indicator, signifying whether an\nindividual‚Äôs annual income exceeds $50,000.\nThe Heart Disease dataset 11 is originally from\nthe CDC, which conducts annual telephone surveys\nto collect data on the health status of U.S. residents.\nIt contains 18 health-related attributes per sample,\nwith the primary target variable being whether an\nindividual has heart disease.\nThe Comprehensive Reading (Comp. Reading)\ndataset 12 comprises 15,000 multiple-choice ques-\ntions, balanced and annotated by question type\nacross four distinct domains: news, user stories, fic-\ntion, and blogs. The questions are crafted to assess\nthe reader‚Äôs comprehension of the accompanying\ntext passages. Each question has four choices.\nThe Highlight Generation dataset 13 contains\n10https://archive.ics.uci.edu/dataset/2/adult\n11https://www.kaggle.com/datasets/kamilpytlak/\npersonal-key-indicators-of-heart-disease\n12https://www.kaggle.com/\ndatasets/thedevastator/\nintroducing-quail-a-comprehensive-reading-compre?\nresource=download\n13https://huggingface.co/datasets/abisee/cnn_\ndailymail\n",
  "14": "Optimization\nAB (hit@10)\nAT (hit@10)\nMR (acc)\nES (b-acc)\nCI (b-acc)\nHD (b-acc)\nCR (acc)\nHG (cos)\nGPT-4\nN/A\n0.292\n0.415\n0.957\n0.949\n0.635\n0.648\n0.671\n0.755\nLlama + GPT-4\nManual\n0.257\n0.371\n0.856\n0.859\n0.692\n0.705\n0.607\n0.687\nLlama + GPT-4\nAPE\n0.261\n0.376\n0.867\n0.851\n0.699\n0.701\n0.611\n0.691\nLlama + GPT-4\nOPRO\n0.268\n0.383\n0.871\n0.862\n0.707\n0.712\n0.618\n0.697\nTable 7: Model Performance. For A + B, A is the obfuscation LLM and B is the inference LLM. GPT-4 refers to\ninferencing with unobfuscated prompts.\nunique news articles written by journalists at CNN\nand the Daily Mail, where each article is accom-\npanied by a highlight written by the article author.\nThe highlight has a mean token count of 56, while\nthe article has a mean token count of 781.\nFor non-English datasets, the Multilingual Spam\nDetection data 14 consists 5,157 unique passages\nin three languages, with 13% labeled as spam. The\nAmazon Review Sentiment data 15 consists cus-\ntomer reviews in various languages. The Article\nSummarization data 16 is from the official daily\nnews, where each article is accompanied by a head-\nline summarizing its content. For the Heart Attack\ndataset, we employ an LLM to translate all feature\nvalues from the Heart Disease dataset into Spanish.\nA.3.2\nMetrics Employed\nFor Amazon Beauty and Toy datasets, we use\nHit@k as the evaluation metric; it computes the\npercentage of ranking lists that include at least one\npositive item in the top-K highest ranked items (we\nset K = 10, thus denoting it as hit@10). For the\nMovie Review and Comp. Reading datasets, we\nuse accuracy as the evaluation metric, as the senti-\nment label and the question types are both balanced\n(i.e., with 50% of reviews being positive, and the\nrest being negative). For the Email Spam, Census\nIncome, and Heart Disease datasets, we employ\nBalanced Accuracy as the evaluation metric, due to\nthe imbalanced nature of the target feature within\neach dataset. To clarify, Balanced Accuracy is com-\nputed as the average of the proportion of correctly\npredicted instances in each class, thus ensuring a\nfair assessment of model performance across both\nmajority and minority classes. For the Highlight\nGeneration dataset, we utilize Cosine Similarity as\nmetric to quantitatively assess the semantic congru-\nence between the target highlights (author gener-\nated) and those generated by the LLMI.\n14https://www.kaggle.com/datasets/rajnathpatel/\nmultilingual-spam-data/data\n15https://github.com/tyqiangz/\nmultilingual-sentiment-datasets/tree/main/data\n16https://www.kaggle.com/datasets/noxmoon/\nchinese-official-daily-news-since-2016\nA.3.3\nData Processing for Modeling\nTo conduct experiments on the Amazon Beauty\nand Toy datasets, we extract the complete purchase\nhistory for each user and retain the 15 most recently\npurchased items to assess the LLM‚Äôs performance\nin sequential recommendation. Inspired by prior\nworks (Geng et al., 2022; Hua et al., 2023; Ji et al.,\n2023a), we designate the most recently purchased\nitem as the ground truth or positive sample, with the\nremaining 14 items as the user‚Äôs interaction history.\nGiven the finite context window size of the LLM,\nwe adhere to the evaluation methodology proposed\nby (Geng et al., 2022; Liu et al., 2023) by randomly\nselecting 99 items from the entire set of beauty/toy\nproducts to serve as negative samples. These 100\nsampled items collectively form a list of potential\ncandidates for the LLMI to rank on. Subsequently,\nwe submit both the user‚Äôs interaction history, in\nobfuscated form, and the sampled candidate list\nto the LLMI, then prompt it to generate a list of\ntop-K recommended items for the user, based on\nthe user‚Äôs interaction history (we set K = 10).\nFor the Movie Reviews dataset, we furnish the\nLLMI with a user‚Äôs movie review in obfuscated\nform and instruct it to produce a binary sentiment\nclassification (positive or negative) for the review,\ndevoid of any accompanying explanations. A sim-\nilar practice is applied to the Email Spam dataset,\nwhere we prompt the LLMI with an obfuscated\nemail and request the model to determine whether\nit is spam, also without additional explanations.\nFor the Census Income and Heart Disease\ndatasets, we provide the LLMI with obfuscated\nversions of each individual‚Äôs attribute-value pairs.\nWe then prompt the model to make a binary deci-\nsion by responding with either ‚Äúyes‚Äù or ‚Äúno‚Äù, with-\nout further elaboration. Specifically, the LLMI is\nasked to determine whether an individual‚Äôs annual\nincome exceeds $50,000 or whether the individual\nhas heart disease, respectively. Given that the Cen-\nsus Income dataset was collected in 1994, we ex-\nplicitly instruct the LLMI to determine whether the\nindividual‚Äôs income would have exceeded $50,000\nin that year, in order to account for inflation.\n",
  "15": "For the Comp. Reading dataset, we provide the\nLLMI with the privatized passage, along with the\nquestion statement as well as the four choices in\nnatural language form. Subsequently, we prompt\nthe LLMI to select the most appropriate choice that\nanswers the question based on its comprehension\nof the obfuscated passage. Lastly, for the Highlight\nGeneration dataset, we provide the LLMI with the\nprivatized article and then prompt it to generate\na highlight for the article, while ensuring that the\ntoken count of the LLM-generated highlight is less\nthan or equal to that of the target highlight.\nA.3.4\nBudget Information\nFor this work, we allocate a total of $1750 as we\nperform a vast amount of evaluations, covering\nboth performance and simulated inference attacks.\nA.4\nAtomic Obfuscation Against Leakage\nFor the Trusted scenario, our atomic-level obfus-\ncation helps to mitigate the risk of privacy leakage\nfrom jailbreaking attacks. Even if attackers man-\nage to obtain one or more queries from the LLM‚Äôs\nplatform through triggering prompts (as discussed\nin Section 1), these queries remain largely unin-\nterpretable because the user privacy is obfuscated.\nTo accurately recover user privacy, attackers must\nacquire the complete set of text-obfuscation pairs\nfor all entities in the task domain. For instance,\nrecovering a user‚Äôs purchase history would require\nrecovering the representations of all product titles,\nwhich could number in the millions. This require-\nment substantially increases the difficulty of the\nattack compared to simply accessing a few queries.\nFor the Untrusted scenario, our atomic-level\nobfuscation preserves user privacy from both the\nobfuscation LLM and the inference LLM. For\nthe obfuscation LLM, we prompt it to obfuscate\neach individual entity instead of the entire piece\nof private text. For instance, on product recom-\nmendation task, we prompt it to obfuscate each\nindividual product title instead of the entire user\npurchase/interaction history. The rationale is that,\nwhile a user‚Äôs purchase history is sensitive, the\nindividual products are not, as they are publicly\navailable on platforms for customers to browse.\nSimilarly, for review (or text in natural language in\ngeneral) obfuscations, the obfuscation LLM only\nsees the highly segmented clauses instead of the\nentire piece of text; such clauses can be merged in\nnumerous ways, rendering it ambiguous on the real\ncontent of the text. Thus, even if the server host\nand/or third-party probers obtain such information,\nit would still be challenging for them to restore\nthe user privacy. For the inference LLM, we only\nprompt it to perform task inferencing with user pri-\nvacy in obfuscated form, so that even if the server\nhost and/or third-party obtain such information, it\nwould be difficult to interpret the user privacy, as\nthey are represented in non-natural language.\nA.5\nBaseline Specifics\nFor TokEmbPriv, we perturb the token embedding\nby incorporating stochastic noise prior to uploading\nto the server. This perturbation is implemented\nby introducing random noise Z drawn from a d-\ndimensional distribution characterized by p(N) ‚àù\nexp(‚àíŒ∑‚à•N‚à•). We set the privacy parameter Œ∑ =\n100 to balance utility and privacy, according to the\nevaluations presented in the paper. We also employ\nthe text-to-text privatization (this post-processing\nprocedure does not affect privacy guarantees).\nFor InferDPT, it comprises two modules: (1) the\nPerturbation Module, which generates a perturbed\ntext via œµ-LDP by replacing each token in the text\nwith another from a predefined vocabulary, and (2)\nthe Extraction Module, a locally hosted LLM (less\ncapable than the inference LLM) that reconstructs\nthe noisy output from the cloud-based inference\nLLM to better align with the original prompt.\nWe adhere to the methodology described in the\npaper by employing RANTEXT as the differential\nprivacy mechanism, which, according to the paper,\noffers superior perturbation performance compared\nto existing state-of-the-art mechanisms. At its core,\nInferDPT randomly selects replacement tokens that\nare sufficiently close to the original token in the\nembedding space, with probabilities exponentially\nproportional to proximity. We replace Insw with\nour task-specific instruction. We set œµ = 10 to bal-\nance between utility and privacy, as demonstrated\nby the synonym evaluation section of the paper.\nAs for the Extraction Module, we utilize Gemma\n(2B) (Team et al., 2024) to refine open-text out-\nputs from the inference LLM (i.e., outputs that are\nnot confined to a predefined set of tokens). For\nTabular datasets, we alter the sampling ranges for\nnumerical values to enhance performances.\nFor Split-N-Denoise, it also adopts dx-privacy\nto perform LDP based token-level perturbations,\nwhile introducing a novel trained local de-noising\nLLM to denoise outputs generated by the infer-\nence LLM with perturbed inputs. This de-noising\nLLM takes as input the raw user input embedding,\n",
  "16": "AB (hit@10)\nAT (hit@10)\nMR (acc)\nES (b-acc)\nCI (b-acc)\nHD (b-acc)\nCR (acc)\nHG (cos)\nGemini-1.5\n0.263\n0.379\n0.955\n0.951\n0.641\n0.653\n0.662\n0.759\nGemini-1.5 + Gemini-1.5\n0.258\n0.363\n0.887\n0.891\n0.719\n0.687\n0.645\n0.724\nGPT-3.5 + Gemini-1.5\n0.246\n0.349\n0.863\n0.872\n0.692\n0.681\n0.618\n0.703\nLlama + Gemini-1.5\n0.243\n0.338\n0.857\n0.859\n0.704\n0.695\n0.611\n0.689\nTable 8: Model Performance. For A + B, A is the obfuscation LLM and B is the inference LLM. Gemini-1.5 refers\nto inferencing with unobfuscated prompts. We employ OPRO for obfuscation prompt optimization.\nAB (hit@10)\nAT (hit@10)\nMR (acc)\nES (b-acc)\nCI (b-acc)\nHD (b-acc)\nCR (acc)\nHG (cos)\nClaude-3.5\n0.271\n0.386\n0.960\n0.949\n0.702\n0.711\n0.664\n0.753\nClaude-3.5 + Claude-3.5\n0.262\n0.381\n0.891\n0.884\n0.719\n0.725\n0.639\n0.715\nGPT-3.5 + Claude-3.5\n0.241\n0.369\n0.874\n0.869\n0.715\n0.731\n0.612\n0.699\nLlama + Claude-3.5\n0.237\n0.362\n0.862\n0.853\n0.722\n0.727\n0.609\n0.684\nTable 9: Model Performance. For A + B, A is the obfuscation LLM and B is the inference LLM. Claude-3.5 refers\nto inferencing with unobfuscated prompts. We employ OPRO for obfuscation prompt optimization.\nthe noise matrix, and the noisy embedding from\nthe cloud-based inference LLM. To train the de-\nnoising LLM in a privacy-preserving manner, a\npublic dataset similar to the modeling dataset is\nneeded. In this work, we utilize the validation set\nreserved for prompt optimization to train the lo-\ncal de-noising LLM. Since cloud-based inference\nLLMs, such as GPT-4 Turbo, require text as in-\nput, we adopt the text-to-text privatization method\nfrom TokEmbPriv, mapping perturbed token em-\nbeddings to their nearest tokens in the embedding\nspace. We also map inference LLM outputs to their\nembeddings to facilitate de-noising LLM training.\nLastly, we refer to the following GitHub reposito-\nries (while modifying code if needed) for executing\nthe baseline models: InferDPT is hosted at https:\n//github.com/mengtong0110/InferDPT, while\nTokEmbPriv\nand\nSplit-N-Denoise\nare\navail-\nable at https://github.com/NusIoraPrivacy/\neaas-privacy/tree/master.\nA.6\nPrompt Optimization Specifics\nWe reserve 1,000 samples per dataset as validation\nset for prompt search, with the remaining data used\nfor performance evaluation. Firstly, we manually\ntune the obfuscation prompt for both ‚ÄúGPT-4 +\nGPT-4‚Äù and ‚ÄúGemini + GPT-4‚Äù. We then employ\nAPE and OPRO for automatic prompt optimization\non the obfuscation prompt for both model variants.\nFor both optimization algorithms, we use GPT-3.5\nTurbo as the prompt generator to propose candi-\ndate obfuscation prompts based on a fixed, manu-\nally crafted meta-prompt. Also, we do not provide\ninput-output pairs in the meta-prompt, as there is\nno universal ground-truth in how an entity should\nbe obfuscated into its non-linguistic form.\nFor APE, during the Monte Carlo search, 7 can-\ndidate prompts are generated per iteration. To en-\nhance efficiency, we implement early-stopping dur-\ning the evaluation of each candidate prompt and\nconcluded the search after 6 iterations. Specifically,\nfor every 50 samples, we compare the average per-\nformance of the current candidate prompt with that\nof the best-performing prompt. If the current candi-\ndate prompt underperforms the best prompt for two\nconsecutive comparisons, we terminate its evalu-\nation and move on to the next candidate. If the\ncurrent prompt exceeds the performance of the\nbest-performing prompt for the entire validation\nset, it will be updated as the new best-performing\nprompt. Additionally, we introduce a slight modi-\nfication to the standard algorithm: instead of con-\nducting a greedy approach that generates candidate\nprompts for the next iteration solely based on the\nbest-performing prompt from the current iteration,\nwe generate candidate prompts for the subsequent\nround from the top two performing prompts.\nFor OPRO, we adopt the original workflow pre-\nsented in the paper, where for each newly generated\nobfuscation prompt, we evaluate its performance\nscore on the validation set (with early-stopping),\nand augment the prompt-score pair to the meta\nprompt. We repeat this process for up to 40 iter-\nations, then employ the prompt with best perfor-\nmance. Nevertheless, we observe that initiating\nthe search with a single obfuscation prompt may\nlead to repeated generation of prompts that are se-\nmantically similar to the first prompt in subsequent\nrounds, thus resulting in only minimal differences\nin performance. To resolve this, we introduce a\nmodification by requesting the prompt generator to\nproduce three distinct prompts and record their re-\nspective performances on the validation set. These\nthree prompt-score pairs are then used to initiate\n",
  "17": "the search process, promoting the generation of\nmore diverse prompts in subsequent rounds.\nA.7\nAdditional Performance Results\nIn this section, we utilize two additional cloud-\nbased LLMs for task inferencing to evaluate\nwhether private data obfuscated via our paradigm\ncan be interpreted by LLMs other than GPT-4\nTurbo. Specifically, we employ Gemini 1.5 Pro\n(denoted as Gemini-1.5) and Claude 3.5 Sonnet\n(Kurokawa et al., 2024) (denoted as Claude-3.5).\nAligning with Section 4, we use the same inference\nLLM as the obfuscation LLM for the Trusted sce-\nnario, while employing GPT-3.5 Turbo (denoted as\nGPT-3.5) and Llama 3.1 (8B) (denoted as Llama) as\nobfuscation LLMs for the Untrusted scenario. We\nemploy OPRO for obfuscation prompt optimiza-\ntion, as it has achieve best overall performance\nfor both scenarios according to Section 4.2. As\ndemonstrated by Table 8 and Table 9, for both in-\nference LLMs, obfuscating user private data from\nnatural to non-natural language retains sufficient in-\nformativeness for task inference by the same LLM,\nas evidenced by the performance of ‚ÄúGemini-1.5\n+ Gemini-1.5‚Äù as well as ‚ÄúClaude-3.5 + Claude-\n3.5‚Äù. Such approach is also extensible when using\ndifferent LLMs for obfuscation and inference, as\ndemonstrated by the performance of ‚ÄúGPT-3.5 +\nGemini-1.5‚Äù, ‚ÄúLlama + Gemini-1.5‚Äù, ‚ÄúGPT-3.5 +\nClaude-3.5‚Äù, and ‚ÄúLlama + Claude-3.5‚Äù.\nA.8\nInference Attack Details\nA.8.1\nLLM-based Attack\nIn this section, we adopt from prior works (Tong\net al., 2023; Mai et al., 2024) by proposing a sim-\nulated attack that leverages the LLMI to recover\nobfuscated texts. We assume that the adversary sup-\nplies the LLMI with details regarding the obfusca-\ntion methodology. Specifically, for EmojiPrompt,\nthe LLMI is informed that the obfuscation involves\ntransforming natural language into a non-natural\nform using LLMs. For all baselines, the LLMI\nis made aware that token-level replacements are\nperformed, with candidate tokens chosen based on\ntheir proximity within the embedding space. The\nLLMI is subsequently tasked with reversing this\ntransformation to recover the original text from\nits obfuscated representation. Furthermore, we as-\nsume that the adversary provides the LLMI with\ntask-specific context, such as indicating that the\nobfuscated text corresponds to a movie review.\nWhile prior works employ the token-level re-\ncovery rate to assess obfuscation robustness, we\nbelieve this approach may be an over-simplification.\nFor instance, even if a recoverer fails to accurately\nrecover the exact tokens in an obfuscated product\nreview, it may still produce synonymous tokens or\ngenerate a recovered text that retains high semantic\nsimilarity to the original content, thus leaking the\nunderlying meaning of the review.\nTo provide a more comprehensive evaluation\nagainst this attack, we employ both semantic and\ntoken-level metrics to assess the degree of seman-\ntic similarity and lexical overlapping between the\nLLM-recovered text and the original text. For se-\nmantic similarity, we compute the Cosine Similar-\nity score between the original and recovered texts,\nemploying the ‚Äútext-embedding-3-small‚Äù model\nfrom OpenAI for embedding vector generations,\nwith the embedding dimension set to 200 to miti-\ngate the curse of dimensionality following a prior\nwork (Lin et al., 2024a). A higher similarity score\nwould suggest lower obfuscation performance, in-\ndicating that the LLMI can decode an obfuscated\ntext to be more semantically similar to the original.\nAdditionally, for token-level metrics, we compute\nboth ROUGE (1, 2, and L) and METEOR scores\nbetween the original and recovered texts, taking\ninto account identical tokens, synonyms, and para-\nphrases in the overlap. Similar to Cosine Similarity,\na higher lexical overlapping would indicate that the\nobfuscation mechanism is less effective.\nWe now proceed to specify how the evaluation is\nconducted. For reusable text, we employ the Ama-\nzon Beauty and Census Income datasets, while for\nnon-reusable text, we utilize the Movie Review and\nComp. Reading datasets. On the Amazon Beauty\ndataset, we present the title of each beauty product\nin its obfuscated form to the LLMI, prompting the\nmodel to infer the original title of the beauty prod-\nuct based on its obfuscated representation. We then\ncompute the Cosine Similarity as well as ROUGE\nand METEOR scores between the original product\ntitle and the inferred product title. Similarly, for the\nCensus Income dataset, we instruct the LLMI to re-\ncover each obfuscated feature value, providing the\nfeature name as context, and then compute the Co-\nsine Similarity as well as ROUGE and METEOR\nscores between the original and recovered feature\nvalues. As for movie reviews and articles (from\nComp. Reading), we direct the LLMI to recover\neach obfuscated review (or article) back to its nat-\nural language form, and then compute the Cosine\n",
  "18": "Amazon Beauty\nMovie Review\nCensus Income\nComp. Reading\nCosSim\nGemini + GPT-4\n0.531\n0.607\n0.467\n0.641\nSnD + GPT-4\n0.609\n0.641\n0.518\n0.673\nInferDPT + GPT-4\n0.617\n0.635\n0.525\n0.682\nTEP + GPT-4\n0.622\n0.657\n0.534\n0.694\nRouge-1\nGemini + GPT-4\n0.211\n0.183\n0.205\n0.228\nSnD + GPT-4\n0.207\n0.187\n0.199\n0.221\nInferDPT + GPT-4\n0.205\n0.193\n0.187\n0.219\nTEP + GPT-4\n0.214\n0.179\n0.201\n0.223\nRouge-2\nGemini + GPT-4\n0.061\n0.035\n0.039\n0.047\nSnD + GPT-4\n0.058\n0.037\n0.043\n0.051\nInferDPT + GPT-4\n0.054\n0.031\n0.045\n0.039\nTEP + GPT-4\n0.063\n0.042\n0.036\n0.045\nRouge-L\nGemini + GPT-4\n0.207\n0.171\n0.198\n0.215\nSnD + GPT-4\n0.201\n0.165\n0.187\n0.209\nInferDPT + GPT-4\n0.198\n0.179\n0.191\n0.212\nTEP + GPT-4\n0.203\n0.163\n0.195\n0.218\nMETEOR\nGemini + GPT-4\n0.183\n0.152\n0.178\n0.191\nSnD + GPT-4\n0.219\n0.193\n0.199\n0.238\nInferDPT + GPT-4\n0.228\n0.189\n0.202\n0.241\nTEP + GPT-4\n0.231\n0.196\n0.207\n0.247\nTable 10: Recovering Robustness on LLM-based Attacks across 5 Metrics. For A + B, A is the obfuscation method,\nand B is the inference LLM. SnD, InferDPT, and TEP are the obfuscation methods used as baselines. Any output\nmore than 1% lower than the best performance among the baseline models (lower is better) is highlighted in bold.\nSimilarity as well as ROUGE and METEOR scores\nbetween the original and the recovered review. For\neach dataset, we compute the mean scores among\nall entities on each metric. All scores for the Emo-\njiPrompt and the baselines are shown in Table 10.\nIn addition to benchmarking our EmojiPrompt\nagainst the 3 baseline models, we introduce a hypo-\nthetical baseline, referred to as ‚ÄúRandom Entities‚Äù,\nto enable a more comprehensive evaluation of ob-\nfuscation robustness. To establish this baseline, we\nrandomly sample a subset of N entities (we set\nN = 5) from the same dataset for each entity. We\nthen compute the mean Cosine Similarity, ROUGE\n(1, 2, and L), and METEOR scores between the\nentity and each of the random entities. For exam-\nple, in the Movie Review dataset, we randomly\nsample five reviews for each original review, calcu-\nlate the Cosine Similarity, ROUGE, and METEOR\nscores between the original review and each of the\nfive selected reviews. We then compute the mean\nscore of the five scores across all three metrics.\nThis procedure is repeated for all reviews in the\ndataset to obtain the overall mean scores for the\nthree metrics. The same methodology is applied to\nall entities in the Amazon Beauty, Census Income,\nand Comp. Reading datasets to compute their re-\nspective overall mean scores. All scores for the\n‚ÄúRandom Entities‚Äù baseline are shown in Table 11.\nSuch scores serve as a strong baseline for obfus-\ncation performance, because if the semantic simi-\nlarity (and degree of lexical overlapping) of a recov-\nered review falls below this baseline, it indicates\nthat the recovering LLM generates a less relevant\noutput compared to a set of randomly selected texts,\ndemonstrating very effective obfuscation.\nAs Table 10 showcases, when compared to the\nbaselines, EmojiPrompt demonstrates similar ro-\nbustness in lexical overlap, as evidenced by the\nROUGE scores, while outperforming them in both\nsynonym overlap and overall semantic similarity,\nas indicated by the METEOR and Cosine Similar-\nity scores. While the scores are higher than those\nfor ‚ÄúRandom Entities‚Äù (as shown in Table 11), this\nis acknowledged, as the goal of the obfuscation is\nnot to render the text entirely random but to obscure\nit while preserving essential information.\nA.8.2\nHuman-based Attack\nIn addition to utilizing an advanced LLM for infer-\nence attacks, we propose two human-based attacks\non EmojiPrompt. All participants are independent\nof this study and were explicitly informed that their\ndata would be used exclusively for experimental\npurposes within the scope of this research.\nItem Identification: on the Amazon Beauty\ndataset, we randomly sample 300 item-obfuscation\npairs, for each item in the pair, we pass a 500-item\nlist (randomly sampled, with the item included)\nand the item obfuscation to all recoverers, asking\nthem to identify the item from the list based on its\nobfuscation. We report the percentage of correctly\nidentified items for each recoverer.\nReview Recovery: on the Movie Review dataset,\nwe randomly sample 300 review-obfuscation pairs,\n",
  "19": "Amazon Beauty\nMovie Review\nCensus Income\nComp. Reading\nCosSim\nGemini + GPT-4\n0.531\n0.607\n0.467\n0.641\nRandom Entities\n0.307\n0.411\n0.295\n0.392\nRouge-1\nGemini + GPT-4\n0.211\n0.183\n0.205\n0.228\nRandom Entities\n0.173\n0.168\n0.161\n0.155\nRouge-2\nGemini + GPT-4\n0.061\n0.035\n0.039\n0.047\nRandom Entities\n0.049\n0.019\n0.021\n0.017\nRouge-L\nGemini + GPT-4\n0.207\n0.171\n0.198\n0.215\nRandom Entities\n0.167\n0.152\n0.147\n0.149\nMETEOR\nGemini + GPT-4\n0.183\n0.152\n0.178\n0.191\nRandom Entities\n0.159\n0.143\n0.139\n0.141\nTable 11: Recovering Robustness on LLM-based Attack benchmarked against Random Entities. Random Entities\ninvolves replacing private entities in each dataset with randomly selected entities from the same dataset.\nfor each sampled review, we provide all recoverers\nwith its obfuscation, asking them to recover the\noriginal review. We then report the mean Cosine\nSimilarity, ROUGE (1, 2, and L), and METEOR\nscores between each human recoverer and their\ncorresponding original reviews.\nFor the Item Identification test, five human re-\ncoverers completed the task, correctly identifying\n31, 28, 24, 17, and 21 items, respectively. This\nresults in a mean identification rate of 8.07%. It is\nimportant to note that while the item identification\ntest employs a list of 500 items (including the tar-\nget item) for evaluators to identify the correct item\nbased on its obfuscation, this is already a simpli-\nfied evaluation. In the domain of beauty products,\nthere are tens of thousands of items available on\nmajor online platforms, assuming web scraping\nis performed, which is substantially larger than\nthe 499 negative samples from our test. Despite\nthis simplification, our obfuscation mechanism still\ndemonstrates solid performance.\nFor the Review Recovery test, three human re-\ncoverers completed the task. Again, we employ\nthe ‚Äútext-embedding-3-small‚Äù model for embed-\nding generation, with embedding vector dimension\nset to 200. The resulting mean similarity scores\nfor the recoverers were: 0.556, 0.493, and 0.587,\nrespectively. For ROUGE and METEOR, we re-\nport the F1 score, as it represents the harmonic\nmean between precision and recall. The scores for\nall recoverers are presented in Table 12. Overall,\nthe performance of the human recoverers generally\naligns with that of the LLMI, further underscoring\nthe effectiveness of the obfuscation. We choose not\nto have the human recoverers identify the correct\nreview from a list of candidate reviews, as we did\nin the Item Identification test, due to the nature of\nthe data. In e-commerce, items are generally ac-\ncessible information from popular online platforms\nROUGE-1\nROUGE-2\nROUGE-L\nMETEOR\nRecoverer 1\n0.189\n0.029\n0.169\n0.167\nRecoverer 2\n0.178\n0.025\n0.153\n0.161\nRecoverer 3\n0.191\n0.031\n0.178\n0.181\nTable 12: Human-based Attack for Gemini + GPT-4 on\nReview Recovery\nand may be scrapped, whereas individual customer\nreviews are not publicly accessible unless released\nby the company or the individuals themselves.\nA.8.3\nDistribution-based Attack\nFurthermore, we propose a hypothetical attack\naimed at tabular datasets, where the adversary tar-\ngets a dataset used for a specific inference task,\nsuch as heart disease classification. Knowing the\ntask, the adversary collects related public datasets\nvia web scraping, which contain similar features to\nthe target dataset. This allows them to analyze the\nvalue distributions for overlapping features.\nThe adversary then examines the distribution\nof each value in the public datasets for features\nthat overlap with the obfuscated features in the\ntarget dataset. Despite not being able to directly\nrecover the obfuscated feature values, the adversary\nuses the distribution information to make educated\nguesses about the obfuscated data. For example, if\na public dataset shows that 70% of entries for the\nfeature ‚ÄúGender‚Äù are ‚ÄúMale‚Äù, and the obfuscated\ndataset has a value comprising 67% of the entries\nfor the same feature, then the adversary may infer\nthat this obfuscated value corresponds to ‚ÄúMale‚Äù.\nBy matching these distributions, the adversary\neffectively maps the obfuscated values to their ac-\ntual values. This technique bypasses the obfusca-\ntion by leveraging statistical patterns rather than\nattempting to recover the obfuscated text directly.\nTo protect tabular datasets against distributional\ninference attacks, we propose a novel generative\nobfuscation mechanism with post-sampling. This\n",
  "20": "approach involves generating multiple obfuscated\nrepresentations for each feature value instead of\nmapping each value to a single obfuscation.\nConsider an example with a feature that has two\npossible values, A and B, where A accounts for\n40% of the dataset instances and B accounts for\n60%. Instead of generating a single obfuscated\nrepresentation for A, we prompt LLMO to produce\nfour distinct and substantially different obfusca-\ntions: A1, A2, A3, and A4. Likewise, for B, we\nprompt LLMO to generate three distinct and sub-\nstantially different obfuscations: B1, B2, and B3.\nDuring the inference stage, one of the obfuscated\nrepresentations is randomly sampled for each in-\nstance of the feature value. Consequently, for fea-\nture value A, the obfuscated representations A1,\nA2, A3, and A4 will each appear in approximately\n10% of instances, while for feature value B, the ob-\nfuscated representations B1, B2, and B3 will each\nappear in approximately 20% of instances.\nThis method offers two key benefits. (1) by de-\ncoupling the obfuscated values from their original\ndistribution, this mechanism prevents adversaries\nfrom accurately deducing the original feature val-\nues through distributional analysis. (2) the variabil-\nity in obfuscated representations allows multiple\ncombinations of obfuscated values to collectively\napproximate the original distribution proportions\nof 40% for A and 60% for B. For instance, the\ncombination of A1, A2, and B1 could collectively\naccount for 40% of the dataset instances. This\nvariability makes it challenging for adversaries to\nascertain which obfuscated representations corre-\nspond to specific feature values, thus obscuring\ndiscernible patterns and enhancing data security.\nTo assess whether our multi-obfuscation method\nretains task performance, we employ both Census\nIncome and Heart Disease datasets to perform an\nablation study against single obfuscated represen-\ntation, with Gemini as LLMO and GPT-4 Turbo as\nLLMI. For each categorical feature, we randomly\nsample two to four unique obfuscated representa-\ntions for each value of the feature. To ensure that\nthe obfuscated representations are sufficiently dif-\nferent, we repeat the generation process until each\nobfuscated representation has a Cosine Similarity\nof 0.5 or less with all other representations. The\nbalanced accuracies of multi-obfuscation versus\nsingle-obfuscation are: 0.657 vs. 0.674 (for Census\nIncome) and 0.691 vs. 0.689 (for Heart Disease).\nThese figures demonstrate that our method retains\nperformance for tabular modeling.\n"
}