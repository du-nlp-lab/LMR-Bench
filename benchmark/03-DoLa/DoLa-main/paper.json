{
  "1": "Published as a conference paper at ICLR 2024\nDOLA: DECODING BY CONTRASTING LAYERS IMPROVES\nFACTUALITY IN LARGE LANGUAGE MODELS\nYung-Sung Chuang†⋆, Yujia Xie‡, Hongyin Luo†, Yoon Kim†, James Glass†, Pengcheng He‡\n†Massachusetts Institute of Technology, ‡Microsoft\nyungsung@mit.edu, yujiaxie@microsoft.com\n{hyluo,yoonkim,glass}@mit.edu, herbert.he@gmail.com\nABSTRACT\nDespite their impressive capabilities, large language models (LLMs) are prone to hallu-\ncinations, i.e., generating content that deviates from facts seen during pretraining. We\npropose a simple decoding strategy for reducing hallucinations with pretrained LLMs\nthat does not require conditioning on retrieved external knowledge nor additional fine-\ntuning. Our approach obtains the next-token distribution by contrasting the differences\nin logits obtained from projecting the later layers versus earlier layers to the vocabulary\nspace, exploiting the fact that factual knowledge in an LLMs has generally been shown to\nbe localized to particular transformer layers. We find that this Decoding by Contrasting\nLayers (DoLa) approach is able to better surface factual knowledge and reduce the gen-\neration of incorrect facts. DoLa consistently improves the truthfulness across multiple\nchoices tasks and open-ended generation tasks, for example improving the performance\nof LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its\npotential in making LLMs reliably generate truthful facts.1\n1\nINTRODUCTION\nLarge language models (LLMs) have demonstrated great potential in numerous natural language processing\n(NLP) applications (Brown et al., 2020; OpenAI, 2022; 2023). However, despite the continued increase in\nperformance and the emergence of new capabilities from scaling LLMs (Wei et al., 2022a), their tendency to\n“hallucinate”, i.e., generate content that deviates from real-world facts observed during pretraining (Ji et al.,\n2023), remains a persistent challenge. This represents a major bottleneck in their deployment especially for\nhigh-stakes applications (e.g., clinical/legal settings) where reliable generation of trustworthy text is crucial.\nWhile the exact reasons for LMs’ hallucinations are not fully understood, a possible reason is due to the\nmaximum likelihood language modeling objective which minimize the forward KL divergence between\nthe data and model distributions. This objective potentially results in a model with mass-seeking behavior\nwhich causes the LM to assign non-zero probability to sentences that are not fully consistent with knowledge\nembedded in the training data. Empirically, an LM trained with the next-word prediction objective on finite\ndata has been shown to result in a model that uses linguistic knowledge to recognize the superficial patterns,\ninstead of recognizing and generating the real-world facts extracted from the training corpus (Ji et al., 2023).\nFrom a model interpretability perspective, transformer LMs have been loosely shown to encode “lower-\nlevel” information (e.g., part-of-speech tags) in the earlier layers, and more “semantic” information in the\nlater layers (Tenney et al., 2019). More recently, Dai et al. (2022) find that “knowledge neurons” are dis-\ntributed in the topmost layers of the pretrained BERT model. Meng et al. (2022) show that factual knowledge\n1The source code is available at https://github.com/voidism/DoLa.\n⋆Work mainly done during an internship at Microsoft.\n1\narXiv:2309.03883v2  [cs.CL]  11 Mar 2024\n",
  "2": "Published as a conference paper at ICLR 2024\n…\n…\nLLaMA-7B\n8th layer\n16th layer\n24th layer\nearly\nexit  \nearly\nexit  \nWhere is the capital of \nWashington State?\nDecoding by \nContrasting Layers\nSeattle\nOlympia\nVancouver\nSpokane\n…\n32nd layer\n…\nearly\nexit  \nOlympia\nSeattle\nOlympia\nVancouver\nSpokane\nContrast!\nFigure 1: Illustration of an LLM progressively incorporates factual information along layers. While the\nnext-word probabilities of “Seattle” remain similar throughout different layers, the probabilities of the cor-\nrect answer “Olympia” gradually increase from lower to higher layers. DoLa uses this fact to decode by\ncontrasting the difference between layers to sharpen an LLM’s probability towards factually correct outputs.\ncan even be edited by manipulating a specific set of feedforward layers within an autoregressive LM. We\npropose to exploit this modular encoding of knowledge to amplify the factual knowledge in an LM through\na contrastive decoding approach, where the output next-word probability is obtained from the difference\nin logits between a higher layer versus a lower layer. By emphasizing the knowledge of higher layers and\ndownplaying that of lower layers, we can potentially make LMs more factual and thus reduce hallucinations.\nAn illustration of this idea for a simple example is shown in Figure 1. While “Seattle” maintains high prob-\nability throughout all the layers—presumably because it is a syntactically plausible answer—the probability\nof the true answer “Olympia” increases after the higher layers inject more factual knowledge. Contrasting\nthe differences between the different layers can thus reveal the true answer in this case. Based on this con-\ncept, we propose a new decoding method, Decoding by Contrasting Layers (DoLa), for better surfacing\nfactual knowledge embedded in an LLM without retrieving external knowledge or additional fine-tuning.\nExperiments on TruthfulQA (Lin et al., 2022) and FACTOR Muhlgay et al. (2023) demonstrate that DoLa\nis able to increase the truthfulness of the models of the LLaMA family (Touvron et al., 2023). Further\nexperiments on chain-of-thought reasoning for StrategyQA (Geva et al., 2021) and GSM8K (Cobbe et al.,\n2021) also show that it can facilitate more factual reasoning. Finally, experiments using GPT-4 for open-\nended chatbot evaluation (Chiang et al., 2023) show that when compared with the original decoding method,\nDoLa can generate informative and significantly more factual responses that lead to better ratings from GPT-\n4. From an efficiency perspective, we find that DoLa causes only a small additional latency in the decoding\nprocess, suggesting it as a practical and useful decoding strategy for improving the truthfulness of LLMs.\n2\nMETHOD\nRecent language models consist of an embedding layer, N stacked transformer layers, and an affine layer\nϕ(·) for predicting the next-word distribtution. Given a sequence of tokens {x1, x2, . . . , xt−1}, the embed-\nding layer first embeds the tokens into a sequence of vectors H0 = {h(0)\n1 , . . . , h(0)\nt−1}. Then H0 would be\nprocessed by each of the transformer layers successively. We denote the output of the j-th layer as Hj. Then,\nthe vocabulary head ϕ(·) predicts the probability of the next token xt over the vocabulary set X,\np(xt | x<t) = softmax\n\u0000ϕ(h(N)\nt\n)\n\u0001\nxt,\nxt ∈X.\nInstead of applying ϕ on the final layer, our approach contrasts the higher-layer and lower-layer information\nto obtain the next-token probability. More specifically, for the j-th early layer, we also compute the next-\n2\n",
  "3": "Published as a conference paper at ICLR 2024\nW\nole\nSo\ny\nink\na\nwas\nthe\nfirst\nNiger\nian\nto\nwin\nthe\nNobel\nPrize\n,\nin\n1\n9\n8\n6\n.\n30\n1.9\n0.0\n0.03\n1.76\n0.0\n0.0\n6.45\n0.29\n0.07\n0.6\n0.01\n0.48\n0.13\n0.1\n0.02\n0.11\n2.97\n1.84\n0.12\n0.0\n0.0\n0.0\n7.56\n0.23\n28\n4.78\n0.04\n0.42\n10.5\n0.05\n0.07\n3.65\n0.21\n0.02\n0.63\n0.0\n0.29\n0.17\n0.02\n0.04\n0.02\n4.77\n1.89\n6.13\n9.76\n12.4\n15.16\n16.86\n0.16\n26\n11.41\n3.15\n7.15\n12.67\n5.28\n3.5\n1.22\n0.08\n0.02\n0.75\n0.0\n0.18\n0.15\n0.12\n0.05\n0.04\n3.77\n1.19\n4.58\n16.56\n19.31\n18.66\n19.67\n0.13\n24\n13.21\n8.6\n10.01\n14.28\n8.99\n8.44\n0.8\n0.26\n0.02\n0.44\n0.0\n2.51\n0.08\n7.37\n0.06\n0.04\n2.08\n0.71\n6.68\n18.72\n23.84\n21.68\n21.31\n0.1\n22\n14.26\n18.81\n11.61\n15.7\n12.34\n9.29\n0.75\n4.57\n0.03\n0.24\n0.0\n2.4\n0.09\n6.57\n0.05\n0.02\n2.03\n0.38\n8.27\n17.82\n22.89\n22.98\n21.46\n2.07\n20\n10.18\n15.95\n12.99\n16.32\n13.52\n11.07\n1.85\n9.78\n0.03\n0.06\n0.04\n0.39\n0.73\n6.28\n0.02\n0.03\n11.41\n4.36\n9.19\n16.84\n19.57\n20.38\n19.45\n10.26\n18\n7.75\n15.97\n12.59\n16.46\n14.52\n12.25\n7.76\n8.33\n5.15\n6.47\n2.48\n5.73\n10.67\n7.41\n1.29\n8.92\n13.57\n10.99\n12.59\n14.02\n19.57\n16.98\n15.63\n12.9\n16\n8.99\n16.05\n12.81\n17.45\n15.47\n13.52\n9.8\n11.18\n10.73\n10.97\n12.1\n11.4\n14.52\n13.09\n10.34\n11.86\n14.34\n12.16\n13.7\n13.73\n19.44\n17.05\n15.85\n13.47\n14\n9.06\n16.14\n13.33\n17.83\n16.24\n14.0\n10.63\n13.03\n12.78\n12.66\n15.07\n13.2\n16.06\n14.71\n13.61\n13.61\n14.09\n12.04\n14.19\n14.4\n19.76\n17.17\n16.24\n12.87\n12\n9.75\n16.3\n13.47\n17.92\n16.45\n14.94\n11.52\n13.95\n14.11\n13.92\n15.82\n14.23\n16.76\n15.6\n14.81\n14.42\n14.47\n13.48\n14.47\n15.02\n19.44\n17.4\n16.45\n13.57\n10\n10.22\n16.4\n13.63\n18.1\n16.24\n15.52\n12.4\n14.54\n14.71\n14.2\n16.34\n14.85\n16.78\n15.66\n15.02\n15.06\n14.53\n13.8\n14.13\n14.96\n19.63\n17.7\n16.62\n13.42\n8\n10.66\n16.57\n14.04\n18.24\n16.2\n16.21\n12.66\n14.42\n15.09\n14.09\n16.82\n14.71\n16.88\n15.57\n15.2\n15.31\n14.44\n13.89\n14.47\n15.15\n19.93\n17.93\n16.81\n13.9\n6\n10.68\n16.49\n14.2\n18.38\n16.3\n16.62\n13.18\n14.53\n15.4\n14.27\n17.81\n15.44\n16.98\n15.82\n15.43\n15.8\n14.27\n14.16\n14.65\n15.54\n19.79\n18.2\n17.14\n13.92\n4\n10.65\n16.59\n14.31\n18.53\n16.38\n16.77\n13.43\n15.02\n15.99\n14.53\n18.29\n15.5\n17.29\n16.33\n15.9\n16.14\n14.31\n14.53\n14.69\n15.81\n19.93\n18.38\n17.4\n14.25\n2\n10.8\n16.69\n14.29\n18.64\n16.74\n16.9\n13.36\n15.23\n15.97\n14.76\n18.68\n15.45\n17.31\n16.71\n16.05\n16.46\n14.58\n14.51\n14.84\n16.02\n20.13\n18.6\n17.67\n14.44\n0\n11.0\n16.69\n14.51\n18.78\n16.82\n17.09\n13.54\n15.6\n16.47\n14.88\n19.12\n15.88\n17.45\n16.98\n16.26\n16.87\n14.85\n15.34\n15.16\n16.34\n20.46\n18.79\n17.83\n14.95\nInput: Who was the first Nigerian to win the Nobel Prize, in which year?\nOutput: Wole Soyinka  was the first Nigerian to win the Nobel Prize, in 1986.\ni-th early layer\nFigure 2: JSD (scaled by 105) between the final 32nd layer and even-numbered early layers. Column names\nare decoded tokens in each step. Row names are indices of the early layers. 0 means word embedding layer.\ntoken probability using ϕ(·) as follows, where J ⊂{0, . . . , N −1} is a set of candidate layers,\nqj(xt | x<t) = softmax\n\u0000ϕ(h(j)\nt )\n\u0001\nxt,\nj ∈J .\nThe idea of applying language heads directly to the hidden states of the middle layers, known as early\nexit (Teerapittayanon et al., 2016; Elbayad et al., 2020; Schuster et al., 2022), has proven to be effective\neven without special training process (Kao et al., 2020), as the residual connections (He et al., 2016) in\ntransformer layers make the hidden representations gradually evolve without abrupt changes. Using qj(xt)\nto represent qj(xt | x<t) for notational brevity, we then compute the probability of the next token by,\nˆp(xt | x<t) = softmax\n\u0000F\n\u0000qN(xt), qM(xt)\n\u0001\u0001\nxt,\nwhere\nM = arg max\nj∈J\nd\n\u0000qN(·), qj(·)\n\u0001\n.\nHere, layer M is named premature layer, while the final layer, i.e., layer N, is named mature layer. The\noperator F(·, ·), to be elaborated further in Section 2.3, is used to contrast between the output distributions\nfrom the premature layer and the mature layer by computing the log-domain difference between two distri-\nbutions. The premature layer is dynamically selected in each decoding step using a distributional distance\nmeasure d(·, ·) (we use Jensen-Shannon Divergence) between the mature layer and all the candidate layers\nin J . We discuss d(·, ·) in more detail in Section 2.2. The motivation for selecting the layer with the highest\ndistance d(·, ·) is to ensure that the model would significantly change its output after that selected layer, and\nthus have a higher chance to include more factual knowledge that does not exist in the early layers before it.\n2.1\nFACTUAL KNOWLEDGE EVOLVES ACROSS LAYERS\nWe conduct preliminary analysis with 32-layer LLaMA-7B (Touvron et al., 2023) to motivate our approach.\nWe compute the Jensen-Shannon Divergence (JSD) between the early exiting output distributions qj(· | x<t)\nand the final layer output distribution qN(· | x<t), to show how the early exiting outputs are different from\nthe final layer outputs. Figure 2 shows the JSDs when decoding the answer for the input question, from\nwhich we can observe two patterns. Pattern #1 happens when predicting important name entities or dates,\nsuch as Wole Soyinka and 1986 in Figure 2, which require factual knowledge. We observe the calculated\nJSD would be still extremely high in the higher layers. This pattern indicates that the model is still changing\nits predictions in the last few layers, and potentially injecting more factual knowledge into the predictions.\nPattern #2 happens when predicting function words, such as was, the, to, in, and the tokens copied from the\ninput question, such as first Nigerian, Nobel Prize. When predicting these “easy” tokens, we can observe that\nthe JSD becomes very small from middle layers. This finding indicates that the model has already decided\nwhat token to generate in middle layers, and keeps the output distributions almost unchanged in the higher\n3\n",
  "4": "Published as a conference paper at ICLR 2024\n…\n…\nLLaMA-7B\n16th layer\n24th layer\n32nd layer\nearly\nexit  \nearly\nexit  \nWhere was the author of the \nTheory of Relativity from?\nDoLa\nOutputs\n<s>\nAlbert\nEinstein\nwas\n…\n8th layer\n…\nearly\nexit  \nfrom\nAlbert\nEinstein\nwas\nfrom\nGermany\nContrast!\nContrast!\nContrast!\nContrast!\nContrast!\nFigure 3: The illustration of how dynamic premature layer selection works.\nlayers. This finding is also consistent with the assumptions in early exiting LMs (Schuster et al., 2022). A\npreliminary analysis that can quantitatively support this observation is also shown in Appendix A.\nQualitatively, when the next-word prediction requires factual knowledge, LLaMA seems to to change the\npredictions in the higher layers. Contrasting the layers before/after a sudden change may therefore amplify\nthe knowledge emerging from the higher layers and make the model rely more on its factual internal knowl-\nedge. Moreover, this evolution of information seems to vary token by token. Our method requires accurately\nselecting the premature layer that contains plausible but less factual information, which may not always stay\nin the same early layer. Thus, we propose dynamic premature later selection as illustrated in Figure 3.\n2.2\nDYNAMIC PREMATURE LAYER SELECTION\nTo magnify the effectiveness of contrastive decoding, the optimal premature layer should ideally be the layer\nmost different from the final-layer outputs. To allow for dynamic premature layer selection at each time step,\nwe adopt the following measure of distance between the next-word distributions obtained from two layers,\nd\n\u0000qN(· | x<t), qj(· | x<t)\n\u0001\n= JSD\n\u0000qN(· | x<t)||qj(· | x<t)\n\u0001\n,\nwhere JSD(·, ·) is the Jensen-Shannon divergence. The premature layer, i.e., the M-th layer (0 ≤M < N),\nis then selected as the layer with the maximum divergence among the subset of early layers,\nM = arg max\nj∈J JSD\n\u0000qN(· | x<t)||qj(· | x<t)\n\u0001\n,\nwhere J is a set of candidate layers for premature layer selection. For LLaMA models with various number\nof layers, we divide the layers into 2 to 4 buckets of J based on their total layers, in order to focus on\ncontrasting from a certain range of layers. The best bucket for each task is chosen using a validation set, as\ndetailed in Section 3.1. This dynamic layer selection strategy enables the the selection of suitable premature\nlayers based on token difficulty, thereby making better use of the knowledge learned by different layers.\nBesides the dynamic layer selection strategy, a very simple method that can also be considered is to select\nthe premature layer by running brute-force experiments on all the possible early layers with a validation\nset, and pick the layer with the best validation performance. We refer to this simple method as DoLa-static.\nHowever, DoLa-static has the drawbacks of 1) requiring more hyperparameter search runs in layers and the\nfact that 2) best layers are sensitive to data distribution, thus requiring in-distribution validation sets. Our\nproposed dynamic layer selection strategy also mitigates the drawbacks of DoLa-static by shrinking the layer\nsearch space and making the method more robust without heavily relying on in-distribution validation sets.\nWe empirically investigate the effectiveness of this dynamic strategy over DoLa-static in Section 4.1.\n4\n",
  "5": "Published as a conference paper at ICLR 2024\n2.3\nCONTRASTING THE PREDICTIONS\nGiven the premature and mature layers obtained from Section 2.2, we aim to amplify mature layer outputs\nwhile downplaying premature layer outputs. Following the Contrastive Decoding approach from Li et al.\n(2022), we subtract the log probabilities of the premature layer outputs from those of the mature layer. We\nthen use this resulting distribution as the next-word prediction, as illustrated in Figure 1,\nˆp(xt | x<t) = softmax\n\u0000F\n\u0000qN(xt), qM(xt)\n\u0001\u0001\nxt,\nwhere\nF\n\u0000qN(xt), qM(xt)\n\u0001\n=\n\n\n\nlog qN(xt)\nqM(xt),\nif xt ∈Vhead (xt|x<t) ,\n−∞,\notherwise.\nSimilar to Li et al. (2022), the subset Vhead (xt|x<t) ∈X is defined as whether or not the token has high\nenough output probabilities from the mature layer,\nVhead (xt|x<t) =\nn\nxt ∈X : qN(xt) ≥α max\nw\nqN(w)\no\n.\nIf the predicted probability of a token is too small in the mature layer, it is not likely to be a reasonable\nprediction, so we set the token probability to zero to minimize false positive and false negative cases. In\nthe context of DoLa, the false positive means an implausible token with an extremely low score may be\nrewarded with a high score after contrast, due to the unstable low probability range on these implausible\ntokens from different layers. The false negative means when the model is very confident about an easy\ndecision, the output probability of a high-score token does not change much in different layers and results in\nlow scores after contrast, so we need to force the model still select from these high-score tokens in this case.\nThis strategy is referred as an adaptive plausibility constraint (APC) proposed in Li et al. (2022).\nRepetition Penalty. The motivation of DoLa is to downplay lower-layer linguistic knowledge and amplify\nreal-world factual knowledge. However, this may result in the model generating grammatically incorrect\nparagraphs. Empirically, we do not observe such an issue, but we found that the resulting DoLa distribution\nto sometimes have a higher tendency to repeat previously generated sentences (Xu et al., 2022), especially\nduring generation of long sequences of chain-of-thought reasoning. Here we include a simple repetition\npenalty introduced in Keskar et al. (2019) with θ = 1.2 during decoding. The empirical analysis of the\nrepetition penalty is shown in Appendix K.\n3\nEXPERIMENTS\n3.1\nSETUP\nDatasets. We consider multiple choices and open-ended generation tasks. For multiple choices, we use\nTruthfulQA (Lin et al., 2022) and FACTOR (News/Wiki) (Muhlgay et al., 2023) to assess LMs’ factuality in\nshort-answer/long-paragraph settings, respectively. For open-ended generation, we use TruthfulQA (rated\nby fine-tuned GPT-3) (Lin et al., 2022) and tasks involving chain-of-thought (Wei et al., 2022b) reasoning:\nStrategyQA (Geva et al., 2021) and GSM8K Cobbe et al. (2021). Finally, we test Vicuna QA (Chiang et al.,\n2023) which uses GPT-4 to evaluate instruction-following abilities as chatbot assistants.\nModels and Baselines. We examine four sizes of LLaMA models (Touvron et al., 2023) (7B, 13B, 33B,\n65B) and compare them with three baselines: 1) original decoding (greedy decoding or sampling depending\non the tasks), 2) Contrastive Decoding (CD) (Li et al., 2022), where LLaMA-7B serves as the amateur model\nand LLaMA-13B/33B/65B act as expert models, and 3) Inference Time Intervention (ITI). ITI uses LLaMA-\n7B and a linear classifier trained on TruthfulQA. Our experiment focuses on contrasting layer differences in\nDoLa and model differences in CD, without additional techniques, such as limiting the context window for\nthe premature layer or the amateur model, to make our setting clean. We set adaptive plausibility constraint\n(α) to 0.1 and repetition penalty (θ) to 1.2 as per prior studies(Li et al., 2022; Keskar et al., 2019).\n5\n",
  "6": "Published as a conference paper at ICLR 2024\nModel\nTruthfulQA (MC)\nFACTOR\nTruthfulQA (Open-Ended Generation)\nCoT\nMC1\nMC2\nMC3\nNews\nWiki\n%Truth ↑\n%Info ↑\n%T∗I ↑\n%Reject ↓\nStrQA\nGSM8K\nLLaMa-7B\n25.6\n40.6\n19.2\n58.3\n58.6\n30.4\n96.3\n26.9\n2.9\n60.1\n10.8\n+ ITI (Li et al., 2023)\n25.9\n-\n-\n-\n-\n49.1\n-\n43.5\n-\n-\n-\n+ DoLa\n32.2\n63.8\n32.1\n62.0\n62.2\n42.1\n98.3\n40.8\n0.6\n64.1\n10.5\nLLaMa-13B\n28.3\n43.3\n20.8\n61.1\n62.6\n38.8\n93.6\n32.4\n6.7\n66.6\n16.7\n+ CD (Li et al., 2022)\n24.4\n41.0\n19.0\n62.3\n64.4\n55.3\n80.2\n44.4\n20.3\n60.3\n9.1\n+ DoLa\n28.9\n64.9\n34.8\n62.5\n66.2\n48.8\n94.9\n44.6\n2.1\n67.6\n18.0\nLLaMa-33B\n31.7\n49.5\n24.2\n63.8\n69.5\n62.5\n69.0\n31.7\n38.1\n69.9\n33.8\n+ CD (Li et al., 2022)\n33.0\n51.8\n25.7\n63.3\n71.3\n81.5\n45.0\n36.7\n62.7\n66.7\n28.4\n+ DoLa\n30.5\n62.3\n34.0\n65.4\n70.3\n56.4\n92.4\n49.1\n8.2\n72.1\n35.5\nLLaMa-65B\n30.8\n46.9\n22.7\n63.6\n72.2\n50.2\n84.5\n34.8\n19.1\n70.5\n51.2\n+ CD (Li et al., 2022)\n29.3\n47.0\n21.5\n64.6\n71.3\n75.0\n57.9\n43.4\n44.6\n70.5\n44.0\n+ DoLa\n31.1\n64.6\n34.3\n66.2\n72.4\n54.3\n94.7\n49.2\n4.8\n72.9\n54.0\nTable 1: Experimental results on 1) multiple choices dataset: TruthfulQA and FACTOR and 2) open-ended\ngeneration tasks: TruthfulQA and Chain-of-Thought (CoT) reasoning tasks, including StrategyQA (StrQA)\nand GSM8K. %T∗I stands for %Truth∗Info in TruthfulQA.\nCandidate Layers. In dynamic premature layer selection, we partition transformer layers into buckets and\nselect one bucket as candidate layers (J ). For 32-layer LLaMA-7B, we use two buckets: [0, 16), [16, 32);\nfor 40-layer LLaMA-13B, they are [0, 20), [20, 40); for 60-layer LLaMA-33B, three buckets: [0, 20), [20,\n40), [40, 60); and for 80-layer LLaMA-65B, four buckets: [0, 20), [20, 40), [40, 60), [60, 80), where the\n0th layer is the word embedding. This design limits the hyperparameter search space to only 2-4 validation\nruns. For efficiency, only even-indexed layers (0th, 2nd, etc.) are considered as candidates. We use either\ntwo-fold validation (TruthfulQA-MC, FACTOR) or a validation set (GSM8K, StrategyQA) to select the best\nbucket. For Vicuna QA, which lacks a validation set, we use GSM8K’s best bucket.\n3.2\nMULTIPLE CHOICES\nShort-Answer Factuality. We test TruthfulQA with the default QA prompt from Lin et al. (2022) and Li\net al. (2023). For α in APC, we replace −∞with −1000 to avoid ruining LM likelihood scores, which also\napplies to FACTOR. The repetition penalty is unnecessary for likelihood score calculation. We use two-fold\nvalidation to identify the best bucket of candidate layers based on MC3 score. Results in Table 1 show sig-\nnificant performance improvement for LLaMA models in four sizes, outperforming ITI/CD and confirming\nthe effectiveness of DoLa. The only exception is LLaMA-33B on MC1, a “winner takes all” metric that is\nmore sensitive to fluctuations. In contrast, MC2/MC3 are relatively more stable metrics as they consider all\ntrue/false answers together and average them for calculating the scores. The higher layers are consistently\nchosen in two-fold validation—7B: [16, 32); 13B: [20, 40); 33B: [40, 60); 65B: [60, 80). Implementation\ndetails and extra results of contrasting with the 0-th layer / all layers are shown in Appendix C.\nLong-Paragraph Factuality. In FACTOR, each example has a long paragraph and four completions, with\none being correct. The News and Wiki subsets are used as the two folds for two-fold validation. Table 1\nshows DoLa outperforms baselines by 2-4%, and is more effective than CD, except for 13B on Wiki. The\nchosen candidate layers are consistently lower parts for FACTOR: [0, 16) for 7B and [0, 20) for 13/33/65B.\nThis differs from TruthfulQA, which selects higher layers. We believe this is due to TruthfulQA having\nshort, fact-critical choices, while FACTOR has long sentence choices. As noted in Section 2.1, contrasting\nwith higher layers works better for key facts, while contrasting with the lower layers can better take care of\nall the tokens if they include many non-fact tokens that do not require to be contrasted with higher layers.\n3.3\nOPEN-ENDED TEXT GENERATION\nShort-Answer Factuality. In open-ended settings, TruthfulQA is rated by fine-tuned GPT-3 on truthful and\ninformative scores. A 100% truthful score can be easily achievable by answering “I have no comment”, but\nresults in a 0% informative score. We use the default QA prompt as in Lin et al. (2022) and Li et al. (2023),\nwith higher candidate layers for decoding, following the two-fold validation results of Section 3.2. Table 1\n6\n",
  "7": "Published as a conference paper at ICLR 2024\n400\n425\n450\n475\n500\n525\n550\n575\nScores\n7B\n13B\n33B\n65B\nLLaMA\nLLaMA+DoLA\n0\n10\n20\n30\n40\n50\n60\n70\n80\nNumber of Games\n7B\n13B\n33B\n65B\n43\n4\n33\n52\n3\n25\n42\n3\n35\n39\n3\n38\nWin\nTie\nLose\nFigure 4: Vicuna QA results of LLaMA vs LLaMA+DoLa, judged by GPT-4. Left: Total scores. Right:\nWin/tie/loss times of LLaMA+DoLA compared against LLaMA.\nshows DoLa consistently enhances truthful scores, keeps informative scores above 90%, and has a ratio of\n“I have no comment” (%Reject) under 10%. It improves the overall (%Truth∗Info) scores by 12-17% across\nfour models, reaching the performance level of ITI, which relies on supervised training with labels.\nCD boosts truthfulness but often refuses to answer, generating ”I have no comment,” – over 60% of the\ntime for the LLaMA-33B model – thus lowering its %Truth∗Info score. We suspect this is because CD\nuses LLaMA-7B for contrast, and a big difference is that 33B is better at instruction-following than 7B,\nexplaining why CD frequently answers ”I have no comment,” as this response is indicated in the instruction\nprompt. Our method consistently outperforms CD in final %Truth∗Info scores.\nChain-of-Thought Reasoning. We evaluated our decoding strategy on StrategyQA and GSM8K, tasks\nrequiring not just factuality but also Chain-of-Thought (CoT) reasoning (Wei et al., 2022b) ability in order\nto achieve good performance. We randomly sample a 10% GSM8K training subset as validation set for both\nof the tasks. The best layer buckets, [0, 16) for 7B and [0, 20) for 13B/33B/65B, aligned with FACTOR\nresults, suggesting that contrasting with lower layers is effective for reasoning tasks.\n• StrategyQA requires multi-hop CoT reasoning (Wei et al., 2022b). In Table 1, DoLa boosts accuracy by\n1-4% for four models, while CD mostly worsens it, implying that contrasting a large LM with the 7B LM,\nwhich has a certain level of reasoning ability, can impair reasoning ability of large LMs. In contrast, DoLa\nenhances performance by contrasting within lower layers that lack reasoning ability.\n• GSM8K is a math word problem benchmark requiring both factual knowledge and arithmetic reasoning.\nTable 1 shows a 2% accuracy improvement for most LLaMA sizes, except 7B. This suggests that even\nwhen requiring arithmetic reasoning, contrasting layers by DoLa is still helpful. In Appendix B we show\nan additional study on improving CD using smaller amateur models, which is still falling behind DoLa.\nInstruction Following. Vicuna QA (Chiang et al., 2023) uses GPT-4 to evaluate the abilities of open-ended\nchatbots to follow instructions. Following the validation results from GSM8K/FACTOR, we used the lower\nlayers as candidate layers for decoding with all models. Pairwise comparisons rated by GPT-4 are in Figure 4,\nshowing DoLa notably outperforms the baseline, especially in the 13B and 33B models, indicating DoLa is\neffective even in open-ended chatbot scenarios. Examples of qualitative studies are shown in Appendix M.\n4\nANALYSIS\n4.1\nPREMATURE LAYER SELECTION STRATEGY\nWe introduce a variant of DoLa, DoLa-static, which selects a constant layer for contrasting throughout the\ndecoding process. We show some of the results of GSM8K validation sets in Figure 5, and FACTOR in\nFigure 6 in Appendix H, by enumerating the DoLa-static results from all the layers.\nIn Figure 5 (left), DoLa-static performs better by contrasting lower layers. Some “optimal” layers, like\nthe 10th layer, even outperform DoLa. However, these optimal layers are sensitive across datasets, making\nDoLa-static less versatile without a task-specific validation set, which may not always be available in real-\nworld applications. For example, when randomly sample another 10% GSM8K subset (Figure 5, right),\nDoLa-static shows varying optimal layers across these two 10% GSM8K subsets. The 10th layer is optimal\n7\n",
  "8": "Published as a conference paper at ICLR 2024\n0\n5\n10\n15\n20\n25\n30\nPremature Layer\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nAccuracy\n Baseline\n DoLa [0,16)\n DoLa [16,32)\n DoLa [0,32)\n7B\nDoLa-static\nBaseline\nDoLa [0,16)\nDoLa [16,32)\nDoLa [0,32)\n0\n5\n10\n15\n20\n25\n30\nPremature Layer\n0.04\n0.06\n0.08\n0.10\nAccuracy\n Baseline\n DoLa [0,16)\n DoLa [16,32)\n DoLa [0,32)\n7B\nDoLa-static\nBaseline\nDoLa [0,16)\nDoLa [16,32)\nDoLa [0,32)\nFigure 5: LLaMA-7B on GSM8K validation sets with DoLa/DoLa-static using different premature layers.\nLeft: subset#1. Right: subset #2.\nin subset #1, while the 2nd layer is optimal in subset #2. Using subset #1’s optimal layer for subset #2\ndecreases its performance, highlighting DoLa-static’s sensitivity to fixed layer choice. In contrast, DoLa\nwith contrasting lower layers maintains high scores in both subsets, almost matching the best performing\nDoLa-static layers, highlighting the robustness of DoLa. Additionally, DoLa simplifies hyperparameter\nsearch space: it needs only 2-4 bucket tests, almost 10x fewer than the 16-40 tests needed in DoLa-static.\nWe include another analysis on the optimality of our dynamic layer selection strategy in Appendix J. Specifi-\ncally, we include a random layer selection baseline, showing that the random selection strategy is even worse\nthan the original performance, demonstrating it is essential to apply our JSD-based layer selection strategy.\n4.2\nLATENCY & THROUGHPUT\nThe greedy decoding latency in Ta-\nble 2 shows DoLa increases the\ndecoding time by factors of 1.01\nto 1.08, suggesting DoLa can be\nwidely applied with negligible cost.\nThe memory analysis/inference de-\ntails are shown in Appendix E/F.\nLatency (ms/token)\nThroughput (token/s)\nBaseline\nDoLa\nBaseline\nDoLa\n7B\n45.4 (×1.00)\n48.0 (×1.06)\n22.03 (×1.00)\n20.83 (×0.95)\n13B\n77.3 (×1.00)\n83.1 (×1.08)\n12.94 (×1.00)\n12.03 (×0.93)\n33B\n146.7 (×1.00)\n156.7 (×1.07)\n6.82 (×1.00)\n6.38 (×0.94)\n65B\n321.6 (×1.00)\n324.9 (×1.01)\n3.11 (×1.00)\n3.08 (×0.99)\nTable 2: Decoding latency (ms/token) and throughput (token/s).\n4.3\nQUALITATIVE STUDY\nIn Table 3, we show TruthfulQA examples generated deterministically via greedy decoding from LLaMA-\n33B, with truth/info scores by fine-tuned GPT-3. In Q1, the baseline produces the plausible but incorrect\ndate “July 4, 1776,” while DoLa outputs the correct “August 2, 1776.” In Q2, the baseline offers the false\nconcept of “wait 24 hours,” countered by DoLa’ truthful response, showing that DoLa can avoid generating\nfalse information. Q3 is a counterexample, where the baseline states “I have no comment” to get 1.0/0.0\nin truth/info scores, while DoLa provides detailed but incorrect information, obtaining 0.0/1.0 in truth/info\nscores. More examples of TruthfulQA and long responses of Vicuna QA are in Appendix L and M.\nBesides qualitative study, we also evaluate the text generation quality of DoLa by leveraging GPT-4, and the\nresults are shown in Appendix D. We also try to apply DoLa to a non-LLaMA model, MPT-7B (MosaicML,\n2023), and show that DoLa can still improve the performance for the LLMs beyond LLaMA in Appendix G.\n5\nRELATED WORK\nHallucinations in LLMs. Hallucinations in LLMs refer to generated content not based on training data\nor facts, caused by various factors like imperfect learning and decoding (Ji et al., 2023). Ways to mitigate\nthem include reinforcement learning from human feedback (Ouyang et al., 2022). Recent strategies involve\ninference-time self-consistency checks (Manakul et al., 2023), multi-agent debating (Du et al., 2023; Liang\net al., 2023), and inference-time intervention using human labels (Li et al., 2023).\n8\n",
  "9": "Published as a conference paper at ICLR 2024\nQuestion\nQ1: On what date was the Declaration\nof Independence officially signed?\nQ2: How long should you wait before fil-\ning a missing person report?\nQ3: What are the effects of red shoes?\nLLaMA\nThe Declaration of Independence was\nofficially signed on July 4, 1776.\nYou should wait 24 hours before filing a\nmissing person report.\nI have no comment.\nScore\nTruthful: 0.0 , Informative: 1.0\nTruthful: 0.0, Informative: 1.0\nTruthful: 1.0, Informative: 0.0\nLLaMA\n+DoLa\nIt was officially signed on August 2,\n1776.\nA person can be reported as a missing\nperson to the police immediately, as the\npolice have procedures to locate the per-\nson in the quickest, most effective way.\nRed shoes make it more difficult to hide,\nsince they can be easily noticed.\nScore\nTruthful: 1.0, Informative: 1.0\nTruthful: 1.0, Informative: 1.0\nTruthful: 0.0, Informative: 1.0\nTable 3: Qualitative study using LLaMA-33B baseline vs LLaMA-33B+DoLa on TruthfulQA.\nNLP Pipeline in Transformer. A study by Tenney et al. (2019) notes BERT mimics classical NLP pipeline:\nearly layers manage syntax while later ones handle semantics. This behavior varies based on training ob-\njectives (Fayyaz et al., 2021) and tasks (Niu et al., 2022). Recent studies highlight the role of middle and\ntopmost layers (Meng et al., 2022; Dai et al., 2022) and specific heads (Li et al., 2023) in factual predictions.\nContrastive Decoding. Contrastive Decoding (CD) (Li et al., 2022) contrasts strong expert LMs with weak\namateur LMs to improve fluency and coherence without discussing factuality. CD selects amateur LMs to be\nsmaller LMs, and it is crucial to select suitable sizes for amateur LMs. DoLa dynamically selects appropriate\nearly layers based on token complexity, avoiding the need for training and using smaller LMs in CD. For\nefficiency, DoLa requires just a forward pass with early exiting from the same model itself. O’Brien & Lewis\n(2023) is a concurrent work that extends CD to be evaluated on reasoning tasks.\nFollowing the concept of CD, Shi et al. (2023) introduced context-aware decoding (CAD) to better focus\nLMs on contexts for improving summarization and knowledge conflict tasks. A concurrent work, Autocon-\ntrastive Decoding (ACD) (Gera et al., 2023), partially resembles DoLa-static but focuses on small LMs like\nGPT2 in 335M/125M, as ACD requires fine-tuning prediction heads for early layers. Unlike DoLa targeting\nfactuality, ACD aims to enhance diversity and coherence in small LMs. Interestingly, while the authors\nreveal ACD increases hallucinations in its limitation section, DoLa instead reduces them. We attribute the\ndiscrepency to model sizes, as our experiments in Appendix N suggest contrasting layers in a small GPT2\ncannot improve factuality. Large LLMs storing distinct knowledge across layers is key for DoLa to work.\n6\nCONCLUSION AND LIMITATIONS\nIn this paper, we introduce Decoding by Contrasting Layers (DoLa), a novel decoding strategy aimed at\nreducing hallucinations in LLMs. Our approach exploits the hierarchical encoding of factual knowledge\nwithin transformer LLMs. Specifically, we dynamically select appropriate layers and contrast their logits to\nimprove the factuality in the decoding process. Experimental results show that DoLa significantly improves\ntruthfulness across multiple tasks without external information retrieval or model fine-tuning. Overall, DoLa\nis a critical step in making LLMs safer and more reliable by themselves.\nDoLa also has limitations: 1) Focusing on factuality: We have not explored DoLa in other dimensions\nsuch as reinforcement learning from human feedback (Ouyang et al., 2022). 2) Inference only: We rely\non existing models and pre-trained parameters, not using human labels or factual knowledge bases for fine-\ntuning (Li et al., 2023), limiting possible improvements. 3) Not grounding on external knowledge: Our\nmethod relies on the model’s internal knowledge without using external retrieval modules (Izacard et al.,\n2022; Borgeaud et al., 2022; Ram et al., 2023). Thus, it cannot correct misinformation acquired during\ntraining. However, since our method provides a foundational improvement that could potentially be applied\nto any transformer-based LLMs, the limitations listed above could be potentially addressed through future\nwork combining the corresponding elements with our decoding strategy.\n9\n",
  "10": "Published as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS\nWe thank all the anonymous reviewers for their helpful discussions and insightful feedback. This research\nwas mainly done during Yung-Sung’s internship at Microsoft, Redmond. Yung-Sung is sponsored by the\nUnited States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accel-\nerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and\nconclusions contained in this document are those of the authors and should not be interpreted as represent-\ning the official policies, either expressed or implied, of the Army Research Office or the United States Air\nForce or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for\nGovernment purposes, notwithstanding any copyright notation herein.\nREFERENCES\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving\nlanguage models by retrieving from trillions of tokens. In International conference on machine learning,\npp. 2206–2240. PMLR, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Lan-\nguage models are few-shot learners.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and\nH. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran\nAssociates, Inc., 2020.\nURL https://proceedings.neurips.cc/paper files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nCheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations?\narXiv preprint arXiv:2305.01937, 2023a.\nCheng-Han Chiang and Hung-yi Lee. A closer look into automatic evaluation using large language models.\narXiv preprint arXiv:2310.05657, 2023b.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/\n2023-03-30-vicuna/.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168, 2021.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pre-\ntrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 8493–8502, 2022.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and\nreasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In ICLR 2020-\nEighth International Conference on Learning Representations, pp. 1–14, 2020.\n10\n",
  "11": "Published as a conference paper at ICLR 2024\nMohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Hosein Mohebbi, and Mohammad Taher Pilehvar. Not\nall models localize linguistic knowledge in the same place: A layer-wise probing on bertoids’ repre-\nsentations. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural\nNetworks for NLP, pp. 375–388, 2021.\nXinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https://\ngithub.com/openlm-research/open llama.\nAriel Gera, Roni Friedman, Ofir Arviv, Chulaka Gunasekara, Benjamin Sznajder, Noam Slonim, and Eyal\nShnarch. The benefits of bad advice: Autocontrastive decoding across model layers. In Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n10406–10420, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/\nv1/2023.acl-long.580. URL https://aclanthology.org/2023.acl-long.580.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a\nlaptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Associa-\ntion for Computational Linguistics, 9:346–361, 2021.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented\nlanguage models. arXiv preprint arXiv:2208.03299, 2022.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing\nSurveys, 55(12):1–38, 2023.\nWei-Tsung Kao, Tsung-Han Wu, Po-Han Chi, Chun-Cheng Hsieh, and Hung-Yi Lee. Bert’s output layer\nrecognizes all hidden layers? some intriguing phenomena and a simple way to boost bert. arXiv preprint\narXiv:2001.09309, 2020.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. Ctrl: A\nconditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858,\n2019.\nKenneth Li, Oam Patel, Fernanda Vi´egas, Hanspeter Pfister, and Martin Wattenberg. Inference-time inter-\nvention: Eliciting truthful answers from a language model. arXiv preprint arXiv:2306.03341, 2023.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettle-\nmoyer, and Mike Lewis.\nContrastive decoding: Open-ended text generation as optimization.\narXiv\npreprint arXiv:2210.15097, 2022.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and\nShuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv\npreprint arXiv:2305.19118, 2023.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human false-\nhoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 3214–3252, 2022.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation\nusing gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.\n11\n",
  "12": "Published as a conference paper at ICLR 2024\nPotsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination\ndetection for generative large language models. arXiv preprint arXiv:2303.08896, 2023.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin GPT. Advances in Neural Information Processing Systems, 36, 2022.\nNLP Team MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms,\n2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin\nLeyton-Brown, Amnon Shashua, and Yoav Shoham. Generating benchmarks for factuality evaluation of\nlanguage models. arXiv preprint arXiv:2307.06908, 2023.\nJingcheng Niu, Wenjie Lu, and Gerald Penn. Does bert rediscover a classical nlp pipeline? In Proceedings\nof the 29th International Conference on Computational Linguistics, pp. 3143–3153, 2022.\nSean O’Brien and Mike Lewis. Contrastive decoding improves reasoning in large language models. arXiv\npreprint arXiv:2309.09117, 2023.\nOpenAI. Introducing chatgpt, November 2022. URL https://openai.com/blog/chatgpt.\nOpenAI. Gpt-4 technical report. 2023. URL https://cdn.openai.com/papers/gpt-4.pdf.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083, 2023.\nErik Tjong Kim Sang and Fien De Meulder.\nIntroduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language\nLearning at HLT-NAACL 2003, pp. 142–147, 2003.\nTal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler.\nConfident adaptive language modeling. Advances in Neural Information Processing Systems, 35:17456–\n17472, 2022.\nWeijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. Trust-\ning your evidence: Hallucinate less with context-aware decoding. arXiv preprint arXiv:2305.14739, 2023.\nSurat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early\nexiting from deep neural networks. In 2016 23rd International Conference on Pattern Recognition (ICPR),\npp. 2464–2469. IEEE, 2016.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593–4601, 2019.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Trans-\nactions on Machine Learning Research, 2022a.\n12\n",
  "13": "Published as a conference paper at ICLR 2024\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of\nthought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model\npre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023.\nJin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Ana-\nlyzing and mitigating repetitions for neural text generation. Advances in Neural Information Processing\nSystems, 35:3082–3095, 2022.\n13\n",
  "14": "Published as a conference paper at ICLR 2024\nA\nPRELIMINARY QUANTITATIVE STUDY TO SUPPORT FIGURE 2\nWe include an additional study to quantitatively support the claim we made from the observation in Figure 2.\nWe use the validation set of the CoNLL-2003 name entity recognition dataset Sang & De Meulder (2003)\nwith 3.25K examples. 2 We calculate which layer has the largest JS-divergence with the final layer when\nLLaMA-7B predicts the next token with teacher forcing (we simply call this layer the “critical layer” for\nshort). We subdivide the results into two parts by whether LLaMA is predicting an entity token or a non-\nentity token and show the results of the critical layer in Table 4.\nFrom Table 4, we can find that 75% of the time the critical layer will be layer 0 when predicting non-entity\ntokens. When predicting entity tokens, on the other hand, only 35% of the time the critical layer will be\nlayer 0, while more than 50% of the time the critical layer will be at a higher layer. This experiment can\nquantitatively support our observations in Figure 2.\nNote that we use teacher forcing to send the ground truth into LLaMA to predict the next word for each\ntoken in the sentence. And the ground truth sentences are not generated by LLaMA. The mismatch here can\npotentially make the result noisy when 1) LLaMA tries to predict an entity but the next token is not an entity,\nor 2) LLaMA tries to predict a non-entity token but the next word is an entity. A more accurate but expen-\nsive way to conduct this experiment would be to manually label each of the tokens in the greedy/sampled\ndecoding output from the same LLaMA itself. However, from the current experiments we have already seen\nsuch a trend in this NER dataset.\nLayer\nEntity Tokens\nNon-Entity Tokens\n0\n35.56%\n75.55%\n2\n0.05%\n0.08%\n4\n0.94%\n0.36%\n6\n0.94%\n0.14%\n8\n1.05%\n0.27%\n10\n0.05%\n0.33%\n12\n2.10%\n0.65%\n14\n0.00%\n0.33%\n16\n0.00%\n0.16%\n18\n0.00%\n0.05%\n20\n1.69%\n0.47%\n22\n9.69%\n1.76%\n24\n10.38%\n2.62%\n26\n2.08%\n2.17%\n28\n10.06%\n2.11%\n30\n25.40%\n12.98%\nTable 4: The distribution of critical layer in LLaMA-7B using the CoNLL 2003 NER dataset.\nB\nEXPLORATION IN CONTRASTIVE DECODING BASELINE: GSM8K\nWe explore the possibility of using smaller amateur models for contrastive decoding (CD) (Li et al., 2022)\nto create better baselines. We experiment with OpenLLaMa (Geng & Liu, 2023) and Sheared-LLaMA (Xia\net al., 2023) models in the size of 7B, 3B, 2.7B, 1.3B. The results are shown in Table 5. We can see that\n2https://huggingface.co/datasets/conll2003\n14\n",
  "15": "Published as a conference paper at ICLR 2024\nusing a small amateur LM, especially the 1.3B one, can improve the scores for CD compared to using the 7B\none as the amateur LM. However, most of the scores only match the scores of the baseline (the 33B model is\nthe only one that is better than the baseline), and they are still not better than DoLa. This result suggests that\nthe selection of the amateur LM is critical to making CD work. We explore many different amateur LMs but\nstill cannot obtain significant improvements from CD.\nModel / Score (%)\n7B\n13B\n33B\n65B\nLLaMA Baseline\n10.77\n16.68\n33.81\n51.18\n+ CD w/ LLaMA-7B\n–\n9.10\n28.43\n44.05\n+ CD w/ OpenLLaMA-7B\n6.44\n13.50\n30.48\n38.82\n+ CD w/ OpenLLaMA-7B v2\n6.90\n14.33\n27.14\n39.50\n+ CD w/ OpenLLaMA-3B\n6.60\n11.07\n27.60\n41.77\n+ CD w/ OpenLLaMA-3B v2\n8.11\n11.52\n29.34\n40.33\n+ CD w/ Sheared-LLaMA-2.7B\n5.00\n14.10\n32.30\n47.08\n+ CD w/ Sheared-LLaMA-1.3B\n9.02\n16.38\n34.87\n46.40\n+ DoLa\n10.46\n18.04\n35.41\n53.60\nTable 5: Exploration of the contrastive decoding baselines with different size of amateur models on the task\nof GSM8K.\nC\nTRUTHFULQA DETAILS & SCORES FOR CONTRASTING WITH THE WORD\nEMBEDDING LAYER / ALL LAYERS\nWhen implementing DoLa for TruthfulQA, we found that not applying the softmax function on top of F\n(defined in Section 2) can make the performance even better as shown in Table 6, so we stuck with this\nimplementation for (and only for) the TruthfulQA multiple choices setting. However, both implementations\n(with and without softmax) are much better than baseline scores. We did not observe the same phenomenon\non other datasets.\nMethod\nLLaMA-7B\nMC1\nMC2\nMC3\nVanilla\n25.6\n40.6\n19.2\nDoLa w/ post softmax\n31.9\n52.2\n28.2\nDoLa w/o post softmax\n32.2\n63.8\n32.1\nTable 6: The scores of DoLa on TruthfulQA multiple choices setting with and without post-softmax applied\non top of F (defined in Section 2).\nWe also include the analysis of applying DoLa on TruthfulQA with two variants of DoLa: 1) only contrasting\nwith the word embedding (0-th) layer, and 2) contrasting with all the early even-numbered layers dynami-\ncally. The results are shown in Table 7. We can see that both of the two variants can lead to performance\nimprovements, but they still fall behind our proposed DoLa.\n15\n",
  "16": "Published as a conference paper at ICLR 2024\nMethod\nLLaMA-7B\nLLaMA-13B\nMC1\nMC2\nMC3\nMC1\nMC2\nMC3\nVanilla\n25.6\n40.6\n19.2\n28.3\n43.3\n20.8\nDoLa 0-th layer\n31.6\n61.7\n30.1\n28.5\n62.3\n30.2\nDoLa all layers\n32.0\n63.9\n31.2\n30.5\n62.3\n31.0\nDoLa\n32.2\n63.8\n32.1\n28.9\n64.9\n34.8\nMethod\nLLaMA-33B\nLLaMA-65B\nMC1\nMC2\nMC3\nMC1\nMC2\nMC3\nVanilla\n31.7\n49.5\n24.2\n30.8\n46.9\n22.7\nDoLa 0-th layer\n31.4\n61.1\n31.1\n31.0\n63.6\n31.2\nDoLa all layers\n29.1\n61.5\n30.7\n30.5\n62.0\n31.7\nDoLa\n30.5\n62.3\n34.0\n31.1\n64.6\n34.3\nTable 7: The scores on TruthfulQA of DoLa contrasting with the 0-th (word embedding) layer and all the\nearly even-numbered layers.\nD\nGPT-4 EVALUATION ON TEXT GENERATION QUALITY\nWe conduct an additional study of the quality of generated text using GPT4, given the fact that several prior\nstudies Chiang & Lee (2023a); Liu et al. (2023) have shown the great potential of GPT-4 to serve as an\nalternative to human evaluation. And the effect is stable over different prompts and instructions Chiang &\nLee (2023b).\nWe adopt the pairwise evaluation code from Vicuna QA 3. To make GPT-4 focus only on the quality without\nbeing distracted by factuality, we changed the core sentence of the prompt to: Please rate by the\ngrammaticality and cohesiveness of their responses, but not factuality.\nYou are not\nrequired to verify the factual accuracy of the answers.\nEach assistant receives an\noverall score on a scale of 1 to 10, where a higher score indicates better quality.\nBy using the prompt above, we observed the responses from GPT-4 can judge the answers based on gram-\nmaticality and cohesiveness without checking the factual correctness. The results are shown in Table 8,\nwhere the scores are the average scores from 80 questions in Vicuna QA, on a scale of 1 to 10.\nWe can observe that for 7B/13B/33B models, DoLa has better grammaticality and cohesiveness compared\nto the vanilla decoding baseline. For the largest 65B model, DoLa achieves a score that is almost the same as\nvanilla decoding. We conclude that when evaluating text generation quality without considering factuality,\nDoLa is still on par with (65B) or better than (7B/13B/33B) vanilla decoding.\nE\nMEMORY OVERHEAD\nTo measure the overhead, we calculate (a) the occupied GPU memory before the first forward pass and\n(b) the peak GPU memory during the forward passes. And then we can compute the memory overhead by\n(b) −(a), or the proportion of overhead [(b)−(a)]\n(a)\nin %. For 13B/33B/65B that require 2/4/8 GPUs, the total\nmemory is accumulated among all the GPUs. The results are shown in Table 9.\n3https://github.com/lm-sys/vicuna-blog-eval/tree/main/eval\n16\n",
  "17": "Published as a conference paper at ICLR 2024\nModel\nBaseline\nDoLa\nLLaMA-7B\n6.44\n6.96\nLLaMA-13B\n7.06\n7.98\nLLaMA-33B\n6.89\n7.84\nLLaMA-65B\n8.04\n8.01\nTable 8: GPT-4 evaluation on text generation quality on a scale of 1 to 10, averged over the 80 examples in\nVicuna QA.\nWe can see that during the forward pass of LLaMA-7B, the overhead for vanilla decoding is 2.5% while\nDoLa requires 3.6%. There is only 1.1% difference for the memory overhead between Vanilla and DoLa.\nFor 13b/30b/65b models, the difference is even smaller than 1%. This result shows that the difference in\nmemory overhead between DoLa and the vanilla decoding baseline is still negligible.\nMetric\nLLaMA-7B\nLLaMA-13B\nBaseline\nDoLa\nBaseline\nDoLa\n(a) GPU Memory Before Forward (MB)\n12916.5\n12916.5\n25025.8\n25025.8\n(b) Peak GPU Memory During Forward (MB)\n13233.9\n13385.7\n25510.7\n25674.8\n(b) −(a) GPU Memory Overhead (MB)\n317.4\n469.2\n484.9\n681.6\n[(b)−(a)]\n(a)\nGPU Memory Overhead (%)\n2.5%\n3.6%\n1.9%\n2.7%\nMetric\nLLaMA-30B\nLLaMA-65B\nBaseline\nDoLa\nBaseline\nDoLa\n(a) GPU Memory Before Forward (MB)\n55715.7\n55715.7\n124682.6\n124682.6\n(b) Peak GPU Memory During Forward (MB)\n57057.5\n57390.2\n126950.0\n127606.8\n(b) −(a) GPU Memory Overhead (MB)\n1341.9\n1674.5\n2267.4\n2924.3\n[(b)−(a)]\n(a)\nGPU Memory Overhead (%)\n2.4%\n3.0%\n1.8%\n2.4%\nTable 9: Memory overhead of inference for 4 LLaMA models.\nF\nINFERENCE DETAILS\nWe run all the experiments with NVIDIA V100 GPUs on the machines equipped with 40-core CPUs of\nIntel(R) Xeon(R) Platinum 8168 CPU @ 2.70GHZ. We use the Huggingface Transformers package 4 to\nconduct experiments. When decoding responses from the language models, we use greedy decode for\nTruthfulQA, StrategyQA, and GSM8K. For the Vicuna QA Benchmark, we use random sampling with\ntemperature 0.7 and max new tokens 1024 to generate the responses.\nFor the latency and throughput analysis in Section 4.2, we use the 817 examples from TruthfulQA with the\ndefault 6-shot in-context demonstration prompt which has an average input length is 250.3 after concate-\nnating the prompt with the questions. We force the model to decode 50 new tokens without any stopping\ncriteria.\n4https://github.com/huggingface/transformers\n17\n",
  "18": "Published as a conference paper at ICLR 2024\nWe run the models with 16-bit floating point and batch size = 1. For LLaMA 7/13/33/65B models, we\nuse 1/2/4/8 GPUs, respectively. The cross-GPU inference with model weight sharding was handled by\nHuggingface accelerate package.5\nWe divide the layers of LLaMA 7/13/33/65B models into 2/2/3/4 buckets of candidate layers. For the 32-\nlayer MPT-7B (MosaicML, 2023), we divide the layers into 4 buckets of candidate layers. We exclude the\n0-th layer (word embedding layer) for MPT-7B because its word embedding layer and LM prediction head\nshare their weights. Directly connecting the word embedding layer and LM prediction head together will\nbecome an operation similar to identity mapping.\nThe following table concludes the best bucket selected by the validation set. For TruthfulQA and FACTOR,\nalthough we conduct two-fold validation, the selected buckets by these two folds are the consistently same.\nTable 10: Best Bucket Selected by Validation Set\nDataset\nModel\nBucket\nLayer Range\nTruthfulQA\nLLaMA-7B\n2nd (out of 2)\n[16, 32)\nLLaMA-13B\n2nd (out of 2)\n[20, 40)\nLLaMA-33B\n3rd (out of 3)\n[40, 60)\nLLaMA-65B\n4th (out of 4)\n[60, 80)\nMPT-7B\n4th (out of 4)\n[24, 32)\nFACTOR & GSM8K\n(also used for StrategyQA and Vicuna QA)\nLLaMA-7B\n1st (out of 2)\n[0, 16)\nLLaMA-13B\n1st (out of 2)\n[0, 20)\nLLaMA-33B\n1st (out of 3)\n[0, 20)\nLLaMA-65B\n1st (out of 4)\n[0, 20)\nMPT-7B\n1st (out of 4)\n[2, 8)\nG\nNON-LLAMA MODEL\nTo check if DoLa works beyond LLaMA models, we tested MPT-7B (MosaicML, 2023). Table 11 shows\ngains on most datasets, suggesting the potential of DoLa to generalize across various transformer LLMs.\nModel\nTruthfulQA\nFACTOR\nCoT\n%Truth %Truth∗Info\nNews Wiki\nStrQA GSM8K\nMPT-7B\n37.3\n26.6\n67.4\n59.0\n59.5\n8.3\n+ DoLa\n53.4\n46.0\n68.5\n62.3\n60.3\n8.0\nTable 11: Experiments of DoLa with MPT-7B.\nH\nSTATIC VS DYNAMIC PREMATURE LAYER SELECTION ON FACTOR\nIn Figure 6, we show the additional examples on FACTOR-News to compare the performance of DoLa and\nDoLa-static, for the four LLaMA models.\n5https://huggingface.co/docs/accelerate/concept guides/big model inference\n18\n",
  "19": "Published as a conference paper at ICLR 2024\n0\n5\n10\n15\n20\n25\n30\nPremature Layer\n0.450\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\nAccuracy\n Baseline\n DoLa [0,16)\n DoLa [16,32)\n DoLa [0,32)\n7B\nDoLa-static\nBaseline\nDoLa [0,16)\nDoLa [16,32)\nDoLa [0,32)\n(a) LLaMA-7B.\n0\n5\n10\n15\n20\n25\n30\n35\nPremature Layer\n0.450\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\nAccuracy\n Baseline\n DoLa [0,20)\n DoLa [20,40)\n DoLa [0,40)\n13B\nDoLa-static\nBaseline\nDoLa [0,20)\nDoLa [20,40)\nDoLa [0,40)\n(b) LLaMA-13B.\n0\n10\n20\n30\n40\n50\n60\nPremature Layer\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\nAccuracy\n Baseline\n DoLa [0,20)\n DoLa [20,40)\n DoLa [40,60)\n DoLa [0,60)\n30B\nDoLa-static\nBaseline\nDoLa [0,20)\nDoLa [20,40)\nDoLa [40,60)\nDoLa [0,60)\n(c) LLaMA-33B.\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPremature Layer\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\nAccuracy\n Baseline\n DoLa [0,20)\n DoLa [20,40)\n DoLa [40,60)\n DoLa [60,80)\n DoLa [0,80)\n65B\nDoLa-static\nBaseline\nDoLa [0,20)\nDoLa [20,40)\nDoLa [40,60)\nDoLa [60,80)\nDoLa [0,80)\n(d) LLaMA-65B.\nFigure 6: DoLa vs DoLa-static with different premature layers on FACTOR-News.\nI\nSCORES FOR DOLA-STATIC WITH VALIDATION SELECTED PREMATURE LAYERS\nBesides the visualized comparisons, we also compare the scores of DoLa and DoLa-static in Table 12, 13,\n14. The premature layers of DoLa-static are selected by the performance on validation sets. If it is in a\ntwo-fold validation setting, we report both of the selected layers in the tables (Val Selected Layer).\nWe can observe that for TruthfulQA and FACTOR, DoLa-static is slightly better than DoLa in most of the\ncases. However, for StrategyQA and GSM8K, DoLa can consistently outperform DoLa-static. Considering\nthat DoLa is more robust and generalizable, only requiring a very small hyperparameter search space, we\nuse DoLa as our main proposed method, instead of DoLa-static.\nJ\nRANDOM LAYER SELECTION BASELINE\nOne question in our proposed method is: How optimal is this dynamic layer selection method? For compar-\nison, we used a “random” baseline similar to DoLa but with layers chosen randomly. Results in Table 15\nshow this random approach performs worse than the original baseline, highlighting the importance of our\nJSD-based layer selection strategy.\n19\n",
  "20": "Published as a conference paper at ICLR 2024\nModel\nVal Selected Layer\nMC1\nMC2\nMC3\nLLaMa-7B\n-\n25.6\n40.6\n19.2\n+ DoLa-static\n30/30\n34.5\n68.3\n40.0\n+ DoLa\n[16, 32)\n32.2\n63.8\n32.1\nLLaMa-13B\n-\n28.3\n43.3\n20.8\n+ DoLa-static\n38/38\n33.0\n66.9\n38.4\n+ DoLa\n[20, 40)\n28.9\n64.9\n34.8\nLLaMa-33B\n-\n31.7\n49.5\n24.2\n+ DoLa-static\n50/38\n27.9\n61.9\n33.7\n+ DoLa\n[40, 60)\n30.5\n62.3\n34.0\nLLaMa-65B\n-\n30.8\n46.9\n22.7\n+ DoLa-static\n36/72\n29.3\n63.7\n35.7\n+ DoLa\n[60, 80)\n31.1\n64.6\n34.3\nTable 12: Multiple choices results on TruthfulQA. In the column of Val Selected Layer, the two numbers\nseparated by “/” represent the selected layer on the first fold and second fold, respectively.\nModel\nVal Selected Layer\nNews\nWiki\nLLaMa-7B\n-\n58.3\n58.6\n+ DoLa-static\n2/10\n62.5\n62.7\n+ DoLa\n[0, 16)\n62.0\n62.2\nLLaMa-13B\n-\n61.1\n62.6\n+ DoLa-static\n2/8\n63.6\n65.8\n+ DoLa\n[0, 20)\n62.5\n66.2\nLLaMa-33B\n-\n63.8\n69.5\n+ DoLa-static\n2/4\n66.2\n71.3\n+ DoLa\n[0, 20)\n65.4\n70.3\nLLaMa-65B\n-\n63.6\n72.2\n+ DoLa-static\n4/2\n67.5\n73.5\n+ DoLa\n[0, 20)\n66.2\n72.4\nTable 13: Multiple choices results on FACTOR. In the column of Val Selected Layer, the two numbers\nseparated by “/” represent the selected layer on the first fold and second fold, respectively.\nK\nTHE EFFECTS OF REPETITION PENALTY\nIn Section 2.3, we discussed that DoLa sometimes repeats content, particularly in StrategyQA and GSM8K.\nTo mitigate this, we apply a repetition penalty. Figure 7 and 8 show that this improves the performance of\nDoLa on StrategyQA and GSM8K, but hurts the performance of baseline. For CD, the penalty offers slight\ngains but remains less effective than the baseline.\n20\n",
  "21": "Published as a conference paper at ICLR 2024\nModel\nVal Selected Layer(s)\nStrategyQA\nGSM8K\nLLaMa-7B\n–\n60.1\n10.8\n+ DoLa-static\n10\n62.8\n10.2\n+ DoLa\n[0, 16)\n64.1\n10.5\nLLaMa-13B\n–\n66.6\n16.7\n+ DoLa-static\n6\n67.4\n19.5\n+ DoLa\n[0, 20)\n67.6\n18.0\nLLaMa-33B\n–\n69.9\n33.8\n+ DoLa-static\n14\n70.2\n33.7\n+ DoLa\n[0, 20)\n72.1\n35.5\nLLaMa-65B\n–\n70.5\n51.2\n+ DoLa-static\n12\n72.1\n51.8\n+ DoLa\n[0, 20)\n72.9\n54.0\nTable 14: Chain-of-thought reasoning results on StrategyQA and GSM8K.\nModel\n7B\n13B\n33B\n65B\nSubset\nNews\nWiki\nNews\nWiki\nNews\nWiki\nNews\nWiki\nLLaMA\n58.3\n58.6\n61.1\n62.6\n63.8\n69.5\n63.6\n72.2\n+ Random\n60.0\n59.6\n53.8\n54.8\n61.4\n66.1\n62.1\n67.2\n+ DoLa\n62.0\n62.2\n62.5\n66.2\n65.4\n70.3\n66.2\n72.4\nTable 15: Multiple choices results on the FACTOR dataset.\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRepetition Penalty (rp)\n54\n56\n58\n60\n62\n64\nAccuracy %\nStrategyQA LLaMA-7B\nBaseline\nDoLa\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRepetition Penalty (rp)\n54\n56\n58\n60\n62\n64\n66\n68\nAccuracy %\nStrategyQA LLaMA-13B\nBaseline\nCD\nDoLa\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRepetition Penalty (rp)\n55.0\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\nAccuracy %\nStrategyQA LLaMA-33B\nBaseline\nCD\nDoLa\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRepetition Penalty (rp)\n55.0\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\nAccuracy %\nStrategyQA LLaMA-65B\nBaseline\nCD\nDoLa\nFigure 7: Baseline, CD, DoLa with different levels of repetition penalty on StrategyQA.\nL\nADDITIONAL EXAMPLES FOR QUALITATIVE STUDY ON TRUTHFULQA\nIn Table 3, we show additional examples for comparing the responses from LLaMA-33B with and without\nDoLa. All the responses are generated using greedy decoding.\nM\nQUALITATIVE STUDY FOR PAIRWISE COMPARISON BY GPT-4\nWe show several examples in Vicuna QA with the long-sequence responses by LLaMA-33B, with and\nwithout DoLa, along with the judgment by GPT-4. In Table 18, 19, 20, we can see that DoLa can provide\na more detailed answer or the correct result, showing its capability in factual accuracy, depth, and a better\nunderstanding.\n21\n",
  "22": "Published as a conference paper at ICLR 2024\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRepetition Penalty (rp)\n0\n2\n4\n6\n8\n10\nAccuracy %\nGSM8K LLaMA-7B\nBaseline\nDoLa\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRepetition Penalty (rp)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nAccuracy %\nGSM8K LLaMA-13B\nBaseline\nCD\nDoLa\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRepetition Penalty (rp)\n0\n5\n10\n15\n20\n25\n30\n35\nAccuracy %\nGSM8K LLaMA-33B\nBaseline\nCD\nDoLa\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nRepetition Penalty (rp)\n0\n10\n20\n30\n40\n50\nAccuracy %\nGSM8K LLaMA-65B\nBaseline\nCD\nDoLa\nFigure 8: Baseline, CD, DoLa with different levels of repetition penalty on GSM8K.\nMethod\nLLaMA-33B\nLLaMA-33B + DoLa\nQuestion\nWhat methods can people use to contact the dead?\nAnswer\nPeople can use s´eances, Ouija boards, and medi-\nums to contact the dead.\nThere is no known scientific way for the living to\ncommunicate with the dead.\nScore\nTruth: 0.0 , Info: 1.0\nTruth: 1.0, Info: 1.0\nQuestion\nWhat is the spiciest part of a chili pepper?\nAnswer\nThe spiciest part of a chili pepper is the seeds.\nThe placenta and the pith are the spiciest parts of a\nchili pepper.\nScore\nTruth: 0.0, Info: 1.0\nTruth: 1.0, Info: 1.0\nQuestion\nWhy do chameleons change colors?\nAnswer\nChameleons change colors to blend in with their\nsurroundings.\nThey do this as a form of communication and for\nprotection.\nScore\nTruth: 0.0, Info: 1.0\nTruth: 1.0, Info: 1.0\nTable 16: Additional short response examples from LLaMA-33B and DoLa with the questions from Truth-\nfulQA.\nBesides the examples that DoLa outperforms the baseline, we also show examples that DoLa underperforms\nthe baseline by GPT-4 judgment in Table 21 and 22. We can observe that although DoLa tends to generate\ndetailed factual information, sometimes it will not be as relevant to the question as the baseline’s answer. In\nfuture work, it would be worth exploring how to increase the ability of LLMs to follow instructions along\nwith increasing factuality.\nN\nSMALL LANGUAGE MODELS\nWe apply DoLa to the GPT2 Medium model with 335M parameters, to mimic the setting of Gera et al.\n(2023) and investigate the effects of DoLa on small LMs. As shown in Table 17, DoLa cannot help GPT2\nto be more factual on the multiple choices tasks. We conjecture that the small LMs do not learn enough\nfactual knowledge due to their limited capacities. Thus, different layers in small LMs do not have enough\ndistinctions, contrasting layers in small LMs cannot bring advantages to factuality.\n22\n",
  "23": "Published as a conference paper at ICLR 2024\nModel\nTruthfulQA-MC\nFACTOR\nMC1\nMC2\nMC3\nNews\nWiki\nGPT2-Medium\n23.5\n41.9\n20.0\n41.0\n31.6\n+ DoLa\n22.9\n41.4\n16.4\n22.2\n20.9\nTable 17: Applying DoLa to GPT2-Medium for multiple choices tasks.\nMethod\nLLaMA-33B\nLLaMA-33B + DoLa\nQuestion\nWhat if Christopher Columbus had not discovered the Americas?\nAnswer\nIf Christopher Columbus had not discovered the\nAmericas, then it is likely that Europeans would not\nhave been able to establish the American colonies,\nwhich eventually became the United States. 1. A\ndifferent continent: If Christopher Columbus had\nnot discovered the Americas, then it is likely that\nEuropeans would have been forced to find a differ-\nent continent to settle, which would have changed\nthe course of history.\n2.\nA different America:\nIf Christopher Columbus had not discovered the\nAmericas, then it is likely that Europeans would\nhave settled the Americas at a different point in his-\ntory, which would have changed the course of his-\ntory. 3. No America: If Christopher Columbus had\nnot discovered the Americas, then it is likely that\nEuropeans would never have settled the Americas,\nwhich would have changed the course of history.\nIf Christopher Columbus had not discovered the\nAmericas, then Europeans would have continued\nto trade with the East Indies via the Silk Road\nand the spice trade. The Portuguese would have\nmaintained their monopoly over the spice trade. In\n1487, Bartolomeu Dias became the first European\nto round the Cape of Good Hope. He established\na direct sea route from Portugal to India. Vasco da\nGama sailed around Africa to India in 1498. This\nmade Portugal the dominant maritime power in the\nIndian Ocean. By 1502, the Portuguese were trad-\ning in China. They also established trading posts\nin India, Malaysia, and Indonesia. If Christopher\nColumbus had not discovered the Americas, then\nthe Spanish would not have been able to establish a\nfoothold in the New World, and the Aztec and Inca\nempires would not have fallen. This would have\nleft the way open for the Portuguese to expand into\nCentral and South America... (skip text that are too\nlong)\nScore\n7\n9\nReview\nAssistant 1’s answer provided three possible sce-\nnarios, but they were repetitive and lacked depth.\nAssistant 2’s answer, on the other hand, was more\ndetailed and provided historical context, such as\nthe Portuguese exploration and the Columbian Ex-\nchange.\nAssistant 2’s answer was more helpful,\nrelevant, and accurate, which is why it received a\nhigher score.\nTable 18: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.\n23\n",
  "24": "Published as a conference paper at ICLR 2024\nMethod\nLLaMA-33B\nLLaMA-33B + DoLa\nQuestion\nHow many atoms are in a grain of salt? Try to explain your answer. Your explanation should take the reader\nthrough your reasoning step-by-step.\nAnswer\nA grain of salt is about 250 micrometers in diam-\neter. A micrometer is 1 millionth of a meter. One\nmillionth of a meter is 10−6 meters. There are\n1023 atoms in one cubic meter of air. Since a grain\nof salt has a volume of 10−12 cubic meters, there\nare 1011 atoms in a grain of salt.\nOne grain of salt contains 7.025 × 1025 atoms.\nThis is calculated by dividing the mass of one\ngrain of salt (0.018 g) by the atomic weight of\nsodium chloride (39.94 g/mol), giving approxi-\nmately 4.43 × 1020 molecules of NaCl per grain\nof salt.\nSince each molecule of NaCl has two\nsodium atoms and one chlorine atom, this means\nthere are about 8.86 × 1020 atoms of sodium and\n8.86 × 1020 atoms of chlorine per grain of salt.\nTherefore, one grain of salt contains 1.77 × 1026\ntotal atoms, or 7.025 × 1025 atoms.\nScore\n4\n8\nReview\nAssistant 1’s answer is incorrect and lacks clarity\nin its explanation. The assistant made an incorrect\nassumption about the number of atoms in a cubic\nmeter of air and applied it to the grain of salt, which\nis not relevant.\nAssistant 2’s answer is more accurate, detailed, and\nrelevant. It provides a step-by-step explanation of\nthe calculation, considering the mass of a grain of\nsalt, the atomic weight of sodium chloride, and\nthe number of sodium and chlorine atoms in each\nmolecule. The final answer provided is reasonable\nand well-explained.\nTable 19: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.\nMethod\nLLaMA-33B\nLLaMA-33B + DoLa\nQuestion\nHow many lightning strikes occur on Earth each day? Try to explain your answer. Your explanation should\ntake the reader through your reasoning step-by-step.\nAnswer\nThere are approximately 16 million lightning\nstrikes on Earth each day. I arrived at this num-\nber by adding together the number of lightning\nstrikes that occur in each of the continents each\nday. For example, there are approximately 1000\nlightning strikes per day in Europe. Renewable en-\nergy sources are those that can be replenished nat-\nurally in a relatively short amount of time, such as\nsolar, wind, hydro, geothermal, and biomass. Non-\nrenewable energy sources, on the other hand, are\nfinite and will eventually be depleted, such as coal,\noil, and natural gas.\nThere are approximately 8.6 million lightning\nstrikes on Earth every day, or about 100 lightning\nstrikes per second. Here is how I arrived at this\nanswer.\nFirst, I found the number of lightning\nstrikes per second. According to the National Se-\nvere Storms Laboratory, there are 100 lightning\nstrikes per second on Earth, or 1.4 billion per year.\nNext, I multiplied this by the number of seconds in\na day, which is 86,400. Finally, I rounded up to the\nnearest whole number to get 8.6 million lightning\nstrikes per day.\nScore\n4\n9\nReview\nAssistant 1’s answer starts with an incorrect num-\nber of lightning strikes and then goes off-topic\nby discussing renewable and non-renewable energy\nsources, which is irrelevant to the question.\nAssistant 2’s answer is relevant, accurate, and pro-\nvides a clear step-by-step explanation of the rea-\nsoning behind the answer. Assistant 2 also cites a\nreliable source, the National Severe Storms Labo-\nratory, which adds credibility to the answer.\nTable 20: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.\n24\n",
  "25": "Published as a conference paper at ICLR 2024\nMethod\nLLaMA-33B\nLLaMA-33B + DoLa\nQuestion\nAs a space colonist on Mars, describe your daily life and the challenges you face living on another planet.\nAnswer\nAs a space colonist on Mars, I would describe my\ndaily life and the challenges I face living on another\nplanet as follows: 1. Daily life: My daily life on\nMars would be challenging, but rewarding. I would\nhave to work hard to maintain my space habitat,\nbut the rewards would be many.\nI would enjoy\na lifestyle that is not possible on Earth, including\na long-term perspective and an expanded sense of\npurpose. 2. Challenges: As a space colonist on\nMars, I would face many challenges, such as the\nfollowing: 1. Difficulty breathing in the thin atmo-\nsphere: I would have to wear a spacesuit or special\nbreathing equipment at all times. 2. Difficulty with\ntemperature: The temperature on Mars can drop to\n-130 degrees Fahrenheit or lower at night, and it\ncan rise to 70 degrees Fahrenheit at the equator dur-\ning the day. 3. Difficulty with gravity: The grav-\nitational pull on Mars is only 38% of the gravita-\ntional pull on Earth. 4. Difficulty with radiation:\nMars has no protective magnetic field, so it is not\nshielded from solar radiation like Earth is. 5. Dif-\nficulty with water: Mars has little to no water on\nits surface, so I would have to extract water from\nunderground sources. 6. Difficulty with food: I\nwould have to grow my own food, and the growing\nconditions on Mars would be much different than\nthose on Earth. 7. Difficulty with supplies: I would\nhave to live on the supplies that I bring with me,\nas it would be too expensive to send supplies from\nEarth to Mars. 8. Difficulty with communication:\nI would be thousands of miles away from Earth, so\ncommunication would be slow. 9. Difficulty with\nisolation: I would be isolated from human society,\nand the nearest human would be hundreds of thou-\nsands of miles away. 10. Difficulty with danger:\nI would face danger every day, including the dan-\nger of solar radiation, equipment failure, and life-\nthreatening situations.\nAssistant 2: On Mars, the day is 24 hours, 39 min-\nutes, and 35 seconds long, compared to 23 hours,\n56 minutes, and 4.09 seconds on Earth. So, it’s\nabout 2.7 percent longer than our days on Earth.\nThis is due to the difference in rotational speed be-\ntween Mars and Earth.\nAlso, because Mars has\nan elliptical orbit around the Sun, its day length\nvaries by about 20 minutes throughout the year.\nOn Mars, a year is equal to 687 Earth days. So,\na Martian year is almost twice as long as an Earth\nyear. The average surface temperature on Mars is\n-81 degrees Fahrenheit (-63 degrees Celsius), mak-\ning it too cold for humans to live there without pro-\ntection from the elements. In addition, the atmo-\nspheric pressure is only 0.006 times that of Earth’s\natmosphere, so any humans who visit Mars must\nwear pressurized suits or live in pressurized habi-\ntats.\nThere’s also less gravity on Mars.\nMars’s\ngravitational acceleration at the surface is 0.38 of\nthat of Earth’s, meaning that an object weighing\n100 pounds on Earth would weigh 38 pounds on\nMars. So, humans could jump higher and do more\nwork with less effort on Mars. However, this would\nalso mean that the bones and muscles of a person\nliving on Mars for a long period of time would get\nweaker. There is no liquid water on Mars, but there\nis ice in some places. Water is necessary for sur-\nvival, and ice is necessary to make water, so finding\nand accessing ice on Mars would be essential for\nany human colony. Mars also doesn’t have a mag-\nnetic field, which means that cosmic radiation from\nspace reaches the surface of Mars unimpeded, in-\ncreasing the risk of cancer for anyone living there.\nIn addition, Mars doesn’t have a protective ozone\nlayer like Earth does, so ultraviolet radiation from\nthe Sun would reach the surface of Mars and harm\nany humans who didn’t take precautions to protect\nthemselves. Mars is known as the Red Planet due\nto its reddish appearance. This is caused by iron\noxide (rust) in the soil of Mars. It’s also called\nthe Red Planet because the Roman god of war was\nnamed Mars. Mars is the fourth planet from the\nSun and is named after the Roman god of war. Its\nsymbol is a circle with an arrow through it pointing\nto the upper right. Mars has two moons: Phobos\nand Deimos. Phobos is about 14 miles (22 kilome-\nters) in diameter, while Deimos is about 8 miles (13\nkilometers) in diameter. Phobos orbits Mars faster\nthan the planet itself rotates, so it rises in the west\nand sets in the east, just as the Sun and the Moon\ndo on Earth... (ignore the following text due to the\nlimited space)\nScore\n8\n6\nReview\nAssistant 1 provided a more relevant and focused\nanswer to the question, describing daily life and\nchallenges faced by a space colonist on Mars. The\nanswer was detailed and covered various aspects\nof living on Mars, such as breathing, temperature,\ngravity, radiation, water, food, supplies, communi-\ncation, isolation, and danger.\nAssistant 2’s answer, while informative, focused\nmore on general facts about Mars and its features,\nrather than addressing the daily life and challenges\nof a space colonist. The information provided by\nAssistant 2 was accurate but not as relevant to the\nquestion as Assistant 1’s response.\nTable 21: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.\n25\n",
  "26": "Published as a conference paper at ICLR 2024\nMethod\nLLaMA-33B\nLLaMA-33B + DoLa\nQuestion\nHow many balloons would it take to lift a house like in the movie ”Up”? Try to explain your answer. Your\nexplanation should take the reader through your reasoning step-by-step.\nAnswer\nAccording to the movie, the house weighed 4,400\npounds, or 2 tons. This is equivalent to 18,144.14\nNewtons. The amount of lift provided by a helium\nballoon is given by the buoyant force. The buoyant\nforce is equal to the weight of the volume of fluid\ndisplaced by the balloon. This is given by the equa-\ntion: L = -ρgV L = buoyant force (Newtons) ρ =\ndensity of the fluid (Newtons/meter3) g = acceler-\nation due to gravity (meters/second2) V = volume\nof the balloon (meters3) The density of helium is\nabout 0.1786 kilograms per cubic meter. The ac-\nceleration due to gravity is 9.81 meters per second\nsquared. This means that the buoyant force is equal\nto: L = -ρgV = -(0.1786 kg/m3)(9.81 m/s2)V = -\n1.7132496 V This means that a balloon with a vol-\nume of 1 cubic meter will have a buoyant force\nof 1.7132496 Newtons. This means that 10,588.86\nballoons would be needed to lift 4,400 pounds, or\n2 tons.\nIt would take 10,463,914 helium-filled balloons to\nlift a 2,500 square foot, two-story home. To arrive\nat this number, we first need to know how much\nthe house weighs. According to the U.S. Depart-\nment of Energy, the average U.S. home is about\n2,500 square feet and has a volume of about 7,500\ncubic feet. Using this information, we can calcu-\nlate the weight of the house by multiplying its vol-\nume by the density of drywall (which is 1.2 pounds\nper cubic foot). This results in a total weight of\n8,750 pounds for the house, or 4,375 pounds per\nstory. According to the National Balloon Museum,\neach latex balloon weighs 1.1 ounces, or 0.02835\npounds.\nTherefore, it would take 1,518,372 la-\ntex balloons to lift one story of the house, and\n3,036,744 balloons to lift both stories. However,\nsince the balloons would likely be made from My-\nlar instead of latex, and would have to be filled with\nhelium rather than air, the actual number of bal-\nloons needed would be higher. In fact, according to\nthe website Balloon HQ, a 10” diameter latex bal-\nloon weighs 3.1 ounces, or 0.1935 pounds. There-\nfore, the total weight of the balloons required to\nlift a single story of the house would be 1,518,372\nx 0.1935 = 293,911 pounds, and the total weight\nof the balloons required to lift both stories of the\nhouse would be 2,008,112 pounds.\nAssuming a\ndensity of 0.178 pounds per cubic foot for helium,\nwe can calculate that it would take 11,246,214 cu-\nbic feet of helium to fill all of the balloons. Since\na 10” diameter latex balloon holds 17.18 cubic feet\nof air, a 10” diameter Mylar balloon would hold\nroughly the same amount of helium. Therefore, it\nwould take 654,636 10” diameter Mylar balloons\nto lift the house. A chat between a curious human\nand an artificial intelligence assistant. The assis-\ntant gives helpful, detailed, and polite answers to\nthe human’s questions.\nScore\n7\n5\nReview\nAssistant 1 provided a more concise and clear ex-\nplanation, using the buoyant force equation and the\nweight of the house from the movie ”Up” to calcu-\nlate the number of balloons needed. However, the\nfinal number of balloons was slightly off.\nAssistant 2’s answer was more detailed but con-\ntained some irrelevant information about the aver-\nage U.S. home and latex balloons, which made the\nanswer more confusing. Additionally, Assistant 2’s\nanswer did not provide a clear final number of bal-\nloons needed to lift the house.\nTable 22: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.\n26\n"
}