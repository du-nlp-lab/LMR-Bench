{
  "1": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 563–578,\nHong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics\n563\nMoverScore: Text Generation Evaluating with Contextualized\nEmbeddings and Earth Mover Distance\nWei Zhao†, Maxime Peyrard†, Fei Liu‡, Yang Gao†, Christian M. Meyer†, Steffen Eger†\n† Computer Science Department, Technische Universit¨at Darmstadt, Germany\n‡ Computer Science Department, University of Central Florida, US\nzhao@aiphes.tu-darmstadt.de, maxime.peyrard@epfl.ch\nfeiliu@cs.ucf.edu, yang.gao@rhul.ac.uk\nmeyer@ukp.informatik.tu-darmstadt.de\neger@aiphes.tu-darmstadt.de\nAbstract\nA robust evaluation metric has a profound im-\npact on the development of text generation sys-\ntems. A desirable metric compares system out-\nput against references based on their seman-\ntics rather than surface forms. In this paper\nwe investigate strategies to encode system and\nreference texts to devise a metric that shows a\nhigh correlation with human judgment of text\nquality. We validate our new metric, namely\nMoverScore, on a number of text generation\ntasks including summarization, machine trans-\nlation, image captioning, and data-to-text gen-\neration, where the outputs are produced by\na variety of neural and non-neural systems.\nOur ﬁndings suggest that metrics combining\ncontextualized representations with a distance\nmeasure perform the best. Such metrics also\ndemonstrate strong generalization capability\nacross tasks.\nFor ease-of-use we make our\nmetrics available as web service.1\n1\nIntroduction\nThe choice of evaluation metric has a signiﬁcant\nimpact on the assessed quality of natural language\noutputs generated by a system. A desirable met-\nric assigns a single, real-valued score to the sys-\ntem output by comparing it with one or more ref-\nerence texts for content matching. Many natural\nlanguage generation (NLG) tasks can beneﬁt from\nrobust and unbiased evaluation, including text-\nto-text (machine translation and summarization),\ndata-to-text (response generation), and image-to-\ntext (captioning) (Gatt and Krahmer, 2018). With-\nout proper evaluation, it can be difﬁcult to judge\non system competitiveness, hindering the develop-\nment of advanced algorithms for text generation.\nIt is an increasingly pressing priority to develop\nbetter evaluation metrics given the recent advances\nin neural text generation. Neural models provide\n1Our code is publicly available at http://tiny.cc/vsqtbz\nthe ﬂexibility to copy content from source text as\nwell as generating unseen words (See et al., 2017).\nThis aspect is hardly covered by existing metrics.\nWith greater ﬂexibility comes increased demand\nfor unbiased evaluation. Diversity-promoting ob-\njectives make it possible to generate diverse nat-\nural language descriptions (Li et al., 2016; Wise-\nman et al., 2018). But standard evaluation met-\nrics including BLEU (Papineni et al., 2002) and\nROUGE (Lin, 2004) compute the scores based pri-\nmarily on n-gram co-occurrence statistics, which\nare originally proposed for diagnostic evaluation\nof systems but not capable of evaluating text qual-\nity (Reiter, 2018), as they are not designed to mea-\nsure if, and to what extent, the system and refer-\nence texts with distinct surface forms have con-\nveyed the same meaning. Recent effort on the ap-\nplicability of these metrics reveals that while com-\npelling text generation system ascend on standard\nmetrics, the text quality of system output is still\nhard to be improved (B¨ohm et al., 2019).\nOur goal in this paper is to devise an auto-\nmated evaluation metric assigning a single holistic\nscore to any system-generated text by comparing\nit against human references for content matching.\nWe posit that it is crucial to provide a holistic mea-\nsure attaining high correlation with human judg-\nments so that various neural and non-neural text\ngeneration systems can be compared directly. In-\ntuitively, the metric assigns a perfect score to the\nsystem text if it conveys the same meaning as the\nreference text. Any deviation from the reference\ncontent can then lead to a reduced score, e.g., the\nsystem text contains more (or less) content than\nthe reference, or the system produces ill-formed\ntext that fails to deliver the intended meaning.\nWe investigate the effectiveness of a spectrum\nof distributional semantic representations to en-\ncode system and reference texts, allowing them\nto be compared for semantic similarity across\n",
  "2": "564\nmultiple natural language generation tasks. Our\nnew metric quantiﬁes the semantic distance be-\ntween system and reference texts by harnessing\nthe power of contextualized representations (Pe-\nters et al., 2018; Devlin et al., 2018) and a power-\nful distance metric (Rubner et al., 2000) for better\ncontent matching. Our contributions can be sum-\nmarized as follows:\n• We formulate the problem of evaluating genera-\ntion systems as measuring the semantic distance\nbetween system and reference texts, assuming\npowerful continuous representations can encode\nany type of semantic and syntactic deviations.\n• We investigate the effectiveness of existing con-\ntextualized representations and Earth Mover’s\nDistance (Rubner et al., 2000) for comparing\nsystem predictions and reference texts, lead-\ning to our new automated evaluation metric\nthat achieves high correlation with human judg-\nments of text quality.\n• Our metric outperforms or performs compara-\nbly to strong baselines on four text generation\ntasks including summarization, machine trans-\nlation, image captioning, and data-to-text gen-\neration, suggesting this is a promising direction\nmoving forward.\n2\nRelated Work\nIt is of fundamental importance to design evalua-\ntion metrics that can be applied to natural language\ngeneration tasks of similar nature, including sum-\nmarization, machine translation, data-to-text gen-\neration, image captioning, and many others. All\nthese tasks involve generating texts of sentence or\nparagraph length. The system texts are then com-\npared with one or more reference texts of similar\nlength for semantic matching, whose scores indi-\ncate how well the systems perform on each task.\nIn the past decades, however, evaluation of these\nnatural language generation tasks has largely been\ncarried out independently within each area.\nSummarization\nA dominant metric for summa-\nrization evaluation is ROUGE (Lin, 2004), which\nmeasures the degree of lexical overlap between a\nsystem summary and a set of reference summaries.\nIts variants consider overlap of unigrams (-1), bi-\ngrams (-2), unigrams and skip bigrams with a max-\nimum gap of 4 words (-SU4), longest common sub-\nsequences (-L) and its weighted version (-W-1.2),\namong others. Metrics such as Pyramid (Nenkova\nand Passonneau, 2004) and BE (Hovy et al., 2006;\nTratz and Hovy, 2008) further compute matches\nof content units, e.g., (head-word, modiﬁer) tu-\nples, that often need to be manually extracted\nfrom reference summaries. These metrics achieve\ngood correlations with human judgments in the\npast. However, they are not general enough to ac-\ncount for the relatedness between abstractive sum-\nmaries and their references, as a system abstract\ncan convey the same meaning using different sur-\nface forms. Furthermore, large-scale summariza-\ntion datasets such as CNN/Daily Mail (Hermann\net al., 2015) and Newsroom (Grusky et al., 2018)\nuse a single reference summary, making it harder\nto obtain unbiased results when only lexical over-\nlap is considered during summary evaluation.\nMachine Translation\nA number of metrics are\ncommonly used in MT evaluation. Most of these\nmetrics compare system and reference translations\nbased on surface forms such as word/character\nn-gram overlaps and edit distance, but not the\nmeanings they convey.\nBLEU (Papineni et al.,\n2002) is a precision metric measuring how well a\nsystem translation overlaps with human reference\ntranslations using n-gram co-occurrence statistics.\nOther metrics include SentBLEU, NIST, chrF,\nTER, WER, PER, CDER, and METEOR (Lavie\nand Agarwal, 2007) that are used and described in\nthe WMT metrics shared task (Bojar et al., 2017;\nMa et al., 2018). RUSE (Shimanaka et al., 2018) is\na recent effort to improve MT evaluation by train-\ning sentence embeddings on large-scale data ob-\ntained in other tasks. Additionally, preprocessing\nreference texts is crucial in MT evaluation, e.g.,\nnormalization, tokenization, compound splitting,\netc. If not handled properly, different preprocess-\ning strategies can lead to inconsistent results using\nword-based metrics (Post, 2018).\nData-to-text Generation\nBLEU can be poorly\nsuited to evaluating data-to-text systems such as\ndialogue response generation and image caption-\ning. These systems are designed to generate texts\nwith lexical and syntactic variation, communicat-\ning the same information in many different ways.\nBLEU and similar metrics tend to reward systems\nthat use the same wording as reference texts, caus-\ning repetitive word usage that is deemed undesir-\nable to humans (Liu et al., 2016). In a similar vein,\nevaluating the quality of image captions can be\nchallenging. CIDEr (Vedantam et al., 2015) uses\ntf-idf weighted n-grams for similarity estimation;\nand SPICE (Anderson et al., 2016) incorporates\n",
  "3": "565\nsynonym matching over scene graphs. Novikova\net al. (2017) examine a large number of word- and\ngrammar-based metrics and demonstrate that they\nonly weakly reﬂect human judgments of system\noutputs generated by data-driven, end-to-end nat-\nural language generation systems.\nMetrics based on Continuous Representations\nMoving beyond traditional metrics, we envision\na new generation of automated evaluation metrics\ncomparing system and reference texts based on se-\nmantics rather than surface forms to achieve better\ncorrelation with human judgments. A number of\nprevious studies exploit static word embeddings\n(Ng and Abrecht, 2015; Lo, 2017) and trained\nclassifers (Peyrard et al., 2017; Shimanaka et al.,\n2018) to improve semantic similarity estimation,\nreplacing lexical overlaps.\nIn contemporaneous work, Zhang et al. (2019)\ndescribe a method comparing system and refer-\nence texts for semantic similarity leveraging the\nBERT representations (Devlin et al., 2018), which\ncan be viewed as a special case of our metrics and\nwill be discussed in more depth later. More re-\ncently, Clark et al. (2019) present a semantic met-\nric relying on sentence mover’s similarity and the\nELMo representations\n(Peters et al., 2018) and\napply them to summarization and essay scoring.\nMathur et al. (2019) introduce unsupervised and\nsupervised metrics based on the BERT represen-\ntations to improve MT evaluation, while Peyrard\n(2019a) provides a composite score combining re-\ndundancy, relevance and informativeness to im-\nprove summary evaluation.\nIn this paper, we seek to accurately measure the\n(dis)similarity between system and reference texts\ndrawing inspiration from contextualized represen-\ntations and Word Mover’s Distance (WMD; Kus-\nner et al., 2015). WMD ﬁnds the “traveling dis-\ntance” of moving from the word frequency distri-\nbution of the system text to that of the reference,\nwhich is essential to capture the (dis)similarity be-\ntween two texts. Our metrics differ from the con-\ntemporaneous work in several facets: (i) we ex-\nplore the granularity of embeddings, leading to\ntwo variants of our metric, word mover and sen-\ntence mover; (ii) we investigate the effectiveness\nof diverse pretrained embeddings and ﬁnetuning\ntasks; (iii) we study the approach to consolidate\nlayer-wise information within contextualized em-\nbeddings; (iii) our metrics demonstrate strong gen-\neralization capability across four tasks, oftentimes\noutperforming the supervised ones. We now de-\nscribe our method in detail.\n3\nOur MoverScore Meric\nWe have motivated the need for better metrics ca-\npable of evaluating disparate NLG tasks. We now\ndescribe our metric, namely MoverScore, built\nupon a combination of (i) contextualized repre-\nsentations of system and reference texts and (ii)\na distance between these representations measur-\ning the semantic distance between system outputs\nand references. It is particularly important for a\nmetric to not only capture the amount of shared\ncontent between two texts, i.e., intersect(A,B), as\nis the case with many semantic textual similarity\nmeasures (Peters et al., 2018; Devlin et al., 2018);\nbut also to accurately reﬂect to what extent the\nsystem text has deviated from the reference, i.e.,\nunion(A,B) - intersect(A,B), which is the intuition be-\nhind using a distance metric.\n3.1\nMeasuring Semantic Distance\nLet x = (x1, . . . , xm) be a sentence viewed as a\nsequence of words. We denote by xn the sequence\nof n-grams of x (i.e., x1 = x is the sequence of\nwords and x2 is the sequence of bigrams). Fur-\nthermore, let f xn ∈R|xn|\n+\nbe a vector of weights,\none weight for each n-gram of xn. We can as-\nsume f T\nxn1 = 1, making f xn a distribution over\nn-grams. Intuitively, the effect of some n-grams\nlike those including function words can be down-\nplayed by giving them lower weights, e.g., using\nInverse Document Frequency (IDF).\nWord Mover’s Distance (WMD) (Kusner et al.,\n2015), a special case of Earth Mover’s Dis-\ntance (Rubner et al., 2000), measures semantic\ndistance between texts by aligning semantically\nsimilar words and ﬁnding the amount of ﬂow trav-\neling between these words.\nIt was shown use-\nful for text classiﬁcation and textual similarity\ntasks (Kusner et al., 2015). Here, we formulate a\ngeneralization operating on n-grams. Let x and\ny be two sentences viewed as sequences of n-\ngrams: xn and yn. If we have a distance metric\nd between n-grams, then we can deﬁne the trans-\nportation cost matrix C such that Cij = d(xn\ni , yn\nj )\nis the distance between the i-th n-gram of x and\nthe j-th n-gram of y. The WMD between the two\nsequences of n-grams xn and yn with associated\nn-gram weights f xn and f yn is then given by:\nWMD(xn, yn) :=\nmin\nF ∈R|xn|×|yn|⟨C, F ⟩,\ns.t. F 1 = f xn, F ⊺1 = f yn.\n",
  "4": "566\nwhere F is the transportation ﬂow matrix with Fij\ndenoting the amount of ﬂow traveling from the i-\nth n-gram xn\ni in xn to the j-th n-gram yn\nj in yn.\nHere, ⟨C, F ⟩denotes the sum of all matrix entries\nof the matrix C ⊙F , where ⊙denotes element-\nwise multiplication. Then WMD(xn, yn) is the\nminimal transportation cost between xn and yn\nwhere n-grams are weighted by f xn and f yn.\nIn practice, we compute the Euclidean dis-\ntance between the embedding representations of\nn-grams: d(xn\ni , yn\nj ) = ||E(xn\ni ) −E(yn\nj )||2 where\nE is the embedding function which maps an n-\ngram to its vector representation. Usually, static\nword embeddings like word2vec are used to com-\npute E but these cannot capture word order or\ncompositionality.\nAlternatively, we investigate\ncontextualized embeddings like ELMo and BERT\nbecause they encode information about the whole\nsentence into each word vector.\nWe compute the n-gram embeddings as the\nweighted sum over its word embeddings.\nFor-\nmally, if xn\ni = (xi, . . . , xi+n−1) is the i-th n-gram\nfrom sentence x, its embedding is given by:\nE(xn\ni ) =\ni+n−1\nX\nk=i\nidf(xk) · E(xk)\n(1)\nwhere idf(xk) is the IDF of word xk computed\nfrom all sentences in the corpus and E(xk) is its\nword vector. Furthermore, the weight associated\nto the n-gram xn\ni is given by:\nf xn\ni = 1\nZ\ni+n−1\nX\nk=i\nidf(xk)\n(2)\nwhere Z is a normalizing constant s.t. f T\nxn1 = 1,\nIn the limiting case where n is larger than the\nsentence’s size, xn contains only one n-gram: the\nwhole sentence. Then WMD(xn, yn) reduces to\ncomputing the distance between the two sentence\nembeddings, namely Sentence Mover’s Distance\n(SMD), denoted as:\nSMD(xn, yn) := ||E(xlx\n1 ) −E(yly\n1 )||\nwhere lx and ly are the size of sentences.\nHard and Soft Alignments\nIn contempora-\nneous work, BERTScore (Zhang et al., 2019) also\nmodels the semantic distance between system and\nreference texts for evaluating text generation sys-\ntems. As shown in Figure 1, BERTScore (pre-\ncision/recall) can be intuitively viewed as hard\nSystem x: A guy with a red jacket is standing on a boat\nguy\nman\nwearing\nlifevest\nsitting\ncanoe\nred\njacket\nstanding\nboat\nguy\nman\nwearing\nlifevest\nsitting\ncanoe\nred\njacket\nstanding\nboat\nWord Embeddings\nWord Embeddings\nRef y: A man wearing a lifevest is sitting in a canoe\n𝒱\n𝒱\nBERTScore (precision/recall)\nMoverScore(unigram)\nFigure 1: An illustration of MoverScore and BERTScore.\nalignments (one-to-one) for words in a sentence\npair, where each word in one sequence travels to\nthe most semantically similar word in the other\nsequence. In contrast, MoverScore goes beyond\nBERTScore as it relies on soft alignments (many-\nto-one) and allows to map semantically related\nwords in one sequence to the respective word in\nthe other sequence by solving a constrained opti-\nmization problem: ﬁnding the minimum effort to\ntransform between two texts.\nThe formulation of Word Mover’s Distance pro-\nvides an important possibility to bias the metric\ntowards precision or recall by using an asymmet-\nric transportation cost matrix, which bridges a gap\nbetween MoverScore and BERTScore:\nProposition 1 BERTScore (precision/recall) can\nbe represented as a (non-optimized) Mover Dis-\ntance ⟨C, F ⟩, where C is a transportation cost\nmatrix based on BERT and F is a uniform trans-\nportation ﬂow matrix.2\n3.2\nContextualized Representations\nThe task formulation naturally lends itself to deep\ncontextualized representations for inducing word\nvectors E(xi). Despite the recent success of multi-\nlayer attentive neural architectures (Devlin et al.,\n2018; Peters et al., 2018), consolidating layer-wise\ninformation remains an open problem as different\nlayers capture information at disparate scales and\ntask-speciﬁc layer selection methods may be lim-\nited (Liu et al., 2018, 2019). Tenney et al. (2019)\nfound that a scalar mix of output layers trained\nfrom task-dependent supervisions would be effec-\ntive in a deep transformer-based model. Instead,\nwe investigate aggregation functions to consol-\nidate layer-wise information, forming stationary\nrepresentations of words without supervision.\nConsider a sentence x passed through contextu-\nalized encoders such as ELMo and BERT with L\nlayers. Each layer of the encoders produces a vec-\n2See the proof in the appendix.\n",
  "5": "567\ntor representation for each word xi in x. We de-\nnote by zi,l ∈Rd the representation given by layer\nl, a d-dimensional vector. Overall, xi receives L\ndifferent vectors (zi,1, . . . , zi,L). An aggregation\nφ maps these L vectors to one ﬁnal vector:\nE(xi) = φ(zi,1, . . . , zi,L)\n(3)\nwhere E(xi) is the aggregated representation of\nthe word xi.\nWe study two alternatives for φ: (i) the con-\ncatenation of power means (R¨uckl´e et al., 2018)\nas a generalized pooling mechanism, and (ii) a\nrouting mechanism for aggregation (Zhao et al.,\n2018, 2019). We relegate the routing method to\nappendix, as it does not yield better results than\npower means.\nPower Means\nPower means is an effective gen-\neralization of pooling techniques for aggregating\ninformation. It computes a non-linear average of a\nset of values with an exponent p (Eq. (4)). Follow-\ning R¨uckl´e et al. (2018), we exploit power means\nto aggregate vector representations (zi,l)L\nl=1 per-\ntaining to the i-th word from all layers of a deep\nneural architecture. Let p ∈R ∪{±∞}, the p-\nmean of (zi,1, . . . , zi,L) is:\nh(p)\ni\n=\n \nzp\ni,1 + · · · + zp\ni,L\nL\n!1/p\n∈Rd\n(4)\nwhere exponentiation is applied elementwise.\nThis generalized form can induce common named\nmeans such as arithmetic mean (p = 1) and geo-\nmetric mean (p = 0). In extreme cases, a power\nmean reduces to the minimum value of the set\nwhen p = −∞, and the maximum value when\np = +∞. The concatenation of p-mean vectors\nwe use in this paper is denoted by:\nE(xi) = h(p1)\ni\n⊕· · · ⊕h(pK)\ni\n(5)\nwhere ⊕is vector concatenation; {p1, . . . , pK} are\nexponent values, and we use K = 3 with p =\n1, ±∞in this work.\n3.3\nSummary of MoverScore Variations\nWe investigate our MoverScore along four dimen-\nsions: (i) the granularity of embeddings, i.e., the\nsize of n for n-grams, (ii) the choice of pretrained\nembedding mechanism, (iii) the ﬁne-tuning task\nused for BERT3 (iv) the aggregation technique (p-\nmeans or routing) when applicable.\n3ELMo usually requires heavy layers on the top, which\nrestricts the power of ﬁne-tuning tasks for ELMo.\nGranularity\nWe used n = 1 and n = 2 as well\nas full sentences (n = size of the sentence).\nEmbedding Mechanism\nWe obtained word em-\nbeddings from three different methods: static em-\nbedding with word2vec as well as contextualized\nembedding with ELMo and BERT. If n > 1, n-\ngram embeddings are calculated by Eq. (1). Note\nthat they represent sentence embeddings when\nn = size of the sentence.\nFine-tuning Tasks\nNatural Language Inference\n(NLI) and paraphrasing pose high demands in\nunderstanding sentence meaning.\nThis moti-\nvated us to ﬁne-tune BERT representations on\ntwo NLI datasets, MultiNLI and QANLI, and one\nParaphrase dataset, QQP—the largest datasets in\nGLUE (Wang et al., 2018). We ﬁne-tune BERT\non each of these, yielding different contextualized\nembeddings for our general evaluation metrics.\nAggregation\nFor ELMo, we aggregate word\nrepresentations given by all three ELMo layers,\nusing p-means or routing (see the appendix). Word\nrepresentations in BERT are aggregated from the\nlast ﬁve layers, using p-means or routing since the\nrepresentations in the initial layers are less suited\nfor use in downstream tasks (Liu et al., 2019).\n4\nEmpirical Evaluation\nIn this section, we measure the quality of dif-\nferent metrics on four tasks: machine transla-\ntion, text summarization, image captioning and di-\nalogue generation. Our major focus is to study the\ncorrelation between different metrics and human\njudgment. We employ two text encoders to embed\nn-grams: BERTbase, which uses a 12-layer trans-\nformer, and ELMOoriginal, which uses a 3-layer\nBiLSTM. We use Pearson’s r and Spearman’s ρ to\nmeasure the correlation. We consider two variants\nof MoverScore: word mover and sentence mover,\ndescribed below.\nWord Mover\nWe denote our word mover\nnotation containing four ingredients as: WMD-\nGranularity+Embedding+Finetune+Aggregation.\nFor example, WMD-1+BERT+MNLI+PMEANS\nrepresents the semantic metric using word mover\ndistance where unigram-based word embeddings\nﬁne-tuned on MNLI are aggregated by p-means.\nSentence\nMover\nWe\ndenote\nour\nsentence\nmover\nnotation\nwith\nthree\ningredients\nas:\nSMD+Embedding+Finetune+Aggregation.\nFor\nexample, SMD+W2V represents the semantic\n",
  "6": "568\nmetric using sentence mover distance where\ntwo sentence embeddings are computed as the\nweighted sum over their word2vec embeddings\nby Eq. (1).\nBaselines\nWe select multiple strong baselines\nfor each task for comparison: SentBLEU, ME-\nTEOR++ (Guo et al., 2018), and a supervised\nmetric RUSE for machine translation; ROUGE-\n1 and ROUGE-2 and a supervised metric S3\nbest\n(Peyrard et al., 2017) for text summarization;\nBLEU and METEOR for dialogue response gen-\neration, CIDEr, SPICE, METEOR and a super-\nvised metric LEIC (Cui et al., 2018) for image cap-\ntioning. We also report BERTScore (Zhang et al.,\n2019) for all tasks (see §2). Due to the page limit,\nwe only compare with the strongest baselines, the\nrest can be found in the appendix.\n4.1\nMachine Translation\nData\nWe obtain the source language sentences,\ntheir system and reference translations from the\nWMT 2017 news translation shared task (Bojar\net al., 2017). We consider 7 language pairs: from\nGerman (de), Chinese (zh), Czech (cs), Latvian\n(lv), Finnish (ﬁ), Russian (ru), and Turkish (tr),\nresp. to English. Each language pair has approxi-\nmately 3,000 sentences, and each sentence has one\nreference translation and multiple system transla-\ntions generated by participating systems. For each\nsystem translation, at least 15 human assessments\nare independently rated for quality.\nResults\nTable 1: In all language pairs, the best\ncorrelation is achieved by our word mover met-\nrics that use a BERT pretrained on MNLI as the\nembedding generator and PMeans to aggregate\nthe embeddings from different BERT layers, i.e.,\nWMD-1/2+BERT+MNLI+PMeans. Note that our\nunsupervised word mover metrics even outper-\nforms RUSE, a supervised metric. We also ﬁnd\nthat our word mover metrics outperforms the sen-\ntence mover. We conjecture that important infor-\nmation is lost in such a sentence representation\nwhile transforming the whole sequence of word\nvectors into one sentence embedding by Eq. (1).\n4.2\nText Summarization\nWe use two summarization datasets from the\nText Analysis Conference (TAC)4: TAC-2008 and\nTAC-2009, which contain 48 and 44 clusters, re-\nspectively. Each cluster includes 10 news articles\n4http://tac.nist.gov\n(on the same topic), four reference summaries,\nand 57 (in TAC-2008) or 55 (in TAC-2009) sys-\ntem summaries generated by the participating sys-\ntems. Each summary (either reference or system)\nhas fewer than 100 words, and receives two human\njudgment scores: the Pyramid score (Nenkova and\nPassonneau, 2004) and the Responsiveness score.\nPyramid measures how many important semantic\ncontent units in the reference summaries are cov-\nered by the system summary, while Responsive-\nness measures how well a summary responds to\nthe overall quality combining both content and lin-\nguistic quality.\nResults\nTables 2: We observe that lexical met-\nrics like ROUGE correlate above-moderate on\nTAC 2008 and 2009 datasets. In contrast, these\nmetrics perform poorly on other tasks like Di-\nalogue Generation (Novikova et al., 2017) and\nImage Captioning (Anderson et al., 2016). Ap-\nparently, strict matches on surface forms seems\nreasonable for extractive summarization datasets.\nHowever, we still see that our word mover met-\nrics, i.e., WMD-1+BERT+MNLI+PMeans, per-\nform better than or come close to even the super-\nvised metric S3\nbest.\n4.3\nData-to-text Generation\nWe use two task-oriented dialogue datasets:\nBAGEL (Mairesse et al., 2010) and SFHOTEL\n(Wen et al., 2015), which contains 202 and 398\ninstances of Meaning Representation (MR). Each\nMR instance includes multiple references, and\nroughly two system utterances generated by dif-\nferent neural systems. Each system utterance re-\nceives three human judgment scores: informa-\ntiveness, naturalness and quality score (Novikova\net al., 2017). Informativeness measures how much\ninformation a system utterance provides with re-\nspect to an MR. Naturalness measures how likely\na system utterance is generated by native speak-\ners. Quality measures how well a system utterance\ncaptures ﬂuency and grammar.\nResults\nTables 3: Interestingly, no metric pro-\nduces an even moderate correlation with human\njudgments, including our own. We speculate that\ncurrent contextualizers are poor at representing\nnamed entities like hotels and place names as well\nas numbers appearing in system and reference\ntexts. However, best correlation is still achieved\nby our word mover metrics combining contextual-\nized representations.\n",
  "7": "569\nDirect Assessment\nSetting\nMetrics\ncs-en\nde-en\nﬁ-en\nlv-en\nru-en\ntr-en\nzh-en\nAverage\nBASELINES\nMETEOR++\n0.552\n0.538\n0.720\n0.563\n0.627\n0.626\n0.646\n0.610\nRUSE(*)\n0.624\n0.644\n0.750\n0.697\n0.673\n0.716\n0.691\n0.685\nBERTSCORE-F1\n0.670\n0.686\n0.820\n0.710\n0.729\n0.714\n0.704\n0.719\nSENT-MOVER\nSMD + W2V\n0.438\n0.505\n0.540\n0.442\n0.514\n0.456\n0.494\n0.484\nSMD + ELMO + PMEANS\n0.569\n0.558\n0.732\n0.525\n0.581\n0.620\n0.584\n0.595\nSMD + BERT + PMEANS\n0.607\n0.623\n0.770\n0.639\n0.667\n0.641\n0.619\n0.652\nSMD + BERT + MNLI + PMEANS\n0.616\n0.643\n0.785\n0.660\n0.664\n0.668\n0.633\n0.667\nWORD-MOVER\nWMD-1 + W2V\n0.392\n0.463\n0.558\n0.463\n0.456\n0.485\n0.481\n0.471\nWMD-1 + ELMO + PMEANS\n0.579\n0.588\n0.753\n0.559\n0.617\n0.679\n0.645\n0.631\nWMD-1 + BERT + PMEANS\n0.662\n0.687\n0.823\n0.714\n0.735\n0.734\n0.719\n0.725\nWMD-1 + BERT + MNLI + PMEANS\n0.670\n0.708\n0.835\n0.746\n0.738\n0.762\n0.744\n0.743\nWMD-2 + BERT + MNLI + PMEANS\n0.679\n0.710\n0.832\n0.745\n0.736\n0.763\n0.740\n0.743\nTable 1: Absolute Pearson correlations with segment-level human judgments in 7 language pairs on WMT17 dataset.\nTAC-2008\nTAC-2009\nResponsiveness\nPyramid\nResponsiveness\nPyramid\nSetting\nMetrics\nr\nρ\nr\nρ\nr\nρ\nr\nρ\nBASELINES\nS3\nbest (*)\n0.715\n0.595\n0.754 0.652\n0.738\n0.595\n0.842 0.731\nROUGE-1\n0.703\n0.578\n0.747 0.632\n0.704\n0.565\n0.808 0.692\nROUGE-2\n0.695\n0.572\n0.718 0.635\n0.727\n0.583\n0.803 0.694\nBERTSCORE-F1\n0.724\n0.594\n0.750 0.649\n0.739\n0.580\n0.823 0.703\nSENT-MOVER\nSMD + W2V\n0.583\n0.469\n0.603 0.488\n0.577\n0.465\n0.670 0.560\nSMD + ELMO + PMEANS\n0.631\n0.472\n0.631 0.499\n0.663\n0.498\n0.726 0.568\nSMD + BERT + PMEANS\n0.658\n0.530\n0.664 0.550\n0.670\n0.518\n0.731 0.580\nSMD + BERT + MNLI + PMEANS\n0.662\n0.525\n0.666 0.552\n0.667\n0.506\n0.723 0.563\nWORD-MOVER\nWMD-1 + W2V\n0.669\n0.549\n0.665 0.588\n0.698\n0.520\n0.740 0.647\nWMD-1 + ELMO + PMEANS\n0.707\n0.554\n0.726 0.601\n0.736\n0.553\n0.813 0.672\nWMD-1 + BERT + PMEANS\n0.729\n0.595\n0.755 0.660\n0.742\n0.581\n0.825 0.690\nWMD-1 + BERT + MNLI + PMEANS\n0.736\n0.604\n0.760 0.672\n0.754\n0.594\n0.831 0.701\nWMD-2 + BERT + MNLI + PMEANS\n0.734\n0.601\n0.752 0.663\n0.753\n0.586\n0.825 0.694\nTable 2: Pearson r and Spearman ρ correlations with summary-level human judgments on TAC 2008 and 2009.\n4.4\nImage Captioning\nWe use a popular image captioning dataset: MS-\nCOCO (Lin et al., 2014), which contains 5,000\nimages. Each image includes roughly ﬁve refer-\nence captions, and 12 system captions generated\nby the participating systems from 2015 COCO\nCaptioning Challenge. For the system-level hu-\nman correlation, each system receives ﬁve human\njudgment scores: M1, M2, M3, M4, M5 (Ander-\nson et al., 2016). The M1 and M2 scores mea-\nsure overall quality of the captions while M3, M4\nand M5 scores measure correctness, detailedness\nand saliency of the captions. Following Cui et al.\n(2018), we compare the Pearson correlation with\ntwo system-level scores: M1 and M2, since we fo-\ncus on studying metrics for the overall quality of\nthe captions, leaving metrics understanding cap-\ntions in different aspects (correctness, detailedness\nand saliency) to future work.\nResults\nTable 4: Word mover metrics outper-\nform all baselines except for the supervised metric\nLEIC, which uses more information by consider-\ning both images and texts.\n4.5\nFurther Analysis\nHard and Soft Alignments\nBERTScore is the\nharmonic mean of BERTScore-Precision and\nBERTScore-Recall, where both two can be de-\ncomposed as a combination of “Hard Mover Dis-\ntance” (HMD) and BERT (see Prop. 1).\nWe use the representations in the 9-th BERT\nlayer for fair comparison of BERTScore and\nMoverScore and show results on the machine\ntranslation task in Table 5. MoverScore outper-\nforms both asymmetric HMD factors, while if they\nare combined via harmonic mean, BERTScore\nis on par with MoverScore. We conjecture that\nBERT softens hard alignments of BERTScore\nas contextualized embeddings encode information\nabout the whole sentence into each word vec-\ntor. We also observe that WMD-BIGRAMS slightly\noutperforms WMD-UNIGRAMS on 3 out of 4 lan-\nguage pairs.\n",
  "8": "570\nBAGEL\nSFHOTEL\nSetting\nMetrics\nInf\nNat\nQual\nInf\nNat\nQual\nBASELINES\nBLEU-1\n0.225\n0.141\n0.113\n0.107\n0.175\n0.069\nBLEU-2\n0.211\n0.152\n0.115\n0.097\n0.174\n0.071\nMETEOR\n0.251\n0.127\n0.116\n0.111\n0.148\n0.082\nBERTSCORE-F1\n0.267\n0.210\n0.178\n0.163\n0.193\n0.118\nSENT-MOVER\nSMD + W2V\n0.024\n0.074\n0.078\n0.022\n0.025\n0.011\nSMD + ELMO + PMEANS\n0.251\n0.171\n0.147\n0.130\n0.176\n0.096\nSMD + BERT + PMEANS\n0.290\n0.163\n0.121\n0.192\n0.223\n0.134\nSMD + BERT + MNLI + PMEANS\n0.280\n0.149\n0.120\n0.205\n0.239\n0.147\nWORD-MOVER\nWMD-1 + W2V\n0.222\n0.079\n0.123\n0.074\n0.095\n0.021\nWMD-1 + ELMO + PMEANS\n0.261\n0.163\n0.148\n0.147\n0.215\n0.136\nWMD-1 + BERT + PMEANS\n0.298\n0.212\n0.163\n0.203\n0.261\n0.182\nWMD-1 + BERT + MNLI + PMEANS\n0.285\n0.195\n0.158\n0.207\n0.270\n0.183\nWMD-2 + BERT + MNLI + PMEANS\n0.284\n0.194\n0.156\n0.204\n0.270\n0.182\nTable 3: Spearman correlation with utterance-level human judgments for BAGEL and SFHOTEL datasets.\nSetting\nMetric\nM1\nM2\nBASELINES\nLEIC(*)\n0.939\n0.949\nMETEOR\n0.606\n0.594\nSPICE\n0.759\n0.750\nBERTSCORE-RECALL\n0.809\n0.749\nSENT-MOVER\nSMD + W2V\n0.683\n0.668\nSMD + ELMO + P\n0.709\n0.712\nSMD + BERT + P\n0.723\n0.747\nSMD + BERT + M + P\n0.789\n0.784\nWORD-MOVER\nWMD-1 + W2V\n0.728\n0.764\nWMD-1 + ELMO + P\n0.753\n0.775\nWMD-1 + BERT + P\n0.780\n0.790\nWMD-1 + BERT + M + P\n0.813\n0.810\nWMD-2 + BERT + M + P\n0.812\n0.808\nTable 4: Pearson correlation with system-level human judg-\nments on MSCOCO dataset. ’M’ and ’P’ are short names.\nMetrics\ncs-en\nde-en\nﬁ-en\nlv-en\nRUSE\n0.624\n0.644\n0.750\n0.697\nHMD-F1 + BERT\n0.655\n0.681\n0.821\n0.712\nHMD-RECALL + BERT\n0.651\n0.658\n0.788\n0.681\nHMD-PREC + BERT\n0.624\n0.669\n0.817\n0.707\nWMD-UNIGRAM + BERT\n0.651\n0.686\n0.823\n0.710\nWMD-BIGRAM + BERT\n0.665\n0.688\n0.821\n0.712\nTable 5: Comparison on hard and soft alignments.\nDistribution of Scores\nIn Figure 2, we take a\ncloser look at sentence-level correlation in MT.\nResults reveal that the lexical metric SENTBLEU\ncan correctly assign lower scores to system trans-\nlations of low quality, while it struggles in judg-\ning system translations of high quality by assign-\ning them lower scores. Our ﬁnding agrees with\nthe observations found in Chaganty et al. (2018);\nNovikova et al. (2017): lexical metrics correlate\nbetter with human judgments on texts of low qual-\nity than high quality.\nPeyrard (2019b) further\nshow that lexical metrics cannot be trusted because\nbad\ngood\nHuman Judgments\n0.0\n0.5\n1.0\nSentBLEU\nbad\ngood\nHuman Judgments\n0.0\n0.5\n1.0\nWordMover\nFigure 2: Score distribution in German-to-English pair.\n1\n0.82\n1\n0.84\n0.97\n1\n0.61\n0.76\n0.75\n1\n0.73\n0.84\n0.83\n0.89\n1\n0.42\n0.51\n0.52\n0.61\n0.69\n1\n0\n0.12\n0.25\n0.38\n0.5\n0.62\n0.75\n0.88\n1\nSentBLEU\nBEER\nCHRF++\nSentMover\nWordMover\nDA\nMachine Translation (de−en)\nMachine Translation (de−en)\nMachine Translation (de-en)\n1\n0.8\n1\n0.84\n0.96\n1\n0.55\n0.78\n0.74\n1\n0.71\n0.83\n0.83\n0.84\n1\n0.51\n0.58\n0.59\n0.61\n0.74\n1\n0\n0.12\n0.25\n0.38\n0.5\n0.62\n0.75\n0.88\n1\nSentBLEU\nBEER\nCHRF++\nSentMover\nWordMover\nDA\nMachine Translation (zh−en)\nMachine Translation (zh−en)\nMachine Translation (zh-en)\nFigure 3: Correlation in similar language (de-en) and distant\nlanguage (zh-en) pair, where bordered area shows correla-\ntions between human assessment and metrics, the rest shows\ninter-correlations across metrics and DA is direct assessment\nrated by language experts.\nthey strongly disagree on high-scoring system out-\nputs. Importantly, we observe that our word mover\nmetric combining BERT can clearly distinguish\ntexts of two polar qualities.\nCorrelation Analysis\nIn Figure 3,\nwe ob-\nserve existing metrics for MT evaluation attaining\nmedium correlations (0.4-0.5) with human judg-\nments but high inter-correlations between them-\nselves.\nIn contrast, our metrics can attain high\ncorrelations (0.6-0.7) with human judgments, per-\nforming robust across different language pairs. We\nbelieve that our improvements come from clearly\ndistinguishing translations that fall on two ex-\ntremes.\nImpact of Fine-tuning Tasks\nFigure 4 com-\n",
  "9": "571\nOriginal\nMNLI\nQNLI\nQQP\n0.65\n0.70\n0.75\nPearson Correlation\nFigure 4: Correlation is averaged over 7 language pairs.\npares Pearson correlations with our word mover\nmetrics combining BERT ﬁne-tuned on three dif-\nferent tasks.\nWe observe that ﬁne-tuning on\nclosely related tasks improves correlations, espe-\ncially ﬁne-tuning on MNLI leads to an impressive\nimprovement by 1.8 points on average.\n4.6\nDiscussions\nWe showed that our metric combining contextual-\nized embeddings and Earth Mover’s Distance out-\nperforms strong unsupervised metrics on 3 out of\n4 tasks, i.e., METEOR++ on machine translation\nby 5.7 points, SPICE on image captioning by 3.0\npoints, and METEOR on dialogue response gen-\neration by 2.2 points.\nThe best correlation we\nachieved is combining contextualized word em-\nbeddings and WMD, which even rivals or exceeds\nSOTA task-dependent supervised metrics across\ndifferent tasks. Especially in machine translation,\nour word mover metric pushes correlations in ma-\nchine translation to 74.3 on average (5.8 points\nover the SOTA supervised metric and 2.4 points\nover contemporaneous BERTScore). The major\nimprovements come from contextualized BERT\nembeddings rather than word2vec and ELMo, and\nfrom ﬁne-tuning BERT on large NLI datasets.\nHowever, we also observed that soft alignments\n(MoverScore) marginally outperforms hard align-\nments (BERTScore). Regarding the effect of n-\ngrams in word mover metrics, unigrams slightly\noutperforms bigrams on average. For the effect\nof aggregation functions, we suggested effective\ntechniques for layer-wise consolidations, namely\np-means and routing, both of which are close to\nthe performance of the best layer and on par with\neach other (see the appendix).\n5\nConclusion\nWe investigated new unsupervised evaluation met-\nrics for text generation systems combining contex-\ntualized embeddings with Earth Mover’s Distance.\nWe experimented with two variants of our metric,\nsentence mover and word mover. The latter has\ndemonstrated strong generalization ability across\nfour text generation tasks, oftentimes even outper-\nforming supervised metrics. Our metric provides\na promising direction towards a holistic metric\nfor text generation and a direction towards more\n‘human-like’ (Eger et al., 2019) evaluation of text\ngeneration systems.\nIn future work, we plan to avoid the need\nfor costly human references in the evaluation of\ntext generation systems, and instead base evalua-\ntion scores on source texts and system predictions\nonly, which would allow for ‘next-level’, unsuper-\nvised (in a double sense) and unlimited evaluation\n(Louis and Nenkova, 2013; B¨ohm et al., 2019).\nAcknowledgments\nWe thank the anonymous reviewers for their com-\nments, which greatly improved the ﬁnal version of\nthe paper. This work has been supported by the\nGerman Research Foundation as part of the Re-\nsearch Training Group Adaptive Preparation of In-\nformation from Heterogeneous Sources (AIPHES)\nat the Technische Universit¨at Darmstadt under\ngrant No. GRK 1994/1. Fei Liu is supported in\npart by NSF grant IIS-1909603.\nReferences\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016.\nSPICE: semantic proposi-\ntional image caption evaluation. In Computer Vision\n- ECCV 2016 - 14th European Conference, Amster-\ndam, The Netherlands, October 11-14, 2016, Pro-\nceedings, Part V, pages 382–398.\nFlorian B¨ohm, Yang Gao, Christian M. Meyer, Ori\nShapira, Ido Dagan, and Iryna Gurevych. 2019. Bet-\nter rewards yield better summaries:\nLearning to\nsummarise without references.\nIn Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing, Hong Kong, China.\nOndrej Bojar, Yvette Graham, and Amir Kamran.\n2017. Results of the WMT17 metrics shared task.\nIn Proceedings of the Conference on Machine Trans-\nlation (WMT).\nArun Chaganty, Stephen Mussmann, and Percy Liang.\n2018. The price of debiasing automatic metrics in\nnatural language evalaution. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n643–653.\nElizabeth Clark, Asli Celikyilmaz, and Noah A. Smith.\n2019. Sentence mover’s similarity: Automatic eval-\nuation for multi-sentence texts. In Proceedings of\n",
  "10": "572\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2748–2760, Florence,\nItaly. Association for Computational Linguistics.\nDorin Comaniciu and Peter Meer. 2002. Mean shift:\nA robust approach toward feature space analysis.\nIEEE Transactions on Pattern Analysis & Machine\nIntelligence, (5):603–619.\nYin Cui, Guandao Yang, Andreas Veit, Xun Huang,\nand Serge Belongie. 2018. Learning to evaluate im-\nage captioning. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition,\npages 5804–5812.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018.\nBERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv:1810.04805.\nSteffen Eger, G¨ozde G¨ul S¸ahin, Andreas R¨uckl´e, Ji-\nUng Lee, Claudia Schulz, Mohsen Mesgar, Kr-\nishnkant Swarnkar, Edwin Simpson, and Iryna\nGurevych. 2019. Text processing like humans do:\nVisually attacking and shielding NLP systems. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages\n1634–1647, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nAlbert Gatt and Emiel Krahmer. 2018. Survey of the\nstate of the art in natural language generation: Core\ntasks, applications and evaluation. Journal of Artiﬁ-\ncial Intelligence Research (JAIR).\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNEWSROOM: A dataset of 1.3 million summaries\nwith diverse extractive strategies. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (NAACL-HLT).\nYinuo Guo, Chong Ruan, and Junfeng Hu. 2018. Me-\nteor++: Incorporating copy knowledge into machine\ntranslation evaluation. In Proceedings of the Third\nConference on Machine Translation: Shared Task\nPapers, pages 740–745.\nKarl Moritz Hermann,\nTomas Kocisky,\nEdward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015.\nTeaching ma-\nchines to read and comprehend. In Proceedings of\nNeural Information Processing Systems (NIPS).\nEduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi\nFukumoto. 2006. Automated summarization eval-\nuation with basic elements. In Proceedings of the\nFifth Conference on Language Resources and Eval-\nuation (LREC 2006), pages 604–611.\nMatt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kil-\nian Q. Weinberger. 2015. From word embeddings to\ndocument distances. In Proceedings of the Interna-\ntional Conference on Machine Learning (ICML).\nAlon Lavie and Abhaya Agarwal. 2007. Meteor: An\nAutomatic Metric for MT Evaluation with High\nLevels of Correlation with Human Judgments. In\nProceedings of the Second Workshop on Statistical\nMachine Translation, StatMT ’07, pages 228–231,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting objec-\ntive function for neural conversation models. In Pro-\nceedings of the North American Chapter of the As-\nsociation for Computational Linguistics (NAACL).\nChin-Yew Lin. 2004.\nROUGE: A Package for Au-\ntomatic Evaluation of summaries. In Proceedings\nof ACL workshop on Text Summarization Branches\nOut, pages 74–81, Barcelona, Spain.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nChia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael\nNoseworthy, Laurent Charlin, and Joelle Pineau.\n2016. How NOT to evaluate your dialogue system:\nAn empirical study of unsupervised evaluation met-\nrics for dialogue response generation. In Proceed-\nings of the Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nLiyuan Liu, Xiang Ren, Jingbo Shang, Xiaotao Gu,\nJian Peng, and Jiawei Han. 2018.\nEfﬁcient con-\ntextualized representation: Language model pruning\nfor sequence labeling. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nNelson F Liu,\nMatt Gardner,\nYonatan Belinkov,\nMatthew Peters, and Noah A Smith. 2019. Linguis-\ntic knowledge and transferability of contextual rep-\nresentations. arXiv preprint arXiv:1903.08855.\nChi-kiu Lo. 2017.\nMEANT 2.0: Accurate semantic\nMT evaluation for any output language. In Proceed-\nings of the Second Conference on Machine Transla-\ntion, WMT 2017, Copenhagen, Denmark, September\n7-8, 2017, pages 589–597.\nAnnie Louis and Ani Nenkova. 2013. Automatically\nassessing machine summary content without a gold\nstandard.\nComputational Linguistics, 39(2):267–\n300.\nQingsong Ma, Ondrej Bojar, and Yvette Graham. 2018.\nResults of the WMT18 metrics shared task. In Pro-\nceedings of the Third Conference on Machine Trans-\nlation (WMT).\nFranc¸ois Mairesse, Milica Gaˇsi´c, Filip Jurˇc´ıˇcek, Simon\nKeizer, Blaise Thomson, Kai Yu, and Steve Young.\n2010. Phrase-based statistical language generation\n",
  "11": "573\nusing graphical models and active learning. In Pro-\nceedings of the 48th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1552–\n1561. Association for Computational Linguistics.\nNitika Mathur, Timothy Baldwin, and Trevor Cohn.\n2019. Putting evaluation in context: Contextual em-\nbeddings improve machine translation evaluation.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n2799–2808, Florence, Italy. Association for Compu-\ntational Linguistics.\nAni Nenkova and Rebecca J. Passonneau. 2004. Evalu-\nating content selection in summarization: The pyra-\nmid method.\nIn Proceedings of the 2004 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 145–152. Association for\nComputational Linguistics.\nJun-Ping Ng and Viktoria Abrecht. 2015. Better sum-\nmarization evaluation with word embeddings for\nrouge. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1925–1930, Lisbon, Portugal. Associa-\ntion for Computational Linguistics.\nJekaterina Novikova, Ondˇrej Duˇsek, Amanda Cer-\ncas Curry, and Verena Rieser. 2017. Why We Need\nNew Evaluation Metrics for NLG. In Proceedings\nof the 2017 Conference on Empirical Methods in\nNatural Language Processing, pages 2241–2252,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A Method for Automatic\nEvaluation of Machine Translation. In Proceedings\nof the 40th Annual Meeting on Association for Com-\nputational Linguistics, ACL ’02, pages 311–318,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the North American\nChapter of the Association for Computational Lin-\nguistics (NAACL).\nMaxime Peyrard. 2019a. A simple theoretical model\nof importance for summarization. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1059–1073, Florence,\nItaly. Association for Computational Linguistics.\nMaxime Peyrard. 2019b.\nStudying summarization\nevaluation metrics in the appropriate scoring range.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n5093–5100, Florence, Italy. Association for Compu-\ntational Linguistics.\nMaxime\nPeyrard,\nTeresa\nBotschen,\nand\nIryna\nGurevych. 2017.\nLearning to score system\nsummaries for better content selection evaluation.\nIn Proceedings of the Workshop on New Frontiers\nin Summarization.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. In Proceedings of the Third Conference on\nMachine Translation (WMT).\nEhud Reiter. 2018. A structured review of the validity\nof BLEU.\nComputational Linguistics, 44(3):393–\n401.\nYossi Rubner, Carlo Tomasi, and Leonidas J. Guibas.\n2000.\nThe earth mover’s distance as a metric for\nimage retrieval. International Journal of Computer\nVision.\nAndreas R¨uckl´e, Steffen Eger, Maxime Peyrard, and\nIryna Gurevych. 2018. Concatenated power mean\nword embeddings as universal cross-lingual sen-\ntence representations. CoRR, abs/1803.01400.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL).\nHiroki Shimanaka, Tomoyuki Kajiwara, and Mamoru\nKomachi. 2018. RUSE: Regressor using sentence\nembeddings for automatic machine translation eval-\nuation. In Proceedings of the Third Conference on\nMachine Translation (WMT).\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R Bowman, Dipan-\njan Das, et al. 2019.\nWhat do you learn from\ncontext?\nprobing for sentence structure in con-\ntextualized word representations.\narXiv preprint\narXiv:1905.06316.\nStephen Tratz and Eduard H Hovy. 2008.\nSumma-\nrization Evaluation Using Transformed Basic Ele-\nments. In Proceedings of the text analysing confer-\nence, (TAC 2008).\nRamakrishna Vedantam, C. Lawrence Zitnick, and\nDevi Parikh. 2015. CIDEr: Consensus-based Im-\nage Description Evaluation.\nIn IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2015, Boston, MA, USA, June 7-12, 2015, pages\n4566–4575.\nMatt P Wand and M Chris Jones. 1994. Kernel smooth-\ning. Chapman and Hall/CRC.\nAlex Wang, Amapreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\n",
  "12": "574\nTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-\nHao Su, David Vandyke, and Steve Young. 2015.\nSemantically conditioned lstm-based natural lan-\nguage generation for spoken dialogue systems.\narXiv preprint arXiv:1508.01745.\nSam Wiseman, Stuart M. Shieber, and Alexander M.\nRush. 2018. Learning neural templates for text gen-\neration. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nSuofei Zhang, Wei Zhao, Xiaofu Wu, and Quan Zhou.\n2018. Fast dynamic routing based on weighted ker-\nnel density estimation. CoRR, abs/1805.10807.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2019.\nBertscore:\nEvaluating text generation with BERT.\nCoRR,\nabs/1904.09675.\nWei Zhao, Haiyun Peng, Steffen Eger, Erik Cambria,\nand Min Yang. 2019.\nTowards scalable and reli-\nable capsule networks for challenging NLP appli-\ncations. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1549–1559, Florence, Italy. Association\nfor Computational Linguistics.\nWei Zhao, Jianbo Ye, Min Yang, Zeyang Lei, Suofei\nZhang, and Zhou Zhao. 2018.\nInvestigating cap-\nsule networks with dynamic routing for text classiﬁ-\ncation. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Process-\ning. Association for Computational Linguistics.\n",
  "13": "575\nA\nSupplemental Material\nA.1\nProof of Prop. 1\nIn this section, we prove Prop. 1 in the paper about viewing BERTScore (precision/recall) as a (non-\noptimized) Mover Distance.\nAs a reminder, the WMD formulation is:\nWMD(xn, yn) :=\nmin\nF ∈R|xn|×|yn|\nX\ni,j\nCij · Fij\ns.t. 1⊺F ⊺1 = 1, 1⊺F 1 = 1.\nwhere F ⊺1 = f n\nx and F 1 = f n\ny. Here, f n\nx and f n\ny denote vectors of weights for each n-gram of xn and\nyn.\nBERTScore is deﬁned as:\nRBERT =\nP\ny1\ni ∈y1 idf(y1\ni ) maxx1\nj∈x1 E(x1\nj)⊺E(y1\ni )\nP\ny1\ni ∈y1 idf(y1\ni )\nPBERT =\nP\nx1\nj∈x1 idf(x1\nj) maxy1\ni ∈y1 E(y1\ni )⊺E(x1\nj)\nP\nx1\nj∈x1 idf(x1\nj)\nFBERT = 2 PBERT · RBERT\nPBERT + RBERT\n.\nThen, RBERT can be formulated in a “quasi” WMD form:\nRBERT(x1, y1) :=\nX\ni,j\nCij · Fij\nFij =\n( 1\nM\nif xj = arg maxˆx1\nj∈x1 E(y1\ni )⊺E(ˆx1\nj)\n0\notherwise\nCij =\n(M\nZ idf(y1\ni )E(x1\nj)⊺E(y1\ni )\nif xj = arg maxˆx1\nj∈x1 E(y1\ni )⊺E(ˆx1\nj)\n0\notherwise\nwhere Z = P\ny1\ni ∈y1 idf(y1\ni ) and M is the size of n-grams in x1. Similarly, we can have PBERT in a quasi\nWMD form (omitted). Then, FBERT can be formulated as harmonic-mean of two WMD forms of PBERT\nand RBERT.\nA.2\nRouting\nIn this section, we study the aggregation function φ with a routing scheme, which has achieved good\nresults in other NLP tasks (Zhao et al., 2018, 2019). Speciﬁcally, we introduce a nonparametric clustering\nwith Kernel Density Estimation (KDE) for routing since KDE bridges a family of kernel functions with\nunderlying empirical distributions, which often leads to computational efﬁciency (Zhang et al., 2018),\ndeﬁned as:\nmin\nv,γ f(z) =\nL\nX\ni=1\nT\nX\nj=1\nγijk(d(vj −zi,j))\ns.t.\n∀i, j : γij > 0,\nL\nX\nj=1\nγij = 1.\n",
  "14": "576\nwhere d(·) is a distance function, γij denotes the underlying closeness between the aggregated vector vj\nand vector zi in the i-th layer, and k is a kernel function. Some instantiations of k(·) (Wand and Jones,\n1994) are:\nGaussian : k(x) ≜exp (−x\n2), Epanechnikov : k(x) ≜\n(\n1 −x\nx ∈[0, 1)\n0\nx ≥1.\nOne typical solution for KDE clustering to minimize f(z) is taking Mean Shift (Comaniciu and Meer,\n2002), deﬁned as:\n∇f(z) =\nX\ni,j\nγijk′(d(vj, zi,j))∂d(vj, zi,j)\n∂v\nFirstly, vτ+1\nj\ncan be updated while γτ+1\nij\nis ﬁxed:\nvτ+1\nj\n=\nP\ni γτ\nijk′(d(vτ\nj , zi,j))zi,j\nP\ni,j k′(d(vτ\nj , zi,j))\nIntuitively, vj can be explained as a ﬁnal aggregated vector from L contextualized layers. Then, we\nadopt SGD to update γτ+1\nij\n:\nγτ+1\nij\n= γτ\nij + α · k(d(vτ\nj , zi,j))\nwhere α is a hyperparameter to control step size. The routing process is summarized in Algorithm 1.\nAlgorithm 1 Aggregation by Routing\n1: procedure ROUTING(zij, ℓ)\n2: Initialize ∀i, j : γij = 0\n3: while true do\n4:\nforeach representation i and j in layer ℓand ℓ+ 1 do γij ←softmax(γij)\n5:\nforeach representation j in layer ℓ+ 1 do\n6:\nvj ←P\ni γijk′(vj, zi)zi/ P\ni k′(vi, zi)\n7:\nforeach representation i and j in layer ℓand ℓ+ 1 do γij ←γij + α · k(vj, zi)\n8:\nloss ←log(P\ni,j γijk(vj, zi))\n9:\nif |loss −preloss| < ϵ then\n10:\nbreak\n11:\nelse\n12:\npreloss ←loss\n13: return vj\nBest Layer and Layer-wise Consolidation\nTable 6 compares our word mover based metric com-\nbining BERT representations on different layers with stronger BERT representations consolidated from\nthese layers (using p-means and routing). We often see that which layer has best performance is task-\ndependent, and our word mover based metrics (WMD) with p-means or routing schema come close to\nthe oracle performance obtained from the best layers.\nExperiments\nTable 7, 8 and 9 show correlations between metrics (all baseline metrics and word\nmover based metrics) and human judgments on machine translation, text summarization and dialogue\nresponse generation, respectively. We ﬁnd that word mover based metrics combining BERT ﬁne-tuned\non MNLI have highest correlations with humans, outperforming all of the unsupervised metrics and even\nsupervised metrics like RUSE and S3\nfull. Routing and p-means perform roughly equally well.\n",
  "15": "577\nDirect Assessment\nMetrics\ncs-en\nde-en\nﬁ-en\nlv-en\nru-en\ntr-en\nzh-en\nWMD-1 + BERT + LAYER 8\n.6361\n.6755\n.8134\n.7033\n.7273\n.7233\n.7175\nWMD-1 + BERT + LAYER 9\n.6510\n.6865\n.8240\n.7107\n.7291\n.7357\n.7195\nWMD-1 + BERT + LAYER 10\n.6605\n.6948\n.8231\n.7158\n.7363\n.7317\n.7168\nWMD-1 + BERT + LAYER 11\n.6695\n.6845\n.8192\n.7048\n.7315\n.7276\n.7058\nWMD-1 + BERT + LAYER 12\n.6677\n.6825\n.8194\n.7188\n.7326\n.7291\n.7064\nWMD-1 + BERT + ROUTING\n.6618\n.6897\n.8225\n.7122\n.7334\n.7301\n.7182\nWMD-1 + BERT + PMEANS\n.6623\n.6873\n.8234\n.7139\n.7350\n.7339\n.7192\nTable 6: Absolute Pearson correlations with segment-level human judgments on WMT17 to-English translations.\nDirect Assessment\nSetting\nMetrics\ncs-en\nde-en\nﬁ-en\nlv-en\nru-en\ntr-en\nzh-en\nAverage\nBASELINES\nBLEND\n0.594\n0.571\n0.733\n0.594\n0.622\n0.671\n0.661\n0.635\nRUSE\n0.624\n0.644\n0.750\n0.697\n0.673\n0.716\n0.691\n0.685\nSENTBLEU\n0.435\n0.432\n0.571\n0.393\n0.484\n0.538\n0.512\n0.481\nCHRF++\n0.523\n0.534\n0.678\n0.520\n0.588\n0.614\n0.593\n0.579\nMETEOR++\n0.552\n0.538\n0.720\n0.563\n0.627\n0.626\n0.646\n0.610\nBERTSCORE-F1\n0.670\n0.686\n0.820\n0.710\n0.729\n0.714\n0.704\n0.719\nWORD-MOVER\nWMD-1 + W2V\n0.392\n0.463\n0.558\n0.463\n0.456\n0.485\n0.481\n0.471\nWMD-1 + BERT + ROUTING\n0.658\n0.689\n0.823\n0.712\n0.733\n0.730\n0.718\n0.723\nWMD-1 + BERT + MNLI + ROUTING\n0.665\n0.705\n0.834\n0.744\n0.735\n0.752\n0.736\n0.739\nWMD-2 + BERT + MNLI + ROUTING\n0.676\n0.706\n0.831\n0.743\n0.734\n0.755\n0.732\n0.740\nWMD-1 + BERT + PMEANS\n0.662\n0.687\n0.823\n0.714\n0.735\n0.734\n0.719\n0.725\nWMD-1 + BERT + MNLI + PMEANS\n0.670\n0.708\n0.835\n0.746\n0.738\n0.762\n0.744\n0.743\nWMD-2 + BERT + MNLI + PMEANS\n0.679\n0.710\n0.832\n0.745\n0.736\n0.763\n0.740\n0.743\nTable 7: Absolute Pearson correlations with segment-level human judgments on WMT17 to-English translations.\nTAC-2008\nTAC-2009\nResponsiveness\nPyramid\nResponsiveness\nPyramid\nSetting\nMetrics\nr\nρ\nr\nρ\nr\nρ\nr\nρ\nBASELINES\nS3\nfull\n0.696\n0.558\n0.753\n0.652\n0.731\n0.552\n0.838\n0.724\nS3\nbest\n0.715\n0.595\n0.754\n0.652\n0.738\n0.595\n0.842\n0.731\nTF∗IDF-1\n0.176\n0.224\n0.183\n0.237\n0.187\n0.222\n0.242\n0.284\nTF∗IDF-2\n0.047\n0.154\n0.049\n0.182\n0.047\n0.167\n0.097\n0.233\nROUGE-1\n0.703\n0.578\n0.747\n0.632\n0.704\n0.565\n0.808\n0.692\nROUGE-2\n0.695\n0.572\n0.718\n0.635\n0.727\n0.583\n0.803\n0.694\nROUGE-1-WE\n0.571\n0.450\n0.579\n0.458\n0.586\n0.437\n0.653\n0.516\nROUGE-2-WE\n0.566\n0.397\n0.556\n0.388\n0.607\n0.413\n0.671\n0.481\nROUGE-L\n0.681\n0.520\n0.702\n0.568\n0.730\n0.563\n0.779\n0.652\nFRAME-1\n0.658\n0.508\n0.686\n0.529\n0.678\n0.527\n0.762\n0.628\nFRAME-2\n0.676\n0.519\n0.691\n0.556\n0.715\n0.555\n0.781\n0.648\nBERTSCORE-F1\n0.724\n0.594\n0.750\n0.649\n0.739\n0.580\n0.823\n0.703\nWORD-MOVER\nWMD-1 + W2V\n0.669\n0.559\n0.665\n0.611\n0.698\n0.520\n0.740\n0.647\nWMD-1 + BERT + ROUTING\n0.729\n0.601\n0.763\n0.675\n0.740\n0.580\n0.831\n0.700\nWMD-1 + BERT + MNLI + ROUTING\n0.734\n0.609\n0.768\n0.686\n0.747\n0.589\n0.837\n0.711\nWMD-2 + BERT + MNLI + ROUTING\n0.731\n0.593\n0.755\n0.666\n0.753\n0.583\n0.827\n0.698\nWMD-1 + BERT + PMEANS\n0.729\n0.595\n0.755\n0.660\n0.742\n0.581\n0.825\n0.690\nWMD-1 + BERT + MNLI + PMEANS\n0.736\n0.604\n0.760\n0.672\n0.754\n0.594\n0.831\n0.701\nWMD-2 + BERT + MNLI + PMEANS\n0.734\n0.601\n0.752\n0.663\n0.753\n0.586\n0.825\n0.694\nTable 8: Correlation of automatic metrics with summary-level human judgments for TAC-2008 and TAC-2009.\n",
  "16": "578\nBAGEL\nSFHOTEL\nSetting\nMetrics\nInf\nNat\nQual\nInf\nNat\nQual\nBASELINES\nBLEU-1\n0.225\n0.141\n0.113\n0.107\n0.175\n0.069\nBLEU-2\n0.211\n0.152\n0.115\n0.097\n0.174\n0.071\nBLEU-3\n0.191\n0.150\n0.109\n0.089\n0.161\n0.070\nBLEU-4\n0.175\n0.141\n0.101\n0.084\n0.104\n0.056\nROUGE-L\n0.202\n0.134\n0.111\n0.092\n0.147\n0.062\nNIST\n0.207\n0.089\n0.056\n0.072\n0.125\n0.061\nCIDER\n0.205\n0.162\n0.119\n0.095\n0.155\n0.052\nMETEOR\n0.251\n0.127\n0.116\n0.111\n0.148\n0.082\nBERTSCORE-F1\n0.267\n0.210\n0.178\n0.163\n0.193\n0.118\nWORD-MOVER\nWMD-1 + W2V\n0.222\n0.079\n0.123\n0.074\n0.095\n0.021\nWMD-1 + BERT + ROUTING\n0.294\n0.209\n0.156\n0.208\n0.256\n0.178\nWMD-1 + BERT + MNLI + ROUTING\n0.278\n0.180\n0.144\n0.211\n0.252\n0.175\nWMD-2 + BERT + MNLI + ROUTING\n0.279\n0.182\n0.147\n0.204\n0.252\n0.172\nWMD-1 + BERT + PMEANS\n0.298\n0.212\n0.163\n0.203\n0.261\n0.182\nWMD-1 + BERT + MNLI + PMEANS\n0.285\n0.195\n0.158\n0.207\n0.270\n0.183\nWMD-2 + BERT + MNLI + PMEANS\n0.284\n0.194\n0.156\n0.204\n0.270\n0.182\nTable 9: Spearman correlation with utterance-level human judgments for BAGEL and SFHOTEL datasets.\n"
}