{
  "1": "Exploring Concept Depth: How Large Language Models Acquire\nKnowledge and Concepts at Different Layers?\nMingyu Jin1, Qinkai Yu2, Jingyuan Huang1, Qingcheng Zeng3,\nZhenting Wang1, Wenyue Hua1, Haiyan Zhao4, Kai Mei1, Yanda Meng2,\nKaize Ding3, Fan Yang5, Mengnan Du4, Yongfeng Zhang1\n1Rutgers University, 2University of Exeter, 3Northwestern University,\n4New Jersey Institute of Technology, 5Wake Forest University,\n{mingyu.jin, chy.huang, yongfeng.zhang}@rutgers.edu , qy269@exeter.ac.uk , mengnan.du@njit.edu ,\nAbstract\nLarge language models (LLMs) have shown\nremarkable performances across a wide range\nof tasks. However, the mechanisms by which\nthese models encode tasks of varying complex-\nities remain poorly understood. In this paper,\nwe explore the hypothesis that LLMs process\nconcepts of varying complexities in different\nlayers, introducing the idea of “Concept Depth”\nto suggest that more complex concepts are\ntypically acquired in deeper layers. Specifi-\ncally, we categorize concepts based on their\nlevel of abstraction, defining them in the order\nof increasing complexity within factual, emo-\ntional, and inferential tasks. We conduct ex-\ntensive probing experiments using layer-wise\nrepresentations across various LLM families\n(Gemma, LLaMA, Qwen) on various datasets\nspanning the three domains of tasks. Our find-\nings reveal that models could efficiently con-\nduct probing for simpler tasks in shallow layers,\nand more complex tasks typically necessitate\ndeeper layers for accurate understanding. Ad-\nditionally, we examine how external factors,\nsuch as adding noise to the input and quantiz-\ning the model weights, might affect layer-wise\nrepresentations. Our findings suggest that these\nfactors can impede the development of a con-\nceptual understanding of LLMs until deeper\nlayers are explored. We hope that our pro-\nposed concept and experimental insights will\nenhance the understanding of the mechanisms\nunderlying LLMs. Our codes are available at\nhttps://github.com/Luckfort/CD.\n1\nIntroduction\nLLMs such as GPT-4 (Achiam et al., 2023) and\nLLaMA (Touvron et al., 2023) have impressive\ngeneration and reasoning capabilities (Chang et al.,\n**Mingyu, Qinkai, Jingyuan, and Qingcheng are the main\ncontributors. The order of these authors is determined by\nflipping a coin.**\nhttps://luckfort.github.io/explore_CD/ is the project\nwebsite.\n2023; Su et al., 2024a,c). It is widely accepted\nthat these models embed substantial knowledge in\ntheir parameters, with performance improving as\nthe number of parameters increases (Ju et al., 2024;\nJin et al., 2024a; Zhou et al., 2024), also known as\nemergent abilities (Wei et al., 2022a). For instance,\nGPT-3 (Brown et al., 2020) shows a large increase\nin performance after scaling up to 13B parameters,\nand a similar phenomenon was also observed for\nLaMDA (Thoppilan et al., 2022) after exceeding\n68B parameters (Wei et al., 2022b). However, it is\nnot well understood how LLMs accurately grasp\nthe concept of knowledge. In this paper, we inves-\ntigate the following research question: Can shal-\nlow layers in LLMs capture meaningful features\nof simple knowledge, while complex concepts need\ndeeper layers to capture their meaningful features?\nlike Figure 2. We hope to explore the connection\nbetween the depth of language models’ neural net-\nworks and their conceptual understanding ability\nby studying this question.\nRecent studies on understanding the reasoning\nabilities of LLMs focus on two main strategies:\nprobing representations and model pruning. Prob-\ning involves using linear classifier probes to an-\nalyze the performance of hidden layer represen-\ntations; for instance, Duan et al. (2024) exam-\nines changes in LLMs’ internal representations\nduring hallucinations, while Ju et al. (2024) in-\nvestigates the performance of different layers in\nthe LLaMA series using synthetic counterfactual\ndatasets. On the other hand, model pruning re-\nmoves redundant parameters based on their im-\nportance in seeing if performance is significantly\naffected. This method, although effective, can be\ncomplex and time-consuming. For example, Zhang\net al. (2023) uses gradient information to decide\nthe pruning components, and Gromov et al. (2024)\neven requires QLoRA fine-tuning (Dettmers et al.,\n2024) to do the pruning. Given these complexities,\nOur work primarily analyzes the representations ob-\narXiv:2404.07066v7  [cs.CL]  4 Feb 2025\n",
  "2": "Fake\nReal\nAcc:0.623\nF1:0.632\nAUC:0.689\nLayer\nLayer\nFinal Layer\n…\nAcc:0.997\nF1:0.997\nAUC:1.000\nAcc:0.597\nF1:0.598\nAUC:0.651\nLarge Language Model and Probes\nProbe Detection on Layer Representation\nQuestion 1: \nDanielle Darrieux’s \nmother tongue is \nFrench.\nQuestion 2: Nissan \nLaurel is created by \nHonda.\nLanguage Model\nText Prompt\nProbe\n…\nResult\n…\nDanielle Darrieux’s \nmother tongue is French.\nNissan Laurel is \ncreated by Honda.\nGalata is in Istanbul.\n…\nLayer\nAcc:0.718\nF1:0.683\nAUC:0.722\nFigure 1: The left figure provides an overview of our analysis process. LLMs respond to text prompts, and the\nprobing process assesses the optimal performance achievable by the current LLM layer. The right figure illustrates a\ndemonstration of layer-wise representations by probe detection. In this demonstration, orange points represent fake\nsamples, while blue points represent real samples. In this case, the probe tries to classify between two categories.\ntained through probing techniques. Building upon\nprevious work, we aim to gain a more compre-\nhensive understanding of the layer representations\nwithin LLMs.\nOur general framework is shown in Figure 1. We\ntrained independent linear probes for each layer of\nLLMs to predict the binary label, thereby determin-\ning the optimal performance achievable with the\nrepresentations of each respective layer. Drawing\nfrom our empirical findings, we propose the notion\nof “Concept Depth” as a novel metric to evaluate\nthe capability of different models in comprehend-\ning varying levels of knowledge across their layers.\nThis is the first time such a concept has been intro-\nduced in the relevant literature.\nOur empirical results ranging from 3 popu-\nlar LLMs families (Gemma (Team et al., 2023),\nLLaMA (Touvron et al., 2023), and Qwen (Bai\net al., 2023)) and 9 datasets reveal that “Concept\nDepth” is widely applicable in existing mainstream\nLLMs. Besides, we conducted comprehensive ro-\nbustness analyses, introducing random strings as\nthe noise or quantization, to further understand how\nthe reasoning of LLMs is sensitive to noise. To con-\nclude, our main contributions could be summarized\nas follows:\n• Concept Depth. We introduce the idea of “Con-\ncept Depth” to measure different layers’ abilities\nto learn different levels of concepts. We first an-\nchored the difficulty of the dataset using LLaMA-\n3-8b-Instruct (Dubey et al., 2024) and then tested\n“Concept Depth” with other models. Our results\nshow that simpler concepts are often learned at\nshallower levels, while complex concepts require\ndeeper levels to understand like Demo 2. This\nphenomenon has been observed across LLMs of\ndifferent model families and different sizes.\n• Experiments on understanding capabilities of\nLLMs. We experimented with multidimensional\ndatasets (fact, emotion, and reasoning) to ana-\nlyze variations in the conceptual depth of LLMs.\nWe observed these differences across various\ndatasets, model parameter counts, and model fam-\nilies (Gemma (Team et al., 2023), LLaMA (Tou-\nvron et al., 2023), and Qwen (Bai et al., 2023)),\nproviding a concise understanding of their impact\non LLM performance and comprehension.\n• Robustness from Concept Depth perspective.\nWe provide a new perspective on LLMs robustness.\nWe conduct ablation experiments on model weight\nquantization and add random noise to the input that\nmay affect the accuracy of LLMs inference. Details\ncan be found in Appendix A.3. The results show\nthat after adding the noise or conducting the quan-\ntization on the weights, the LLMs end up learning\nthe concepts at slower paces and deeper layers.\n2\nRelated Work\n2.1\nConcepts Representation in DNNs\nIdentifying similarities across various examples\nto form concepts, plays a crucial role in human\ndecision-making (Armstrong et al., 1983). Many\nstudies have explained DNNs’ (Deep Neural Net-\nworks) decision-making based on a conceptual per-\nspective, describing the global behavior of DNNs\nin a human-understandable way (Su et al., 2022;\nRäz, 2023; Ren et al., 2023; Deng et al., 2021;\nWen et al., 2024a; Wen, 2024; Wen et al., 2024b).\nFor example, (Yeh et al., 2019) demonstrated that\nDNNs exhibit conceptual representations through\nthe activation patterns observed in their hidden or\noutput layers. Further, Räz (2023) indicates that\nDNNs learned not only conceptual representations\nof predicted categories but also indirect concepts\n",
  "3": "Sentence 1: ‘Q: Will Queen Elizabeth be \nburied in the Pantheon?’\nSentence 2: ‘Mark's father gave him $85. \nMark bought 10 books, each of which cost \n$5. How much money does Mark have left?’\nWhich one is the math question?\nLLM\nStronger LLM\nLayer \nLayer \nLayer \nLayer \nLayer \nLayer \nFinal\nLayer \nFinal\nLayer \nLLM\nStronger LLM\nLayer \nLayer \nLayer \nLayer \nLayer \nLayer \nFinal\nLayer \nFinal\nLayer \nConcept \nEmerges\nConcept \nEmerges\nSentence: a stirring , funny and finally \ntransporting re-imagining of beauty and the \nbeast and 1930s horror films.\nIs this a positive or negative tone?\nComplex level task\nEasy level task\nText Prompt\nText Prompt\nConcept Depth\nConcept Depth\nFigure 2: The LLMs are trying to understand easy tasks and complex tasks. The more complex the task, the deeper\nlayers an LLM needs to understand. The stronger the LLM is, the more complex the task level it can learn.\nthat contribute to the prediction. A notable study\nreveals the existence of a representation bottleneck,\nhighlighting a cognitive disparity between DNNs\nand humans. This phenomenon is characterized by\nDNNs’ tendency to grasp representations of con-\ncepts that are either too simple or overly complex,\nwhile they often struggle with acquiring represen-\ntations of concepts of moderate complexity (Deng\net al., 2021). Motivated by previous work, our pa-\nper aims to cover concepts of different complexities\nto understand concepts within LLMs further.\n2.2\nKnowledge and Concepts in LLMs\nThe impressive performance of the LLMs in var-\nious downstream tasks (e.g.\nLLMs can pre-\ndict factual statements about the world based on\nprompts (Meng et al., 2022)) has led to a great\ndiscussion about whether these capabilities are\n‘stochastic parrots’ or LLMs understands these con-\ncepts. Pioneeringly, Gurnee and Tegmark (2023)\nshowed that LLMs internally store concepts like\nlatitude, longitude, and time. Similarly, another\nwork showed that the internal states of LLMs can\ndetect the truth of a statement (Azaria and Mitchell,\n2023; Su et al., 2024b). Geva et al. (2023) also\ncame to similar conclusions by artificially blocking\nor “knocking out\" specific parts of the LLMs to ob-\nserve their effects on the inference process. These\nrelated studies show the existence of structures for\nunderstanding concepts within LLMs, motivating\nus to explore how concepts at various complexities\nare encoded within various depths of LLMs.\n2.3\nExplorations of Interpretability in LLMs\nMany related studies have deconstructed the inner\nlayers of LLMs from various perspectives to under-\nstand the mechanisms inside such models (Zhao\net al., 2024; Jin et al., 2024b). Fan et al. (2024)\ncomputes stopping signals by evaluating key fea-\ntures to early stop the LLM inference and get\nthe internal performance of the LLMs, conclud-\ning that not all layers are necessary.\nThrough\npruning the LLMs, Gromov et al. (2024) found\nthat the parameters of some layers were not uti-\nlized correctly. Men et al. (2024) also shows a\nhigh level of redundancy in the LLMs’ architec-\nture. Probes trained with logistic regression are a\nwell-established method (Alain and Bengio, 2016)\nthat has been applied in classifying the truthful-\nness of LLMs and has been validated in many\nstudies (Marks and Tegmark, 2023; Azaria and\nMitchell, 2023; Li et al., 2024). The latest work\ndetects different layers in the Llama series respond-\ning to facts or counterfactuals by probing tech-\nniques (Ju et al., 2024). Inspired by these works,\nwe propose Concept Depth to summarize these\nphenomena. Our work focuses on the Concept\nDepth that appears in the LLMs, analyzing it exper-\nimentally by training linear classifier probes, which\nmakes our work different from others.\n3\nAnalyzing Method\nIn this paper, we design a probing framework to\nunderstand how concepts at various levels are en-\ncoded within LLMs and investigate whether the\n",
  "4": "internal representations are robust to concepts. For\ninstance, Figure 1 demonstrates the representation\nproject of the Counterfact dataset.\n3.1\nLinear Classifier Probing\nProbe technology (Alain and Bengio, 2016) is a\nmethod for analyzing and evaluating the internal\nrepresentations of a neural network by applying\na specific probe task, typically a classification or\nregression task, to a particular layer of the model.\nThis technique measures the layer’s ability to repre-\nsent information for the given task, thereby reveal-\ning the features and information captured by dif-\nferent layers of the model. Our approach involves\nextracting the representations from each layer of\nthe large model, training a binary classifier on these\nrepresentations, and validating its accuracy.\nFor one specific task w that contains n questions,\nthe hidden feature set in LLMs is x ∈Rn×dmodel,\nwhere n denotes number of samples, and x(i) ∈\nR1×dmodel represent the representation at a certain\nlayer, where dmodel donate the dimension for the\nhidden layer representation. Binary label y(i) is\nset as 0 or 1. The objective function of such a\nbinary Logistic regression classifier probes with L2\nregularization can be written as:\nJ(θ) = −1\nn\nn\nX\ni=1\nCost(σ(x(i)), y(i)) + λ\n2n\nm\nX\nj=1\nθ2\nj\n(1)\nCost(σ(x(i)), y(i)) = y(i) log\n\u0010\nσ(θT x(i))\n\u0011\n+(1 −y(i)) log\n\u0010\n1 −σ(θT x(i))\n\u0011\n(2)\nwhere θ is the parameter in this Logistic regression\nmodel, λ is the regularization parameter. The lin-\near model predicts LLM’s response to the test set,\ncompared with the true label. This yields a quan-\ntification of how well LLMs understand the current\ndepth. If the binary model gets good accuracy at a\ncertain layer, that means the LLM can distinguish\ntrue or false in this layer.\n4\nExperimental Setting\nOur experiments used nine datasets containing\nthree aspects (emotion understanding, reasoning,\nand fact-checking).\nWe categorized these nine\ndatasets from easy to complex levels according to\nthe performance of LLaMA3-8B-Instruct (Dubey\net al., 2024), GPT-4o-mini (OpenAI, 2024), and\nQWen2-7B-Instruct (Yang et al., 2024) on each\ndataset (see Section 4.2.1) to anchor the difficulty\nof the datasets. Specifically, the datasets in which\nthe linear probes can obtain high classification accu-\nracy at the initial or middle depth of the LLMs are\ncategorized as easy levels. Other datasets where lin-\near probes can only accurately classify at a deeper\nlayer of the model or even fail to classify accurately\nare categorized as complex levels. The average ac-\ncuracy of these datasets on the three models was\nconsistent with the probe results and had a signifi-\ncant correlation. In Section 4.1, we introduce the\nLLMs used for experiments. The nine datasets are\ndescribed in Section 4.2.\n4.1\nModels\nIn this paper, we employ three open-source model\nfamilies: Gemma (2B, 7B) (Team et al., 2024),\nLLaMA-2 (7B, 13B) (Touvron et al., 2023), and\nQwen (0.5B, 1.8B, 4B, 7B, and 14B) (Bai et al.,\n2023) to support our analysis. Table 1 shows the\nnumber of layers in each model. We choose a lin-\near classifier probe for the experiments during the\nprobing analysis. The ratio of the training set to\nthe test set is 8:2, following the usual approach\nof LLMs probing classifier (Duan et al., 2024; Pal\net al., 2023). We extract feature representations\nfrom the final layer in the transformer at each layer\nof LLMs(e.g. 14-th ’post_attention_layernorm’ in\nLlama2-7B (32 Layers in total)) as input to the\nprobing classifier. The other series of models fol-\nlow a similar processing pattern.\nModel\nLayer\nModel\nLayer\nModel\nLayer\nGemma-2B\n18\nQwen-4B\n40\nLLaMA-7B\n32\nQwen-0.5B\n24\nGemma-7B\n28\nQwen-14B\n40\nQwen-1.8B\n24\nQwen-7B\n32\nLLaMA-13B\n40\nTable 1: Number of layers in each LLM.\n4.2\nDatasets\nTable 4 presents the nine datasets we use, on Fact\n(Cities (Marks and Tegmark, 2023), Common-\nClaim (Casper et al., 2023), Counterfact (Meng\net al., 2022)), Emotion (STSA (Kim, 2014),\nIMDb (Maas et al., 2011), Sarcasm (Misra and\nArora, 2023), HateEval (Manolescu et al., 2019)),\nand Reasoning (StrategyQA (Geva et al., 2021),\nCoinflip (Wei et al., 2022b)) for our experiments.\nA detailed description of the dataset can be found\nin the Appendix A.2.\n4.2.1\nAnchoring Difficulties of Each Dataset\nTo ascertain the learning difficulty of each dataset,\nwe have utilized the LLaMA3-8B-Instruct (Dubey\net al., 2024), GPT-4o-mini (OpenAI, 2024), and\nQWen2-7B-Instruct model (Yang et al., 2024).\nOur approach involves testing each sample in the\n",
  "5": "0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nGemma-2B\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nGemma-7B\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nLLaMA-7B\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nLLaMA-13B\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nQwen-0.5B\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nQwen-1.8B\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nQwen-4B\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nQwen-7B\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nQwen-14B\nCities\nCommon\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nStrategyQA\nCoinflip\nFigure 3: Analysis diagrams of Section 5.1. Linear probing accuracy of three LLM families (Gemma, LLaMA,\nQwen) on nine datasets.\ndatasets as a binary classification problem via a\nprompting way. The model generates a response\nfor each sample, from which we infer a judgment,\ncategorizing it as either \"Yes\" or \"No\". By com-\nparing these judgments with the actual labels, we\ncompute the accuracy for each dataset.\nTable 2 presents the results of this analysis. The\ndataset with the highest accuracy is deemed the\neasiest dataset to classify. Conversely, the dataset\nwith the lowest accuracy is considered the most\ndifficult to classify. This method quantitatively\nmeasures the learning difficulty associated with\neach dataset.\n4.3\nMetrics for Accuracy Variation\nDefinition 1 (Variation Rate) Given an LLM prob-\ning classifier M = {q, y, z, d} (q, y, z, and d are\nthe input question, ground truth binary label, pre-\ndicted label and total amount of layers, respec-\ntively), it has the accuracy αi at i-th layer:\nαi = 1\n|z| ∗\n|z|\nX\nk=1\n[yk = zk], i ∈{0, 1, 2, ..., d −1}\nDataset\nAccuracy\nCoinflip\n0.5920\nCommon\n0.6487\nSarcasm\n0.6597\nStrategyQA\n0.6969\nCounterfact\n0.7126\nHateEval\n0.7640\nSTSA\n0.9116\nCities\n0.9204\nIMDb\n0.9380\nTable 2: Average accuracy on nine datasets based on\nLLaMA3-8b-Instruct, GPT-4o-mini and QWen2-7B-\nInstruct. Accuracy based on each model is shown in\nTable 5.\nWe denote the variation rate βi where\nβi = αi/αi−1, i ∈{1, 2, ..., d −1}\nWe introduce two metrics to capture variations in\naccuracy: the jumping point and the converging\npoint and define them by the given definition of\nvariation rate.\nDefinition 2 (Jumping point) We denote the\n",
  "6": "jumping point\nJ(M, D) = min{ i\nd}\ns.t. βi >= 1.1, i ∈{1, 2, ..., d −1}\nwhere M and D = (q, y) are the LLM classifier\nand the dataset.\nWhen a noticeable boost in accuracy is observed,\nthe jumping point signals the model’s recognition\nof a dataset’s critical patterns.\nDefinition 3 (Converging Point) We denote the\nconverging point\nC(M, D) = max{ i\nd}\ns.t.|βi −1| < 0.03, i ∈{1, 2, ..., d −1}\nwhere M and D = (q, y) are the LLM classifier\nand the dataset.\nAs the accuracy plateaus or starts declining, the\nconverging point indicates the model’s learning sat-\nuration or peak learning capacity from the dataset.\nAnalyzing these metrics offers deeper insight into\nthe model’s learning dynamics and adaptability to\nvarious data types.\n5\nExperimental Analysis\nWe conduct experiments to answer the following\nresearch questions about the Concept Depth:\nRQ1: Do different LLMs’ Concept Depths be-\nhave consistently in the same dataset? (Section\n5.1)\nRQ2: Do different size LLMs in the same family\n(e.g., the LLaMA family) have consistent Concept\nDepth? (Section 5.2)\nRQ3: Do LLMs’ Concept Depth of the same\nsize behave consistently? (Section 5.3)\n5.1\nComparison Among the Datasets\nWe delve into an evaluative performance compar-\nison across a range of datasets, utilizing Figure 3\nto detail the layer-wise accuracy of all nine LLMs\nover nine distinct datasets. Table 3 shows the de-\ntailed numerical results for Figure 3, as well as\nthe F1-score and AUC. A performance threshold\nof 0.7 accuracy is applied to assess the models’\neffective comprehension of concepts. This exam-\nination leads to two general observations. Firstly,\nregarding different concepts, LLMs exhibit varying\naccuracy trends across their layers. For example,\nCities approaches perfect accuracy fast; in con-\ntrast, datasets requiring high-level reasoning such\nas StrategyQA will not reliably converge to ac-\ncuracy above 0.7, indicating that they have differ-\nent “Concept Depth”. Within individual concepts,\nhowever, different LLMs tend to display consistent\naccuracy patterns across these layers. Secondly,\nin tasks that require varying levels of conceptual\nunderstanding, the LLMs demonstrate their under-\nstanding across different layers, indicating a lay-\nered approach to processing complex concepts.\nSignificant variations in trends are observed\nacross the models among the three factual concept\ndatasets. Cities exhibits a sharp increase in compre-\nhension at lower layers, stabilizing in higher layers,\nindicating a strong grasp of the concept. Com-\nmonClaim has become stable in early layers. Be-\nsides, the accuracy improvement of the nine LLMs\ntrained on Counterfact was relatively difficult to\nachieve, utilizing deeper layers, and the accuracy\nwas lower than that of many other datasets. There-\nfore, we can conclude that Counterfact is more\ncomplex.\nIn datasets centered on emotional concept com-\nprehension (STSA, IMDb, Sarcasm, and HateE-\nval), despite varying levels of understanding, all\nmodels demonstrate a rise in accuracy at the initial\nlayers, with convergence occurring in the intermedi-\nate layers. Although HateEval essentially reaches\nstable at the initial layers, its accuracy reaches up to\n0.8, suggesting that LLMs primarily aggregate rep-\nresentations from lower layers to grasp emotional\nconcepts. Meanwhile, StrategyQA and Coinflip,\nwhich demand specific reasoning skills, tend to dis-\nplay a bell-shaped accuracy trajectory in all models,\nwith peak accuracy observed in the middle layers.\nSuch patterns underscore the intricate complexity\nassociated with reasoning tasks.\nRemark 1\nWe categorize the performances into three\ntypes. 1) For Cities, STSA, IMDb, and Sar-\ncasm, the LLMs suddenly understand the\ntasks at intermediate layers. 2) For Com-\nmonClaim and HateEval, the LLMs have\nalready understood the tasks in shallower\nlayers. 3) For Counterfact, StrategyQA,\nand Coinflip, The tasks are more difficult\nto understand compared with others. There-\nfore, we consider the tasks in type 1 and 2\neasy tasks, and those in type 3 are complex.\n",
  "7": "Cities\nCommon\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nStrategyQA\nCoinflip\n0\n20\n40\n60\n80\n100\nConverge Proportion (%) of Gemma\nGemma-2B\nGemma-7B\nCities\nCommon\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nStrategyQA\nCoinflip\n0\n20\n40\n60\n80\n100\nConverge Proportion (%) of LLaMA\nLLaMA-7B\nLLaMA-13B\nCities\nCommon\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nStrategyQA\nCoinflip\n0\n20\n40\n60\n80\n100\nConverge Proportion (%) of Qwen\nQwen-0.5B\nQwen-1.8B\nQwen-4B\nQwen-7B\nQwen-14B\n(a) The converging point of each dataset on Gemma, LLaMA, and Qwen represented by the percent depth proportion.\nCities\nCommon\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nStrategyQA\nCoinflip\n0\n20\n40\n60\n80\n100\nPeak Accuracy (%) of Gemma\nGemma-2B\nGemma-7B\nCities\nCommon\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nStrategyQA\nCoinflip\n0\n20\n40\n60\n80\n100\nPeak Accuracy (%) of LLaMA\nLLaMA-7B\nLLaMA-13B\nCities\nCommon\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nStrategyQA\nCoinflip\n0\n20\n40\n60\n80\n100\nPeak Accuracy (%) of Qwen\nQwen-0.5B\nQwen-1.8B\nQwen-4B\nQwen-7B\nQwen-14B\n(b) The peak accuracy of each dataset on Gemma, LLaMA, and Qwen represented by the percent depth proportion.\nFigure 4: Analysis diagrams of Section 5.2. The converge proportion and peak accuracy of each model over the\nnine datasets. (a) shows the converged proportion over the datasets. (b) shows the peak accuracy over the datasets.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.6\n0.8\n1.0\nAccuracy\nStrategyQA\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.6\n0.8\n1.0\nAccuracy\nCoinflip\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.6\n0.8\n1.0\nAccuracy\nCities\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.6\n0.8\n1.0\nAccuracy\nCommon\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.6\n0.8\n1.0\nAccuracy\nCounterfact\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.6\n0.8\n1.0\nAccuracy\nHateEval\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.6\n0.8\n1.0\nAccuracy\nSTSA\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.6\n0.8\n1.0\nAccuracy\nIMDb\nGemma-7B\nLLaMA-7B\nQwen-7B\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.4\n0.6\n0.8\n1.0\nAccuracy\nSarcasm\nFigure 5: Analysis diagrams of Section 5.3. Linear probing accuracy of Gemma-7B, LLaMA-7B, Qwen-7B on nine\ndatasets.\n5.2\nComparison Among the Number of\nParameters\nThis section offers a comparative analysis of LLMs\nwithin their respective families, examining both ac-\ncuracy levels and converging points across the mod-\nels. Figure 4 reveals two recurring patterns within\nthese families: for tasks with accuracy improves\ndramatically by model learning, larger models tend\nto show converging points at earlier layers, suggest-\n",
  "8": "ing they achieve their own peak comprehensions\nof concepts at lower layers; for tasks with accu-\nracy changes little, all LLMs show the converging\npoints at early layers.\nTwo notable exceptions to this trend appear in\nthe Qwen family over the Coinflip and IMDb\ndatasets. For Coinflip, larger models exhibit de-\nlayed convergence. This deviation underscores the\ncomplexity of the reasoning required, illustrating\nhow this task challenges even the larger models\nto extend their depth of understanding further. For\nIMDb, converging points fluctuate with the increas-\ning size of the model because the number of layers\nis different among different sizes of LLMs, which\namplifies the differences. These exceptions are also\nfound in the Gemma family.\nFurthermore, in Figure 4, we explore the peak\naccuracy levels across all layers for LLMs of dif-\nfering sizes. The overarching trend indicates that\nlarger models consistently achieve superior peak\nperformance. This observation not only supports\nthat scaling up models enhances their effectiveness\nbut also suggests that larger models develop more\nrobust internal representations, validating the bene-\nfits of training models with greater capacity.\nRemark 2\nWe have two observations by comparing dif-\nferent sizes of models from the same LLM\nfamily. 1) As the number of parameters in-\ncreases, peak accuracy gradually increases,\nand the converging point gradually advances.\n2) Larger models grasp the concepts earlier\nand better.\n5.3\nComparison Among the LLM Families\nWe examine how LLMs from various families, pos-\nsessing a similar parameter count, process concepts\nas reflected by their converging points and peak ac-\ncuracies. The overarching trends are highlighted in\nFigure 6, with detailed statistics on a layer-by-layer\nbasis provided in Figure 5. Our findings reveal that\nwhile LLMs across different families may reach\nnearly identical peak accuracies, the layers at which\nthey converge to these peaks can vary. For instance,\nin the HateEval and Counterfact datasets, we ob-\nserve models converging at significantly deeper\nlayers. This variation suggests that despite simi-\nlar parameter scales, different models may employ\nvaried mechanisms to tackle the same problems,\nreflecting the diversity in how models interpret and\nCities\nCommon\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nStrategyQA\nCoinflip\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nGemma-7B\nLLaMA-7B\nQwen-7B\nCities\nCommon\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nStrategyQA\nCoinflip\n0.7\n0.8\n0.9\n1.0\nFigure 6: The upper radar image is the converging point\nof each dataset on Gemma-7B, LLaMA-7B, and Qwen-\n7B, represented by the percent depth proportion. The\nbottom radar image is the maximum accuracy of each\ndataset on Gemma-7B, LLaMA-7B, and Qwen-7B, rep-\nresented by the percentage depth proportion.\nprocess complex information.\nRemark 3\nWith the same number of model parameters,\nthe models generally have a comparable un-\nderstanding of the datasets.\n5.4\nAblation Study\nTo quantify the robustness of the LLMs concerning\ntheir internal representation, we conducted ablation\nstudies on noise perturbation and bit quantization.\nThe result shows that adding noises or reducing\nmodel weights to 8 bits can make the accuracy\nconverge slower. Compressing the LLMs to 16 bits\ndoesn’t harm the understanding process too much.\nDetails can be found in Section A.3.\n6\nConclusions\nThis paper proposes Concept Depth, the phe-\nnomenon that different concepts are learned in dif-\nferent layers of LLMs, i.e., more difficult concepts\nare fully acquired with deeper layers. We con-\nducted several experiments around Concept Depth\nusing probe techniques. Our research suggests that\nLLMs tend to effectively categorize easy tasks, in-\ndicating that these concepts are learned in the first\n",
  "9": "few initial layers. In contrast, complex tasks may\nonly be recognizable (if at all) in deeper layers,\nand LLMs of the same size perform largely con-\nsistently across datasets regarding Concept Depth.\nCompressing the model weight to 16-bit representa-\ntions for future LLMs’ designs is also a promising\nmethod for saving computation memory.\n7\nLimitations\nThe paper presents several opportunities for further\nexploration. Firstly, the datasets employed might\nnot encompass the full spectrum of language tasks,\noffering a chance to expand the scope of the find-\nings in a multilingual environment. Secondly, We\ndid not experiment with very large open-source\nlanguage models, thus allowing future researchers\nto investigate how scaling up the model size af-\nfects concept acquisition across different layers\nand enhances robustness. Moreover, we should\nalso try different kinds of classifiers, including non-\nlinear models and neural network-based classifiers,\nto acquire more profound insights into how LLM\nrepresentations differ across layers. These aspects\nhighlight promising directions for continued ad-\nvancement in the field. We will continue to explore\nintermediate representations to help us better un-\nderstand the inner side of LLMs, as this challenge\nmay also be open to other researchers in this field.\nAcknowledgement\nWe thank Taowen Wang, Fei Sun, Wujiang Xu, and\nGuangyan Sun for their valuable discussions and\nsuggestions during the project.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nGuillaume Alain and Yoshua Bengio. 2016. Under-\nstanding intermediate layers using linear classifier\nprobes. arXiv preprint arXiv:1610.01644.\nSharon Lee Armstrong, Lila R Gleitman, and Henry\nGleitman. 1983. What some concepts might not be.\nCognition, 13(3):263–308.\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an llm knows when its lying. arXiv preprint\narXiv:2304.13734.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,\nRunji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,\nKeming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,\nXuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong\nTu, Peng Wang, Shijie Wang, Wei Wang, Shengguang\nWu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian\nYang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi\nYuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,\nYichang Zhang, Zhenru Zhang, Chang Zhou, Jin-\ngren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.\nQwen technical report. Preprint, arXiv:2309.16609.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nStephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and\nDylan Hadfield-Menell. 2023. Explore, establish,\nexploit: Red teaming language models from scratch.\nPreprint, arXiv:2306.09442.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,\nYi Chang, Philip S. Yu, Qiang Yang, and Xing Xie.\n2023. A survey on evaluation of large language mod-\nels. Preprint, arXiv:2307.03109.\nHuiqi Deng, Qihan Ren, Hao Zhang, and Quanshi\nZhang. 2021. Discovering and explaining the rep-\nresentation bottleneck of dnns.\narXiv preprint\narXiv:2111.06236.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2024. Qlora: Efficient finetuning\nof quantized llms. Advances in Neural Information\nProcessing Systems, 36.\nHanyu Duan, Yi Yang, and Kar Yan Tam. 2024. Do\nllms know about hallucination?\nan empirical in-\nvestigation of llm’s hidden states. arXiv preprint\narXiv:2402.09733.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nSiqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng\nHan, Shuo Shang, Aixin Sun, Yequan Wang, and\nZhongyuan Wang. 2024.\nNot all layers of llms\nare necessary during inference.\narXiv preprint\narXiv:2403.02181.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual asso-\nciations in auto-regressive language models. arXiv\npreprint arXiv:2304.14767.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did Aristotle\nUse a Laptop? A Question Answering Benchmark\n",
  "10": "with Implicit Reasoning Strategies. Transactions of\nthe Association for Computational Linguistics, 9:346–\n361.\nAndrey Gromov, Kushal Tirumala, Hassan Shapourian,\nPaolo Glorioso, and Daniel A Roberts. 2024. The un-\nreasonable ineffectiveness of the deeper layers. arXiv\npreprint arXiv:2403.17887.\nWes Gurnee and Max Tegmark. 2023.\nLanguage\nmodels represent space and time. arXiv preprint\narXiv:2310.02207.\nMingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang,\nWenyue Hua, Ruixiang Tang, William Yang Wang,\nand Yongfeng Zhang. 2024a. Disentangling mem-\nory and reasoning ability in large language models.\narXiv preprint arXiv:2411.13504.\nMingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao,\nWenyue Hua, Yanda Meng, Yongfeng Zhang, and\nMengnan Du. 2024b. The impact of reasoning step\nlength on large language models.\narXiv preprint\narXiv:2401.04925.\nTianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan,\nZhaochun Ren, and Gongshen Liu. 2024.\nHow\nlarge language models encode context knowl-\nedge? a layer-wise probing study. arXiv preprint\narXiv:2402.16061.\nYoon Kim. 2014.\nConvolutional neural networks\nfor sentence classification. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Linguis-\ntics.\nKenneth Li, Oam Patel, Fernanda Viégas, Hanspeter\nPfister, and Martin Wattenberg. 2024.\nInference-\ntime intervention: Eliciting truthful answers from\na language model. Advances in Neural Information\nProcessing Systems, 36.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y. Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nMihai Manolescu, Denise Löfflad, Adham Nasser Mo-\nhamed Saber, and Masoumeh Moradipour Tari. 2019.\nTuEval at SemEval-2019 task 5: LSTM approach\nto hate speech detection in English and Spanish. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation, pages 498–502, Minneapolis,\nMinnesota, USA. Association for Computational Lin-\nguistics.\nSamuel Marks and Max Tegmark. 2023. The geometry\nof truth: Emergent linear structure in large language\nmodel representations of true/false datasets. arXiv\npreprint arXiv:2310.06824.\nXin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang,\nHongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng\nChen. 2024.\nShortgpt: Layers in large language\nmodels are more redundant than you expect. arXiv\npreprint arXiv:2403.03853.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associ-\nations in gpt. Advances in Neural Information Pro-\ncessing Systems, 35:17359–17372.\nRishabh Misra and Prahal Arora. 2023. Sarcasm detec-\ntion using news headlines dataset. AI Open, 4:13–18.\nOpenAI. 2024. Hello gpt-4o. OpenAI Blog.\nKoyena Pal, Jiuding Sun, Andrew Yuan, Byron C Wal-\nlace, and David Bau. 2023. Future lens: Anticipating\nsubsequent tokens from a single hidden state. arXiv\npreprint arXiv:2311.04897.\nTim Räz. 2023. Methods for identifying emergent con-\ncepts in deep neural networks. Patterns, 4(6).\nJie Ren, Mingjie Li, Qirui Chen, Huiqi Deng, and Quan-\nshi Zhang. 2023. Defining and quantifying the emer-\ngence of sparse concepts in dnns. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 20280–20289.\nZhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xi-\naoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, et al.\n2024a. Living in the moment: Can large language\nmodels grasp co-temporal reasoning? arXiv preprint\narXiv:2406.09072.\nZhaochen Su, Zecheng Tang, Xinyan Guan, Juntao Li,\nLijun Wu, and Min Zhang. 2022. Improving tem-\nporal generalization of pre-trained language mod-\nels with lexical semantic change.\narXiv preprint\narXiv:2210.17127.\nZhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu\nLi, Jiashuo Sun, Juntao Li, Min Zhang, and Yu Cheng.\n2024b. Conflictbank: A benchmark for evaluating\nthe influence of knowledge conflicts in llm. In Ad-\nvances in neural information processing systems.\nZhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao\nLi, Min Zhang, and Yu Cheng. 2024c. Timo: To-\nwards better temporal reasoning for language models.\narXiv preprint arXiv:2406.14192.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai,\nAnja Hauth, et al. 2023.\nGemini: a family of\nhighly capable multimodal models. arXiv preprint\narXiv:2312.11805.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivière, Mihir Sanjay\nKale, Juliette Love, Pouya Tafti, Léonard Hussenot,\nAakanksha Chowdhery, Adam Roberts, Aditya\nBarua, Alex Botev, Alex Castro-Ros, Ambrose Slone,\n",
  "11": "Amélie Héliou, Andrea Tacchetti, Anna Bulanova,\nAntonia Paterson, Beth Tsai, Bobak Shahriari, Char-\nline Le Lan, Christopher A. Choquette-Choo, Clé-\nment Crepy, Daniel Cer, Daphne Ippolito, David\nReid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng\nYan, George Tucker, George-Christian Muraru, Grig-\nory Rozhdestvenskiy, Henryk Michalewski, Ian Ten-\nney, Ivan Grishchenko, Jacob Austin, James Keel-\ning, Jane Labanowski, Jean-Baptiste Lespiau, Jeff\nStanway, Jenny Brennan, Jeremy Chen, Johan Fer-\nret, Justin Chiu, Justin Mao-Jones, Katherine Lee,\nKathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa\nLee, Lucas Dixon, Machel Reid, Maciej Mikuła,\nMateo Wirth, Michael Sharman, Nikolai Chinaev,\nNithum Thain, Olivier Bachem, Oscar Chang, Oscar\nWahltinez, Paige Bailey, Paul Michel, Petko Yotov,\nPier Giuseppe Sessa, Rahma Chaabouni, Ramona\nComanescu, Reena Jana, Rohan Anil, Ross McIl-\nroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Se-\nbastian Borgeaud, Sertan Girgin, Sholto Douglas,\nShree Pandya, Siamak Shakeri, Soham De, Ted Kli-\nmenko, Tom Hennigan, Vlad Feinberg, Wojciech\nStokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao\nGong, Tris Warkentin, Ludovic Peran, Minh Giang,\nClément Farabet, Oriol Vinyals, Jeff Dean, Koray\nKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,\nDouglas Eck, Joelle Barral, Fernando Pereira, Eli\nCollins, Armand Joulin, Noah Fiedel, Evan Senter,\nAlek Andreev, and Kathleen Kenealy. 2024. Gemma:\nOpen models based on gemini research and technol-\nogy. Preprint, arXiv:2403.08295.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nXiming Wen. 2024.\nLanguage model meets proto-\ntypes: Towards interpretable text classification mod-\nels through prototypical networks. arXiv preprint\narXiv:2412.03761.\nXiming Wen, Wenjuan Tan, and Rosina O Weber. 2024a.\nGaprotonet: A multi-head graph attention-based pro-\ntotypical network for interpretable text classification.\narXiv preprint arXiv:2409.13312.\nXiming Wen, Rosina O Weber, Anik Sen, Darryl Han-\nnan, Steven C Nesbit, Vincent Chan, Alberto Goffi,\nMichael Morris, John C Hunninghake, Nicholas E\nVillalobos, et al. 2024b.\nThe impact of an xai-\naugmented approach on binary classification with\nscarce data. arXiv preprint arXiv:2407.06206.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nChih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang\nLi, Pradeep Ravikumar, and Tomas Pfister. 2019. On\nconcept-based explanations in deep neural networks.\nMingyang Zhang, Chunhua Shen, Zhen Yang, Linlin\nOu, Xinyi Yu, Bohan Zhuang, et al. 2023. Prun-\ning meets low-rank parameter-efficient fine-tuning.\narXiv preprint arXiv:2305.18403.\nHaiyan Zhao, Fan Yang, Himabindu Lakkaraju, and\nMengnan Du. 2024. Opening the black box of large\nlanguage models: Two views on holistic interpretabil-\nity. arXiv preprint arXiv:2402.10688.\nZihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan\nYe, Wei Liu, Wei Wang, Xiaowei Huang, and Kaizhu\nHuang. 2024. Mathattack: Attacking large language\nmodels towards math solving ability. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 38, pages 19750–19758.\n",
  "12": "A\nAppendix\nHere, we provide our supplementary materials.\nA.1\nMetrics for Parts of the Layers\nTable 3 shows the experimental results for the accu-\nracy, F1-score, and AUC metrics of parts of the first,\n25% depth, 50% depth, 67% depth, 83% depth, and\nthe last layer of each model over the nine datasets.\nA.2\nDescription of the dataset\nCities (Marks and Tegmark, 2023): consists of\nstatements about the location of cities and their\nveracity labels (e.g., The city of Zagreb is in Japan,\nwhich is wrong). We use 1496 of these samples.\nCommonClaim (Casper et al., 2023): A dataset of\nboolean statements, each labeled by two humans\nas common-knowledge-true, common-knowledge-\nfalse, or neither. We use 6000 of these samples.\nCounterfact (Meng et al., 2022): Counterfact in-\ncludes myriad counterfactuals that allows quantita-\ntive testing of specificity and generalization when\nlearning a counterfactual. We use 4000 of these\nsamples.\nHateEval (Manolescu et al., 2019): HateEval has\nEnglish tweets which were annotated hierarchically.\nWe use 6000 of these samples.\nSTSA (Kim, 2014): STSA includes movie reviews,\nhalf of which were considered positive and the\nother half negative. Each label is extracted from a\nlonger movie review and reflects the writer’s over-\nall intention for this review. We use 6920 of these\nsamples.\nIMDb (Maas et al., 2011): IMDb is a benchmark\ndataset for binary sentiment classification. We use\n2000 of these samples.\nSarcasm (Misra and Arora, 2023): Sarcasm is a\nsuperior news headline dataset that tells if the head-\nlines are sarcastic. We use 6000 of these samples.\nStrategyQA (Geva et al., 2021): StrategyQA con-\ntains questions across all knowledge domains to\nelicit creative and diverse yes/no questions that re-\nquire implicit reasoning steps. We use 2290 of\nthese samples.\nCoinflip (Wei et al., 2022b): Coinflip includes coin\nflipping queries, asking if a coin remains heads up\nafter it is either flipped or left unflipped by individ-\nuals. We use 500 of these samples.\nA.3\nAblation Study\nAdding Noise.\nTo quantify the robustness of\nthe LLMs concerning their internal representation,\nwhen we input these questions into the LLM, we\nadd a random string of noise in front of the question\nto interfere. For instance, we perturbate a question\nq into q′ = N ⊕q. Here, ⊕is the concatenate oper-\nation, and the noise string N ∈{S1, S2} disrupts\nthe classification tasks, satisfying\nP(N = S1) = P(N = S2) = 50%\nThe probability distribution of N is unrelated to\ntheir labels. The following is an example of STSA\nwith a positive label.\nBefore adding noise:\nThe\nproduction\nvalues\nare\nof\nthe\nhighest\nand\nthe\nperformances\nattractive without being memorable.\nThe sentence above is a movie review\nand\nreflects\nthe\nwriter’s\noverall\nintention for this review. According\nto the sentence, judge whether the\nemotion is Positive or Negative.\nAfter adding noise:\naaa The\nproduction\nvalues\nare\nof\nthe\nhighest\nand\nthe\nperformances\nattractive without being memorable.\nThe sentence above is a movie review\nand\nreflects\nthe\nwriter’s\noverall\nintention for this review. According\nto the sentence, judge whether the\nemotion is Positive or Negative.\naaa\ncan\nbe\nsubstituted\nby\nbbb,\nboth\ncases have equal probability.\nQuantization Settings. Quantization has been a\nrule of thumb for faster LLMs’ inference. The\ngeneral fact is that using a lower precision level\nallows LLMs to be run on less capable hardware\nwith an acceptable reduction in their ability and\naccuracy. Our approach to applying quantization is\nto explore its effect on Concept Depth, whether it\nstill maintains the previous Concept Depth or slows\ndown the process of understanding knowledge. We\nquantize the model weights to 8, 16, and 32 bits.\nResults. Figure 7 illustrates the effect of adding\nnoise and reducing bit representations to the\nGemma-2B model. The addition of noise causes\nthe learning curve to shift to the right, indicating a\nreduction in the converging speed. This suggests\nthat the presence of noise in the input data can hin-\nder the LLM’s learning, slowing down its ability to\n",
  "13": "0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.7\n0.8\n0.9\nAccuracy\nSTSA\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nProportion (%)\n0.7\n0.8\n0.9\nAccuracy\nSarcasm\nGemma-2B 8-bit\nGemma-2B 16-bit\nGemma-2B Full\nGemma-2B Noise 8-bit\nGemma-2B Noise 16-bit\nGemma-2B Noise Full\nFigure 7: Noise and quantization evaluation on STSA\nand Sarcasm of Gemma-2B.\nconverge to an optimal solution. We also see that\nthere isn’t much difference between 32 and 16 bits,\nand the convergence rate slows when we reduce the\nmodel to 8 bits. Therefore, we may compress the\nLLMs to 16 bits for future designs.\nRemark 4\nNoises or 8-bit-quantization can cause the\naccuracy to converge more slowly. Com-\npressing the LLMs to 16 bits doesn’t harm\nthe understanding process too much. The\nlayer-wise representations of LLMs are sus-\nceptible to noise and high-ratio quantization.\nTherefore, it is crucial to proceed cautiously\nwhen conducting high-ratio quantization in-\nference.\nA.4\nExamples of Each Dataset\nFor the 9 bi-classification tasks, we provide two\nexamples from each class for each task. The blue\nsentences are the data samples, while the black\nsentences are the prompts.\nCities\n“True” example:\nJudge the statement is True or False. The\ncity of Tokyo is in Japan.\n“False” example:\nJudge the statement is True or False. The\ncity of Lodz is in the Dominican Republic.\nCommonClaim\n“True” example:\nJudge\nthe\nstatement\nis\nTrue\nor\nFalse.\nSalmon\nwill\noften\nreturn\nto\nthe\nsame\nspawning ground where then were born.\n“False” example:\nJudge the statement is True or False. A\nchicken has two right wings.\nCounterfact\n“True” example:\nJudge the statement is True or False. The\ncity of Tokyo is in Japan.\n“False” example:\nJudge\nthe\nstatement\nis\nTrue\nor\nFalse.\nKanata South Ward is in Wisconsin.\nHateEval\n“Yes” example:\nHere it is not about Refugees or Illegal\nimmigrants. It is about whether one has\ndocuments before 1971 or not.\nNow, it\nis difficult for slum people and beggars\nto show valid documents, except the name\nin voter list. According to the comment,\ntell whether they present hate speech or\nnot.\n“No” example:\nLabor\nmigrants\ntransfer\nalmost\n$10\nbillion a year to Ukraine. According to\nthe comment, tell whether they present\nhate speech or not.\nSTSA\n“Positive” example:\nThe production values are of the highest\nand the performances attractive without\nbeing\nmemorable.\nThe\nsentence\nabove\nis\na\nmovie\nreview\nand\nreflects\nthe\nwriter’s\noverall\nintention\nfor\nthis\nreview.\nAccording\nto\nthe\nsentence,\njudge whether the emotion is Positive or\nNegative.\n“Negative” example:\nLess\na\nstory\nthan\nan\ninexplicable\nnightmare, right down to the population’s\nshrugging acceptance to each new horror.\nThe sentence above is a movie review and\nreflects the writer’s overall intention\nfor\nthis\nreview.\nAccording\nto\nthe\n",
  "14": "sentence, judge whether the emotion is\nPositive or Negative.\nIMDb\n“Positive” example:\nThis\nis\nthe\ndefinitive\nmovie\nversion\nof Hamlet.\nBranagh cuts nothing,\nbut\nthere are no wasted moments. According\nto the movie review, judge whether it is\nPositive or Negative.\n“Negative” example:\nThis is without a doubt the worst movie\nI have ever seen.\nIt is not funny.\nIt\nis not interesting and should not have\nbeen made. According to the movie review,\njudge whether it is Positive or Negative.\nSarcasm\n“Yes” example:\nTask:\nDetect sarcasm, help me identify\nwhether this sentence is sarcastic. First,\nwe need to understand what sarcasm is.\nSarcasm is a form of verbal irony, where\nthe intended meaning of the words is the\nopposite of the literal meaning. In other\nwords, the speaker is saying one thing\nbut meaning the opposite. Bashar al-Assad\ntries a tiny bit of sarin gas on self\nto see what it’s like. Think carefully\naccording to the sentence. Is there any\nsarcasm in this sentence? Please answer\nYes or No.\n“No” example:\nTask:\nDetect sarcasm, help me identify\nwhether this sentence is sarcastic. First,\nwe need to understand what sarcasm is.\nSarcasm is a form of verbal irony, where\nthe intended meaning of the words is the\nopposite of the literal meaning. In other\nwords, the speaker is saying one thing but\nmeaning the opposite. This ceo will send\nyour kids to school, if you work for his\ncompany. Think carefully according to the\nsentence.\nIs there any sarcasm in this\nsentence? Please answer Yes or No.\nStrategyQA\nNote: This dataset was created in 2021. Queen\nElizabeth was alive then.\n“Yes” example:\nJudge the question is true or false? Q:\nWill Queen Elizabeth be buried in the\nPantheon? Let us think step by step. The\nstem of the sentence is Queen Elizabeth,\nburial, pantheon. Inference: First, the\nPantheon is a church, so it is possible\nthat she could be buried there. Second,\nQueen Elizabeth II is still alive, so she\nhas not been buried yet. Third, even if\nshe were to be buried in the Pantheon, it\nis unlikely that we would know about it\nahead of time, so it is hard to say for\nsure. pred_ans: no. Do hamsters provide\nfood for any animals?\nLet us think step\nby step...\n“No” example:\nJudge the question is true or false? Q:\nWill Queen Elizabeth be buried in the\nPantheon? Let us think step by step. The\nstem of the sentence is Queen Elizabeth,\nburial, pantheon. Inference: First, the\nPantheon is a church, so it is possible\nthat she could be buried there. Second,\nQueen Elizabeth II is still alive, so she\nhas not been buried yet. Third, even if\nshe were to be buried in the Pantheon, it\nis unlikely that we would know about it\nahead of time, so it is hard to say for\nsure. pred_ans: no. Could a llama birth\ntwice during the War in Vietnam (1945-46)?\nLet us think step by step...\nCoinflip\n“Yes” example:\nA coin is heads up. Whitney flips the coin.\nErika does not flip the coin. Tj does not\nflip the coin. Benito flips the coin. Is\nthe coin still heads up? Note that \"flip\"\nhere means \"reverse\". According to the\nflipping process above, determine if a\ncoin remains heads up after it is either\nflipped or left unflipped by individuals.\nTherefore, the answer (Yes or No) is?\n“No” example:\nA coin is heads up. Lucky does not flip\nthe coin. Mireya flips the coin. Jj flips\nthe coin.\nKc flips the coin.\nIs the\ncoin still heads up?\nNote that \"flip\"\nhere means \"reverse\". According to the\nflipping process above, determine if a\ncoin remains heads up after it is either\nflipped or left unflipped by individuals.\nTherefore, the answer (Yes or No) is?\n",
  "15": "Gemma-2B (18 Layers)\nStrategyQA\nCoinflip\nCities\nCommon Claim\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nMetrics\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\n1st-layer\n0.556\n0.601\n0.588\n0.635\n0.667\n0.626\n0.446\n0.411\n0.422\n0.582\n0.61\n0.584\n0.502\n0.509\n0.518\n0.74\n0.822\n0.742\n0.666\n0.729\n0.68\n0.722\n0.788\n0.725\n0.731\n0.808\n0.714\n25%-layer\n0.602\n0.642\n0.629\n0.62\n0.64\n0.6\n0.94\n0.987\n0.939\n0.637\n0.683\n0.633\n0.675\n0.745\n0.664\n0.793\n0.89\n0.794\n0.872\n0.942\n0.874\n0.884\n0.953\n0.885\n0.857\n0.935\n0.852\n50%-layer\n0.639\n0.7\n0.665\n0.65\n0.705\n0.632\n0.983\n0.999\n0.983\n0.648\n0.699\n0.642\n0.729\n0.826\n0.716\n0.802\n0.891\n0.8\n0.915\n0.972\n0.917\n0.941\n0.982\n0.941\n0.89\n0.955\n0.887\n67%-layer\n0.683\n0.751\n0.708\n0.695\n0.783\n0.69\n0.992\n1.0\n0.992\n0.671\n0.727\n0.666\n0.766\n0.852\n0.758\n0.809\n0.891\n0.806\n0.929\n0.976\n0.93\n0.936\n0.984\n0.936\n0.893\n0.96\n0.889\n83%-layer\n0.62\n0.668\n0.637\n0.585\n0.625\n0.579\n0.985\n0.999\n0.985\n0.652\n0.703\n0.645\n0.704\n0.782\n0.696\n0.807\n0.89\n0.803\n0.911\n0.97\n0.914\n0.926\n0.977\n0.926\n0.882\n0.944\n0.878\nlast-layer\n0.592\n0.602\n0.626\n0.525\n0.554\n0.532\n0.988\n0.999\n0.988\n0.647\n0.697\n0.644\n0.685\n0.752\n0.679\n0.803\n0.892\n0.801\n0.895\n0.957\n0.898\n0.941\n0.981\n0.941\n0.837\n0.92\n0.831\nGemma-7B (28 Layers)\nStrategyQA\nCoinflip\nCities\nCommon Claim\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nMetrics\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nLayer\n0.519\n0.565\n0.569\n0.69\n0.712\n0.656\n0.59\n0.444\n0.439\n0.737\n0.609\n0.053\n0.496\n0.506\n0.514\n0.737\n0.817\n0.741\n0.668\n0.737\n0.679\n0.73\n0.811\n0.735\n0.723\n0.793\n0.713\n25%-layer\n0.617\n0.653\n0.648\n0.605\n0.654\n0.591\n0.95\n0.987\n0.951\n0.719\n0.687\n0.422\n0.696\n0.773\n0.686\n0.795\n0.875\n0.794\n0.89\n0.958\n0.893\n0.9\n0.958\n0.899\n0.885\n0.953\n0.882\n50%-layer\n0.658\n0.716\n0.678\n0.765\n0.844\n0.761\n1.0\n0.999\n0.998\n0.75\n0.747\n0.498\n0.835\n0.912\n0.829\n0.784\n0.871\n0.782\n0.929\n0.979\n0.93\n0.932\n0.979\n0.932\n0.907\n0.968\n0.904\n67%-layer\n0.733\n0.809\n0.744\n0.88\n0.922\n0.875\n1.0\n0.998\n0.995\n0.756\n0.75\n0.515\n0.814\n0.9\n0.809\n0.784\n0.868\n0.782\n0.936\n0.981\n0.937\n0.922\n0.979\n0.923\n0.913\n0.972\n0.91\n83%-layer\n0.675\n0.746\n0.695\n0.785\n0.818\n0.786\n0.98\n0.997\n0.982\n0.743\n0.74\n0.492\n0.776\n0.867\n0.768\n0.812\n0.897\n0.809\n0.935\n0.979\n0.937\n0.925\n0.977\n0.926\n0.901\n0.963\n0.898\nlast-layer\n0.604\n0.666\n0.625\n0.57\n0.602\n0.578\n0.95\n0.996\n0.972\n0.759\n0.748\n0.481\n0.729\n0.808\n0.721\n0.82\n0.901\n0.817\n0.92\n0.975\n0.922\n0.938\n0.984\n0.938\n0.862\n0.932\n0.86\nLlaMA-7B (32 Layers)\nStrategyQA\nCoinflip\nCities\nCommon Claim\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nMetrics\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\n1st-layer\n0.584\n0.608\n0.641\n0.545\n0.525\n0.674\n0.487\n0.472\n0.472\n0.74\n0.617\n0.031\n0.493\n0.491\n0.522\n0.725\n0.814\n0.732\n0.678\n0.744\n0.703\n0.73\n0.799\n0.736\n0.705\n0.773\n0.697\n25%-layer\n0.657\n0.712\n0.688\n0.615\n0.612\n0.621\n0.93\n0.978\n0.929\n0.736\n0.699\n0.441\n0.684\n0.754\n0.681\n0.806\n0.887\n0.792\n0.513\n0.913\n0.676\n0.914\n0.972\n0.914\n0.89\n0.961\n0.886\n50%-layer\n0.74\n0.827\n0.754\n0.915\n0.977\n0.907\n0.997\n1.0\n0.997\n0.753\n0.738\n0.477\n0.797\n0.894\n0.793\n0.805\n0.883\n0.791\n0.847\n0.954\n0.866\n0.941\n0.984\n0.941\n0.922\n0.976\n0.919\n67%-layer\n0.729\n0.805\n0.744\n0.9\n0.966\n0.89\n0.995\n1.0\n0.995\n0.744\n0.729\n0.466\n0.775\n0.872\n0.77\n0.795\n0.88\n0.779\n0.896\n0.957\n0.902\n0.939\n0.985\n0.939\n0.92\n0.973\n0.918\n83%-layer\n0.699\n0.774\n0.72\n0.88\n0.961\n0.871\n0.993\n1.0\n0.993\n0.734\n0.719\n0.464\n0.744\n0.832\n0.735\n0.798\n0.887\n0.785\n0.913\n0.967\n0.915\n0.939\n0.984\n0.939\n0.908\n0.966\n0.905\nlast-layer\n0.67\n0.744\n0.69\n0.815\n0.9\n0.8\n0.993\n1.0\n0.993\n0.743\n0.731\n0.464\n0.739\n0.818\n0.731\n0.831\n0.914\n0.83\n0.935\n0.984\n0.937\n0.944\n0.987\n0.944\n0.895\n0.955\n0.892\nLlaMA-13B (40 Layers)\nStrategyQA\nCoinflip\nCities\nCommon Claim\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nMetrics\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\n1st-layer\n0.567\n0.601\n0.628\n0.475\n0.509\n0.575\n0.481\n0.463\n0.47\n0.741\n0.62\n0.034\n0.486\n0.485\n0.526\n0.719\n0.807\n0.727\n0.697\n0.769\n0.709\n0.732\n0.795\n0.736\n0.692\n0.756\n0.688\n25%-layer\n0.676\n0.732\n0.701\n0.53\n0.59\n0.515\n0.985\n0.999\n0.985\n0.733\n0.716\n0.457\n0.763\n0.859\n0.758\n0.832\n0.914\n0.829\n0.93\n0.98\n0.931\n0.942\n0.983\n0.943\n0.915\n0.972\n0.913\n50%-layer\n0.763\n0.844\n0.771\n0.825\n0.886\n0.819\n0.993\n1.0\n0.993\n0.758\n0.751\n0.515\n0.812\n0.897\n0.809\n0.839\n0.92\n0.836\n0.939\n0.984\n0.94\n0.945\n0.984\n0.945\n0.936\n0.983\n0.934\n67%-layer\n0.716\n0.806\n0.729\n0.795\n0.882\n0.794\n0.993\n1.0\n0.993\n0.751\n0.745\n0.499\n0.776\n0.866\n0.772\n0.838\n0.919\n0.834\n0.938\n0.984\n0.939\n0.94\n0.987\n0.94\n0.924\n0.978\n0.921\n83%-layer\n0.71\n0.795\n0.719\n0.7\n0.797\n0.703\n0.99\n1.0\n0.99\n0.741\n0.731\n0.487\n0.768\n0.856\n0.762\n0.832\n0.912\n0.829\n0.937\n0.983\n0.938\n0.941\n0.985\n0.942\n0.922\n0.974\n0.919\nlast-layer\n0.693\n0.772\n0.704\n0.645\n0.715\n0.664\n0.99\n1.0\n0.99\n0.75\n0.743\n0.499\n0.76\n0.841\n0.752\n0.835\n0.913\n0.833\n0.935\n0.984\n0.937\n0.946\n0.988\n0.946\n0.91\n0.969\n0.908\nQwen-0.5B (24 Layers)\nStrategyQA\nCoinflip\nCities\nCommon Claim\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nMetrics\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\n1st-layer\n0.557\n0.578\n0.607\n0.535\n0.649\n0.657\n0.482\n0.46\n0.464\n0.735\n0.622\n0.11\n0.499\n0.503\n0.527\n0.759\n0.851\n0.764\n0.73\n0.801\n0.733\n0.764\n0.837\n0.762\n0.729\n0.799\n0.72\n25%-layer\n0.583\n0.63\n0.62\n0.545\n0.582\n0.508\n0.731\n0.797\n0.722\n0.732\n0.651\n0.257\n0.52\n0.524\n0.523\n0.785\n0.864\n0.783\n0.751\n0.829\n0.756\n0.804\n0.887\n0.806\n0.811\n0.895\n0.799\n50%-layer\n0.619\n0.686\n0.649\n0.62\n0.652\n0.596\n0.935\n0.979\n0.935\n0.744\n0.695\n0.379\n0.68\n0.754\n0.676\n0.793\n0.88\n0.792\n0.846\n0.921\n0.848\n0.884\n0.949\n0.883\n0.838\n0.92\n0.831\n67%-layer\n0.644\n0.688\n0.673\n0.585\n0.617\n0.561\n0.933\n0.982\n0.934\n0.742\n0.705\n0.375\n0.668\n0.74\n0.665\n0.789\n0.874\n0.786\n0.868\n0.946\n0.87\n0.894\n0.956\n0.893\n0.827\n0.911\n0.821\n83%-layer\n0.583\n0.61\n0.612\n0.62\n0.668\n0.612\n0.923\n0.971\n0.924\n0.746\n0.706\n0.364\n0.604\n0.657\n0.6\n0.791\n0.867\n0.791\n0.85\n0.927\n0.854\n0.866\n0.941\n0.865\n0.824\n0.902\n0.819\nlast-layer\n0.55\n0.567\n0.584\n0.55\n0.613\n0.541\n0.912\n0.971\n0.912\n0.742\n0.703\n0.357\n0.579\n0.616\n0.579\n0.784\n0.866\n0.781\n0.844\n0.922\n0.848\n0.879\n0.951\n0.88\n0.825\n0.9\n0.82\nQwen-1.8B (24 Layers)\nStrategyQA\nCoinflip\nCities\nCommon Claim\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nMetrics\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\n1st-layer\n0.57\n0.6\n0.63\n0.49\n0.634\n0.648\n0.482\n0.458\n0.464\n0.739\n0.619\n0.071\n0.516\n0.514\n0.539\n0.724\n0.819\n0.732\n0.693\n0.762\n0.703\n0.718\n0.784\n0.72\n0.721\n0.796\n0.713\n25%-layer\n0.607\n0.638\n0.643\n0.58\n0.59\n0.584\n0.583\n0.626\n0.582\n0.736\n0.658\n0.317\n0.521\n0.541\n0.525\n0.809\n0.882\n0.807\n0.775\n0.844\n0.781\n0.81\n0.899\n0.81\n0.833\n0.909\n0.825\n50%-layer\n0.658\n0.726\n0.676\n0.595\n0.655\n0.58\n0.975\n0.997\n0.975\n0.741\n0.708\n0.419\n0.688\n0.767\n0.683\n0.808\n0.89\n0.807\n0.895\n0.961\n0.897\n0.914\n0.974\n0.915\n0.87\n0.947\n0.864\n67%-layer\n0.664\n0.733\n0.685\n0.695\n0.759\n0.655\n0.977\n0.996\n0.977\n0.741\n0.717\n0.423\n0.695\n0.776\n0.689\n0.809\n0.886\n0.804\n0.893\n0.963\n0.895\n0.904\n0.968\n0.905\n0.865\n0.943\n0.859\n83%-layer\n0.631\n0.666\n0.649\n0.7\n0.747\n0.674\n0.972\n0.995\n0.972\n0.735\n0.706\n0.419\n0.657\n0.734\n0.651\n0.791\n0.877\n0.788\n0.89\n0.956\n0.893\n0.891\n0.957\n0.893\n0.835\n0.922\n0.829\nlast-layer\n0.595\n0.642\n0.604\n0.615\n0.667\n0.605\n0.973\n0.996\n0.973\n0.74\n0.704\n0.404\n0.638\n0.713\n0.631\n0.795\n0.877\n0.794\n0.879\n0.949\n0.882\n0.906\n0.962\n0.907\n0.812\n0.896\n0.808\nQwen-7B (40 Layers)\nStrategyQA\nCoinflip\nCities\nCommon Claim\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nMetrics\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\n1st-layer\n0.54\n0.559\n0.602\n0.545\n0.597\n0.588\n0.437\n0.403\n0.418\n0.735\n0.635\n0.169\n0.475\n0.466\n0.498\n0.785\n0.867\n0.786\n0.753\n0.826\n0.757\n0.782\n0.858\n0.785\n0.771\n0.856\n0.765\n25%-layer\n0.59\n0.625\n0.631\n0.62\n0.635\n0.631\n0.806\n0.89\n0.803\n0.721\n0.657\n0.348\n0.556\n0.57\n0.553\n0.798\n0.879\n0.795\n0.782\n0.869\n0.787\n0.825\n0.916\n0.827\n0.852\n0.924\n0.848\n50%-layer\n0.705\n0.782\n0.724\n0.66\n0.719\n0.667\n0.985\n0.998\n0.985\n0.744\n0.733\n0.452\n0.731\n0.825\n0.722\n0.802\n0.885\n0.795\n0.912\n0.969\n0.914\n0.929\n0.975\n0.929\n0.882\n0.956\n0.878\n67%-layer\n0.702\n0.773\n0.722\n0.635\n0.719\n0.64\n0.983\n0.997\n0.983\n0.746\n0.728\n0.465\n0.721\n0.801\n0.716\n0.793\n0.88\n0.789\n0.904\n0.965\n0.907\n0.915\n0.968\n0.913\n0.89\n0.955\n0.888\n83%-layer\n0.678\n0.751\n0.703\n0.685\n0.762\n0.693\n0.978\n0.996\n0.978\n0.747\n0.724\n0.45\n0.68\n0.747\n0.667\n0.795\n0.881\n0.79\n0.899\n0.961\n0.901\n0.916\n0.965\n0.915\n0.872\n0.941\n0.868\nlast-layer\n0.676\n0.718\n0.693\n0.585\n0.623\n0.587\n0.982\n0.995\n0.981\n0.75\n0.727\n0.453\n0.657\n0.718\n0.643\n0.782\n0.869\n0.778\n0.902\n0.964\n0.905\n0.92\n0.973\n0.919\n0.848\n0.921\n0.845\nQwen-14B (32 Layers)\nStrategyQA\nCoinflip\nCities\nCommon Claim\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nMetrics\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\n1st-layer\n0.582\n0.608\n0.624\n0.555\n0.599\n0.594\n0.436\n0.399\n0.429\n0.738\n0.627\n0.205\n0.487\n0.481\n0.491\n0.77\n0.864\n0.772\n0.749\n0.827\n0.754\n0.768\n0.864\n0.768\n0.785\n0.866\n0.773\n25%-layer\n0.605\n0.645\n0.644\n0.555\n0.609\n0.566\n0.858\n0.93\n0.855\n0.713\n0.667\n0.362\n0.558\n0.601\n0.555\n0.8\n0.879\n0.796\n0.818\n0.898\n0.821\n0.861\n0.936\n0.864\n0.876\n0.946\n0.871\n50%-layer\n0.695\n0.775\n0.709\n0.69\n0.731\n0.69\n0.99\n0.999\n0.99\n0.744\n0.742\n0.483\n0.771\n0.861\n0.762\n0.812\n0.897\n0.807\n0.918\n0.972\n0.919\n0.946\n0.983\n0.946\n0.901\n0.966\n0.897\n67%-layer\n0.734\n0.82\n0.749\n0.685\n0.753\n0.693\n0.992\n0.997\n0.992\n0.762\n0.754\n0.5\n0.769\n0.848\n0.765\n0.802\n0.889\n0.798\n0.914\n0.974\n0.916\n0.93\n0.978\n0.93\n0.898\n0.963\n0.895\n83%-layer\n0.737\n0.814\n0.751\n0.68\n0.74\n0.677\n0.992\n0.996\n0.992\n0.751\n0.732\n0.485\n0.726\n0.808\n0.719\n0.806\n0.886\n0.805\n0.91\n0.972\n0.912\n0.925\n0.974\n0.925\n0.892\n0.957\n0.887\nlast-layer\n0.714\n0.785\n0.729\n0.65\n0.681\n0.646\n0.982\n0.996\n0.981\n0.753\n0.74\n0.483\n0.707\n0.779\n0.701\n0.802\n0.881\n0.798\n0.908\n0.97\n0.91\n0.932\n0.979\n0.933\n0.871\n0.943\n0.867\nQwen-14B (40 Layers)\nStrategyQA\nCoinflip\nCities\nCommon Claim\nCounterfact\nHateEval\nSTSA\nIMDb\nSarcasm\nMetrics\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\n1st-layer\n0.569\n0.59\n0.626\n0.605\n0.639\n0.646\n0.461\n0.422\n0.448\n0.733\n0.62\n0.162\n0.481\n0.486\n0.504\n0.766\n0.86\n0.769\n0.75\n0.824\n0.753\n0.778\n0.856\n0.776\n0.772\n0.859\n0.762\n25%-layer\n0.603\n0.637\n0.631\n0.68\n0.704\n0.66\n0.866\n0.945\n0.865\n0.72\n0.669\n0.393\n0.621\n0.67\n0.615\n0.8\n0.89\n0.797\n0.842\n0.919\n0.845\n0.862\n0.939\n0.861\n0.874\n0.946\n0.869\n50%-layer\n0.735\n0.835\n0.747\n0.71\n0.754\n0.681\n0.995\n1.0\n0.995\n0.76\n0.748\n0.515\n0.791\n0.875\n0.787\n0.808\n0.891\n0.807\n0.932\n0.981\n0.934\n0.935\n0.984\n0.935\n0.919\n0.974\n0.916\n67%-layer\n0.798\n0.883\n0.808\n0.7\n0.791\n0.674\n0.995\n1.0\n0.995\n0.761\n0.76\n0.5\n0.775\n0.859\n0.769\n0.816\n0.894\n0.814\n0.933\n0.98\n0.934\n0.93\n0.978\n0.93\n0.914\n0.968\n0.911\n83%-layer\n0.796\n0.872\n0.807\n0.715\n0.768\n0.705\n0.992\n1.0\n0.992\n0.753\n0.735\n0.493\n0.753\n0.843\n0.748\n0.816\n0.896\n0.811\n0.927\n0.978\n0.927\n0.936\n0.978\n0.936\n0.912\n0.964\n0.91\nlast-layer\n0.783\n0.863\n0.792\n0.605\n0.67\n0.586\n0.99\n1.0\n0.99\n0.757\n0.749\n0.481\n0.725\n0.814\n0.723\n0.815\n0.897\n0.812\n0.922\n0.978\n0.923\n0.931\n0.978\n0.93\n0.89\n0.952\n0.887\nTable 3: The experimental results of the nine datasets on nine LLMs. For each LLM, we select six layers (first, 25%\ndepth, 50% depth, 67% depth, 83% depth, last) to record the accuracy, F1-score, and AUC.\n",
  "16": "Category\nDataset\nFact\nCities\nCommon\nCounterfact\nEmotion\nHateEval\nSTSA\nIMDb\nSarcasm\nReasoning\nStrategyQA\nCoinflip\nTable 4: The category that each dataset belongs to.\nDataset\nLLaMA3-8B-Instruct GPT-4o-mini QWen2-7B-Instruct Average\nCoinflip\n0.5080\n0.7620\n0.5060\n0.5920\nCommon\n0.5606\n0.6905\n0.6950\n0.6487\nSarcasm\n0.6575\n0.6770\n0.6445\n0.6597\nStrategyQA\n0.7035\n0.8803\n0.5069\n0.6969\nCounterfact\n0.5277\n0.7990\n0.8110\n0.7126\nHateeval\n0.7640\n0.7300\n0.7952\n0.7631\nSTSA\n0.9030\n0.9211\n0.9108\n0.9116\nCities\n0.7687\n0.9973\n0.9953\n0.9204\nIMDb\n0.9365\n0.9370\n0.9405\n0.9380\nTable 5: Accuracy on nine datasets based on LLaMA3-\n8b-Instruct, GPT-4o-mini and QWen2-7B-Instruct.\nA.5\nLLM structure\nHere, we give an introduction to the model struc-\nture (using LLaMA2-7B as an example).\nLlamaForCausalLM(\n(model): LlamaModel(\n(embed_tokens):Embedding(32000,4096)\n(layers):ModuleList(\n(0-31):32 x lamaDecoderLayer(\n(self_attn): LlamaSdpaAttention(\n(g_proj):Linear(in_features=4096,out_features=4096,bias=False)\n(k proj): Linear(in_features=4096,out_features=4096,bias=False)\n(v proj):Linear(in_features=4096,out_features=4096, bias=False)\n(o proj):Linear(in_features=4096, out_features=4096, bias=False)\n(rotary emb):LlamaRotaryEmbedding()\n)\n(mlp):LlamaMLP(\n(gate_proj): Linear(in_features=4096,out_features=11008,bias=False)\n(up_proj): Linear(in_features=4096,out_features=11008,bias=False)\n(down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n(act_fn): siLu()\n)\n(input_layernorm):LlamaRMSNorm()\n(post_attention_layernorm):LlamaRMSNorm()\n)\n)\n(norm):LlamaRMSNorm()\n)\n(lm_head): Linear(in_features=4096,out_features=32000,bias=False)\n)\n"
}