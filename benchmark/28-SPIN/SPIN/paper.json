{
  "1": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language\nModels\nZixiang Chen * 1 Yihe Deng * 1 Huizhuo Yuan * 1 Kaixuan Ji 1 Quanquan Gu 1\nAbstract\nHarnessing the power of human-annotated data\nthrough Supervised Fine-Tuning (SFT) is pivotal\nfor advancing Large Language Models (LLMs).\nIn this paper, we delve into the prospect of grow-\ning a strong LLM out of a weak one without the\nneed for acquiring additional human-annotated\ndata.\nWe propose a new fine-tuning method\ncalled Self-Play fIne-tuNing (SPIN), which starts\nfrom a supervised fine-tuned model. At the heart\nof SPIN lies a self-play mechanism, where the\nLLM refines its capability by playing against in-\nstances of itself.\nMore specifically, the LLM\ngenerates its own training data from its previ-\nous iterations, refining its policy by discerning\nthese self-generated responses from those ob-\ntained from human-annotated data. Our method\nprogressively elevates the LLM from a nascent\nmodel to a formidable one, unlocking the full po-\ntential of human-annotated demonstration data\nfor SFT. Theoretically, we prove that the global\noptimum to the training objective function of our\nmethod is achieved only when the LLM policy\naligns with the target data distribution. Empiri-\ncally, we evaluate our method on several bench-\nmark datasets including the HuggingFace Open\nLLM Leaderboard, MT-Bench, and datasets from\nBig-Bench. Our results show that SPIN can sig-\nnificantly improve the LLM’s performance across\na variety of benchmarks and even outperform\nmodels trained through direct preference opti-\nmization (DPO) supplemented with extra GPT-4\npreference data. This sheds light on the promise\nof self-play, enabling the achievement of human-\nlevel performance in LLMs without the need\nfor expert opponents.\nCodes are available at\nhttps://github.com/uclaml/SPIN.\n*Equal contribution 1Department of Computer Science, Univer-\nsity of California, Los Angeles, CA 90095, USA. Correspondence\nto: Quanquan Gu <qgu@cs.ucla.edu>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\n1\nIntroduction\nLarge Language Models (LLMs) have began a ground-\nbreaking era in artificial general intelligence (AGI), demon-\nstrating extraordinary capabilities across a wide range of\ndomains that require intricate reasoning and specialized\nknowledge. These models excel in areas such as math-\nematical reasoning/problem solving (Cobbe et al., 2021;\nWei et al., 2022; Lewkowycz et al., 2022), code genera-\ntion/programming (Chen et al., 2021; Austin et al., 2021; Li\net al., 2022), text generation (Bubeck et al., 2023; Anil et al.,\n2023; Touvron et al., 2023), summarization and creative\nwriting, among others. A significant advancement in LLMs\nis the post-pre-training alignment with the more desirable\nbehaviors (Mishra et al., 2021; Victor et al., 2022; Chung\net al., 2022; Thoppilan et al., 2022), a process often reliant\non the costly human-annotated data. Typical alignment\nmethods include Supervised Fine-Tuning (SFT) (Ouyang\net al., 2022; Tunstall et al., 2023a) based on human demon-\nstrations, and Reinforcement Learning from Human Feed-\nback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019;\nStiennon et al., 2020; Bai et al., 2022a) based on human\npreferences.\nAll the aforementioned alignment methods require a sub-\nstantial volume of human annotated data. Therefore, there\nis increasing interest in developing fine-tuning methods that\ncan effectively utilize human data, thereby streamlining the\nalignment process. This motivates us to study fine-tuning\nLLMs without the need for additional human-annotated data\nbeyond the fine-tuning dataset. Our study is also related to\nthe broader goal of converting weak models to strong mod-\nels without the requirement for extra training data, which\nis of central interest in machine learning that can be traced\nback to the boosting algorithms (Kearns & Valiant, 1994;\nSchapire, 1990; Freund, 1995; Freund & Schapire, 1997).\nThe self-training algorithm (Vapnik, 1999; Grandvalet &\nBengio, 2004; Lee, 2013) has also been proved to be able to\nconvert weak learners to strong learners in mixture models\nwithout the need for additional labeled data (Frei et al., 2022;\nKou et al., 2022). However, the pursuit of autonomously\nenhancing a weak LLM without external guidance is both\nintriguing and understudied. This raises the following ques-\ntion:\n1\narXiv:2401.01335v3  [cs.LG]  14 Jun 2024\n",
  "2": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nCan we empower a weak LLM to improve itself without\nacquiring additional human annotated data?\nIn this paper, we answer this question affirmatively. Inspired\nby the success of self-play mechanisms (Samuel, 2000) in\ngames, exemplified by AlphaGo Zero (Silver et al., 2017b),\nAlphaZero (Silver et al., 2017a), with historical roots traced\nback to TD-Gammon (Tesauro et al., 1995), we propose to\nconvert a weak LLM to a strong one through the lens of\nself-play, where the model is enhanced by playing against\nitself without requiring any direct supervision. In particu-\nlar, we propose a novel fine-tuning method called Self-Play\nfIne-tuNing (SPIN), which begins from a supervised fine-\ntuned model. SPIN allows the LLM to engage in self-play,\neliminating the need for an expert annotator such as a hu-\nman or more advanced LLMs like GPT-4. In detail, with\nthe LLM from previous iteration t denoted by pθt, we em-\nploy it to generate responses y′ to the prompts x in the\nhuman-annotated SFT dataset. The subsequent objective\nis to find a new LLM pθt+1, capable of distinguishing the\nresponses y′ generated by pθt from the responses y gener-\nated by humans. This process can be seen as a two-player\ngame: the main player, or the new LLM pθt+1, seeks to dis-\ncern between the responses of the opponent player pθt and\nhuman-generated responses, while the opponent, or the old\nLLM pθt, generates responses as similar as possible to those\nin the human-annotated SFT dataset. The new LLM pθt+1 is\nobtained by fine-tuning the old one pθt to prefer responses\nfrom pdata over pθt, resulting in a distribution pθt+1 that\nis more aligned with pdata. In the next iteration, the newly\nobtained LLM pθt+1 becomes the opponent for response\ngeneration, with the self-play process aiming for the LLM\nto eventually converge to pθ∗= pdata, so that the strongest\npossible LLM can no longer differentiate the responses gen-\nerated by its previous version and those generated by the\nhuman.\nInterestingly, our method exhibits similarity with the re-\ncently introduced direct preference optimization (DPO)\nmethod (Rafailov et al., 2023), with the notable distinc-\ntion being the self-play nature of our method.\nConse-\nquently, our approach stands out by eliminating the need\nfor extra human preference data, a requirement present in\nthe DPO method. Additionally, the self-play mechanism\nin our method resembles the idea of generative adversar-\nial networks (GAN) (Goodfellow et al., 2014; Arjovsky\net al., 2017), albeit that both the discriminator (main player)\nand the generator (the opponent) in our method are in-\nstances of the same LLM from different iterations. The-\noretically, we prove that our method converges when the\ndistribution of the LLM is identical to the target data dis-\ntribution, i.e., pθt = pdata. Our experimental results on\nzephyr-7b-sft-full (Tunstall et al., 2023a), a fine-\ntuned LLM based on Mistral-7B (Jiang et al., 2023), show\nthat while continued training using SFT on its own SFT\ndataset Ultrachat200k (Ding et al., 2023) reaches a perfor-\nmance plateau or even diminished evaluation scores, our\nmethod consistently improves zephyr-7b-sft-full\nacross successive iterations while leveraging only a 50k sub-\nset of Ultrachat200k dataset. Ultimately, SPIN effectively\nimproves the base model’s average score from 58.14 to\n63.16 on the HuggingFace Open LLM Leaderboard (Beech-\ning et al., 2023) with remarkable 10%+ improvement in\nscores on GSM8k and TruthfulQA, and from 5.94 to 6.78\non MT-Bench (Zheng et al., 2023). Notably, SPIN achieves\nresults that are even comparable to models trained on ad-\nditional 62k preference dataset (Tunstall et al., 2023a) on\nOpen LLM leaderboard and MT-Bench.\nConcurrent to our work, Singh et al. (2023) proposed the\nuse of synthetic data with binary feedback in self-training,\nreducing the reliance on human data. In contrast, our ap-\nproach eliminates the need for additional binary feedback\nfrom humans or an extra reward model thanks to the self-\nplay mechanism. Additionally, Burns et al. (2023) employed\na weak LLM model as the guidance to train stronger LLMs\nin a fashion of weak-to-strong generation. Unlike Burns\net al. (2023), which necessitates both a weak supervisor and\na strong model, our SPIN operates effectively with a single\nLLM.\nNotation. We use lowercase letters and lowercase bold-\nface letters to denote scalars and vectors, respectively. We\nuse [N] to denote the index set {1, . . . , N}. In the func-\ntion space, let F be the function class. The symbol qdata\ndesignates the target data distribution, while p represents\nthe conditional probability of LLM’s response (i.e., LLM\npolicy).\n2\nRelated Work\nSelf-Play. Self-play (Samuel, 1959; Tesauro et al., 1995),\nwhere the algorithm learns by playing against itself, has\ngained notable attention due to its effectiveness in multi-\nagent reinforcement learning (MARL). This method in-\nvolves agents engaging in interactions with copies of them-\nselves, enabling an increasing level of challenge and com-\nplexity within the learning environment. A fundamental\nwork in the field of self-play is AlphaGo Zero (Silver\net al., 2017b), which demonstrated exceptional performance\nagainst human players using a self-play learning scheme.\nSubsequent research has expanded upon the concept of\nself-play, exploring various adaptations and implementa-\ntions (Anthony et al., 2017; Lanctot et al., 2017; Bansal\net al., 2018; Hernandez-Leal et al., 2018; Muller et al., 2019;\nVinyals et al., 2019). Our method takes the self-play ap-\nproach akin to AlphaGo Zero, which can convert a weak\nmodel to a strong one without additional human-annotated\ndata. While the effectiveness of self-play in MARL is well-\nestablished, to our knowledge, our work is the first to apply\n2\n",
  "3": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nthis approach to the enhancement of LLMs.\nSynthetic Data for LLMs. In the context of supervised\nfine-tuning (SFT) of LLMs, human-crafted data has proven\nto be a remarkably effective source that enhances the perfor-\nmance of LLMs on tasks such as code generation (Roziere\net al., 2023; Yang et al., 2023) and mathematical reason-\ning (Yuan et al., 2023; Luo et al., 2023). While human data\ntypically exhibits high quality, acquiring sufficient amount\nof such data poses a challenge in cost. In light of this con-\nsideration, the use of synthetic data has become increasingly\npopular and considered as a proxy for human data. This ap-\nproach primarily leverages advanced LLMs such as the GPT\nseries (Radford et al., 2019; Brown et al., 2020; OpenAI,\n2023) as the guidance to generate high-quality data (Josi-\nfoski et al., 2023; Taori et al., 2023; Chiang et al., 2023;\nLi et al., 2023). Recent research has also highlighted the\nrephrasing capability of LLMs in prompting for better LLM\nresponse (Deng et al., 2023; Prasad et al., 2023) as well\nas augmenting synthetic data for more effective SFT (Yu\net al., 2023; Liu et al., 2023). In contrast to prior studies\nthat utilized more advanced models for synthetic data gener-\nation when pre-training or fine-tuning a target model, our\napproach directly generates synthetic data from the target\nmodel itself.\n3\nProblem Setting and Preliminaries\nWe consider a Large Language Model (LLM) parameter-\nized by θ and denoted by pθ.\nThe model takes as in-\nput a sequence x = [x1, . . . , xn], commonly referred to\nas the prompt, to generate the corresponding response\ny = [y1, . . . , ym]. The response y is therefore considered\nas a sample from the conditional probability distribution\npθ(·|x). In LLMs, xi and yj represent individual tokens\nfrom a predetermined vocabulary within the sequences x\nand y, respectively. The auto-regressive model pθ generates\ntokens sequentially for a given position, leveraging only the\nsequence of previously generated tokens. This model there-\nfore constitutes a Markov process, where the conditional\nprobability distribution pθ(y|x) can be expressed through a\ndecomposition as follows:\npθ(y|x) =\nm\nY\nj=1\npθ(yj|x, y<j),\nwhere y<1 is null and y<j = [y1, . . . , yj−1] for j =\n2, . . . , m. In the following, we review two major fine-tuning\nmethods for LLMs: supervised fine-tuning and reinforce-\nment learning (RL) fine-tuning.\n3.1\nSupervised Fine-Tuning\nSupervised fine-tuning (SFT) is employed to tailor a pre-\ntrained LLM to specific downstream tasks, leveraging rel-\natively smaller dataset of labeled examples in comparison\nto the large-scale pre-training data (Ouyang et al., 2022;\nYu et al., 2023). In this context, we consider a specific\ntask where the prompts, denoted by x, are derived from\na specified distribution q(·). The notation pdata(·|x) then\nrepresents the probability distribution of the associated high-\nquality responses y from the training data. Consequently,\nSFT involves training the LLM to minimize the following\nnegative log-likelihood loss associated with these distribu-\ntions,\nLSFT(θ) = −Ex∼q(·),y∼pdata(·|x)\nh\nlog pθ\n\u0000y|x\n\u0001i\n.\n(3.1)\nIt should be noted that excluding x ∼q(·) from the expecta-\ntion term yields the typical cross-entropy loss, expressed as\n−Ey∼pdata(·|x)[log pθ(y|x)]. LSFT(θ) attains its minimum\nwhen the model’s predictive distribution pθ(y|x) aligns\nperfectly with the distribution of the labeled high-quality\nresponses pdata(y|x).\nConsequently, the LLM after SFT is anticipated to generate\nresponses that closely resemble those from pdata(y|x). This\nprocedure is therefore expected to significantly enhance the\nmodel’s performance in generating appropriate responses\nfor a specific task.\n3.2\nRL Fine-Tuning\nRL fine-tuning (Christiano et al., 2017; Bai et al., 2022a;\nGao et al., 2023a) offers another method for enhancing the\nspecific capabilities of general-purpose pre-trained models.\nTypically, RL fine-tuning is employed subsequent to SFT\nto achieve improved alignment for LLMs (Tunstall et al.,\n2023a).\nFor a given sequence pair (x, y), RL fine-tuning necessitates\na deterministic reward function r(x, y). The higher the\nreward r(x, y), the better the response y is to the given\nprompt x. The objective of the RL fine-tuning process is\nthen to maximize the following objective function:\nLRL(θ) = Ex∼q(·),y∼pθ(·|x)[r(x, y)]\n−λEx∼q(·)KL\n\u0000pθ(·|x)||pref(·|x)\n\u0001\n,\nwhere the Kullback-Leibler (KL) regularization enforces\nthe new model pθ to be close to the reference model pref,\nand λ > 0 is the regularization parameter to control the\ndeviation of the new model pθ from the reference model\npref. In practice, the reference model pref is often initialized\nas the supervised fine-tuned model. The inclusion of KL\nregularization is vital for preventing excessive deviation\nfrom the reference model, which in turn reduces the risk of\nmode collapse.\nMeanwhile, the primary challenge in RL fine-tuning lies\nin finding a good reward function. Typically, this function\nrequires training on a preference dataset. The compila-\ntion of such a dataset demands significant resources, often\n3\n",
  "4": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\ninvolving comprehensive evaluations either by human an-\nnotators, i.e., reinforcement learning from human feedback\n(RLHF) (Christiano et al., 2017; Bai et al., 2022a) or strong\nAI agents, i.e., reinforcement learning from AI feedback\n(RLAIF) (Bai et al., 2022b).\n4\nMethod\nIn this section, we introduce a new fine-tuning method for\nenhancing the performance of LLMs without relying on\nadditional human or AI feedback. Consider a high-quality\nsupervised fine-tuning (SFT) dataset SSFT = {(x, y)}n\ni=1,\nwhich are sampled from the marginal distribution q(x) and\npdata(y|x). Given a supervised fine-tuned LLM pθ0, further\napplication of the SFT approach in (3.1) with SSFT will be\nineffective and potentially lead to worse performance. In\naddition, without human and/or AI feedback, it becomes\ninfeasible to acquire a preference dataset for RL fine-tuning\n(e.g., RLHF and RLAIF). This hinders the application of\nRL fine-tuning techniques.\nWe evaluate pθ0 against SSFT, where pθ0 is the LLM\nachieved by SFT using (3.1). We notice a persistent quality\ngap between the groundtruth response y from SSFT and the\nLLM-generated response y′ ∼pθ(·|x) (refer to Figure 1).\nThis disparity indicates that there is still room for improve-\nment over pθ0. Therefore, we propose to use the synthetic\ndata generated by the LLM to enhance LLM’s performance\nstarting from pθ0 iteratively.\n4.1\nSelf-Play Fine-Tuning (SPIN)\nLet us consider a two-player game, where the main player’s\nobjective is to distinguish the responses generated by the\nLLM and those generated by the human. Meanwhile, the\nopponent’s role is to generate responses that are indistin-\nguishable from the human’s responses. The core of our\nmethod is the self-play mechanism, where both the main\nplayer and the opponent are the same LLM, but from dif-\nferent iterations. More specifically, the opponent is the old\nLLM from the previous iteration, and the main player is the\nnew LLM to be learned in the current iteration.\nIn iteration t+1, the opponent is the LLM from the previous\niteration, denoted by pθt, which generates responses y′ for\nthose prompts x in the SFT dataset according to pθt(·|x).\nOur method, therefore, consists of the following two steps at\niteration t+1: (1) training the main player, and (2) updating\nthe opponent player.\nTraining the Main Player. We begin with illustrating how\nwe expect a main player is trained to distinguish LLM re-\nsponses from human responses. Motivated by integral prob-\nability metric (IPM) (Müller, 1997), we formulate our ob-\njective function such that the main player ft+1 maximizes\nthe expected value gap between the target data distribution\npdata and the opponent player’s distribution pθt:\nft+1 = argmax\nf∈Ft\nE\n\u0002\nf(x, y) −f(x, y′)\n\u0003\n,\n(4.1)\nwhere the expectation is computed over the distributions\nx ∼q(·), y ∼pdata(·|x), y′ ∼pθt(·|x), and Ft is a se-\nquence of highly expressive function classes that we will\ndetermine in later deduction. The subscript t in Ft is due\nto that the function class is dependent on pθt. Given such a\nft+1 and a response sequence y to the prompt x, the value\nof ft+1(x, y) reflects the main player’s degree of belief that\ny originates from pdata rather than pθt. Ideally, the main\nplayer ft+1 should yield a high value when y ∼pdata(·|x)\nand a low value when y′ ∼pθt(·|x), where pθt is the op-\nponent’s distribution. Instead of solving (4.1), we can also\nsolve the following more general optimization problem,\nft+1 = argmin\nf∈Ft\nE\n\u0002\nℓ\n\u0000f(x, y) −f(x, y′)\n\u0001\u0003\n,\n(4.2)\nwhere the expectation is computed over the distribution\nx ∼q(·), y ∼pdata(·|x), y′ ∼pθt(·|x), and ℓ(·) is a loss\nfunction that is both monotonically decreasing and convex.\nFor example, a linear loss function ℓ(t) = −t reduces (4.2)\nto the minimization version of (4.1). However, the use of\na linear loss function results in an unbounded objective\nvalue, which, during continuous training, leads to a negative\ninfinite value of f(x, y′) on the opponent player’s responses.\nTherefore, in our work, we choose the logistic loss function\nℓ(t) := log(1+exp(−t)) for its non-negativity, smoothness,\nand exponentially decaying tail as t →∞. Such a choice\nof loss function aids in preventing the excessive growth in\nthe absolute value of f.\nUpdating the Opponent Player. Previously we have dis-\ncussed the training of ft+1 given the opponent player’s\ndistribution pθt. Now suppose we have optimized our main\nplayer ft+1 that can distinguish pdata from pθt, within a cer-\ntain function class Ft, we elaborate how we get parameter\nθt+1 of the opponent player. Specifically, when presented\nwith two responses y and y′ to the same prompt x, ft+1\nassesses the values ft+1(x, y) and ft+1(x, y′). It then in-\nfers that the response with the higher value is from the real\ndata distribution pdata and the response with lower value\nis attributed to the LLM pθt. Subsequently, the objective\nof the opponent player is to find a better LLM that gener-\nates responses indistinguishable from pdata for the main\nplayer. This is achieved by maximizing the expected value\nEx∼q(·),y∼p(·|x)[ft+1(x, y)]. In addition, to prevent exces-\nsive deviation of pθt+1 from pθt and stabilize the self-play,\nwe incorporate a Kullback-Leibler (KL) regularization term.\nPutting these together gives rise to the following optimiza-\ntion problem:\nargmax\np\nEx∼q(·),y∼p(·|x)[ft+1(x, y)]\n4\n",
  "5": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nIn Southampton, what is the most popular form of \ntransportation for commuters?\nI don't have access to current data. However, \nhistorically, the most popular form of transporta-\ntion for commuters in southampton is buses. The \ncity also has a good network of trains and taxis. \nIn Southampton, what is the most popular form of \ntransportation for commuters?\n55% of commuters in Southampton use cars as \ntheir primary mode of transportation, while 23% \nuse public transportation, 12% use bikes, and \n10% walk.\nChosen: ground truth completion\nRejected: model generation @Iter 0\nI do not have access to the latest updates and statistics of southampton. However, based on \nprevious reports, the most popular form of transportation for commuters in southampton is \nthe public transport network, which includes buses, trains, and ferries. The city also has a \nwell-developed cycling infrastructure and many people choose to cycle or walk to work. \nAdditionally, private cars and taxis are also commonly used for commuting.\nModel generation @Iter 1\nFigure 1. Example of ground truth completion compared to the fine-tuned model generation at iteration 0 and 1. We can observe that\nthe model generation at iteration 0, although fluent, incorrectly quantifies transportation preferences with specific percentages that are\npotentially hallucinations. The model generation at iteration 1 provides a qualitative summary of the transportation forms at Southampton\nwithout specific percentage, aligning more closely with the ground truth while adding more details.\n−λEx∼q(·)KL\n\u0000p(·|x)||pθt(·|x)\n\u0001\n,\n(4.3)\nwhere λ > 0 is the regularization parameter. Notably, (4.3)\nhas a closed-form solution bp(·|x):\nbp(y|x) ∝pθt(y|x) exp\n\u0000λ−1ft+1(x, y)\n\u0001\n.\n(4.4)\nIt is worth noting that bp(·|x) is not guaranteed to be belong\nto the LLM space {pθ(·|x)|θ ∈Θ}. Since we hope that the\nclosed-form solution bp in the probability space can be real-\nized by an LLM with parameter θ, i.e., pθ(y|x) = bp(y|x),\nsolving for pθ(y|x)\n∝\npθt(y|x) exp\n\u0000λ−1ft+1(x, y)\n\u0001\ngives ft+1(x, y) = λ · log pθ(·|x)\npθt(·|x). This suggests the fol-\nlowing function class Ft for ft+1:\nFt =\n\u001a\nλ · log pθ(y|x)\npθt(y|x)\n\f\f\f\fθ ∈Θ\n\u001b\n,\n(4.5)\nwhere Θ is the parameter space of LLMs being considered.\nGiven the choice of Ft in (4.5), optimizing (4.2) gives ft+1\nparameterized by θt+1 in the following form:\nft+1(x, y) = λ · log pθt+1(y|x)\npθt(y|x) .\n(4.6)\nSubstituting (4.6) into (4.4) yields bp(y|x) = pθt+1(y|x). In\nother words, θt+1 learned from (4.2) is exactly the LLM\nparameter for our ideal opponent selection.\nEnd-to-end Training Objective. We integrate the previ-\nously discussed two steps into a single end-to-end train-\ning objective with an update rule of θt+1. Specifically,\nplugging (4.5) into (4.2) arrives at the update rule θt+1 =\nargminθ∈Θ LSPIN(θ, θt), where LSPIN is the training ob-\njective defined as follows\nLSPIN = E\n\u0014\nℓ\n\u0012\nλ log pθ(y|x)\npθt(y|x) −λ log pθ(y′|x)\npθt(y′|x)\n\u0013\u0015\n,\n(4.7)\nwhere the expectation is computed over the distribution x ∼\nq(·), y ∼pdata(·|x), y′ ∼pθt(·|x). We summarize the\niterative self-play process of our method SPIN as follows,\n. . .\n→\npθt(·|x)\n| {z }\nOpponent Player at t\n→\nλ · log pθt+1(·|x)\npθt(·|x)\n|\n{z\n}\nMain Player at t + 1\n→\npθt+1(·|x)\n|\n{z\n}\nOpponent Player at t + 1\n→\n. . .\nNamely, the opponent player chosen from the previous it-\neration t is employed to train the main player at iteration\nt+1, resulting in the LLM parameterized by θt+1. Then we\ndetermine the next opponent player at iteration t + 1 by di-\nrectly copying the LLM parameter θt+1, which is then used\nin training the main player at iteration t + 2. The detailed\nalgorithm is presented in Algorithm 1.\n4.2\nComparison between SPIN and DPO\nIn Section 4.1, we propose Self-Play Fine-Tuning (SPIN)\nwith an end-to-end training objective (4.7) for each iter-\nation. (4.7) bears resemblance to direct preference opti-\nmization (DPO) (Rafailov et al., 2023) for RL fine-tuning.\nHowever, SPIN and DPO are fundamentally different.\nDPO is based on the Bradley-Terry (BT) model: p(y1 ≻\ny2|x) =\nexp(r∗(x,y1))\nexp(r∗(x,y1))+exp(r∗(x,y2)), and maximizes the\n5\n",
  "6": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nAlgorithm 1 Self-Play Fine-Tuning (SPIN)\nInput: {(xi, yi)}i∈[N]: SFT Dataset, pθ0: LLM with\nparameter θ0, T: Number of iterations.\nfor t = 0, . . . , T −1 do\nfor i = 1, . . . N do\nGenerate synthetic data y′\ni ∼pθt(·|xi).\nend for\nUpdate θt+1 = argminθ∈Θ\nP\ni∈[N] ℓ\n\u0010\nλ log pθ(yi|xi)\npθt(yi|xi)−\nλ log pθ(y′\ni|xi)\npθt(y′\ni|xi)\n\u0011\n.\nend for\nOutput: θT .\nlog-likelihood of p(y1 ≻y2|x) by direct policy optimiza-\ntion without explicit reward estimation. In contrast, SPIN\nrelies on maximizing the IPM to compete with an increas-\ningly stronger version of itself. More detailed comparisons\nare highlighted as follows:\n1. DPO does not inherently lead to iterative training. More\nspecifically, DPO aims to match the preference prob-\nability p(y1 ≻y2|x) induced from its reward model\nwith the data distribution pdata(y1 ≻y2|x) in a single\niteration. On the contrary, SPIN’s self-play mechanism\nnaturally leads to an iterative training procedure. SPIN\niteratively refines its generation distribution pθ(y|x) to\nmatch the target distribution pdata(y|x) across iterations.\n2. SPIN only requires the SFT dataset, represented by pairs\n(x, y). In contrast, DPO necessitates a preference dataset,\nrepresented by (x, yw, yl), where yw and yl denote the\nwinner (chosen) and loser (rejected) responses, respec-\ntively. Moreover, SPIN can be applied between SFT and\nRL fine-tuning.\n3. In SPIN, we can choose different loss functions ℓwhich\nonly need to be convex and decreasing (detailed later\nin Theorem 5.2), which includes correlation loss, hinge\nloss, and logistic loss. Only when ℓis chosen as the lo-\ngistic loss would the training objective of SPIN become\nsimilar to that of DPO.\nRecently, Xu et al. (2023) proposed to use iterative prefer-\nence optimization with the Pairwise Cringe Loss (PCO), and\ngeneralized DPO to iterative DPO. Concurrent to our work,\nYuan et al. (2024) further proposed a framework named\n“self-rewarding language models”, which leverages the LLM\nitself as the reward model to provide the preference feed-\nback, and employs iterative DPO to train the LLM. Com-\npared with Xu et al. (2023); Yuan et al. (2024), SPIN’s\nself-assessment is implicit, as no intermediate reward or\npreference feedback is required.\n5\nTheoretical Analysis\nIn this section, we provide a theoretical analysis for Algo-\nrithm 1 in Section 4. Under monotonicity and convexity\nassumption of the objective function ℓ, we show that the\nglobal optimum is obtained if and only if parameter θt gen-\nerates data distribution. We summarize our assumptions as\nfollows:\nAssumption 5.1. The loss function ℓ(t) : R →R is\nmonotonically decreasing, i.e., ∀t, ℓ′(t) ≤0 and satisfies\nℓ′(0) < 0. In addition, ℓ(t) is a convex function.\nAssumption 5.1 holds for a wide range of loss functions\ncommonly used in machine learning, including correlation\nloss ℓ(t) = 1 −t, hinge loss ℓ(t) = max(0, 1 −t), ex-\nponential loss ℓ(t) = exp(−t) and logistic loss ℓ(t) =\nlog(1 + exp(−t)). Under Assumptions 5.1, we present the\nfollowing theorem, which is pivotal in understanding the\noptimization dynamics of our method.\nTheorem 5.2. Under Assumption 5.1, suppose there exists\npθ(·|x) = pdata(·|x), then we have that\n• (Sufficiency) If pθt(·|x) = pdata(·|x), then θt is the\nglobal minimum of (4.7) for any λ ≥0.\n• (Necessity) If pθt(·|x) ̸= pdata(·|x), there exists an appro-\npriately chosen λ, such that θt is not the global minimum\nof (4.7).\nRemark 5.3. Theorem 5.2 suggests that under certain con-\nditions, the optimization process of our method naturally\nstops at the point pθ(·|x) = pdata(·|x), implying the effec-\ntiveness of our approach in aligning the LLM’s distribution\nwith the target data distribution. Moreover, Theorem 5.2\nalso indicates that the optimization process only stops when\nthe global optimality is achieved, i.e., the LLM’s distribution\naligns with the target data distribution.\nFor the logistic loss function ℓ(t) = log(1 + exp(−t)), the\nfollowing theorem gives a more precise characterization\nof the opponent player, enabling a better understanding of\nSPIN.\nTheorem 5.4. Consider the choice of logistic loss\nℓ(t)\n=\nlog(1 + exp(−t)) in SPIN.\nSuppose that\npθt(y|x)\n\u0000pdata(y|x)/pθt(y|x)\n\u00011/λ lies in the LLM space\n{pθ(y|x)|θ\n∈\nΘ} and θt+1 is global minimum of\nLSPIN(θ, θt), then the opponent player at iteration t + 1\nsatisfies\npθt+1(y|x) ∝pθt(y|x)\n\u0000pdata(y|x)/pθt(y|x)\n\u00011/λ.\nRemark 5.5. According to Theorem 5.4, the model update\nfrom pθt(y|x) to pθt+1(y|x) tends to increase the probabil-\nity pθt+1(y|x) when pθt(y|x) is less than pdata(y|x), and\ndecrease it when pθt(y|x) is greater than pdata(y|x). Thus,\nTheorem 5.4 further confirms that our method’s optimiza-\ntion process naturally converges to the point where pθ(·|x)\nequals pdata(·|x). The update of the opponent player is con-\ntrolled by\n\u0000pdata(y|x)/pθt(y|x)\n\u00011/λ, which is regulated by\n6\n",
  "7": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nthe factor 1/λ. A smaller λ results in a larger change of the\nopponent player, while a larger λ leads to a smaller change.\nTherefore, as pθ(·|x) approaches pdata(·|x), increasing λ\nenhances the stability of LLM training. This observation\naligns with (4.3), where λ is the regularization parameter\nof the KL regularization that is employed to control the\ndeviation of the opponent player.\n6\nExperiments\nThis section provides a detailed empirical analysis of\nSPIN. Our findings highlight several key points: (1) SPIN\nmarkedly enhances model performance across a wide range\nof evaluation benchmarks by breaking the limit of SFT;\n(2) even without introducing new human annotated data,\nSPIN at iteration 0 achieves performance on par to DPO\ntraining that utilizes even more data; (3) iterative training\nis a necessary component in SPIN as it breaks the limit of\nmulti-epoch training.\n6.1\nExperiment Setup\nModel and Datasets.\nIn this study,\nwe adopt\nzephyr-7b-sft-full as our base model. This model\nderives from the pre-trained Mistral-7B (Jiang et al., 2023)\nand has been further fine-tuned on the SFT dataset Ultra-\nchat200k1 by HuggingFace. Ultrachat200k represents a\nhigh-quality 200k subset of the larger UltraChat (Ding et al.,\n2023) corpus, which comprises approximately 1.4M dia-\nlogues produced using OpenAI’s Turbo APIs. From Ultra-\nChat200k, We randomly sample 50k prompts and use the\nbase model to generate the synthetic responses. We subse-\nquently follow the optimization method described in Section\n4.1 for further training. In multiple iterations, we leverage\nthe synthetic data from the most recent iteration and add\nto the newly generated synthetic data, therefore resulting\nin a synthetic dataset size of 50k at iteration 0 and 100k at\niteration 1, 2 and 3. At each iteration, we train our model\nfor 2 epochs.\nEvaluation. We employed the widely used Huggingface\nOpen LLM Leaderboard (Beeching et al., 2023) as our eval-\nuation benchmark, using the same Language Model Eval-\nuation Harness library (Gao et al., 2023b). This leader-\nboard encompasses 6 different datasets, each focusing\non a a specific capability of LLMs. Collectively, these\ndatasets provide a thorough assessment framework, evalu-\nating LLMs on commonsense reasoning (Arc (Clark et al.,\n2018), HellaSwag (Zellers et al., 2019), Winogrande (Sak-\naguchi et al., 2021)), multi-task language understanding\n(MMLU(Hendrycks et al., 2020)), human falsehood mimic\n(TruthfulQA (Lin et al., 2021)) and math problem solving\n(GSM8k (Cobbe et al., 2021)). We leave further implemen-\n1https://huggingface.co/datasets/\nHuggingFaceH4/ultrachat_200k\ntation details to Appendix B with detailed evaluation setting\nadopted by both the leaderboard and our experiments.\n6.2\nSPIN Effectively Improves Benchmark\nPerformance\nSFT\nSPIN\niter-0\nSPIN\niter-1\nSPIN\niter-2\nSPIN\niter-3\n58\n59\n60\n61\n62\n63\nAverage Accuracy (%)\n58.14\n60.80\n62.12\n62.97\n63.16\nHuggingFace Open LLM Benchmark\nFigure 2. The\naverage\nscore\nof\nSPIN\nat\ndifferent\nitera-\ntions on the HuggingFace Open LLM leaderboard datasets.\nFor “SFT”, we report the performance of our base model\nzephyr-7b-sft-full, which has been fine-tuned on the\nsame dataset we use to generate synthetic data.\nIn Figure 2, we demonstrate the effectiveness of SPIN us-\ning HuggingFace Open LLM Leaderboard as the evalua-\ntion. In Figure 3, we compare the performance of our fine-\ntuned model by SPIN after iterations 0 to 3 with the base\nmodel zephyr-7b-sft-full on each task included in\nthe leaderboard. Detailed performances are presented in\nTable 4 in Appendix B. We can observe that SPIN ex-\nhibits remarkable effectiveness in improving the model’s\nperformance by further leveraging the SFT dataset, on\nwhich the base model has already been fully fine-tuned.\nAt iteration 0, where model responses are generated from\nzephyr-7b-sft-full, we observe an overall improve-\nment of 2.66% on the average score. The improvement\nis particularly significant on the TruthfulQA and GSM8k\nbenchmarks, with improvement exceeding 5% and 10% re-\nspectively. At iteration 1, we employ the LLM model from\niteration 0 to generate new responses for SPIN, adhering\nto the procedure outlined in Algorithm 1. This iteration\nyields further enhancements of 1.32% on average, and es-\npecially significant on the Arc Challenge and TruthfulQA\nbenchmarks. Subsequent iterations continue this trend of\nincremental improvement across various tasks. Meanwhile,\nthe improvement at iteration t + 1 is naturally smaller than\nthat at iteration t. As the iterative training progresses, the\ndegree of improvement gradually approaches zero, suggest-\ning that the model has reached a limiting point in the last\niteration.\nComparison with DPO. zephyr-7b-beta is a model\nderived from zephyr-7b-sft-full, trained with DPO\n7\n",
  "8": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\non approximately 62k preference data. This data, the Ultra-\nFeedback Binarized dataset (Cui et al., 2023)2, comprises\nboth chosen and rejected completions evaluated by GPT-4.\nWe note that, DPO requires either human input or advanced\nlanguage model feedback to determine the preference, mak-\ning data generation a rather expensive procedure. In contrast,\nour SPIN only requires the initial model itself. Moreover,\nunlike DPO which requires new data source, our method\nexclusively leverages the existing SFT dataset. In Figure 3,\nwe show the performance comparison of SPIN at iterations\n0 and 1 (employing 50k SFT data) with DPO training, from\nthe same SFT checkpoint. We can observe that, while DPO\nleverages more data from new sources, SPIN based on the\nexisting SFT data can already achieve comparable average\nperformance to DPO training at iteration 0. From iteration\n1, SPIN even surpasses the performance of DPO on the\nleaderboard benchmark.\n6.3\nAblation Studies\nIn this subsection, we examine the effect of synthetic dataset\nsize and training epochs within an iteration. Our analysis\ndemonstrates the effectiveness of the synthetic data used by\nSPIN compared to the SFT data, as well as the necessity\nof iterative training in SPIN. In Appendix B, we present\nassessment of SPIN on additional benchmark tasks.\nTraining Size. We investigate the effect of varying training\ndata size on the performance of SPIN. In Figure 5, we\ndemonstrate the effect of training size for SPIN during it-\neration 0 and additionally compare with SFT with the full\noriginal dataset. Specifically, for the SFT baseline, we fully\nfine-tune Mistral-7B on Ultrachat200k for three epochs and\nreport first epoch performance as the starting point (with\nx-axis 0) in the figure for SFT. For SPIN, we report the\nzephyr-7b-sft-full checkpoint as the starting point,\nwhich has also been fine-tuned on Ultrachat200k for one\nepoch. We select the training size of SPIN at iteration 0\nto be 14k, 26k, and 50k and generate the data accordingly,\nensuring that the larger dataset encompasses the smaller\ndataset. The performance of SPIN was then evaluated after\n1 epoch of self-play fine-tuning for each training size. We\ncan observe that, while SPIN results in notable improve-\nment with increasing training sizes, SFT on further epochs 2\nand 3 fails to yield more than 1% improvement. Additional\nresults are deferred to Appendix B.\nIterative Training v.s. Training for More Epochs. We\nfurther study the training within iteration 0 and compare\nwith the performance achieved in iteration 1, particularly\ncontrasting the test performance obtained from extended\ntraining duration with that from next iteration. Figure 4 de-\npicts the performance trajectory of the model trained using\n2https://huggingface.co/datasets/\nHuggingFaceH4/ultrafeedback_binarized\nSPIN over multiple epochs at iteration 0. It is evident that\nthe most substantial improvement occurs during the first\ntwo epochs, followed by only modest gains in subsequent\nepochs. Notably, SPIN exhibits robustness and stability; ex-\ntending the training duration does not diminish performance\nbut rather maintains a rather consistent level. Neverthe-\nless, the observation suggests an inherent limitation to the\nperformance achievable within a single iteration, thereby\nunderscoring the necessity for iterative training. As shown\nby the test performance achieved at iteration 1 in the fig-\nures, extending the training in iteration 0 fails to reach the\nperformance comparable to iteration 1.\n7\nConclusion and Discussion\nThis paper introduces a novel fine-tuning method SPIN, to\nconvert a weak LLM to a strong LLM by unleashing the full\npower of human-annotated data. Central to this method is a\nself-play mechanism, wherein a main player (the LLM) is\nfine-tuned to differentiate the responses of opponent player\n(the LLM from previous iteration) from the target data dis-\ntribution, and the LLM is iteratively aligned with the target\ndata distribution. Therefore, SPIN facilitates the LLM’s\niterative self-evaluation and enhancement through self-play.\nIn comparison to supervised fine-tuning and RL fine-tuning\nmethods, SPIN enables the LLM to self-improve without\nadditional human data or feedback from stronger LLMs.\nEmpirical results demonstrate that SPIN significantly en-\nhances LLM performance across diverse benchmarks, even\noutperforming models trained with additional human data\nor AI feedback.\nLimitation and Future Work.\nOur theoretical results\ndemonstrate that the optimization process of SPIN con-\nverges if and only if the LLM’s distribution aligns with\npdata. Therefore, our study focuses on a fixed target data\ndistribution generated by humans, which inherently imposes\na ceiling on the performance of fine-tuned LLM. Explor-\ning the dynamically changing target data distribution is an\nimportant direction to overcome this limitation and elevate\nthe LLM’s performance beyond this ceiling or even to a\nsuper-human level. Moreover, considering the resource\ndemands of synthetic data generation, another promising\navenue for further exploration is to reduce the volume of\nrequired synthetic data.\nAcknowledgement\nWe thank the anonymous reviewers and area chair for their\nhelpful comments. ZC, YD, HY, KJ, and QG are supported\nin part by the National Science Foundation CAREER Award\n1906169, IIS-2008981, CHE-2247426 and the Sloan Re-\nsearch Fellowship. The views and conclusions contained\nin this paper are those of the authors and should not be\ninterpreted as representing any funding agencies.\n8\n",
  "9": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nArc\nTruthfulQA\nWinogrande\nGSM8k\nHellaswag\nMMLU\nAverage\n0\n10\n20\n30\n40\n50\n60\n70\n80\nScores\nZephyr-SFT\nZephyr-DPO\nSPIN-iter-0\nSPIN-iter-1\nSPIN-iter-2\nSPIN-iter-3\nFigure 3. Performance comparison with DPO training across the six benchmark datasets. Self-play at iteration 0 achieves comparable\nperformance to DPO training with 62k new data. At iteration 1, self-play has already surpassed DPO training on the majority of datasets.\n0\n1\n2\n3\n4\n5\nEpoch\n61\n62\n63\n64\n65\nAccuracy (%)\niter 0\niter 1 (epoch 2)\n(a) Arc Challenge.\n0\n1\n2\n3\n4\n5\nEpoch\n44\n46\n48\n50\n52\n54\nAccuracy (%)\niter 0\niter 1 (epoch 2)\n(b) TruthfulQA.\n0\n1\n2\n3\n4\n5\nEpoch\n58\n59\n60\n61\n62\nAverage Accuracy (%)\niter 0\niter 1 (epoch 2)\n(c) Average.\nFigure 4. The SPIN training dynamics of zephyr-7b-sft-full on the 50k synthetic data with regard to the number of training\nepochs during iteration 0. We can observe that iterative training is pivotal as training for more epochs during iteration 0 reaches a limit\nand cannot surpass iteration 1.\n0\n14k\n26k\n50k\n200k\n400k\nTraining Size\n58.0\n58.5\n59.0\n59.5\n60.0\n60.5\n61.0\nAverage Score\n59.04\n59.82\n59.27\n58.14\n59.55\n60.16\n60.83\nPerformance Comparison\nSFT\nSPIN\nFigure 5. The scaling effect of training size of SPIN compared to\nSFT on the average score of Open LLM Leaderboard. For SPIN,\nwe consider training data of sizes 14k, 26k and 50k where the\nlarger dataset contains the smaller dataset. The starting point for\nSPIN (with x-axis 0) is the zephyr-7b-sft-full checkpoint,\nwhich has been fine-tuned on Ultrachat200k for 1 epoch. We\nreport the model performance trained for 1 epoch with SPIN on\nthe varying sizes of dataset. We additionally compare with SFT,\nwhere we fine-tune Mistral-7B on Ultrachat200k for 3 consecutive\nepochs and report the model performance at the first epoch as the\nstarting point (with x-axis 0).\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Large Language Models. We believe that our work con-\ntribute meaningfully to the field, specifically on leveraging\nsynthetic data to enhance LLM without the requirement for\nhuman preference annotations. The synthetic data generated\nby SPIN may be used to further augment the training of\nvarious language models. Moreover, SPIN demonstrated\na substantial improvement in LLMs’ capabilities, opening\nnew avenues for their application in various downstream\ntasks. This advancement underscores the transformative po-\ntential of LLM fine-tuning in both technological and societal\ncontexts.\nReferences\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\nZ., et al.\nPalm 2 technical report.\narXiv preprint\narXiv:2305.10403, 2023.\nAnthony, T., Tian, Z., and Barber, D. Thinking fast and slow\n9\n",
  "10": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nwith deep learning and tree search. Advances in neural\ninformation processing systems, 30, 2017.\nArjovsky, M., Chintala, S., and Bottou, L. Wasserstein gen-\nerative adversarial networks. In International conference\non machine learning, pp. 214–223. PMLR, 2017.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\net al. Training a helpful and harmless assistant with rein-\nforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022a.\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,\nJones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-\nnon, C., et al. Constitutional ai: Harmlessness from ai\nfeedback. arXiv preprint arXiv:2212.08073, 2022b.\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mor-\ndatch, I. Emergent complexity via multi-agent competi-\ntion. In International Conference on Learning Represen-\ntations, 2018.\nBeeching, E., Fourrier, C., Habib, N., Han, S., Lambert,\nN., Rajani, N., Sanseviero, O., Tunstall, L., and Wolf, T.\nOpen llm leaderboard, 2023.\nbench authors, B. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models.\nTransactions on Machine Learning Research, 2023. ISSN\n2835-8856.\nBengio, Y., Louradour, J., Collobert, R., and Weston, J.\nCurriculum learning. In Proceedings of the 26th annual\ninternational conference on machine learning, pp. 41–48,\n2009.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877–1901, 2020.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y.,\nLundberg, S., et al. Sparks of artificial general intel-\nligence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712, 2023.\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L.,\nAschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M.,\nLeike, J., et al. Weak-to-strong generalization: Eliciting\nstrong capabilities with weak supervision. 2023.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\nCheng, P., Yang, Y., Li, J., Dai, Y., and Du, N. Adversarial\npreference optimization, 2023.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\nStoica, I., and Xing, E. P. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality, March 2023.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg,\nS., and Amodei, D. Deep reinforcement learning from\nhuman preferences. Advances in neural information pro-\ncessing systems, 30, 2017.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, Y., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nCirik, V., Hovy, E., and Morency, L.-P. Visualizing and\nunderstanding curriculum learning for long short-term\nmemory networks. arXiv preprint arXiv:1611.06204,\n2016.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\nCui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G.,\nLiu, Z., and Sun, M. Ultrafeedback: Boosting language\nmodels with high-quality feedback, 2023.\nDao, T.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning. arXiv preprint\narXiv:2307.08691, 2023.\nDeng, Y., Zhang, W., Chen, Z., and Gu, Q. Rephrase and\nrespond: Let large language models ask better questions\nfor themselves. arXiv preprint arXiv:2311.04205, 2023.\nDing, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu,\nZ., Sun, M., and Zhou, B. Enhancing chat language mod-\nels by scaling high-quality instructional conversations.\narXiv preprint arXiv:2305.14233, 2023.\nFrei, S., Zou, D., Chen, Z., and Gu, Q. Self-training converts\nweak learners to strong learners in mixture models. In\nInternational Conference on Artificial Intelligence and\nStatistics, pp. 8003–8021. PMLR, 2022.\n10\n",
  "11": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nFreund, Y. Boosting a weak learning algorithm by majority.\nInformation and computation, 121(2):256–285, 1995.\nFreund, Y. and Schapire, R. E. A decision-theoretic general-\nization of on-line learning and an application to boosting.\nJournal of computer and system sciences, 55(1):119–139,\n1997.\nGao, L., Schulman, J., and Hilton, J. Scaling laws for reward\nmodel overoptimization. In International Conference on\nMachine Learning, pp. 10835–10866. PMLR, 2023a.\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H.,\nMcDonell, K., Muennighoff, N., Ociepa, C., Phang, J.,\nReynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L.,\nTang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A\nframework for few-shot language model evaluation, 12\n2023b.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.\nGenerative adversarial nets. Advances in neural informa-\ntion processing systems, 27, 2014.\nGrandvalet, Y. and Bengio, Y. Semi-supervised learning by\nentropy minimization. Advances in neural information\nprocessing systems, 17, 2004.\nGugger, S., Debut, L., Wolf, T., Schmid, P., Mueller, Z.,\nMangrulkar, S., Sun, M., and Bossan, B. Accelerate:\nTraining and inference at scale made simple, efficient and\nadaptable., 2022.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and\nCourville, A. C. Improved training of wasserstein gans.\nAdvances in neural information processing systems, 30,\n2017.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J.\nMeasuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\nHernandez-Leal, P., Kartal, B., and Taylor, M. E. Is mul-\ntiagent deep reinforcement learning the answer or the\nquestion? a brief survey. learning, 21:22, 2018.\nHinton, G., Srivastava, N., and Swersky, K. Neural networks\nfor machine learning lecture 6a overview of mini-batch\ngradient descent. Cited on, 14(8):2, 2012.\nHo, J. and Ermon, S. Generative adversarial imitation learn-\ning. Advances in neural information processing systems,\n29, 2016.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nJolicoeur-Martineau, A. The relativistic discriminator: a\nkey element missing from standard gan. arXiv preprint\narXiv:1807.00734, 2018.\nJosifoski, M., Sakota, M., Peyrard, M., and West, R. Ex-\nploiting asymmetry for synthetic training data generation:\nSynthie and the case of information extraction. arXiv\npreprint arXiv:2303.04132, 2023.\nKearns, M. and Valiant, L. Cryptographic limitations on\nlearning boolean formulae and finite automata. Journal\nof the ACM (JACM), 41(1):67–95, 1994.\nKou, Y., Chen, Z., Cao, Y., and Gu, Q. How does semi-\nsupervised learning with pseudo-labelers work? a case\nstudy.\nIn The Eleventh International Conference on\nLearning Representations, 2022.\nKumar, M., Packer, B., and Koller, D. Self-paced learning\nfor latent variable models. Advances in neural informa-\ntion processing systems, 23, 2010.\nLanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A.,\nTuyls, K., Pérolat, J., Silver, D., and Graepel, T. A uni-\nfied game-theoretic approach to multiagent reinforcement\nlearning. Advances in neural information processing\nsystems, 30, 2017.\nLee, D.-H. Pseudo-label: The simple and efficient semi-\nsupervised learning method for deep neural networks. In\nICML Challenges in Representation Learning Workshop,\n2013.\nLee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T.,\nBishop, C., Carbune, V., and Rastogi, A. Rlaif: Scal-\ning reinforcement learning from human feedback with ai\nfeedback. arXiv preprint arXiv:2309.00267, 2023.\nLee, Y. J. and Grauman, K. Learning the easy things first:\nSelf-paced visual category discovery. In CVPR 2011, pp.\n1721–1728. IEEE, 2011.\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E.,\nMichalewski, H., Ramasesh, V., Slone, A., Anil, C.,\nSchlag, I., Gutman-Solo, T., et al. Solving quantitative\nreasoning problems with language models. Advances in\nNeural Information Processing Systems, 35:3843–3857,\n2022.\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J.,\nLeblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago,\nA., et al. Competition-level code generation with alpha-\ncode. Science, 378(6624):1092–1097, 2022.\n11\n",
  "12": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nLi, Y., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar,\nS., and Lee, Y. T. Textbooks are all you need ii: phi-1.5\ntechnical report, 2023.\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. arXiv preprint\narXiv:2109.07958, 2021.\nLiu, B., Bubeck, S., Eldan, R., Kulkarni, J., Li, Y., Nguyen,\nA., Ward, R., and Zhang, Y. Tinygsm: achieving> 80%\non gsm8k with small language models. arXiv preprint\narXiv:2312.09241, 2023.\nLiu, C., He, S., Liu, K., Zhao, J., et al. Curriculum learning\nfor natural answer generation. In IJCAI, pp. 4223–4229,\n2018.\nLiu, F., Ge, S., and Wu, X. Competence-based multimodal\ncurriculum learning for medical report generation. In Pro-\nceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Vol-\nume 1: Long Papers), pp. 3001–3012, 2021.\nLuo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C.,\nGeng, X., Lin, Q., Chen, S., and Zhang, D. Wizard-\nmath: Empowering mathematical reasoning for large lan-\nguage models via reinforced evol-instruct. arXiv preprint\narXiv:2308.09583, 2023.\nMao, X., Li, Q., Xie, H., Lau, R. Y., Wang, Z., and\nPaul Smolley, S. Least squares generative adversarial\nnetworks. In Proceedings of the IEEE international con-\nference on computer vision, pp. 2794–2802, 2017.\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a\nsuit of armor conduct electricity? a new dataset for open\nbook question answering. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language\nProcessing, pp. 2381–2391, 2018.\nMishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Cross-\ntask generalization via natural language crowdsourcing\ninstructions. arXiv preprint arXiv:2104.08773, 2021.\nMroueh, Y. and Sercu, T. Fisher gan. Advances in neural\ninformation processing systems, 30, 2017.\nMüller, A. Integral probability metrics and their generating\nclasses of functions. Advances in applied probability, 29\n(2):429–443, 1997.\nMuller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Perolat,\nJ., Liu, S., Hennes, D., Marris, L., Lanctot, M., Hughes,\nE., et al. A generalized training approach for multiagent\nlearning. arXiv preprint arXiv:1909.12823, 2019.\nOpenAI. Gpt-4 technical report, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730–27744, 2022.\nPrasad, A., Stengel-Eskin, E., and Bansal, M. Rephrase, aug-\nment, reason: Visual grounding of questions for vision-\nlanguage models.\narXiv preprint arXiv:2310.05861,\n2023.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\nC. D., and Finn, C. Direct preference optimization: Your\nlanguage model is secretly a reward model. arXiv preprint\narXiv:2305.18290, 2023.\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero:\nMemory optimizations toward training trillion parameter\nmodels. In SC20: International Conference for High Per-\nformance Computing, Networking, Storage and Analysis,\npp. 1–16. IEEE, 2020.\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,\nTan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950, 2023.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. Communications of the ACM, 64(9):99–106,\n2021.\nSamuel, A. L. Some studies in machine learning using\nthe game of checkers.\nIBM Journal of research and\ndevelopment, 3(3):210–229, 1959.\nSamuel, A. L. Some studies in machine learning using\nthe game of checkers.\nIBM Journal of research and\ndevelopment, 44(1.2):206–226, 2000.\nSaunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J.,\nand Leike, J. Self-critiquing models for assisting human\nevaluators. arXiv preprint arXiv:2206.05802, 2022.\nSchapire, R. E. The strength of weak learnability. Machine\nlearning, 5:197–227, 1990.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,\nM., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-\npel, T., et al. Mastering chess and shogi by self-play\nwith a general reinforcement learning algorithm. arXiv\npreprint arXiv:1712.01815, 2017a.\n12\n",
  "13": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou,\nI., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\nBolton, A., et al. Mastering the game of go without\nhuman knowledge. nature, 550(7676):354–359, 2017b.\nSingh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil,\nP., Liu, P. J., Harrison, J., Lee, J., Xu, K., Parisi, A.,\net al.\nBeyond human data: Scaling self-training for\nproblem-solving with language models. arXiv preprint\narXiv:2312.06585, 2023.\nSoviany, P., Ionescu, R. T., Rota, P., and Sebe, N. Cur-\nriculum learning: A survey. International Journal of\nComputer Vision, 130(6):1526–1565, 2022.\nSpitkovsky, V. I., Alshawi, H., and Jurafsky, D. Baby steps:\nHow “less is more” in unsupervised dependency parsing.\nIn NIPS 2009 Workshop on Grammar Induction, Repre-\nsentation of Language and Language Learning, 2009.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\nVoss, C., Radford, A., Amodei, D., and Christiano,\nP. F. Learning to summarize with human feedback. Ad-\nvances in Neural Information Processing Systems, 33:\n3008–3021, 2020.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Stanford\nalpaca: An instruction-following llama model, 2023.\nTesauro, G. et al. Temporal difference learning and td-\ngammon. Communications of the ACM, 38(3):58–68,\n1995.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y., et al. Lamda: Language models for dialog appli-\ncations. arXiv preprint arXiv:2201.08239, 2022.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nTunstall, L., Beeching, E., Lambert, N., Rajani, N., Ra-\nsul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier,\nC., Habib, N., et al. Zephyr: Direct distillation of lm\nalignment. arXiv preprint arXiv:2310.16944, 2023a.\nTunstall, L., Beeching, E., Lambert, N., Rajani, N., Rush,\nA. M., and Wolf, T. The alignment handbook, 2023b.\nVapnik, V. The nature of statistical learning theory. Springer\nscience & business media, 1999.\nVictor, S., Albert, W., Colin, R., Stephen, B., Lintang, S.,\nZaid, A., Antoine, C., Arnaud, S., Arun, R., Manan, D.,\net al. Multitask prompted training enables zero-shot task\ngeneralization. In International Conference on Learning\nRepresentations, 2022.\nVinyals, O., Babuschkin, I., Chung, J., Mathieu, M.,\nJaderberg, M., Czarnecki, W., Dudzik, A., Huang, A.,\nGeorgiev, P., Powell, R., Ewalds, T., Horgan, D., Kroiss,\nM., Danihelka, I., Agapiou, J., Oh, J., Dalibard, V.,\nChoi, D., Sifre, L., Sulsky, Y., Vezhnevets, S., Molloy,\nJ., Cai, T., Budden, D., Paine, T., Gulcehre, C., Wang,\nZ., Pfaff, T., Pohlen, T., Yogatama, D., Cohen, J., McK-\ninney, K., Smith, O., Schaul, T., Lillicrap, T., Apps, C.,\nKavukcuoglu, K., Hassabis, D., and Silver, D. AlphaS-\ntar: Mastering the Real-Time Strategy Game StarCraft II,\n2019.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\nChi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought\nprompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:\n24824–24837, 2022.\nWu, J., Liang, Y., Akbari, H., Wang, Z., Yu, C., et al. Scaling\nmultimodal pre-training via cross-modality gradient har-\nmonization. Advances in Neural Information Processing\nSystems, 35:36161–36173, 2022.\nXu, J., Lee, A., Sukhbaatar, S., and Weston, J.\nSome\nthings are more cringe than others: Preference opti-\nmization with the pairwise cringe loss. arXiv preprint\narXiv:2312.16682, 2023.\nYang, Y., Singh, A. K., Elhoushi, M., Mahmoud, A., Tiru-\nmala, K., Gloeckle, F., Rozière, B., Wu, C.-J., Morcos,\nA. S., and Ardalani, N. Decoding data quality via syn-\nthetic corruptions: Embedding-guided pruning of code\ndata. arXiv preprint arXiv:2312.02418, 2023.\nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok,\nJ. T., Li, Z., Weller, A., and Liu, W. Metamath: Boot-\nstrap your own mathematical questions for large language\nmodels. arXiv preprint arXiv:2309.12284, 2023.\nYuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J.,\nand Weston, J. Self-rewarding language models. arXiv\npreprint arXiv:2401.10020, 2024.\nYuan, Z., Yuan, H., Li, C., Dong, G., Tan, C., and Zhou,\nC. Scaling relationship on learning mathematical rea-\nsoning with large language models.\narXiv preprint\narXiv:2308.01825, 2023.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sentence?\narXiv preprint arXiv:1905.07830, 2019.\n13\n",
  "14": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nZhang, D., Meng, D., Li, C., Jiang, L., Zhao, Q., and Han,\nJ. A self-paced multiple-instance learning framework\nfor co-saliency detection. In Proceedings of the IEEE\ninternational conference on computer vision, pp. 594–\n602, 2015.\nZhang, X., Kumar, G., Khayrallah, H., Murray, K., Gwin-\nnup, J., Martindale, M. J., McNamee, P., Duh, K., and\nCarpuat, M. An empirical exploration of curriculum\nlearning for neural machine translation. arXiv preprint\narXiv:1811.00739, 2018.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. arXiv\npreprint arXiv:2306.05685, 2023.\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,\nA., Amodei, D., Christiano, P., and Irving, G. Fine-tuning\nlanguage models from human preferences. arXiv preprint\narXiv:1909.08593, 2019.\n14\n",
  "15": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nA\nFurther Related Work\nCurriculum Learning. In deep learning, it has been observed that training models using data samples arranged in a\nstrategically meaningful order can lead to improved performance compared to training on randomly shuffled data. This\napproach is commonly known as curriculum learning (Bengio et al., 2009; Soviany et al., 2022). Initial studies in curriculum\nlearning introduced efficient algorithms that adhere to an ‘easy-to-hard’ progression (Spitkovsky et al., 2009; Kumar et al.,\n2010; Lee & Grauman, 2011; Zhang et al., 2015). In the field of Natural Language Processing (NLP), criteria such as\nsentence length and term frequency are commonly utilized (Cirik et al., 2016; Zhang et al., 2018; Liu et al., 2018). More\nrecent developments include the application of curriculum learning algorithms in multi-modal learning (Liu et al., 2021; Wu\net al., 2022). Our work shares a similar idea to curriculum learning, wherein the training data evolves iteratively—beginning\nwith responses that are easy to distinguish from human-annotated data and gradually progressing to more challenging\ninstances.\nGenerative Adversarial Networks. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) represent a\ndistinct class of generative models, characterized by their unique adversarial process. To enhance training stability and data\nquality, Mao et al. (2017) introduced the Least Squares GAN, employing a least squares loss function for the discriminator.\nA significant advancement in GANs involves the use of Integral Probability Metrics (IPM) (Müller, 1997), particularly\nhighlighted in the development of Wasserstein GAN by Arjovsky et al. (2017). This model employs IPM in its loss design,\nenhancing training stability. Since then, IPMs have become crucial in GAN design (Mroueh & Sercu, 2017; Gulrajani et al.,\n2017), particularly in constraining the discriminator to a specific function class, thereby preventing it from overpowering the\ngenerator. Furthermore, Jolicoeur-Martineau (2018) generalized IPM-based GANs by introducing relativistic discriminator\nand proposed Relativistic GAN. It is worth noting that the objective function defined in our (4.2) is similar to Relativistic\nGAN (Jolicoeur-Martineau, 2018) and reduces to an IPM framework in Wasserstein GAN (Arjovsky et al., 2017) with a\nlinear loss. However, our approach differs in both the choice of the function class and the training procedure. Inspired by\nGAN, Cheng et al. (2023) proposed an adversarial learning framework named Adversarial Preference Optimization (APO)\nthat trains the LLM and a reward model in an adversarial game. Similarly related to the spirit of our method, Generative\nAdversarial Imitation Learning (GAIL) (Ho & Ermon, 2016) was proposed to train separate discriminator and policy\nnetworks in each iteration. In contrast to the above methods, SPIN relies on self-play where both the main player and the\nopponent player are the same LLM from two consecutive iterations.\nAlignment with AI Feedback. The objective of alignment is to fine-tune LLMs to align with human intentions. In addition\nto using human demonstrations, AI feedback is emerging as a crucial component in the alignment process. Constitutional AI\n(Bai et al., 2022b) leveraged AI feedback to align language models through a combination of both supervised learning and\nreinforcement learning (RL) phases. In the RL phase, (Bai et al., 2022b) applied Reinforcement Learning from AI Feedback\n(RLAIF), training a reward model using AI-generated preferences, followed by RL using the reward. Lee et al. (2023)\ndemonstrated that AI feedback can achieve comparable or superior performance to human feedback in RL fine-tuning. They\nalso demonstrated that RLAIF can improve upon an SFT policy even when the LLM labeler has the same size as the policy.\nSaunders et al. (2022) studied the scaling properties of self-critique and introduced a framework for comparing the critique\nability to generation and discrimination ability. Self-critique models employ the LLM itself to generate natural language\ncritiques through behavioral cloning, assisting human evaluators.\nB\nExperiments\nB.1\nHyperparameters and Implementation Details\nTable 1. Detailed information of HuggingFace Open LLM Leaderboard. For each evaluation dataset, we present the number of few-shot\nexamples and metric adopted for evaluation.\nDatasets\nArc\nTruthfulQA\nWinogrande\nGSM8k\nHellaSwag\nMMLU\n# few-shot\n25\n0\n5\n5\n10\n5\nMetric\nacc_norm\nmc2\nacc\nacc\nacc_norm\nacc\nWe use the Alignment Handbook library (Tunstall et al., 2023b) as the codebase for our self-play fine-tuning method SPIN,\nwhich includes DeepSpeed ZeRO-3 (Rajbhandari et al., 2020) and FlashAttention-2 (Dao, 2023) to reduce training cost. We\ntrain our models with RMSProp (Hinton et al., 2012) optimizer with no weight decay for all iterations as commonly used\nin fine-tuning LLMs for alignment, with a global batch size of 64, 10% warmup steps and bfloat16 precision. We set the\npeak learning rate to be 5e-7 for iterations 0 and 1, and decay this peak learning rate to 1e-7 for iteration 2 and 3 as we are\n15\n",
  "16": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\napproaching the end of self-play fine-tuning. Lastly, we choose β = 0.1 and max sequence length to be 2048 tokens as in\nTunstall et al. (2023b). We note that at the last iteration (iter-3) where the model is close to convergence, we increase the\nvalue of β to 5.0. We use the Accelerate library (Gugger et al., 2022) to generate our synthetic data using distributed inference\nwith multiple GPUs with a global batch size of 64. We consider the prompting template “### Instruction: {prompt}\\n\\n###\nResponse: ” as commonly used in Taori et al. (2023). For Ultrachat200k containing multi-round conversations, we only\nsample the first round as our prompt and ground truth completion pairs.\nB.2\nTraining Overhead\nThe cost overhead introduced by SPIN is mostly the generation of synthetic data from the LLM that we train. The cost of\nthe fine-tuning process remains computationally equal to that of SFT and DPO. We report both the generation and training\ntime for SPIN in Table 2 . Results were obtained using a machine with 8xA100 (80G) GPUs. For per 64 examples, the\ngeneration time and training time are 6.69s and 10s respectively.\nTable 2. Generation and Training Times for Different Iterations\nIteration\nIter 0\nIter 1\nIter 2\nIter 3\nProcess\nGeneration\nTraining\nGeneration\nTraining\nGeneration\nTraining\nGeneration\nTraining\nTime\n1.45h\n4.32h\n1.45h\n8.64h\n1.45h\n8.64h\n1.45h\n8.64h\nIt is evident that the generation time is dominated by the training time at each iteration. The estimated time in Table 2 is\nbased on the fact that we generate 50k examples per iteration. Please note that the doubled training time from iter 1 to iter 3\nis attributed to the utilization of a double-sized dataset (the combination of 50k synthetic data from the previous iteration\nand 50k synthetic data in the current iteration), as discussed in our Section 6.1.\nB.3\nAdditional Experiment Result for SPIN+DPO\nSPIN requires only the SFT data to improve over the traditional SFT stage and can sit between SFT and RL finetuning.\nSuppose additional preference data is provided, we can use the additional data to further improve the performance of the\nmodel after SPIN using RL fine-tuning.\nStarting at SPIN iteration 3, we further train the model with DPO for two epochs on the 62k preference data from the\nUltraFeedback Binarized dataset (Cui et al., 2023), which consists of both chosen and rejected responses evaluated by\nGPT-4. Detailed performances are presented in Table 3.\nTable 3. Performance of SPIN + DPO based on zephyr-7b-sft-full across HuggingFace Open LLM Leaderboard datasets,\ncompared with all baselines. We also denote the average improvement over last iteration in the Average column.\nModel\nArc\nTruthfulQA\nWinogrande\nGSM8k\nHellaSwag\nMMLU\nAverage\nzephyr-7b-dpo-full\n63.65\n55.19\n72.61\n33.43\n84.44\n58.52\n61.31\nzephyr-7b-sft-full\n60.41\n43.73\n74.19\n26.76\n82.85\n60.92\n58.14\nSPIN iteration 0\n63.40\n49.18\n72.69\n35.10\n84.38\n60.03\n60.80(+2.66)\nSPIN iteration 1\n65.19\n55.17\n72.30\n35.78\n84.96\n59.34\n62.12(+1.32)\nSPIN iteration 2\n65.96\n54.91\n73.56\n38.06\n85.41\n59.93\n62.97(+0.85)\nSPIN iteration 3\n65.87\n54.90\n73.72\n38.97\n85.54\n59.99\n63.16(+0.19)\nSPIN iteration 3 + DPO\n66.47\n60.07\n78.06\n37.98\n86.17\n59.68\n64.05(+0.89)\nWe can observe that the checkpoint trained by SPIN can be further improved using DPO, yielding an enhancement of 0.89%\non average. Notably, the improvement is particularly significant on the TruthfulQA benchmark with around 5%.\nB.4\nFurther Experiment Results\nIn Table 4, we show the detailed performance of SPIN at different iterations on each of the task in Open LLM Leaderboard.\nIn Table 5, we also show the performance of SFT from zephyr-7b-sft-full on Ultrachat200k for one epoch. While\nself-play fine-tuning with synthetic data from zephyr-7b-sft-full effectively improves its performance, simply\nfine-tuning it again on the SFT data leads to degraded performance, as similarly observed in Figure 5.\nFurther Investigation on More Tasks. Here, we further investigate the performance of SPIN on a broader variety of\n16\n",
  "17": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nTable 4. Test performance of SPIN based on zephyr-7b-sft-full across HuggingFace Open LLM Leaderboard datasets. We also\ndenote the average improvement over last iteration in the Average column.\nModel\nArc\nTruthfulQA\nWinogrande\nGSM8k\nHellaSwag\nMMLU\nAverage\nzephyr-7b-sft-full\n60.41\n43.73\n74.19\n26.76\n82.85\n60.92\n58.14\nSPIN iteration 0\n63.40\n49.18\n72.69\n35.10\n84.38\n60.03\n60.80(+2.66)\nSPIN iteration 1\n65.19\n55.17\n72.30\n35.78\n84.96\n59.34\n62.12(+1.32)\nSPIN iteration 2\n65.96\n54.91\n73.56\n38.06\n85.41\n59.93\n62.97(+0.85)\nSPIN iteration 3\n65.87\n54.90\n73.72\n38.97\n85.54\n59.99\n63.16(+0.19)\nTable 5. Test performance of zephyr-7b-sft-full fine-tuned on Ultrachat200k for 1 more epoch across HuggingFace Open LLM\nbenchmark datasets. SFT fails to further leverage the fine-tuning data for performance enhancement and even results in degraded\nperformance.\nModel\nArc\nTruthfulQA\nWinogrande\nGSM8k\nHellaSwag\nMMLU\nAverage\nzephyr-7b-sft-full\n60.41\n43.73\n74.19\n26.76\n82.85\n60.92\n58.14\nSFT epoch 1\n57.76\n44.39\n75.77\n25.85\n81.69\n57.89\n57.23\ntasks, including MT-Bench (Zheng et al., 2023), Big-Bench (bench authors, 2023) and OpenBookQA (Mihaylov et al.,\n2018) in addition to the Open LLM Leaderboard tasks. Specifically, we use the following tasks from Big-Bench-Hard for\na more comprehensive evaluation, including Causal Judgment (causal reasoning), Sports Understanding (commonsense\nreasoning) and Formal Fallacies (logical reasoning). In Table 6, we show the resulting scores of SPIN on MT-Bench as\nwell as those tasks from Big-Bench. In Figure 6, we detail the model performances on MT-Bench with regard to different\ntypes of questions. We can see a notably robust improvement in the performance of SPIN on various tasks besides the\nHuggingFace Benchmark, without major degradation. Notably, on MT-Bench, the model fine-tuned by SPIN has surpassed\nthe performance of vicuna-13b-v1.5 (Chiang et al., 2023) with a score of 6.57.\nTable 6. Test performance on other reasoning benchmark datasets for SPIN at different iterations and zephyr-7b-sft-full. We\nreport the average score for MT-Bench and the accuracy score for Big Bench datasets under standard few-shot CoT evaluation. On\nOpenBookQA, we report acc_norm with 1-shot example as used in Anil et al. (2023). As similar to Open LLM Leaderboard evaluation,\nwe observe a steady improvement in performance on the other benchmark tasks, with no significant degradation.\nModel\nMT-Bench\nBB-causal\nBB-formal\nBB-sports\nOpenBookQA\nzephyr-7b-sft-full\n5.94\n56.15\n49.6\n96.0\n45.4\nSPIN iteration 0\n6.46(+0.52)\n57.75\n51.6\n95.2\n46.8\nSPIN iteration 1\n6.65(+0.19)\n58.82\n51.2\n95.2\n47.2\nSPIN iteration 2\n6.78(+0.13)\n59.36\n51.2\n94.4\n47.6\nB.5\nGeneration Examples\nIn Tables 7 and 8, we further provide the generation examples of our fine-tuned model by SPIN from different iterations.\nWe can observe an improvement in response quality as compared to the generation of the SFT checkpoint. Meanwhile,\nthe model generations at higher iterations typically becomes more concise than iteration 0 and resemble the ground truth\ncompletion better.\n17\n",
  "18": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nWriting\nRoleplay\nReasoning\nMath\nCoding\nExtraction\nSTEM\nHumanities\n0 1 2 3 4 5 6 7 8 9\nmodel\nSPIN iter-2\nSPIN iter-1\nSPIN iter-0\nSFT\nFigure 6. Model performance on MT-Bench. We compare SPIN across different iterations with the base SFT model. Starting from\niteration 1, our fine-tuned model by SPIN robustly outperforms the SFT checkpoint on all evaluation aspects.\nTable 7. Generation example of our fine-tuned model by SPIN at different iterations.\nPrompt\nHow does the economic growth of Horsham compare to nearby towns?\nGround truth\nI do not have access to the latest economic data on horsham and nearby towns. However,\nyou can check the latest economic statistics and reports provided by the local government\nor the official websites of horsham and nearby towns for a detailed comparison of their\neconomic growth.\nzephyr-7b=sft-full\nAccording to the given material, Horsham was growing \"significantly\" faster than nearby\ntowns, indicating that it was economically thriving.\\n However, the given material does\nnot specify the exact rates or levels of economic growth of Horsham compared to nearby\ntowns. Without this information, it is challenging to determine whether the differences in\neconomic growth between Horsham and nearby towns were significant or minimal.\nIteration 0\nI do not have access to the latest economic data for horsham and nearby towns. However,\nhorsham is known to be a prosperous town in west sussex, uk, with a diverse economy that\nincludes agriculture, manufacturing, and services. The town has a strong retail sector, and\nit is also home to several large companies, including the global pharmaceutical company\nnovartis. In terms of comparison with nearby towns, horsham is generally considered\nto be one of the more affluent towns in the region. However, the economic growth of\nhorsham and its neighboring towns may vary depending on various factors such as industry,\ninvestment, and government policies.\nIteration 1\nI do not have access to real-time data or information about the most recent economic\ngrowth of horsesham and nearby towns. However, according to the recent reports from\nvarious sources such as the office for national statistics, horsesham’s economy primarily\nrelies on retail, healthcare, education, and public administration industries. Compared to\nnearby towns such as lewes, chichester, and bognor regis, horsesham’s economy may have\na different composition and growth rate based on their specific industries and economic\npolicies.\nIteration 2\nI don’t have access to the most recent economic data. However, according to a 2019 report\nby pwc, horsham’s expected economic growth up to 2025 was projected to be around\n2.3% per annum, which is slightly below the average for the south east of england (2.8%).\nNeighbouring towns like crawley and bognor regis were projected to experience similar\ngrowth rates. note: this may not reflect the impact of the covid-19 pandemic that occurred\nin 2020.\n18\n",
  "19": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nTable 8. Another generation example of our fine-tuned model by SPIN at different iterations.\nPrompt\nCan you clarify if Poppelsdorfer Allee 45 and the old site of the Mathematical Institute\nare the same location? Answer according to: Go straight to Poppelsdorfer Allee 45 and\nsign-in.\\n There is a big lecture hall at the old site of the Mathematical Institute (Wegelerstr.\n10, Bonn).\nGround truth\nNo, Poppelsdorfer Allee 45 and the old site of the Mathematical Institute (Wegelerstr. 10,\nBonn) are not the same location. To sign-in, one should go straight to Poppelsdorfer Allee\n45. However, there is a big lecture hall at the old site of the Mathematical Institute.\nzephyr-7b=sft-full\nYes, Poppelsdorfer Allee 45 and the old site of the Mathematical Institute are the same\nlocation.\nIteration 0\nYes, Poppelsdorfer Allee 45 and the old site of the Mathematical Institute (Wegelerstr. 10,\nBonn) are the same location. The Mathematical Institute has been relocated and renamed\nas the Institute for Mathematics and Theoretical Physics, but the building where it used to\nbe is still referred to as the old site. The big lecture hall at the old site still exists and may\nbe used for events or classes.\nIteration 1\nNo, Poppelsdorfer Allee 45 and the old site of the Mathematical Institute are not the\nsame location. While Poppelsdorfer Allee 45 is where attendees should sign-in, the old\nsite of the Mathematical Institute (located at Wegelerstr. 10, Bonn) has a large lecture\nhall mentioned in the statement. Therefore, it can be assumed that the two locations are\nseparate and not the same.\nIteration 2\nPoppelsdorfer Allee 45 and the old site of the Mathematical Institute (Wegelerstr. 10, Bonn)\nare two different locations. As per the instruction, you need to go straight to Poppelsdorfer\nAllee 45 and sign-in. However, there is a lecture hall mentioned at the old site of the\nMathematical Institute (Wegelerstr. 10, Bonn).\nC\nProof of Theorems in Section 5\nC.1\nProof of Theorem 5.2\nProof of Theorem 5.2. To begin with, we prove the “Sufficiency” in Theorem 5.2. Since pdata(·|x) = pθt(·|x), by symmetry\nproperty of y and y′, we have for any θ ∈Θ that\n2LSPIN(θ, θt) = Ex∼q(·),y∼pdata(·|x),y′∼pθt(·|x)\n\u0014\nℓ\n\u0012\nγ log pθ(y|x)\npθt(y|x) −γ log pθ(y′|x)\npθt(y′|x)\n\u0013\u0015\n+ Ex∼q(·),y′∼pdata(·|x),y∼pθt(·|x)\n\u0014\nℓ\n\u0012\nγ log pθ(y|x)\npθt(y|x) −γ log pθ(y′|x)\npθt(y′|x)\n\u0013\u0015\n= Ex∼q(·),y∼pdata(·|x),y′∼pθt(·|x)\n\u0014\nℓ\n\u0012\nγ log pθ(y|x)\npθt(y|x) −γ log pθ(y′|x)\npθt(y′|x)\n\u0013\n+ ℓ\n\u0012\nγ log pθ(y′|x)\npθt(y′|x) −γ log pθ(y|x)\npθt(y|x)\n\u0013\u0015\n≥2Ex∼q(·),y∼pdata(·|x),y′∼pθt(·|x)\n\u0014\nℓ\n\u0012γ\n2 log pθ(y|x)\npθt(y|x) −γ\n2 log pθ(y′|x)\npθt(y′|x)\n+ γ\n2 log pθ(y′|x)\npθt(y′|x) −γ\n2 log pθ(y|x)\npθt(y|x)\n\u0013\u0015\n= 2ℓ(0),\nwhere the inequality is due to Jensen’s inequality (recalling that ℓis convex in Assumption 5.1). Therefore, we have that\nLSPIN(θ, θt) ≥ℓ(0) = LSPIN(θt, θt), which means that θt is the global optimum of (4.7). As a consequence, the gradient\nat the point θt is zero, which concludes θt+1 = θt.\n19\n",
  "20": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nNext, we prove the “Necessity”. Define g(λ) as follows:\ng(λ) = Ex∼q(·),y∼pdata(·|x),y′∼pθt(·|x)\n\u0014\nℓ\n\u0012\nλ log pdata(y|x)\npθt(y|x) −λ log pdata(y′|x)\npθt(y′|x)\n\u0013\u0015\n.\nThen we have g(0) = ℓ(0) and\ng′(0) = Ex∼q(·),y∼pdata(·|x),y′∼pθt(·|x)\n\u0014\nℓ′(0)\n\u0012\nlog pdata(y|x)\npθt(y|x) −log pdata(y′|x)\npθt(y′|x)\n\u0013\u0015\n= ℓ′(0)\n\u0012\nEx∼q(·),y∼pdata(·|x)\n\u0014\nlog pdata(y|x)\npθt(y|x)\n\u0015\n−Ex∼q(·),y′∼pθt(·|x)\n\u0014\nlog pdata(y′|x)\npθt(y′|x)\n\u0015\u0013\n= ℓ′(0)\nh\nKL\n\u0000pdata(·|x)\n\f\f\f\fpθt(·|x)\n\u0001\n+ KL\n\u0000pθt(·|x)\n\f\f\f\fpdata(·|x)\n\u0001i\n< 0,\nwhere the last inequality is due to the condition that ℓ′(0) < 0. Therefore, there exist a λ0 such that for all 0 < λ < λ0, we\nhave g(λ) < ℓ(0). Choose θ∗such that pθ∗(y|x) = pdata(y|x). For those 0 < λ < λ0, we have that\nLSPIN(θ∗, θt) = Ex∼q(·),y∼pθ∗(·|x),y′∼pθt(·|x)\n\u0014\nℓ\n\u0012\nλ log pθ∗(y|x)\npθt(y|x) −λ log pθ∗(y′|x)\npθt(y′|x)\n\u0013\u0015\n= Ex∼q(·),y∼pdata(·|x),y′∼pθt(·|x)\n\u0014\nℓ\n\u0012\nλ log pdata(y|x)\npθt(y|x) −λ log pdata(y′|x)\npθt(y′|x)\n\u0013\u0015\n= g(λ)\n< g(0)\n= LSPIN(θt, θt),\nwhere the second equality holds by the choice of pθ∗(·|x), and the inequality holds due to the choice of λ. Therefore, we\nconclude that θt is not the global optimum of (4.7) if pθt(·|x) ̸= pdata(·|x).\nC.2\nProof Theorem 5.4\nWe need the following auxiliary lemma before we prove Theorem 5.4.\nLemma C.1. Suppose that ℓ(t) = log(1 + exp(−t)) and for a, b > 0, the following inequality holds\naℓ(t) + bℓ(−t) ≥a log(1 + b/a) + b log(1 + a/b),\nthe equality holds if and only if t = log(a/b).\nProof of Lemma C.1. Define g(t) = aℓ(t) + bℓ(−t) = a log(1 + exp(−t)) + b log(1 + exp(t)), then we have\ng′(t) = −a exp(−t)\n1 + exp(−t) +\nb exp(t)\n1 + exp(t) = −a + b exp(t)\n1 + exp(t)\n.\nTherefore, g′(t) < 0 when t < log(a/b), g′(t) > 0 when t > log(a/b), which indicates that g achieves it minimum at\nt = log(a/b) which concludes the proof.\nLemma C.1 shows that the global minimum of aℓ(t) + bℓ(−t) is achieved when t = log(a/b). Based on Lemma C.1, we\ncan further prove that (4.2) with the logistic loss function has a closed-form solution if we ignore the constraint set Ft.\nLemma C.2. Denote p+(y, y′, x) = q(x) · pdata(y|x) · pθt(y′|x) and p−(y, y′, x) = q(x) · pθt(y′|x) · pdata(y|x),\nEx∼q(·),y∼pdata(·|x),y′∼pθt(·|x)\n\u0002\nℓ\n\u0000f(x, y) −f(x, y′)\n\u0001\u0003\n≥log 2 −JSD(p+∥p−),\nwhere JSD(p+∥p−) represents the Jensen–Shannon divergence which is defined as follows\nJSD\n\u0010\np\n\r\r\rq\n\u0011\n= 1\n2KL\n\u0010\np\n\r\r\rp + q\n2\n\u0011\n+ 1\n2KL\n\u0010\nq\n\r\r\rp + q\n2\n\u0011\n,\n20\n",
  "21": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\nwhere KL(·∥·) is KL-divergence. JSD is always non-negative and equals zero if and only if p+ and p−are identical.\nMoreover, the global minimum value log 2 −JSD(p+∥p−) is achieved by f ∗if and only if,\nf ∗(x, y) = Z(x) + log\n\u0012pdata(y|x)\npθt(y|x)\n\u0013\n,\nwhere Z(x) is any function that is possibly dependent on x.\nProof of Lemma C.2. We rewrite the objective function in the following formula,\n2Ex∼q(·),y∼pdata(·|x),y′∼pθt(·|x)\n\u0002\nℓ\n\u0000f(x, y) −f(x, y′)\n\u0001\u0003\n=\nZ\nq(x)pdata(y|x)pθt(y′|x)\n\u0002\nℓ\n\u0000f(x, y) −f(x, y′)\n\u0001\u0003\ndydy′\n+\nZ\nq(x)pdata(y′|x)pθt(y|x)\n\u0002\nℓ\n\u0000f(x, y′) −f(x, y)\n\u0001\u0003\ndydy′\n=\nZ\nq(x)pdata(y|x)pθt(y′|x)ℓ\n\u0000f(x, y) −f(x, y′)\n\u0001\n+ q(x)pdata(y′|x)pθt(y|x)ℓ\n\u0000f(x, y′) −f(x, y)\n\u0001\ndydy′\n(i)\n≥\nZ\nq(x)pdata(y|x)pθt(y′|x) log\n\u0012\n1 + pdata(y′|x)pθt(y|x)\npdata(y|x)pθt(y′|x)\n\u0013\n+ q(x)pdata(y′|x)pθt(y|x) log\n\u0012\n1 + pdata(y|x)pθt(y′|x)\npdata(y′|x)pθt(y|x)\n\u0013\ndydy′,\nwhere the inequality is due to aℓ(t) + bℓ(−t) ≥a log(1 + b/a) + b log(1 + a/b) in Lemma C.1 with a =\nq(x)pdata(y|x)pθt(y′|x), b = q(x)pdata(y′|x)pθt(y|x), t = f(x, y) −f(x, y′). The equality (i) holds if and only if\nthe following equation holds almost surely for any x, y, y′,\nf(x, y) −f(x, y′) = log\n\u0012pdata(y|x)pθt(y′|x)\npdata(y′|x)pθt(y|x)\n\u0013\n.\n(C.1)\nEquation (C.1) is equivalent to\nf(x, y) −log\n\u0012pdata(y|x)\npθt(y|x)\n\u0013\n= f(x, y′) −log\n\u0012pdata(y′|x)\npθt(y′|x)\n\u0013\nholds almost surely for any x, y, y′. Therefore, the equality (i) holds if and only if there exists some Z(x) such that\nf(x, y) = Z(x) + log\n\u0012pdata(y|x)\npθt(y|x)\n\u0013\n.\nRecall that p+(y, y′|x) = pdata(y|x) · pθt(y|x) and p−(y, y′|x) = pθt(y|x) · pdata(y|x). Then, the right-hand side of (i)\ncan be written as\nZ\nq(x)pdata(y|x)pθt(y′|x) log\n\u0012\n1 + pdata(y′|x)pθt(y|x)\npdata(y|x)pθt(y′|x)\n\u0013\n+ q(x)pdata(y′|x)pθt(y|x) log\n\u0012\n1 + pdata(y|x)pθt(y′|x)\npdata(y′|x)pθt(y|x)\n\u0013\ndydy′\n=\nZ\np+(y, y′|x) log\n\u0012\n1 + p−(y, y′|x)\np+(y, y′|x)\n\u0013\n+ p−(y, y′|x) log\n\u0012\n1 + p+(y, y′|x)\np−(y, y′|x)\n\u0013\ndydy′\n= 2 log 2 +\nZ\np+(y, y′|x) log\n\u00121/2[p−(y, y′|x) + p+(y, y′|x)]\np+(y, y′|x)\n\u0013\n+ p−(y, y′|x) log\n\u00121/2[p−(y, y′|x) + p+(y, y′|x)]\np−(y, y′|x)\n\u0013\ndydy′\n21\n",
  "22": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\n= 2 log 2 −KL\n\u0012\np+\n\r\r\r\r\np+ + p−\n2\n\u0013\n−KL\n\u0012\np−\n\r\r\r\r\np+ + p−\n2\n\u0013\n= 2 log 2 −2 · JSD(p+∥p−),\nwhere the last equality is by the definition of JSD. This concludes the proof.\nLemma C.2 provides a closed-form solution to (4.2) if we ignore the constraint set Ft. If this closed-form solution belongs\nto Ft, then it should also be the solution to (4.2). This observation is the key to the proof of Theorem 5.4.\nProof of Theorem 5.4. Under the condition of Theorem 5.4, there exists a pθ such that\npθ(y|x) ∝pθt(y|x)\n\u0000pdata(y|x)/pθt(y|x)\n\u00011/λ.\nTherefore, there exists a function bZ(x) such that\npθ(y|x) = bZ(x) · pθt(y|x)\n\u0000pdata(y|x)/pθt(y|x)\n\u00011/λ.\n(C.2)\nApplying logarithm function on both side of (C.2) yields\nλ log( bZ(x)) + log\n\u0012pdata(y|x)\npθt(y|x)\n\u0013\n= λ log\n\u0012 pθ(y|x)\npθt(y|x)\n\u0013\n∈Ft.\nBy Lemma C.2, f ∗(x, y) = λ log( bZ(x)) + log\n\u0000 pdata(y|x)\npθt(y|x)\n\u0001\nis the global minimum of the following minimization problem,\nargmin\nf\nEy∼pdata(·|x),y′∼pθt(·|x)\n\u0002\nℓ\n\u0000f(x, y) −f(x, y′)\n\u0001\u0003\n.\n(C.3)\nSince f ∗∈Ft, f ∗(x, y) = λ log( bZ(x)) + log\n\u0000 pdata(y|x)\npθt(y|x)\n\u0001\nis also the global optimum of the optimization problem (4.2),\nargmin\nf∈Ft\nEy∼pdata(·|x),y′∼pθt(·|x)\n\u0002\nℓ\n\u0000f(x, y) −f(x, y′)\n\u0001\u0003\n.\nTherefore, we have proved that\nmin\nf\nEy∼pdata(·|x),y′∼pθt(·|x)\n\u0002\nℓ\n\u0000f(x, y) −f(x, y′)\n\u0001\u0003\n= min\nf∈Ft Ey∼pdata(·|x),y′∼pθt(·|x)\n\u0002\nℓ\n\u0000f(x, y) −f(x, y′)\n\u0001\u0003\n= min\nθ∈Θ LSPIN(θ, θt).\n(C.4)\nSince θt+1 is the global minimum of LSPIN(θ, θt). Then by (C.4), λ log\n\u0010 pθt+1(y|x)\npθt(y|x)\n\u0011\nshould be the global minimum of\nproblem (C.3). By Lemma C.2, there exists Z(x) such that\nλ log\n\u0012pθt+1(y|x)\npθt(y|x)\n\u0013\n= Z(x) + log\n\u0012pdata(y|x)\npθt(y|x)\n\u0013\n,\nwhich leads to the result that pθt+1(y|x) ∝pθt(y|x)\n\u0000pdata(y|x)/pθt(y|x)\n\u00011/λ.\n22\n"
}