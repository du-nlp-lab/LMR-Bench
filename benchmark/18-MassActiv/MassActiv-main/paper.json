{
  "1": "Massive Activations in Large Language Models\nMingjie Sun1\nXinlei Chen2\nJ. Zico Kolter1,3\nZhuang Liu2\n1Carnegie Mellon University\n2Meta AI Research\n3Bosch Center for AI\nSummer\nis\nwarm\n.\nWinter\nis\ncold\n.\n\\n\n1415\n2533\n0\n1k\n2k\nSummer\nis\nwarm\n\\n\nWinter\nis\ncold\n.\n\\n\n1415\n2533\n0\n1k\n2k\nWhy\nis\nsummer\nwarm\nand\nwinter\ncold\n?\n1415\n2533\n0\n1k\n2k\nIt\nis\nthe\nbest\nof\ntimes\n.\n\\n\n1415\n2533\n0\n1k\n2k\nFigure 1: Activation Magnitudes (z-axis) in LLaMA2-7B. x and y axes are sequence and feature dimensions.\nFor this specific model, we observe that activations with massive magnitudes appear in two fixed feature dimensions\n(1415, 2533), and two types of tokens—the starting token, and the first period (.) or newline token (\\n).\nAbstract\nWe observe an empirical phenomenon in Large Language Models (LLMs)—very few activations exhibit\nsignificantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First,\nwe demonstrate the widespread existence of massive activations across various LLMs and characterize\ntheir locations. Second, we find their values largely stay constant regardless of the input, and they\nfunction as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration\nof attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-\nattention output. Last, we also study massive activations in Vision Transformers. Code is available at\nhttps://github.com/locuslab/massive-activations.1\n1\nIntroduction\nLarge Language Models (LLMs) (Brown et al., 2020, OpenAI, 2023) have demonstrated remarkable capabilities.\nThe majority of existing studies conducted on these models are focused on their external behaviors, e.g.,\nevaluating their performance on various tasks (Katz et al., 2023, Bubeck et al., 2023), developing prompts\nto elicit accurate responses (Wei et al., 2022, Yang et al., 2023). While these studies are encouraging and\nhighlight the potential of these models, it is also important to gain insights into their internal mechanisms,\nespecially as they are being increasingly integrated into many real-world applications. However, research on\nthe internal workings of these models remains relatively limited.\nIn this work, we discover and study a surprising phenomenon in the internal representations of LLMs.\nExamining the hidden states in these models, we find that certain activations exhibit huge magnitudes, e.g.,\nmore than 4 orders of magnitude larger than the median, and could take on absolute values larger than\n15,000 in LLaMA2-70B (Touvron et al., 2023), despite the presence of normalization layers. These activations\nare also extremely rare, often numbering fewer than 10 among tens of millions of total activations. Figure 1\n1Published as a conference paper in First Conference on Language Modeling (COLM), 2024.\n1\narXiv:2402.17762v2  [cs.CL]  14 Aug 2024\n",
  "2": "illustrates this phenomenon in LLaMA2-7B. As these activations are so much larger in magnitudes compared\nto others, we name them massive activations. We demonstrate their presence in a wide range of LLMs,\nspanning different model sizes and families.\nWe explore where massive activations are located in LLMs. Regarding the depth dimension of LLMs, the\nappearance of massive activations is mostly abrupt: they emerge suddenly after a single layer of computation,\nand diminish at the last few layers. Further, we find massive activations occur in a small number of feature\ndimensions that are input agnostic. Many of these activations are found within the starting word token and\ndelimiter tokens. Additionally, we show that massive activations are not the same as outlier features (Dettmers\net al., 2022), a previously known phenomenon in LLMs.\nWe show that massive activations act as fixed but crucial bias terms in LLMs. Here by bias terms, we mean\ncertain internal states of the models that are independent from the inputs, analogous to the bias term b in a\nlinear layer y = Wx + b. First, we show that massive activations play a critical role in LLMs’ capabilities. For\ninstance, in LLaMA2-7B, setting merely four massive activations (out of millions of activations) to zero would\nresult in catastrophic collapse in model performance. Further, setting them to their mean values does not\nhurt the model, suggesting their role is equivalent to simple constant biases. Our analysis reveals that after\nthe initial layers, LLMs repurpose the tokens linked with massive activations to store these important biases.\nIntriguingly, massive activations are closely connected with self-attention. In particular, we show massive\nactivations cause attention to be attracted to the tokens associated with them. Our findings extend the\nobservations from “attention sinks”\n(Xiao et al., 2023b)—we demonstrate that LLMs allocate excessive\nattention to more than just the first token, and provide an in-depth analysis on how such attention concentration\npatterns arise. Our analysis suggests that LLMs try to learn implicit bias components in self-attention via\nmassive activations, during their pretraining phase. We thus experiment with augmenting self-attention with\nadditional key and value embeddings that are explicitly designed as biases. Remarkably, we demonstrate that\ntraining with them eliminates the need for LLMs to learn massive activations.\nFinally, we also observe massive activations in Vision Transformers (ViTs). They appear less frequently than\nthose in LLMs but are still in many of the ViTs we have examined. In these ViTs, they tend to appear at\nfixed feature dimensions, but notably at varying patch tokens. Moreover, we find that these activations act\nsimilarly as fixed biases. Notably, we discuss the connections between massive activations and the recently\nproposed “register tokens” in ViTs (Darcet et al., 2023). We show they both learn values independent of input\nimages, functioning as fixed biases. This offers an alternative interpretation for register tokens than that in\nthe original work (Darcet et al., 2023), where they were hypothesized to aggregate global image information.\n2\nMassive Activations\nWe study autoregressive Transformers, which are built by a stack of L decoding layers. Each layer ℓtakes\nthe previous hidden state hℓ−1 ∈RT ×d as input and outputs a hidden state hℓ∈RT ×d. T is the number of\ntokens and d is the number of features. Transformer layers use residual connections (He et al., 2016), and the\ncomputation can be formulated as:\nhℓ= hℓ−1 + Fℓ(hℓ−1)\n(1)\nwhere Fℓis the residual transformation. Note that this includes both attention and MLP blocks. An activation\ndenotes a specific scalar value in a hidden state. Unless otherwise specified, our study of activations is on the\nhidden state hℓ, i.e., the output of residual summations, not any intermediate states inside Fℓ.\nExistence in LLMs. We start with an illustrative example on LLaMA2-7B. In Figure 1, we visualize the\nintermediate features hℓof interest. We feed this model with short sentences and visualize the activation\nmagnitudes (z-axis) of the hidden states at a middle layer. x and y axes correspond to sequence and feature\ndimensions respectively. Each blue row corresponds to the feature embedding of one token. We observe up to\nfour activations with significantly large magnitudes. The largest activation (about 2,000) is approximately\n2\n",
  "3": "LLaMA2-13B\nFigure 2: Massive activations in LLaMA2-13B. In this model, they appear in two fixed feature dimensions\n(2100, 4743), and are limited to the starting token.\nMixtral-8x7B\nFigure 3: Massive activations in Mixtral-8x7B. In this model, they lie in two feature dimensions (2070,\n3398), and are found within the starting token, delimiter tokens and certain word tokens (“and” and “of”).\n10,000 times larger than the median magnitude (about 0.2). The sheer scale of these activations makes them\nstand out from others. We thus refer to these special activations as massive activations.\nMassive activations are not unique to this specific model LLaMA2-7B, but are widely observed in LLMs.\nIn Figure 2 and Figure 3, we demonstrate the existence of massive activations in both LLaMA2-13B and\nMixtral-8x7B (Jiang et al., 2024). Notably for Mixtral-8x7B, the largest activation magnitude can reach an\nabsolute value of 7,000, around 4 orders of magnitude larger than the median feature magnitude (around 0.3).\nWe refer the reader to Appendix A for results on more pretrained and fine-tuned LLMs.\nProperties. We summarize two main properties of massive activations. The most notable property is that\nthese activations possess massive values and their magnitudes are significantly larger than other activations,\noften several orders of magnitude larger than the median value. Another property is that they are exceptionally\nfew in number. For LLaMA2-7B in Figure 1, there are approximately 40,000 total activations in each presented\nhidden state but at most four massive activations can be identified.\nQuantitatively, we present the values of the top activation magnitudes in Table 1. We also provide a loose\nbut broad definition: an activation qualifies as a massive activation if its magnitude surpasses 100 and is at\nleast or around 1,000 times larger than the median magnitude of its hidden state. We find this criterion to\neffectively identify these activations of interest across various LLMs, which are emphasized in bold in Table 1.\nModel\nTop 1\nTop 2\nTop 3\nTop 4\nTop 5\nTop-10\nTop-100\nTop 1%\nTop 10%\nmedian\nLLaMA2-7B\n2622.0\n1547.0\n802.0\n477.3\n156.9\n45.7\n10.6\n1.1\n0.6\n0.2\nLLaMA2-13B\n1264.0\n781.0\n51.0\n50.5\n47.1\n43.5\n16.6\n1.9\n1.1\n0.4\nMixtral-8x7B\n7100.0\n5296.0\n1014.5\n467.8\n302.8\n182.8\n90.8\n3.0\n1.0\n0.3\nTable 1: Five largest, top 1% and 10%, and the median activation magnitudes at a hidden state of three\nLLMs. The activations that are considered as massive activations are highlighted in bold.\n3\n",
  "4": "1\n8\n16\n24\n32\nLayers\n0\n1k\n2k\n3k\nMagnitudes\nLLaMA2-7B\n1\n10\n20\n30\n40\nLayers\n0\n500\n1k\nLLaMA2-13B\nTop 1\nTop 2\nTop 3\nMedian\n1\n8\n16\n24\n32\nLayers\n0\n500\n1k\nPhi-2\nFigure 4: Three largest activation magnitudes and the median magnitude at each layer in LLMs.\nNext, we identify the locations of massive activations within LLMs. For a comprehensive analysis, rather than\nusing short sentences as inputs, we collect 100 sequences (each with 4,096 tokens) from RedPajama (Together\nComputer, 2023). We run LLMs on these 100 sequences and collect the hidden states from each layer.\n2.1\nWhich Layers?\nWe determine the layers whose output hidden states exhibit massive activations. In Figure 4, we visualize the\nthree largest activation magnitudes and the median of the hidden state output of each layer, with results\naveraged over 100 sequences. We examine three models: LLaMA2-7B, 13B and Phi-2 (Javaheripi et al., 2023)\n(see Appendix A.4 for more LLMs). In all cases, each of the top three activations comes from the same\nposition in the hidden state across most of the middle layers. Generally, we observe the following:\nMassive activations exist and remain as largely constant values throughout most of the intermediate layers.\nThey emerge in the initial layers and start to diminish in the last few layers.\nIn LLaMA2-7B, massive activations first appear in layer 2 and remain nearly constant values until layer 30.\nIntriguingly, for LLaMA2-7B and 13B, massive activations emerge very rapidly from one layer of computation,\ne.g., layer 2 and layer 4 respectively. This means that they do not emerge as a result of gradual accumulation\nthrough many layers, and are caused by a rather different mechanism.\n2.2\nWhich Feature and Sequence Dimensions?\nWe determine the locations of massive activations within hidden states, i.e., their feature and sequence\ndimensions. Since we have shown that their values largely stay constant in middle layers, we take on any\nsuch layer for this analysis.\nLLaMA2-7B. In this model, massive activations are identified in two feature dimensions (1415 and 2533).\nRegarding sequence dimensions, we find that massive activations appear at: 1. the starting word token, 2.\nthe token representing the first period (.) or newline token (\\n) in the sequence. Figure 1 illustrates these\nfindings for LLaMA2-7B. This is also consistent on long sequences. In cases where the input contains a “.” or\n“\\n” token, four massive activations are observed. For the less common scenario where neither “.” nor “\\n” is\npresent, we can see two massive activations, both of which are associated with the initial token.\nLLaMA2-13B. We find that massive activations in this model consistently appear in two feature dimensions,\n2100 and 4743. These activations are exclusively located within the starting token of the sequence, regardless\nof its semantics. Figure 2 illustrates these behaviors within LLaMA2-13B. For any given input sequence,\nonly two massive activations are present, corresponding to features 2100 and 4743 of the first word token.\nMixtral-8x7B. For this model, massive activations lie in two feature dimensions, i.e., 2070 and 3398. For\nsequence dimensions, we find that they are associated with the starting token, delimiter tokens and also certain\nword tokens, e.g., token “and” and token “of”. These word tokens tend to be conjunctions and prepositions,\nrepresenting relatively few semantics. Figure 3 showcases these patterns in Mixtral-8x7B. Generally, for\ninputs of 4096 tokens in length, these tokens are predominantly located in the early part of sequence.\n4\n",
  "5": "Summary. We summarize our findings for LLMs beyond the three models discussed above. We also put\nother models into categories based on empirical observations.\n• For feature dimensions, massive activations are consistently present in very few fixed dimensions.\n• For sequence dimensions, we classify LLMs into three categories based on massive activations’ locations:\na) Starting token only.\nModels include LLaMA2-13B, MPT and GPT-2.\nb) Starting token and the first “strong” delimiter token (i.e., “.” or “\\n”)\nModels include LLaMA2-7B and LLaMA2-7B-Chat.\nc) Starting token, delimiter tokens (such as “.”, “\\n”, “’” or “,”), and certain word tokens with weak\nsemantics (such as “and”, “from”, “of” or “2”2)\nModels include LLaMA2-70B, Mistral-7B, Mixtral-8x7B, Falcon-40B and Phi-2.\n2.3\nDifference from Outlier Features\nWith an understanding of the nature and locations of massive activations, we now discuss the differences\nbetween them and outlier features, a seemingly similar phenomenon in LLMs. Dettmers et al. (2022) have\nidentified the existence of outlier features characterized by large magnitudes within LLMs.\nConceptually, a massive activation is a scalar value, determined jointly by the sequence and feature dimensions;\nin contrast, an outlier feature is a vector, corresponding to activations at all tokens. Further, massive activations\nare present at extremely few tokens, while outlier features expect most activations in them to be large.\nIn practice, we find that massive activations do not overlap with outlier feature dimensions. We identify\noutlier features in LLaMA2-7B and 13B using the definition in Dettmers et al. (2022): a feature is deemed\nas an outlier feature if activation magnitudes exceed 6.0 at more than 25% of layers and 6% of tokens, on\nmore than 90 out of 100 sequences. We discover 10 and 25 outlier features in these two models respectively.\nHowever, none of them correspond to the feature dimensions of massive activations.\n3\nMassive Activations Act as Biases in LLMs\nWhile we have demonstrated the existence of massive activations and identified their locations, their functional\nrole within LLMs is not yet clear. Are they important for internal computation? Or are they simply redundant\nactivations with no effect? This section will delve deeper into LLMs to answer these questions. Different from\nthe previous passive observations, we take a more proactive approach by inspecting how modifying massive\nactivations affects the external behavior of LLMs.\nWe first measure the variances of massive activations across input sequences. Besides massive activations,\nwe choose three other positions based on their average magnitudes, corresponding to the top 1%/10%, and\nthe median within the hidden state. In Table 2, we show the mean and standard deviation of the activation\nvalues at these positions across 100 sequences, for LLaMA2-7B and 13B. We find that the variances of massive\nactivations are considerably smaller relative to their mean values when compared to other activations.\nWe then modify the inference of LLMs by intervening massive activations at one layer—for a hidden state\nexhibiting massive activations, we manually set these activations to chosen fixed values. Then the altered\nhidden state is fed into the next layer, and the computation afterwards continues as normal. We modify\nmassive activations in LLaMA2-7B and 13B. We evaluate the perplexity on WikiText, C4 and PG-19 and the\nmean zero-shot accuracy on BoolQ, PIQA, WinoGrande, Arc-Easy and Arc-Challenge. For each model, we\n2Such numeric tokens exhibit massive activations only in certain contexts, e.g., dates and years. Refer to Figure 17 for an\nillustration on LLaMA2-70B.\n5\n",
  "6": "Model\nTop 1\nTop 2\nTop 1%\nTop 10%\nMedian\nLLaMA2-7B\n2556.8 ± 141.0\n-1507.0 ± 83.0\n-0.14 ± 0.6\n0.0 ± 0.5\n0.2 ± 0.3\nLLaMA2-13B\n-1277.5 ± 14.6\n-787.8 ± 8.0\n0.9 ± 0.7\n-0.3 ± 0.8\n-0.3 ± 0.6\nTable 2: The mean and variance of activation values at several positions, corresponding to the 2 largest, top\n1% and 10%, and the median magnitudes within the hidden state. We find that the variation in massive\nactivations is significantly lower in comparison to other activations.\nLLaMA2-7B\nLLaMA2-13B\nIntervention\nWikiText\nC4\nPG-19\nMean Zero-Shot\nWikiText\nC4\nPG-19\nMean Zero-Shot\nOriginal\n5.47\n7.85\n8.57\n68.95%\n4.88\n7.22\n7.16\n71.94%\nSet to zero\ninf\ninf\ninf\n36.75%\n5729\n5526\n4759\n37.50%\nSet to mean\n5.47\n7.86\n8.59\n68.94%\n4.88\n7.22\n7.16\n71.92%\nTable 3: Intervention analysis of massive activations in LLaMA2-7B and 13B. We set massive activations to\nfixed values and evaluate the perplexity (↓) and zero-shot accuracy (%, ↑) of intervened models.\nperform the intervention once on the hidden state where massive activations first appear. This corresponds\nto layer 2 and layer 4 in LLaMA2-7B and 13B respectively.\nSetting massive activations to zero. We evaluate the performance of LLMs without massive activations.\nWe set their values to zero in the hidden state when they first appear, i.e., removing massive activations\nfrom intervened LLMs. The results (denoted by Set to zero) are shown in Table 3. Intriguingly, there is a\nsignificant degradation in model performance, e.g., exploding perplexity numbers. For comparative analysis,\nan equal number of activations—those with average magnitudes close to the median magnitude—are similarly\nset to zero. We find this leads to no performance drop. These results highlight the crucial role that massive\nactivations play in the internal computation of LLMs.\nSetting massive activations to mean values. We remove the small variances in the values of massive\nactivations. Specifically, we adjust the values of massive activations to their empirical mean values. The\nmeans are computed on 100 sequences from RedPajama. The results of this intervention (denoted by Set to\nmean) are shown in Table 3. We find that there are negligible changes in perplexity and zero-shot accuracy.\nThis shows that their values are constants and input agnostic, i.e., functioning similarly to bias terms.\nTo summarize our findings:\nMassive activations act as fixed but important biases in LLMs.\nWhy these layers and tokens?\nThe fact that these activations act as biases may explain why LLMs\nstore them at certain layers and tokens:\n• The tendency of these activations to appear at the starting token could be attributed to the fact that\nevery autoregressive training instance contains an initial token. Since LLMs are based on next word\nprediction, the starting token is the only token used in all forward passes within a sequence.\n• The existence of these activations in delimiter tokens might be due to the relatively low semantic value\nof these tokens, rendering them a low-cost option for storing such biases. Conversely, tokens with rich\nsemantics would risk significant loss of input information, if they are repurposed to store biases.\n• The fact that massive activations emerge only after a few initial layers may be because LLMs would\nrequire some initial layers to process the meaning of the tokens associated with massive activations.\nAt these layers, their semantics may be transferred to other token positions via self-attention, and\npreserved moving forward.\n6\n",
  "7": "4\nEffects on Attention\nIn this section, we explore and study the internal mechanism of massive activations in LLMs, particularly in\nrelation to self-attention.\n4.1\nAttention is Concentrated on Massive Activations\nWe observe a stark contrast in attention patterns when comparing layers before and after the appearance\nof massive activations in LLMs. Figure 5 shows the attention logits (before softmax), averaged over all\nheads per layer in LLaMA2-7B. The input is a prompt from MMLU (Hendrycks et al., 2021): “The following\nare multiple choice questions (with answers) about machine learning.\\n\\n ...”. Recall that in LLaMA2-7B,\nmassive activations first appear in the output of layer 2 (see Figure 4). We find that in layer 3 and deeper\nlayers (e.g., layer 31), attention is mostly concentrated on the two tokens associated with massive activations.\nOur observations are also consistent across various LLMs. Figure 6 demonstrates such attention concentration\npatterns in LLaMA2-13B and Phi-2, on the same input. See Appendix B.1 for results on more LLMs.\nWe notice that there is a consistent pattern across models on the distribution of attention logit values. In\nFigure 5 and Figure 6, many attention logits tend to be negative following massive activations. They are\nmostly computed by the inner product between query and key states of tokens without massive activations.\nHowever, when the key states belong to tokens associated with massive activations, the resulting attention\nlogits are slightly positive. Thus in the attention softmax (computed along each row), these special attention\nlogits will attract most of the attention probability.\nRecently, Xiao et al. (2023b) showed that LLMs attend heavily to the starting token. Our findings on\nLLaMA2-13B in Figure 6a align with their results. Empirically, we find it is true for LLMs where massive\nThe\n.\n1415\n2533\n0\n1k\n2k\nLLaMA2-7B, Layer 2\nKey\nQuery\nLLaMA2-7B, Layer 2\n1\n0\n1\n2\nKey\nQuery\nmassive activations\nLLaMA2-7B, Layer 3\n8\n6\n4\n2\n0\nKey\nQuery\nmassive activations\nLLaMA2-7B, Layer 31\n6\n4\n2\n0\nFigure 5: Attention patterns before and after massive activations appear in LLaMA2-7B. For each layer, we\nvisualize average attention logits (unnormalized scores before softmax) over all heads, for an input sequence.\nThe\n2100\n4743\n0\n1k\nLLaMA2-13B, Layer 4\nKey\nQuery\nThe\nLLaMA2-13B, Layer 5\n6\n4\n2\n0\n(a) LLaMA2-13B\nThe\n\\n\n293\n743\n0\n500\n1k\nPhi-2, Layer 8\nKey\nQuery\nThe\n\\n\nPhi-2, Layer 9\n6\n4\n2\n0\n(b) Phi-2\nFigure 6: Attention patterns after massive activations emerge in LLaMA2-13B (left) and Phi-2 (right).\n7\n",
  "8": "activations are only found within the starting token. However, our results on LLaMA2-7B and Phi-2 indicate\nthat LLMs also allocate substantial attention to other tokens and they are associated with massive activations.\nFurthermore, our results reveal a deeper cause for the emergence of these attention concentration patterns.\n4.2\nMassive Activations Impose Implicit Attention Biases\nIn this part, we delve into the computation within the attention block and demonstrate that LLMs use\nmassive activations to enforce an implicit bias term in self-attention.\nAttention LayerNorm and QKV projections. We study the impact of massive activations on the query,\nkey and value states (Q/K/V) in self-attention. In LLMs, at each layer, input features are processed by layer\nnormalization3 (Ba et al., 2016) and then transformed into query, key and value states via linear projections,\nas illustrated in Figure 7a. This design choice is introduced in GPT-2 (Radford et al., 2019) and widely\nadopted in modern LLMs.\nQ\nK\nV\nLinear\nLayerNorm\nNormalize\nScale\nOutput\nMiddle\nX\nInput\n(a) Attention LayerNorm and QKV\nlinear projections.\nSummer\nis\nwarm\n.\n-1k\n0\n1k\n2k\nLayerNorm Input\nSummer\nis\nwarm\n.\n25\n0\n25\n50\nLayerNorm Middle\nSummer\nis\nwarm\n.\n2.5\n0.0\n2.5\nLayerNorm Output\nSummer\nis\nwarm\n.\n5\n0\n5\nQ\nSummer\nis\nwarm\n.\n10\n0\n10\nK\nSummer\nis\nwarm\n.\n1\n0\n1\nV\n(b) Layer 3, LLaMA2-7B. We highlight the embeddings of the two tokens\nwhere massive activations appear: the starting token and the period token.\nFigure 7: Activation trajectory starting from input hidden states to query, key and value states.\nFigure 7b visualizes all hidden states computed in this schematic (LLaMA2-7B, layer 3). We find that at\nall stages, features of the two tokens associated with massive activations are drastically different from other\ntokens. Specifically, after the first “normalize” step, the embeddings of these two tokens appear as a sparse\nvector with two distinct non-zero elements. Notably, the subsequent QKV states exhibit considerably smaller\nvariations within each embedding. We hypothesize that the attention LayerNorm may play a pivotal role in\nthis process (see Appendix B.2 for further discussion).\nAttention output decomposition. Given that attention is also concentrated on the tokens associated\nwith massive activations (Section 4.1), we thus isolate these tokens and study their effects on the attention\noutput (the layer of attention matrix multiplying value vectors). In Equation 2, we decompose the attention\noutput at each token k into two parts: value updates from the tokens C where attention is concentrated; and\nvalue updates aggregated from other tokens.\nAttention(Q, K, V )k =\nX\ni≤k\npk\ni vi =\nX\ni∈C\npk\ni vi +\nX\ni/∈C\npk\ni vi\n(2)\n3LLaMA2 uses a variant of layer normalization: RMSNorm (Zhang and Sennrich, 2019).\n8\n",
  "9": "Attention Output\nValue Updates\nValue Updates\nSame across all tokens\nFigure 8: Value updates from tokens associated with massive activations are essentially the same.\nwhere pk\ni is the attention distribution of query token k to token i, and vi is the value state of token i.\nFigure 8 visualizes the decomposed value updates and the attention output in LLaMA2-7B, with the input\nprompt “Summer is warm. Winter is cold.”. In this case, the set C consists of token Summer and the first\nperiod token. We can see that the value updates from C are nearly identical across tokens, i.e., they serve\nas additive bias terms, although not explicitly imposed. Furthermore, we note that this pattern of value\nupdate is strikingly similar across various inputs. We refer the reader to Appendix B.3 for additional analysis.\nOverall, our results indicate that LLMs use massive activations to allocate substantial attention at certain\ntokens. These tokens are then utilized to form a constant bias term when computing the attention output.\n4.3\nExplicit Attention Biases Eliminate Massive Activations\nGiven the strong need of LLMs to learn implicit attention biases during pretraining, we thus experiment with\ndirectly augmenting self-attention with additional bias terms. Intriguingly, we find that models augmented\nwith explicit attention biases do not exhibit massive activations.\nFormulation. The idea is to model such attention biases explicitly, except not through repurposing existing\ntokens in the input sequence. Thus we introduce additional learnable parameters k′, v′ ∈Rd for each head.\nSpecifically, given input query, key and value matrices Q, K, V ∈RT ×d, the augmented attention with explicit\nattention biases is computed as:\nAttention(Q, K, V ; k′, v′) = softmax\n \nQ\n\u0002\nKT k′\u0003\n√\nd\n! \u0014 V\nv′T\n\u0015\n(3)\nwhere k′ and v′ are each concatenated with the key and value matrices K/V. The proposed attention can be\nused as a drop-in replacement of standard attention, without modifying other parts of Transformers, e.g.,\npositional embeddings and MLP blocks.\nResults. We train three GPT-2 models: the standard model, GPT-2 prepended with a sink token (Xiao\net al., 2023b) and GPT-2 with explicit attention biases. See Appendix B.4 for training setups. We find\nthat the three models have the same performance at convergence but differ significantly in the status of\nmassive activations, as demonstrated in Figure 9. Notably, in GPT-2 with explicit attention biases, massive\nactivations disappear, as compared to the default GPT-2 and one with a sink token.\nFigure 10 shows the three largest activation magnitudes at each layer. Notably, with explicit attention biases,\ntop activation magnitudes in GPT-2 are increasing gradually as layers go deeper. These results indicate that\nexplicit attention biases negate the necessity for LLMs to develop massive activations during the pretraining\nphase. We leave it as future work to investigate other aspects of our alternative attention formulation, e.g.\ntraining stability (Wortsman et al., 2023).\n9\n",
  "10": "Summer\nis\nwarm\n.\nWinter\nis\ncold\n.\n0\n1k\n2k\nGPT-2 Default\nSINK\nSummer\nis\nwarm\n.\nWinter\nis\ncold\n.\n0\n1k\n2k\nGPT-2 with Sink Token\nSummer\nis\nwarm\n.\nWinter\nis\ncold\n.\n0\n1k\n2k\nGPT-2 with Attention Bias\nFigure 9: Massive activations disappear when training GPT-2 with explicit attention bias (Equation 3).\n1\n3\n6\n9\n12\nLayers\n0\n500\n1k\nMagnitudes\nGPT-2 Default\n1\n3\n6\n9\n12\nLayers\n0\n1k\n2k\nGPT-2 with Sink Token\nTop 1\nTop 2\nTop 3\n1\n3\n6\n9\n12\nLayers\n100\n200\nGPT-2 with Attention Bias\nFigure 10: Three largest activation magnitudes in the output feature of each layer for three GPT-2 models.\nTo summarize our findings in this section:\nMassive activations are connected to self-attention. LLMs use massive activations to concentrate substantial\nattention on very few tokens, injecting implicit bias terms in the attention computation. Further, massive\nactivations can be eliminated by augmenting LLMs with explicit attention biases.\n5\nMassive Activations in Vision Transformers\nIn this section, we study if Vision Transformers (ViTs) (Dosovitskiy et al., 2021) exhibit massive activations.\nWe note that while ViTs and LLMs are both based on self-attention, ViTs employ global token mixing, which\ncontrasts with the autoregressive nature of LLMs.\nMassive activations in ViTs. We explore several model families based on ViTs: CLIP (Radford et al.,\n2021), MAE (He et al., 2021) and DINOv2 (Oquab et al., 2024). We examine the ViT-L models from\nthese families. The activation magnitudes in the penultimate layer for an input image are illustrated in\nFigure 11. We find that massive activations exist in CLIP and DINOv2 ViT-L, where we highlight the\ncorresponding sequence dimensions. In these two models, there are extremely few activations (fewer than\nfour) with significantly larger magnitudes than others. In addition, these activations are located in specific\nfeature dimensions and appear in random patch tokens. However, we do not observe massive activations in\nMAE ViT-L. In this model, a feature dimension (927) exhibits uniformly large values across all tokens.\nMassive activations are biases in ViTs. Figure 13 shows the three largest activation magnitudes and the\nmedian per layer in CLIP and DINOv2 ViT-L, averaged over 1k images. We find that massive activations are\nconsistently present across images and their values remain largely the same around the mean values. It is\nworth noting that unlike LLMs, massive activations start to appear only in the later stages of ViTs.\n10\n",
  "11": "Patch Tokens\nPatch Tokens\nPatch Tokens\nFigure 11: Massive activations are present in ViT-L from CLIP and DINOv2, but not MAE.\n1\n6\n12\n18\n24\nLayers\n0\n100\n200\nMagnitudes\nCLIP ViT-L\nTop 1\nTop 2\nTop 3\nMedian\n1\n6\n12\n18\n24\nLayers\n0\n200\n400\nDINOv2 ViT-L\nFigure 13: Three largest activation magnitudes and the median magni-\ntude at each layer in CLIP and DINOv2 ViT-L.\nCLIP ViT-L, layer 13\nIntervention\nImageNet acc (%)\nOriginal\n75.5\nSet to zero\n59.8\nSet to mean\n75.5\nTable 4:\nIntervention analysis of\nmassive activations in CLIP ViT-L.\nFollowing our methodology in Section 3, we perform intervention analysis on CLIP ViT-L. We modify the\ntwo largest massive activations to zero and mean values respectively. The intervention is conducted on layer\n13, where massive activations first appear within this model. Results are shown in Table 4, where we evaluate\nthe zero-shot accuracy on ImageNet. We can see that setting massive activations to zero leads to significant\ndrop in accuracy while setting to their means results in negligible accuracy drop. These results indicate that\nmassive activations function as fixed but crucial biases in ViTs, aligned with our observations in Section 3.\nPatch Tokens\nFigure 14: DINOv2-reg ViT-G.\nRegisters are biases in ViTs. Recently Darcet et al. (2023) propose\nto augment standard ViTs with additional learnable tokens, which they\nname as register tokens. They show that training ViTs with register\ntokens leads to smooth attention maps, and the resulting model family,\nnamely DINOv2-reg, achieves superior downstream performance over\nDINOv2. Examining the largest ViT-G model in DINOv2-reg, we observe\nthe existence of massive activations, as shown in Figure 14. However,\ndifferent from standard ViTs, massive activations do not appear in\npatch tokens but exclusively within a fixed register token, i.e., register 3.\nThis suggests that this model uses register 3 to store these activations.\nFigure 16 visualizes the attention distribution of the [CLS] token in the\nlast layer. We find that most of the attention is allocated to register 3,\nechoing our previous findings in attention patterns (Section 4.1).\nFurther, we conduct intervention analysis to analyze the role of registers. We replace all register features at\nthe output of every layer with their means, averaged over 10k ImageNet training images. This intervention\nremoves the intended purpose of registers to aggregate global input information (Darcet et al., 2023). Table 5\nshows the results. We find that ViTs with fixed register features achieve accuracy comparable to original\n11\n",
  "12": "CLS\nReg 1\nReg 2\nReg 3\nReg 4\nPatch\n0%\n20%\n40%\n60%\nAttention Distribution, DINOv2-reg ViT-G\nFigure 16: Average attention of the [CLS] token.\nDINOv2-reg with 4 registers\nImageNet acc (%)\nViT-S\nViT-B\nViT-L\nViT-G\nOriginal\n81.9\n84.8\n86.3\n87.0\nFix-Reg-Mean\n81.7\n85.0\n86.2\n87.0\nTable 5: We fix all register features at every layer to\ntheir means and evaluate the intervened ViTs.\nmodels, suggesting that registers act as learned biases in ViTs. This leads to constant key and value states at\nregister tokens, effectively introducing bias terms to self-attention (extra k′ and v′ in Equation 3). Thus a\nViT with register tokens function equivalently to a standard ViT augmented with explicit attention biases.\nTo summarize our findings:\nMassive activations exist in many but not all ViTs. Similar to those in LLMs, these activations act as\nconstant biases. We also show the recently proposed register tokens have a similar function.\n6\nRelated Work\nIntriguing properties of autoregressive Transformers. Timkey and Schijndel (2021) observed that in\nGPT-2’s penultimate layer, there are feature dimensions containing activations with magnitudes up to 3,000.\nThey found that these few dimensions dominate several standard measures for evaluating representation\nsimilarity. Heimersheim and Turner (2023) found that the feature norm of the initial token in GPT-2 grows\nmuch faster than other tokens. Kovaleva et al. (2021) and Zhao et al. (2023) demonstrated the existence of\noutlier weights in the LayerNorm of GPT-2 and LLaMA2-13B and showed that setting them to zero leads to\ncatastrophic drop in model performance. Notably, the feature dimension of this weight in LLaMA2-13B (i.e.,\n2100) corresponds to that of a massive activation (Figure 2).\nOutlier features. Various existing works in quantization (Dettmers et al., 2022, Zeng et al., 2022, Xiao\net al., 2023a, Lin et al., 2023, Ahmadian et al., 2023) have studied the existence of outlier features in\nLLMs. Dettmers et al. (2022) showed that outlier features have large activation values in most of their\nsequence dimensions. While massive activations can be seemingly similar to outlier features, we discussed\ntheir fundamental differences in Section 2.3. More importantly, we show that massive activations can not be\nattributed to the existence of outlier features.\nAttention concentration patterns. Clark et al. (2019b), Kovaleva et al. (2019) and Bondarenko et al.\n(2021) discovered that attention in BERT (Devlin et al., 2018) tends to focus on the “separate” token [SEP].\nXiao et al. (2023b) showed that LLMs assign most of the attention to the starting word token. Darcet et al.\n(2023) revealed the existence of attention artifacts in ViTs. Robinson et al. (2023) found sparse activation\npatterns in ViTs that attract attention to certain tokens. Our work provides an in-depth analysis as to why\nthese patterns emerge, specifically in relation to massive activations.\nBiases in self-attention. There can be various notion of biases in the self-attention mechanism. First, simple\nadditive bias terms can be used in linear layers for computing the query, key and value states (Namazifar\net al., 2023). Second, position biases can be inserted in self-attention to encode positional information of each\ntoken (Su et al., 2021, Press et al., 2021). There are also variants of biases with manually designed softmax\noperator (Miller, 2023, Bondarenko et al., 2023, Hu et al., 2024). Our work reveals that LLMs, even with\nstandard self-attention formulation, would impose implicit bias components in the attention computation\nthrough massive activations.\n12\n",
  "13": "7\nConclusion and Discussion\nAutoregressive training of large Transformers has brought significant advances in natural language processing.\nThis study reveals the widespread existence of massive activations in these Large Language Models (LLMs).\nThe values of these activations are input agnostic but crucial for model performance, despite their extremely\nrare quantity. We establish a close connection between massive activations and the self-attention mechanism.\nWe show that LLMs use them to implement an implicit form of biases for attention computation. Our findings\nalso generalize well to Vision Transformers (ViTs). We hope the new results presented in this work contribute\nto a deeper understanding of today’s large-scale foundation models.\nWe discuss some practical implications and future directions of this work. First, the presence of activations\nwith large magnitudes has been widely known as a major challenge in effectively quantizing LLMs (Dettmers\net al., 2022, Xiao et al., 2023a). This paper identifies a new type of outlier activations in LLMs, and we hope\nour findings will be of value to research on LLM compression. Second, attention maps that allocate excessive\nattention probabilities to a few fixed tokens may be undesirable for mechanistic interpretability (Olsson\net al., 2022). Our proposed attention formulation could make the resulting attention maps in LLMs more\ninterpretable, and potentially benefit downstream applications (Darcet et al., 2023). Finally, our investigation\nof the new attention formulation is focused on its effects on massive activations, and our experiments were\nlimited to a small GPT-2 model due to computational resource constraints. It would be interesting to see\nhow our results generalize to models at larger scales, and how our attention formulation could affect the\ntraining stability (Wortsman et al., 2023) of modern LLMs.\nAcknowledgments\nWe thank Sachin Goyal, Jeremy Cohen, Timothée Darcet, Koustuv Sinha and Mike Rabbat for valuable\ndiscussions. Mingjie Sun was supported by funding from the Bosch Center for Artificial Intelligence.\nReferences\nArash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Stephen Gou, Phil Blunsom, Ahmet\nÜstün, and Sara Hooker. Intriguing properties of quantization at scale. In NeurIPS, 2023.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, and et al. The falcon\nseries of open language models. arXiv preprint arXiv:2311.16867, 2023.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization.\narXiv preprint\narXiv:1607.06450, 2016.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical\ncommonsense in natural language. arXiv preprint arXiv:1911.11641, 2019.\nYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges\nof efficient transformer quantization. arXiv:2109.12948, 2021.\nYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers: Removing outliers\nby helping attention heads do nothing. arXiv preprint arXiv:2306.12929, 2023.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712, 2023.\n13\n",
  "14": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\nBoolQ: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044,\n2019a.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does bert look at? an\nanalysis of bert’s attention. arXiv preprint arXiv:1906.04341, 2019b.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\narXiv:1803.05457, 2018.\nTimothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers.\narXiv:2309.16588, 2023.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical\nimage database. In CVPR, 2009.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication\nfor transformers at scale. In NeurIPS, 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanove. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.\nAn image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, et al. The llama\n3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace\nHe, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of\ndiverse text for language modeling. arXiv preprint arXiv:2101.00027, 2021.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nCVPR, 2016.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders\nare scalable vision learners. arXiv:2111.06377, 2021.\nStefan\nHeimersheim\nand\nAlex\nTurner.\nResidual\nstream\nnorms\ngrow\nexponentially\nover\nthe\nforward\npass,\n2023.\nURL\nhttps://www.alignmentforum.org/posts/8mizBCm3dyc432nK8/\nresidual-stream-norms-grow-exponentially-over-the-forward.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. In ICLR, 2021.\nJerry Yao-Chieh Hu, Pei-Hsuan Chang, Robin Luo, Hong-Yu Chen, Weijian Li, Wei-Po Wang, and Han Liu.\nOutlier-efficient hopfield layers for large transformer-based models. arXiv preprint arXiv:2404.03828, 2024.\nMojan\nJavaheripi,\nSébastien\nBubeck,\nand\net\nal.\nPhi-2:\nThe\nsurprising\npower\nof\nsmall\nlanguage\nmodels,\n2023.\nURL\nhttps://www.microsoft.com/en-us/research/blog/\nphi-2-the-surprising-power-of-small-language-models/.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\n14\n",
  "15": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, et al. Mixtral of experts. arXiv\npreprint arXiv:2401.04088, 2024.\nAndrej Karpathy. Nanogpt, 2023. URL https://github.com/karpathy/nanoGPT.\nDaniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. Gpt-4 passes the bar\nexam. SSRN, 2023.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of bert.\narXiv preprint arXiv:1908.08593, 2019.\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. Bert busters: Outlier dimensions\nthat disrupt transformers. In ACL Findings, 2021.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-\naware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.\narXiv preprint arXiv:1609.07843, 2016.\nEvan Miller. Attention is off by one, 2023. URL https://www.evanmiller.org/attention-is-off-by-one.\nhtml.\nMosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL\nwww.mosaicml.com/blog/mpt-7b.\nMahdi Namazifar, Devamanyu Hazarika, and Dilek Hakkani-Tur. Role of bias terms in dot-product attention.\narXiv preprint arXiv:2302.08626, 2023.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario\nAmodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning\nand induction heads. Transformer Circuits Thread, 2022.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nMaxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, and Marc Szafraniec. Dinov2: Learning robust\nvisual features without supervision. arXiv:2304.07193, 2024.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, and et al. Training language models\nto follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\nOfir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input\nlength extrapolation. arXiv preprint arXiv:2108.12409, 2021.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. Technical Report, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, et al. Learning transferable visual models\nfrom natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/\n1911.05507.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nJournal of Machine Learning Research, 2020.\n15\n",
  "16": "Brian S. Robinson, Nathan Drenkow, Colin Conwell, and Michael F. Bonner. A sparse null code emerges in\ndeep neural networks. In NeurIPS UniReps Workshop, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\nWilliam Timkey and Marten van Schijndel. All bark and no bite: Rogue dimensions in transformer language\nmodels obscure representational quality. arXiv:2109.04404, 2021.\nTogether Computer. Redpajama: an open dataset for training large language models, October 2023. URL\nhttps://github.com/togethercomputer/RedPajama-Data.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural\nInformation Processing Systems, 2022.\nMitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie Everett, and et al. Small-scale proxies for large-scale\ntransformer training instabilities. arXiv preprint arXiv:2309.14322, 2023.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate\nand efficient post-training quantization for large language models. In ICML, 2023a.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language\nmodels with attention sinks. arXiv:2309.17453, 2023b.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large\nlanguage models as optimizers. arXiv preprint arXiv:2309.03409, 2023.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, and et al. Glm-130b: An open bilingual pre-trained\nmodel. arXiv preprint arXiv:2210.02414, 2022.\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019.\nJun Zhao, Zhihao Zhang, Yide Ma, Qi Zhang, Tao Gui, Luhui Gao, and Xuanjing Huang. Unveiling a core\nlinguistic region in large language models. arXiv:2310.14928, 2023.\n16\n",
  "17": "Appendix\nA\nAdditional Results on Massive Activations in LLMs\nIn this section, we supplement the main paper with additional results of massive activations in LLMs. This\nincludes results on more pretrained LLMs (Appendix A.1) and fine-tuned LLMs (Appendix A.2), analysis of\nthe the BOS token <s> (Appendix A.3) and layer-level analysis (Appendix A.4).\nA.1\nPretrained LLMs\nIn Section 2, we have demonstrated massive activations in LLaMA2-7B, LLaMA2-13B and Mixtral-8x7B. In\nthis section, we evaluate more pretrained LLMs which cover a wide range of model families. We illustrate\nmassive activations in LLaMA2-70B, LLaMA3 (Dubey et al., 2024), Phi-2, Mistral-7B (Jiang et al., 2023),\nMPT-7B (MosaicML, 2023) and Falcon-7B (Almazrouei et al., 2023). The results are presented in Figure 17,\n18, 19, 20, 21, 22 and 23.\nWe make several observations. First, massive activations are consistently present in these models and they\nexhibit similar characteristics to those described in Section 2. Intriguingly, for LLaMA2-70B, we find that\nmassive activations are found within tokens representing numerical values, e.g., token “0” and token “2”, as\ndepicted in Figure 17. However, they do not appear in all numerical tokens (see the rightmost example in\nFigure 17). Another interesting finding is that the feature dimension of massive activations in both Mistral-7B\n(Figure 21) and Mixtral-8x7B (Figure 3) is identical (i.e., 2070), implying that the latter model may have\nbeen fine-tuned from the former.\nLLaMA2-70B\nFigure 17: Massive activations in LLaMA2-70B.\nLLaMA3-8B\nFigure 18: Massive activations in LLaMA3-8B.\n17\n",
  "18": "LLaMA3-70B\nFigure 19: Massive activations in LLaMA3-70B.\nPhi-2\nFigure 20: Massive activations in Phi-2.\nMistral-7B\nFigure 21: Massive activations in Mistral-7B.\nMPT-7B\nFigure 22: Massive activations in MPT-7B.\n18\n",
  "19": "Falcon-7B\nFigure 23: Massive activations in Falcon-7B.\nA.2\nFine-tuned LLMs\nOur results so far are focused on pretrained LLMs. However, a significant application of LLMs lies in their\nuse for chat purposes. Instruction fine-tuning (Ouyang et al., 2022) is essential for developing models capable\nof generating coherent responses to questions. In this part, we demonstrate massive activations in these\nfine-tuned models. We evaluate fine-tuned models from models in LLaMA2 and Mistral. The results are\nshown in Figure 24, 25, 26 and 27.\nWe can see that massive activations persist after instruction fine-tuning. Moreover, the values and positions\nof massive activations remain largely the same as the original pretrained LLMs. For LLaMA2-7B, this can be\nseen by comparing Figure 24 and Figure 1. However, one exception is Mixtral-8x7B. We find that massive\nactivations disappear from the newline token “\\n” after fine-tuning, as shown by comparing Figure 27 and\nFigure 3. We leave the study on how instruction fine-tuning affects massive activations for future work.\nLLaMA2-7B-Chat\nFigure 24: Massive activations in LLaMA2-7B-Chat.\nLLaMA2-13B-Chat\nFigure 25: Massive activations in LLaMA2-13B-Chat.\n19\n",
  "20": "Mistral-7B-Instruct\nFigure 26: Massive activations in Mistral-7B-Instruct.\nMixtral-8x7B-Instruct\nFigure 27: Massive activations in Mixtral-8x7B-Instruct.\nA.3\nBOS Token <s>\nIn some tokenizers, e.g., LLaMA2, the BOS token <s>, also known as the beginning of sequence token, can\nbe prepended to the input sequence. For the experiments presented in other parts of the paper, we turn off\nthis option, where all sequences do not start with the BOS token.\nIn Figure 28, 29 and 30, we show massive activations in LLaMA2-7B, LLaMA2-13B and Mixtral-8x7B, with\nthe same input sequences as in Section 2. We find that massive activations persist with a prepended BOS\ntoken. In LLaMA2-7B and LLaMA2-13B, the locations of massive activations, i.e., sequence and feature\ndimensions, are not altered. However, for Mixtral-8x7B, some massive activations shift to the BOS token <s>.\nWe leave the study on how the BOS token <s> affects the positions of massive activations for future work.\nLLaMA2-7B\nFigure 28: Massive activations in LLaMA2-7B when the input is prepended with a BOS token <s>.\n20\n",
  "21": "LLaMA2-13B\nFigure 29: Massive activations in LLaMA2-13B when the input sequence is prepended with a BOS token <s>.\nMixtral-8x7B\nFigure 30: Massive activations in Mixtral-8x7B when the input sequence is prepended with a BOS token <s>.\nA.4\nLayer-Level Analysis\nIn Section 2.1, we have presented the layer-level analysis results for LLaMA2-7B, LLaMA2-13B and Phi-2. In\nFigure 31, we provide the comprehensive results for all LLMs examined in this paper (listed in Table 7). This\nincludes LLMs from LLaMA2, Mistral, MPT, Falcon, OPT and GPT-2 model families. For each model, we\nshow the three largest activation magnitudes as well as the median at each layer.\nWe can see that the trend of massive activations we observe in Section 2.1 holds true for LLMs in general.\nMassive activations tend to remain constant in most of the intermediate layers. They emerge in the early\nlayers and disappear in the last layer.\n21\n",
  "22": "1\n8\n16\n24\n32\nLayers\n0\n200\nLLaMA3-8B\n1\n20\n40\n60\n80\nLayers\n0\n250\n500\nLLaMA3-70B\n1\n8\n16\n24\n32\nLayers\n0\n200\nLLaMA3-8B-Instruct\n1\n20\n40\n60\n80\nLayers\n0\n250\n500\nLLaMA3-70B-Instruct\n1\n20\n40\n60\n80\nLayers\n0\n5k\n10k\n15k\nMagnitudes\nLLaMA2-70B\n1\n8\n16\n24\n32\nLayers\n0\n1k\n2k\n3k\nLLaMA2-7B-chat\n1\n10\n20\n30\n40\nLayers\n0\n500\n1k\nLLaMA2-13B-chat\n1\n20\n40\n60\n80\nLayers\n0\n5k\n10k\n15k\nLLaMA2-70B-chat\n1\n8\n16\n24\n32\nLayers\n0\n200\n400\nMagnitudes\nMistral-7B\n1\n8\n16\n24\n32\nLayers\n0\n3k\n6k\nMixtral-8x7B\n1\n8\n16\n24\n32\nLayers\n0\n200\nMistral-7B-Instruct\n1\n8\n16\n24\n32\nLayers\n0\n2k\n4k\nMixtral-8x7B-Instruct\n1\n8\n16\n24\n32\nLayers\n0\n500\n1k\nMagnitudes\nMPT-7B\n1\n12\n24\n36\n48\nLayers\n0\n500\n1k\nMPT-30B\n1\n8\n16\n24\n32\nLayers\n0\n1k\n2k\nFalcon-7B\n1\n15\n30\n45\n60\nLayers\n0\n5k\n10k\n15k\nFalcon-40B\n1\n8\n16\n24\n32\nLayers\n0\n200\nMagnitudes\nOPT-7B\n1\n10\n20\n30\n40\nLayers\n0\n200\n400\nOPT-13B\n1\n12\n24\n36\n48\nLayers\n0\n500\n1k\nOPT-30B\n1\n16\n32\n48\n64\nLayers\n0\n200\n400\nOPT-66B\n1\n3\n6\n9\n12\nLayers\n0\n1k\n2k\n3k\nMagnitudes\nGPT-2\n1\n6\n12\n18\n24\nLayers\n0\n2k\n4k\nGPT-2-Medium\n1\n9\n18\n27\n36\nLayers\n0\n1k\n2k\nGPT-2-Large\n1\n12\n24\n36\n48\nLayers\n0\n1k\n2k\n3k\nGPT-2-XL\nFigure 31: Layer-level analysis of LLMs. For each model, we show the three largest activation magnitudes as\nwell as the median per layer.\n22\n",
  "23": "B\nAdditional Results on Self-Attention\nIn this section, we provide additional results for the analysis on self-attention. This includes results on more\nLLMs (Appendix B.1), analysis of attention LayerNorm (Appendix B.2), more results on implicit attention\nbiases (Appendix B.3) and detailed results on training GPT-2 with explicit attention biases (Appendix B.4).\nB.1\nAttention Concentration on Massive Activations\nIn Section 4, we have demonstrated the attention concentration pattern in LLaMA2-7B, LLaMA2-13B and\nPhi-2. We now illustrate this phenomenon for more LLMs. Figure 32 and Figure 33 show the results for\nLLaMA2-70B and Mistral-7B. For these two models, massive activations are formed in the output feature of\nlayer 9 and layer 2 respectively.\nWe can see that attention is predominantly focused on the sequence dimensions of massive activations. In the\ncase of LLaMA2-70B, as depicted in Figure 32, massive activations are found in the starting word token and\nalso token 2. These two tokens receive substantial attention logits. Additionally, we visualize the attention\nprobability in Figure 34. The attention softmax is computed along each row, thus resulting in these special\ntokens being allocated a much higher attention probability.\nThis\n2\n3758\n7754\n0\n5k\n10k\n15k\nLLaMA2-70B, Layer 9\nThis\n2\nLLaMA2-70B, Layer 10\n6\n4\n2\n0\n2\nThis\n2\nLLaMA2-70B, Layer 40\n5\n4\n3\n2\n1\n0\n1\nThis\n2\nLLaMA2-70B, Layer 60\n6\n4\n2\n0\n2\nFigure 32: Average attention logits over all heads in layers 10, 40 and 60 of LLaMA2-70B. The input sequence\nis “This book, including all illustrations and text, is protected under Copyright©2024 and may not be\nreproduced or transmitted in any form without the prior written permission of the copyright owner.”.\nWilliam\nfrom\n\\n\n2070\n0\n150\n300\nMistral-7B, Layer 2\nWilliam\nfrom\n\\n\nMistral-7B, Layer 3\n6\n4\n2\n0\n2\nWilliam\nfrom\n\\n\nMistral-7B, Layer 10\n4\n2\n0\n2\nWilliam\nfrom\n\\n\nMistral-7B, Layer 20\n6\n4\n2\n0\n2\nFigure 33: Average attention logits over all heads in layers 3, 10 and 20 of Mistral-7B. The input sequence is\n“William Shakespeare was a famous writer from England who wrote plays and poems. He is considered one\nof the best writers ever.\\n His works include famous plays like ’Romeo and Juliet’ and ’Hamlet’.”.\n23\n",
  "24": "Key\nQuery\nLayer 1\n0.2\n0.4\n0.6\n0.8\n1.0\nKey\nQuery\nLayer 3\n0.2\n0.4\n0.6\n0.8\n1.0\nKey\nQuery\nLayer 10\n0.2\n0.4\n0.6\n0.8\n1.0\nKey\nQuery\nLayer 20\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) LLaMA2-7B\nKey\nQuery\nLayer 1\n0.2\n0.4\n0.6\n0.8\n1.0\nKey\nQuery\nLayer 3\n0.2\n0.4\n0.6\n0.8\n1.0\nKey\nQuery\nLayer 10\n0.2\n0.4\n0.6\n0.8\n1.0\nKey\nQuery\nLayer 20\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) Mistral-7B\nKey\nQuery\nLayer 1\n0.2\n0.4\n0.6\n0.8\n1.0\nKey\nQuery\nLayer 3\n0.2\n0.4\n0.6\n0.8\n1.0\nKey\nQuery\nLayer 10\n0.2\n0.4\n0.6\n0.8\n1.0\nKey\nQuery\nLayer 20\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) Phi-2\nFigure 34: Average attention probability over all heads in intermediate layers of LLaMA2-7B, LLaMA2-13B\nand Phi-2. The input prompt is “William Shakespeare was a famous writer from England who wrote plays\nand poems. He is considered one of the best writers ever.\\n His works include famous plays like ’Romeo and\nJuliet’ and ’Hamlet’.”.\nB.2\nAttention LayerNorm\nOur analysis in Section 4.2 indicates that tokens associated with massive activations have drastically different\nkey and value states. In this part, we investigate how attention layernorm plays a crucial role in this process.\nPreliminaries. There are two specific types of layer normalization commonly used in LLMs. One is the\nstandard layer normalization (Ba et al., 2016). Suppose we have a feature vector x ∈Rd, LayerNorm will\nnormalize this feature to fix the mean and variance and then re-scale with element-wise affine transformation:\n¯xi = xi −µ\nσ\n∗gi + bi,\nwhere µ = 1\nd\nd\nX\ni=1\nxi,\nσ =\nv\nu\nu\nt1\nd\nd\nX\ni=1\n(xi −µ)2.\n(4)\nwhere g, b ∈Rd are parameters of the affine transform, also called the gain and bias.\nIn addition to the original LayerNorm, a variant of layer normalization has also been used in LLaMA2 and\nMistral models. Specifically, Root Mean Square Normalization (RMSNorm) (Zhang and Sennrich, 2019)\n24\n",
  "25": "Who\nare\nyou\n?\n\\n\n1000\n0\n1000\n2000\nLLaMA2-7B, RMSNorm Input\nWho\nare\nyou\n?\n\\n\n20\n0\n20\n40\nLLaMA2-7B, RMSNorm Middle\nWho\nare\nyou\n?\n\\n\n4\n2\n0\n2\n4\nLLaMA2-7B, RMSNorm Output\nWho\n are\n you\n?\n\\n\n1000\n500\n0\nPhi-2, LayerNorm Input\nWho\n are\n you\n?\n\\n\n40\n30\n20\n10\n0\nPhi-2, LayerNorm Middle\nWho\n are\n you\n?\n\\n\n4\n2\n0\n2\nPhi-2, LayerNorm Output\nFigure 35: Activation trajectory in the attention LayerNorm of LLaMA2-7B and Phi-2, where the Lay-\nerNorm input contains massive activations. Note that LLaMA2-7B uses a variant of layer normalization:\nRMSNorm (Zhang and Sennrich, 2019) and Phi-2 uses the default LayerNorm (Ba et al., 2016).\nnormalizes the feature x ∈Rd with the root mean square (RMS) statistic:.\n¯xi =\nxi\nRMS(a) ∗gi,\nwhere RMS(x) =\nv\nu\nu\nt1\nd\nd\nX\ni=1\nx2\ni .\n(5)\nwhere g ∈Rd is the gain parameter.\nFor both LayerNorm and RMSNorm, when there are a few activations in x ∈Rd that have significantly large\nmagnitudes, the denominator in the normalization step, i.e., σ in Equation 4 and RMS(x) in Equation 5,\nbecomes large as a result. In fact, the denominator is almost determined by these few massive activations.\nThe large denominator will push all normal values to zero while preserving the outlier nature of massive\nactivations. This will effectively create a drastically different normalized feature, determined by the few\nmassive activations. Figure 35 shows two activation trajectory in both RMSNorm and LayerNorm. We can\nsee that how the normalization step (middle) preserves the outlier activations in tokens Who and \\n and the\nnormalized features at these two tokens become extremely similar.\nB.3\nImplicit Attention Biases\nIn Section 4.2, we have shown how the value updates from the tokens associated with massive activations\ntend to be largely identical. Here we extend those findings by examining additional input prompts and\nlayers within the LLaMA2-7B model. We use four input prompts: “Are you cold?\\n Grab a jacket.”, “Will it\n25\n",
  "26": "snow?\\n Check the forecast.”, “Did she call?\\n I missed it.” and “\"I am doing well. Thank you for asking.\"”.\nWe visualize the value updates in layer 3, layer 15 and layer 30 in Figure 36, Figure 37 and Figure 38\nrespectively. We focus on the latter half of the input sequence, following the two tokens associated with\nmassive activations. We can see that in the same layer, the value updates P\ni∈C pk\ni vi display remarkable\nsimilarity across the different input sequences.\nGrab\na\njack\net\n0.0\n0.1\nSeq 1\nCheck\nthe\nforec\nast\n0.0\n0.1\nSeq 2\nI\nmissed\nit\n.\n0.0\n0.1\nSeq 3\nThank\nyou\nfor\nasking\n0.0\n0.1\nSeq 4\nFigure 36: Value updates P\ni∈C pk\ni vi at layer 3 of LLaMA2-7B, with four input sequences.\nGrab\na\njack\net\n0.5\n0.0\n0.5\nSeq 1\nCheck\nthe\nforec\nast\n0.5\n0.0\n0.5\nSeq 2\nI\nmissed\nit\n.\n0.5\n0.0\n0.5\nSeq 3\nThank\nyou\nfor\nasking\n0.5\n0.0\n0.5\nSeq 4\nFigure 37: Value updates P\ni∈C pk\ni vi at layer 15 of LLaMA2-7B, with four input sequences.\nGrab\na\njack\net\n0.5\n0.0\nSeq 1\nCheck\nthe\nforec\nast\n0.5\n0.0\nSeq 2\nI\nmissed\nit\n.\n0.5\n0.0\nSeq 3\nThank\nyou\nfor\nasking\n0.5\n0.0\nSeq 4\nFigure 38: Value updates P\ni∈C pk\ni vi at layer 30 of LLaMA2-7B, with four input sequences.\nB.4\nExplicit Attention Biases\nExperimental setup. We use the open-source reproduction of GPT-2 from the NanoGPT repository (Karpa-\nthy, 2023). We use the default recommended training setup and optimizer setting. For each of the three\nGPT-2 models, we train for 50,000 iterations, with a total of approximately 2B tokens. For the GPT-2 with\na sink token, we follow Xiao et al. (2023b), where we prepend each training sequence with a learnable sink\n26\n",
  "27": "first token\nKey\nQuery\nGPT-2 Default, Layer 1 Head 0\n0.2\n0.4\n0.6\n0.8\n1.0\nfirst token\nKey\nQuery\nGPT-2 Default, Layer 6 Head 0\n0.2\n0.4\n0.6\n0.8\n1.0\nextra k' v'\nKey\nQuery\nGPT-2 with Attention Bias, Layer 1 Head 0\n0.2\n0.4\n0.6\n0.8\nextra k' v'\nKey\nQuery\nGPT-2 with Attention Bias, Layer 6 Head 0\n0.2\n0.4\n0.6\n0.8\nFigure 39: Attention distribution in default GPT-2 and GPT-2 with explicit attention bias.\ntoken [SINK]. When computing the training loss, we do not include the cross-entropy loss computed on the\nprepended sink token. For GPT-2 with explicit attention biases, we initialize each k′ and v′ with N(0, 0.02I).\nResults. Regarding the performance of the three GPT-2 models we evaluate in Section 4.3, we find that\nafter 50,000 training iterations, they have the same perplexity on the validation split constructed from\nOpenWebText2 (Gao et al., 2021): 3.04.\nIn Figure 39, we visualize the attention distribution in both default GPT-2 and GPT-2 with explicit attention\nbiases, where we plot the average attention probability over 50 sentences each with 30 tokens. First, we find\nthat our observations on the relationship between massive activations and attention concentration hold for\nthe default GPT-2 model. Second, for the GPT-2 model with explicit attention bias, most of the attention\nprobability is assigned to the extra k′ and v′ vectors we inserted. Intriguingly, this also holds for initial layers\nas well (e.g., layer 1), suggesting the strong need for LLMs to form this attention concentration pattern\nduring pretraining.\nWe also experiment with other ways of injecting biases in the self-attention computation:\n1. The first one is a special case of our proposed formulation in Equation 3, where both k′ and v′ are zero\nvectors. Equation 6 shows the computation of this variant of self-attention. This is also equivalent to\nthe previous proposed Softmax-off-by-one (Miller, 2023).\nAttention(Q, K, V ) = softmax\n \nQ\n\u0002\nKT 0\n\u0003\n√dk\n! \u0014 V\n0T\n\u0015\n(6)\n2. Since Equation 3 can be viewed as inserting a sequence dimension, we also experiment with inserting\none extra feature dimension. Specifically, we add learnable parameters q′, k′ ∈RT and concatenate\nthem with the query and key states respectively. This variant of self-attention is as follows:\nAttention(Q, K, V ; q′, k′) = softmax\n\n\n\u0002\nQ q′\u0003 \u0002\nK k′\u0003T\n√dk\n\nV\n(7)\n3. We also experiment with a simple way to enforce constant value updates by injecting an extra value\nparameter v′ ∈Rdk. This variant of self-attention is as follows:\nAttention(Q, K, V ; v′) = softmax\n\u0012QKT\n√dk\n\u0013\nV + v′\n(8)\nFigure 40 visualizes the ten largest activation magnitudes in three GPT-2 models, corresponding to the three\nformulations of biases in Equation 6, 7 and 8. We find that these alternatives are not able to eliminate\nmassive activations during pretraining.\n27\n",
  "28": "1\n3\n6\n9\n12\nLayers\n100\n200\n300\nMagnitudes\nGPT-2 with Softmax-off-by-one\n1\n3\n6\n9\n12\nLayers\n0\n500\n1000\nGPT-2 with QK Bias\nTop 1\nTop 2\nTop 3\nTop 4\nTop 5\nTop 6\nTop 7\nTop 8\nTop 9\nTop 10\n1\n3\n6\n9\n12\nLayers\n0\n200\n400\n600\nGPT-2 with V Bias\nFigure 40: Ten largest activation magnitudes at each layer in three GPT-2 models.\nC\nAdditional Results on Vision Transformer\nIn this section, we provide additional results for Vision Transformers (ViTs). This includes illustration of\nmassive activations in various input images (Appendix C.1), analysis of register tokens in Masked Autoencoders\n(C.2) and more results of layer-level analysis (Appendix C.3).\nC.1\nMassive Activations in ViTs\nWe present results of massive activations in ViTs on 4 images from Figure 41. Results of CLIP ViT-L,\nDINOv2 ViT-L and DINOv2-reg ViT-G are shown in Figure 42, Figure 43 and Figure 44. We highlight the\npatch tokens exhibiting massive activations. For standard ViTs like CLIP ViT-L and DINOv2 ViT-L, massive\nactivations appear in random patch tokens, i.e. the sequence dimensions of massive activations vary across\ninput images. For DINOv2-reg ViT-G, they exist in a fixed register token, i.e., register 3.\nFigure 41: Example images.\n28\n",
  "29": "Patch Tokens\nPatch Tokens\nPatch Tokens\nPatch Tokens\nCLIP ViT-L, Layer 23\nFigure 42: Illustration of massive activations in CLIP ViT-L for the 4 images shown in Figure 41.\nPatch Tokens\nPatch Tokens\nPatch Tokens\nPatch Tokens\nDINOv2 ViT-L, Layer 23\nFigure 43: Illustration of massive activations in DINOv2 ViT-L for the 4 images shown in Figure 41.\nPatch Tokens\nPatch Tokens\nPatch Tokens\nPatch Tokens\nDINOv2-reg ViT-G, Layer 40\nFigure 44: Illustration of massive activations in DINOv2 ViT-G for the 4 images shown in Figure 41.\nC.2\nRegisters are Biases in Masked Autoencoders\nIn Masked Autoencoders (MAEs) (He et al., 2021), a dummy token is added to ViTs during pretraining. In\none fine-tuning pipeline of MAEs, fine-tuning is done based on the average pooled features of all patch tokens.\nIn these MAE models, this dummy token is equivalent to a register token. Here we maintain the register\ntoken features as constant across the output features of all layers in ViTs, which we denote as Fix-Reg-Mean.\nThese fixed values are computed as the average register features over 10k ImageNet training images. Table 6\nshows the results. We can see that setting register features to fixed values does not affect model performance.\nThis result further supports our argument that registers function as biases within ViTs.\n29\n",
  "30": "MAE with 1 register\nImageNet acc\nViT-B\nViT-L\nViT-H\nOriginal\n82.6\n85.5\n86.7\nFix-Reg-Mean\n82.6\n85.5\n86.7\nTable 6: Registers are biases in Masked Autoencoders (MAEs).\nC.3\nLayer-Level Analysis\nFigure 45, 46 and 47 detail the layer-level analysis results for all ViTs examined in this paper (also summarized\nin Table 8). Different from LLMs, some ViTs do not exhibit massive activations, e.g., MAE ViT-B/L and\nDINOv2 ViT-S. For ViTs where we observe massive activations, e.g., CLIP ViT-L and DINOv2 ViT-L, the\ntrend across layers differs from LLMs. For instance, in the case of DINOv2 ViT-L, massive activations are\nobserved in the later stages of this model but are absent in the output of the final layer.\n1\n3\n6\n9\n12\nLayers\n0\n100\nMagnitudes\nMAE ViT-B\n1\n6\n12\n18\n24\nLayers\n0\n100\n200\nMAE ViT-L\n1\n3\n6\n9\n12\nLayers\n0\n50\nOpenAI CLIP ViT-B\n1\n6\n12\n18\n24\nLayers\n0\n100\n200\nOpenAI CLIP ViT-L\nFigure 45: Layer-level analysis for ViTs in MAE and CLIP.\n1\n3\n6\n9\n12\nLayers\n0\n10\nMagnitudes\nDINOv2 ViT-S\n1\n3\n6\n9\n12\nLayers\n0\n200\n400\nDINOv2 ViT-B\n1\n6\n12\n18\n24\nLayers\n0\n200\n400\nDINOv2 ViT-L\n1\n10\n20\n30\n40\nLayers\n0\n200\n400\nDINOv2 ViT-G\nFigure 46: Layer-level analysis for ViTs in DINOv2.\n1\n3\n6\n9\n12\nLayers\n0\n50\nMagnitudes\nDINOv2-reg ViT-S\n1\n3\n6\n9\n12\nLayers\n0\n100\n200\nDINOv2-reg ViT-B\n1\n6\n12\n18\n24\nLayers\n0\n500\nDINOv2-reg ViT-L\n1\n10\n20\n30\n40\nLayers\n0\n500\n1k\nDINOv2-reg ViT-G\nFigure 47: Layer-level analysis for ViTs in DINOv2-reg.\n30\n",
  "31": "D\nModels and Datasets\nTable 7 and Table 8 list the information of the LLM and ViT models used in this paper.\nModel family\nModel name\nLayers Dimensions Heads\nHuggingface model id\nLLaMA2\nLLaMA2-7B\n32\n4096\n32\nmeta-llama/Llama-2-7b-hf\nLLaMA2-13B\n40\n5120\n40\nmeta-llama/Llama-2-13b-hf\nLLaMA2-70B\n60\n6656\n52\nmeta-llama/Llama-2-70b-hf\nLLaMA2-7B-Chat\n32\n4096\n32\nmeta-llama/Llama-7b-chat-hf\nLLaMA2-13B-Chat\n40\n5120\n40\nmeta-llama/Llama-2-13b-chat-hf\nLLaMA2-70B-Chat\n60\n6656\n52\nmeta-llama/Llama-2-70b-chat-hf\nLLaMA3\nLLaMA3-8B\n32\n4096\n32\nmeta-llama/Meta-Llama-3-8B\nLLaMA3-70B\n80\n8192\n64\nmeta-llama/Meta-Llama-3-70B\nMistral\nMistral-7B\n32\n4096\n32\nmistralai/Mistral-7B-v0.1\nMistral-8x7B\n32\n4096\n32\nmistralai/Mistral-8x7B-v0.1\nMistral-7B-Instruct\n32\n4096\n32\nmistralai/Mistral-7B-Instruct-v0.2\nMistral-8x7B-Instruct\n32\n4096\n32\nmistralai/Mistral-8x7B-Instruct-v0.1\nPhi\nPhi-2\n32\n2560\n32\nmicrosoft/phi-2\nMPT\nMPT-7B\n32\n4096\n32\nmosaicml/mpt-7b\nMPT-30B\n48\n7168\n64\nmosaicml/mpt-30b\nFalcon\nFalcon-7B\n32\n4544\n71\ntiiuae/falcon-7b\nFalcon-40B\n60\n8192\n128\ntiiuae/falcon-40b\nOPT\nOPT-7B\n32\n4096\n32\nfacebook/opt-6.7b\nOPT-13B\n40\n5120\n40\nfacebook/opt-13b\nOPT-30B\n48\n7168\n56\nfacebook/opt-30b\nOPT-66B\n64\n9216\n72\nfacebook/opt-66b\nGPT-2\nGPT-2\n12\n768\n12\ngpt2\nGPT-2-Medium\n24\n1024\n16\ngpt2-medium\nGPT-2-Large\n36\n1280\n20\ngpt2-large\nGPT-2-XL\n48\n1600\n25\ngpt2-xl\nTable 7: Relevant information of LLM models we experimented with in this work.\nModel family Model size Layers Dimensions Heads\nHuggingface model id\nDINOv2\nViT-S\n12\n384\n6\ntimm/vit_small_patch14_dinov2.lvd142m\nViT-B\n12\n768\n12\ntimm/vit_base_patch14_dinov2.lvd142m\nViT-L\n24\n1024\n16\ntimm/vit_large_patch14_dinov2.lvd142m\nViT-G\n40\n1536\n24\ntimm/vit_giant_patch14_dinov2.lvd142m\nDINOv2-reg\nViT-S\n12\n384\n6\ntimm/vit_small_patch14_reg4_dinov2.lvd142m\nViT-B\n12\n768\n12\ntimm/vit_base_patch14_reg4_dinov2.lvd142m\nViT-L\n24\n1024\n16\ntimm/vit_large_patch14_reg4_dinov2.lvd142m\nViT-G\n40\n1536\n24\ntimm/vit_giant_patch14_reg4_dinov2.lvd142m\nMAE\nViT-B\n12\n768\n12\ntimm/vit_base_patch16_224.mae\nViT-L\n24\n1024\n16\ntimm/vit_large_patch16_224.mae\nViT-H\n32\n1280\n16\ntimm/vit_huge_patch16_224.mae\nCLIP\nViT-B\n12\n768\n12\ntimm/vit_base_patch16_clip_224.openai\nViT-L\n24\n1024\n16\ntimm/vit_large_patch14_clip_224.openai\nTable 8: Relevant information of ViT models we experimented with in this work.\n31\n",
  "32": "We list the datasets used in this work and relevant license information:\n• RedPajama (Together Computer, 2023):\nApache License, Version 2.0\n• OpenWebText2 (Gao et al., 2021):\nMIT License\n• C4 (Raffel et al., 2020):\nOpen Data Commons Attribution License 1.0 license\n• PG-19 (Rae et al., 2019):\nApache License, Version 2.0\n• WikiText (Merity et al., 2016):\nCreative Commons BY-SA 3.0 license\n• MMLU (Hendrycks et al., 2021):\nMIT License\n• BoolQ (Clark et al., 2019a):\nCreative Commons BY-SA 3.0 license\n• PIQA (Bisk et al., 2019):\nThe license status is unclear\n• WinoGrande (Sakaguchi et al., 2019):\nApache License, Version 2.0\n• ARC easy and challenge (Clark et al., 2018):\nCreative Commons BY 4.0 license\n• ImageNet (Deng et al., 2009):\nThe license status is unclear\n32\n"
}