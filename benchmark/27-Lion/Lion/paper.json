{
  "1": "Symbolic Discovery of Optimization Algorithms\nXiangning Chen1 2 § ∗\nChen Liang1 §\nDa Huang1\nEsteban Real1\nKaiyuan Wang1\nHieu Pham1\nXuanyi Dong1\nThang Luong1\nCho-Jui Hsieh2\nYifeng Lu1\nQuoc V. Le1\n§Equal & Core Contribution\n1Google\n2UCLA\nAbstract\nWe present a method to formulate algorithm discovery as program search, and\napply it to discover optimization algorithms for deep neural network training. We\nleverage efficient search techniques to explore an infinite and sparse program space.\nTo bridge the large generalization gap between proxy and target tasks, we also\nintroduce program selection and simplification strategies. Our method discovers\na simple and effective optimization algorithm, Lion (EvoLved Sign Momentum).\nIt is more memory-efficient than Adam as it only keeps track of the momentum.\nDifferent from adaptive optimizers, its update has the same magnitude for each\nparameter calculated through the sign operation. We compare Lion with widely\nused optimizers, such as Adam and Adafactor, for training a variety of models\non different tasks. On image classification, Lion boosts the accuracy of ViT by\nup to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On\nvision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-\ntuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%,\nrespectively. On diffusion models, Lion outperforms Adam by achieving a better\nFID score and reducing the training compute by up to 2.3x. For autoregressive,\nmasked language modeling, and fine-tuning, Lion exhibits a similar or better\nperformance compared to Adam. Our analysis of Lion reveals that its performance\ngain grows with the training batch size. It also requires a smaller learning rate\nthan Adam due to the larger norm of the update produced by the sign function.\nAdditionally, we examine the limitations of Lion and identify scenarios where its\nimprovements are small or not statistically significant.\n1\nIntroduction\nOptimization algorithms, i.e., optimizers, play a fundamental role in training neural networks.\nThere are a large number of handcrafted optimizers, mostly adaptive ones, introduced in recent\nyears [2, 3, 7, 27, 58, 105]. However, Adam [48] with decoupled weight decay [60], also referred to as\nAdamW, and Adafactor with factorized second moments [86], are still the de facto standard optimizers\nfor training most deep neural networks, especially the recent state-of-the-art language [12, 23, 90],\nvision [21, 26, 102] and multimodal [75, 84, 100] models.\n∗Work done as a student researcher at Google Brain.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n",
  "2": "Table 1: Accuracy of BASIC-L [72] on ImageNet and several robustness benchmarks. We apply Lion\nto both vision tower pre-training and vision-language contrastive training stages. The previous SOTA\nresults on zero-shot and fine-tuning ImageNet accuracy are 86.3% and 91.0% [100].\nOptimizer\nZero-shot\nFine-tune\nImageNet\nV2\nA\nR\nSketch\nObjectNet\nImageNet\nAdafactor\n85.7\n80.6\n85.6\n95.7\n76.1\n82.3\n90.9\nLion\n88.3\n81.2\n86.4\n96.8\n77.2\n82.9\n91.1\nFigure 1: Left: ImageNet fine-tuning accuracy vs. pre-\ntraining cost of ViT models on JFT-300M. Right: FID\nof the diffusion model on 2562 image generation. We\nuse DDPM for 1K steps w/o guidance to decode image.\nAs a reference, the FID of ADM is 10.94 [24].\nProgram 1: Discovered optimizer Lion.\nβ1 = 0.9 and β2 = 0.99 by default are\nderived from Program 4. It only tracks\nmomentum and uses the sign operation to\ncompute the update. The two gray lines\ncompute the standard decoupled weight\ndecay, where λ is the strength.\ndef train(weight, gradient, momentum, lr):\nupdate = interp(gradient, momentum, β1)\nupdate = sign(update)\nmomentum = interp(gradient, momentum, β2)\nweight_decay = weight * λ\nupdate = update + weight_decay\nupdate = update * lr\nreturn update, momentum\nAnother direction is to automatically discover such optimization algorithms. The learning to optimize\n(L2O) approach proposes to discover optimizers by training parameterized models, e.g., neural\nnetworks, to output the updates [1, 54, 63, 64]. However, those black-box optimizers, typically\ntrained on a limited number of small tasks, struggle to generalize to state-of-the-art settings where\nmuch larger models are trained with significantly more training steps. Another line of methods [5, 95]\napply reinforcement learning or Monte Carlo Sampling to discover new optimizers, where the search\nspace is defined by trees composed from predefined operands (e.g., gradient and momentum) and\noperators (e.g., unary and binary math operations). However, to make the search manageable, they\noften limit the search space by using fixed operands and restricting the size of the tree, thereby limiting\nthe potential for discovery. For example, they are unable to modify the tracking of momentum or how\nit contributes to the update, which is an essential component of Lion. Consequently, the algorithms\ndiscovered have not yet reached the state-of-the-art. AutoML-Zero [80] is an ambitious effort that\nattempts to search every component of a machine learning pipeline while evaluating on toy tasks.\nThis work follows the research direction of automatic discovering optimizers and is in particular\ninspired by AutoML-Zero, but aims at discovering effective optimization algorithms that can improve\nthe state-of-the-art benchmarks.\nIn this paper, we present a method to formulate algorithm discovery as program search and apply\nit to discover optimization algorithms. There are two primary challenges. The first one is to find\nhigh-quality algorithms in the infinite and sparse program space. The second one is to further select\nout the algorithms that can generalize from small proxy tasks to much larger, state-of-the-art tasks.\nTo tackle these challenges, we employ a range of techniques including evolutionary search with\nwarm-start and restart, abstract execution, funnel selection, and program simplification.\nOur method discovers a simple and effective optimization algorithm: Lion. This optimizer differs\nfrom various adaptive algorithms by only tracking momentum and leveraging the sign operation\nto calculate updates, leading to lower memory overhead and uniform update magnitudes across all\ndimensions. Despite its simplicity, Lion demonstrates outstanding performance across a range of\nmodels (Transformer, MLP, ResNet, U-Net, and Hybrid) and tasks (image classification, vision-\nlanguage contrastive learning, diffusion, language modeling, and fine-tuning). Notably, we achieve\n88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet by replacing Adafactor with Lion\nin BASIC [72], surpassing the previous best results by 2% and 0.1%, respectively. Additionally,\nLion reduces the pre-training compute on JFT by up to 5x, improves training efficiency on diffusion\nmodels by 2.3x and achieves a better FID score, and offers similar or better performance on language\nmodeling with up to 2x compute savings. Limitations of our work are discussed in Appendix N.\n2\n",
  "3": "Program 2: An example training loop,\nwhere the optimization algorithm that\nwe are searching for is encoded within\nthe train function. The main inputs\nare the weight (w), gradient (g) and\nlearning rate schedule (lr). The main\noutput is the update to the weight. v1\nand v2 are two additional variables for\ncollecting historical information.\nw = weight_initialize()\nv1 = zero_initialize()\nv2 = zero_initialize()\nfor i in range(num_train_steps):\nlr = learning_rate_schedule(i)\ng = compute_gradient(w, get_batch(i))\nupdate, v1, v2 = train(w, g, v1, v2, lr)\nw = w - update\nProgram 3: Initial pro-\ngram (AdamW). The bias\ncorrection and ϵ are omit-\nted for simplicity.\ndef train(w, g, m, v, lr):\ng2 = square(g)\nm = interp(g, m, 0.9)\nv = interp(g2, v, 0.999)\nsqrt_v = sqrt(v)\nupdate = m / sqrt_v\nwd = w * 0.01\nupdate = update + wd\nlr = lr * 0.001\nupdate = update * lr\nreturn update, m, v\nProgram 4:\nDiscovered\nprogram after search, se-\nlection and removing re-\ndundancies in the raw Pro-\ngram 8 (Appendix). Some\nvariables are renamed for\nclarity.\ndef train(w, g, m, v, lr):\ng = clip(g, lr)\ng = arcsin(g)\nm = interp(g, v, 0.899)\nm2 = m * m\nv = interp(g, m, 1.109)\nabs_m = sqrt(m2)\nupdate = m / abs_m\nwd = w * 0.4602\nupdate = update + wd\nlr = lr * 0.0002\nm = cosh(update)\nupdate = update * lr\nreturn update, m, v\n2\nSymbolic Discovery of Algorithms\nWe present an approach that formulates algorithm discovery as program search [11, 49, 80]. We use\na symbolic representation in the form of programs for the following advantages: (1) it aligns with the\nfact that algorithms must be implemented as programs for execution; (2) symbolic representations\nlike programs are easier to analyze, comprehend and transfer to new tasks compared to parameterized\nmodels such as neural networks; (3) program length can be used to estimate the complexity of\ndifferent programs, making it easier to select the simpler, often more generalizable ones. This work\nfocuses on optimizers for deep neural network training, but the method is generally applicable.\n2.1\nProgram Search Space\nWe adhere to the following three criteria while designing the program search space: (1) the search\nspace should be flexible enough to enable the discovery of novel algorithms; (2) the programs should\nbe easy to analyze and incorporate into a machine learning workflow; (3) the programs should focus\non the high-level algorithmic design rather than low-level implementation details. We define the\nprograms to contain functions operating over n-dimensional arrays, including structures like lists and\ndictionaries containing such arrays, in an imperative language. They are similar to Python code using\nNumPy / JAX [10, 36] as well as pseudo code of optimization algorithms. The details of the design\nare outlined below, with an example representation of AdamW in Program 3.\nInput / output signature The program defines a train function, which encodes the optimization\nalgorithm being searched for, where the main inputs are the model weight (w), the gradient (g) and\nthe learning rate schedule value (lr) at the current training step. The main output is the update to\nthe weight. The program also incorporates extra variables initialized as zeros to collect historical\ninformation during training. For example, AdamW requires two extra variables to estimate first\nand second moments. Note that those variables can be used arbitrarily, we use the name m and v\nin Program 3 just for better readability. This simplified code snippet in Program 2 uses the same\nsignature as AdamW to ensure that the discovered algorithms have smaller or equal memory footprints.\nAs opposed to previous optimizer search attempts [5, 95], our method allows discovering better ways\nof updating the extra variables.\nBuilding blocks The train function consists of a sequence of assignment statements, with no\nrestrictions on the number of statements or local variables. Each statement calls a function using\nconstants or existing variables as inputs, and the resulting value is stored in a new or existing variable.\nFor the program, we select 45 common math functions, most of which corresponds to a function in\nNumPy or an operation in linear algebra. Some functions are introduced to make the program more\ncompact, such as the linear interpolation function interp(x, y, a), which is made equivalent to\n(1 - a) * x + a * y. Preliminary experiments have investigated the inclusion of more advanced\nfeatures such as conditional and loop statements, and defining and calling new functions, but these\n3\n",
  "4": "Figure 2: Left: We run hyperparameter tuning on AdamW and random search, both with 4x more\ncompute, to get the best results as two baselines (green and red lines). The evolutionary search, with\nmean and standard error calculated from five runs, significantly outperforms both of them. The use\nof multiple restarts from the initial program is crucial due to the high variance in the search fitness\n(blue curves), and restarting from the best program after 300K progress further improves the fitness\n(orange curves) when the original search plateaus. Right: Example curves of search fitness, the cache\nhit rate, and the percentage of redundant statements. The cache hit rate and the redundant statements\npercentage increase along with the search progress to ∼90% and ∼70%.\ndo not yield improved results, so we leave them out. A detailed description of the functions are\nsummarized in Appendix H. When necessary, the types and shapes of the function arguments are\nautomatically cast, e.g., in the case of adding a dictionary of arrays to a scalar.\nMutations and redundant statements The design of mutations utilized in evolutionary search is\ntightly intertwined with the representation of the program. We include three types of mutations: (1)\ninserting a new statement at a random location with randomly chosen functions and arguments, (2)\ndeleting a random chosen statement, and (3) modifying a random statement by randomly altering\none of its function arguments, which may be either variables or constants. To mutate an argument,\nwe replace it with an existing variable or a newly generated constant obtained by sampling from a\nnormal distribution X ∼N(0 1). Additionally, we can mutate an existing constant by multiplying\nit by a random factor 2a, where a ∼N(0 1). These constants serve as tunable hyperparameters\nin the optimization algorithm, such as the peak learning rate and weight decay in AdamW. The\nmodification mutation makes it easier for the search to tune those constants while keeping most\nof the program unchanged. Note that we allow a program to include redundant statements during\nsearch, i.e., statements that do not impact the final program outputs. This is necessary as mutations\nare limited to only affecting a single statement. Redundant statements therefore serve as intermediate\nsteps towards bigger changes.\nInfinite and sparse search space Given the limitless number of statements and local variables, as\nwell as the presence of mutable constants, the program search space is infinite. Even if we ignore the\nconstants and bound the program length and number of variables, the number of potential programs\nis still intractably large. More importantly, the challenge comes from the sparsity of high-performing\nprograms in the search space. To illustrate this point, we evaluates 2M randomly sampled programs\non a low-cost proxy task. The best program among them is still significantly inferior to AdamW.\n2.2\nEfficient Search Techniques\nWe employ the following techniques to address the challenges posed by the infinite and sparse space.\nEvolution with warm-start and restart We apply regularized evolution as it is simple, scalable,\nand has shown success on many AutoML search tasks [42, 79, 80, 87, 99]. It keeps a population\nof P algorithms that are gradually improved through cycles. Each cycle picks T<P algorithms at\nrandom and the best performer is chosen as the parent, i.e., tournament selection [32]. This parent is\nthen copied and mutated to produce a child algorithm, which is added to the population, while the\noldest algorithm is removed. Normally, evolutionary search starts with random candidates, but we\nwarm-start the initial population as AdamW to accelerate the search. By default, we use a tournament\nsize of two and a population size of 1K. To further improve the search efficiency, we apply two\ntypes of restart: (1) restarting from the initial program, which can lead to different local optima due\nto the randomness in evolution and encourage exploration. This can be done by running multiple\nsearches in parallel. (2) restarting from the best algorithm found thus far to further optimize it,\nencouraging exploitation. Figure 2 (Left) displays the mean and standard error of five evolutionary\nsearch experiments. We run hyperparameter tuning based on AdamW by only allowing mutations of\nconstants in the evolution, and run random search by sampling random programs, both with 4x more\n4\n",
  "5": "compute. Our search significantly outperforms the best results from both baselines, shown as the\ntwo dashed lines. The high variance in the search fitness necessitates running multiple repeats by\nrestarting from the initial program. When the search fitness plateaus after ∼300K progress, restarting\nfrom the best program found thus far further improves the fitness shown by the orange curve.\nPruning through abstract execution We propose to prune the redundancies in the program space\nfrom three sources: programs with syntax or type / shape errors, functionally equivalent programs,\nand redundant statements in the programs. Before a program is actually executed, we perform an\nabstract execution step that (1) infers variable types and shapes to detect programs with errors; (2)\nproduces a hash that uniquely identifies how the outputs are computed from the inputs, allowing us to\ncache and look up semantically duplicate programs [31]; (3) identifies redundant statements that can\nbe ignored during actual execution and analysis. For instance, Program 4 is obtained after removing\nall redundant statements in Program 8 (Appendix). Abstract execution has negligible cost compared\nto the actual execution, with each input and function replaced by customized values, e.g., hash. See\nAppendix I for details of abstract execution. Preliminary experiments have shown that the search\nprocess can become overwhelmed with invalid programs and cannot make progress without filtering\nout invalid programs. As seen in Figure 2 (Right), the percentage of redundant statements and cache\nhit rate both increase as the search proceeds. Based on five search runs, each covering 300K programs,\nthere are 69.8 ± 1.9% redundant statements towards the end, implying that redundant statements\nremoval makes the program ∼3x shorter on average. The cache hit rate is 89.1 ± 0.6%, indicating\nthat using the hash table as cache brings ∼10x reduction on the search cost.\nProxy tasks and search cost To reduce search cost, we create low-cost proxies by decreasing the\nmodel size, number of training examples, and steps from the target tasks. Evaluation on the proxies\ncan be completed on one TPU V2 chip within 20min. We use the accuracy or perplexity on the\nvalidation set as the fitness. Each search experiment utilizes 100 TPU V2 chips and runs for ∼72h.\nThere are a total of 200-300K programs generated during each search experiment. However, the\nnumber of programs that are actually evaluated is around 20-30K, thanks to the use of the cache\nthrough abstract execution. To incorporate restart, we start five repeats of search experiments,\nfollowed by another round of search initializing from the best algorithm found thus far. This results\nin a total cost of ∼3K TPU V2 days. See Appendix F for the details of proxy tasks.\n2.3\nGeneralization: Program Selection and Simplification\nThe search experiments can discover promising programs on proxy tasks. We use performance on\nmeta-validation tasks that are larger than the proxy tasks by increasing the model size and training\nsteps, to select the programs that generalize beyond proxy tasks then further simplify them. The\nphenomenon of meta-overfitting occurs when the search fitness keeps growing, but the meta-validation\nmetric declines, indicating that the discovered algorithms have overfit the proxy tasks. Two examples\nmeta-validation curves are shown in Figure 11 (Left) in the Appendix.\nLarge generalization gap The discovered algorithms face a significant challenge due to the substan-\ntial gap between the proxy tasks during search and the target tasks. While proxy tasks can typically\nbe completed within 20min on one TPU V2 chip, target tasks can be > 104x larger and require days\nof training on 512 TPU V4 chips. Furthermore, we expect the optimizer to perform well on different\narchitectures, datasets and even different domains, so the discovered algorithms need to show strong\nout-of-distribution generalization. The sparse search space and inherent noise in the evolution process\nfurther compound this challenge, leading to inconsistent generalization properties between different\nruns. Our observation suggests that evolutionary search runs that meta-overfit later tend to uncover\noptimization algorithms that generalize better. See more details in Figure 11 (Right) in the Appendix.\nFunnel selection To mitigate the generalization gap, we collect promising programs based on search\nfitness and add an extra selection step using a series of meta-validation tasks to select those generalize\nbetter. To save compute, we apply a funnel selection process that gradually increases the scale of\nthe meta-validation tasks. For example, starting with proxy task A, we create a 10x larger task B\nby increasing the model size and the training steps. Only algorithms that surpass the baseline on\ntask B will be evaluated on task C, which is 100x larger. This approach allows us to gradually filter\nout algorithms that show poor generalization performance, ultimately leading to the selection of\nalgorithms that generalize well to larger tasks.\n5\n",
  "6": "Simplification Simpler programs are easier to understand and our intuition is that they are more likely\nto generalize, so we simplify the programs with the following steps. Firstly, we remove redundant\nstatements that do not contribute to the final output as identified through abstract execution. Secondly,\nwe remove statements that are non-redundant but produce minimal differences when removed. This\nstep can also be achieved through evolution by disabling the insertion of new statements in the\nmutation process. Finally, we rearrange the statements manually, assign clear and descriptive names\nto variables, and convert the program into its simpler, mathematically equivalent form.\n3\nDerivation and Analysis of Lion\nWe arrive at the Lion optimizer due to its simplicity, memory efficiency, and strong performance in\nsearch and meta-validation. The search also discovers other algorithms shown in Appendix D, e.g.,\nsome with better regularization and some resembling AdaBelief [105] and AdaGrad [29].\n3.1\nDerivation\nThe search and funnel selection process lead to Program 4, which is obtained by automatically\nremoving redundant statements from the raw Program 8 (in the Appendix). We further simplify\nit to get the final algorithm (Lion) in Program 1. Several unnecessary elements are removed from\nProgram 4 during the simplification process. The cosh function is removed since m would be\nreassigned in the next iteration (line 3). The statements using arcsin and clip are also removed\nas we observe no quality drop without them. The three red statements translate to a single sign\nfunction. Although both m and v are utilized in Program 4, v only changes how the momentum is\nupdated (two interp functions with constants ∼0.9 and ∼1.1 is equivalent to one with ∼0.99) and\ndoes not need to be separately tracked. Note that the bias correction is no longer needed, as it does\nnot change the direction. Algorithm 2 in the Appendix shows the pseudocode.\n3.2\nAnalysis\nSign update and regularization The Lion algorithm produces update with uniform magnitude\nacross all dimensions by taking the sign operation, which is in principle different from various\nadaptive optimizers. Intuitively, the sign operation adds noise to the updates, which acts as a form of\nregularization and helps with generalization [16, 30, 67]. An evidence is shown in Figure 9 (Right) in\nthe Appendix, where the ViT-B/16 trained by Lion on ImageNet has a higher training error compared\nto AdamW but a 2% higher accuracy on the validation set (as shown in Table 8 from the Appendix).\nAdditionally, the results in Appendix G demonstrate that Lion leads to the convergence in smoother\nregions, which usually results in better generalization.\nMomentum tracking The default EMA factor used to track the momentum in Lion is 0.99 (β2),\ncompared to the commonly used 0.9 in AdamW and momentum SGD. The current gradient and\nmomentum are interpolated with a factor of 0.9 (β1) before the sign operation is applied. This choice\nof EMA factor and interpolation allows Lion to balance between remembering a ∼10x longer history\nof the gradient in momentum and putting more weight on the current gradient in the update. The\nnecessity of both β1 and β2 is further discussed in Appendix L.\nHyperparameter and batch size choices Lion is simpler and has fewer hyperparameters compared\nto AdamW and Adafactor as it does not require ϵ and factorization-related ones. The update is an\nelement-wise binary ±1 if we omit the weight decay term, with larger norm than those produced by\nother optimizers like SGD and adaptive algorithms. As a result, Lion needs a smaller learning rate\nand in turn a larger decoupled weight decay to achieve a similar effective weight decay strength (lr\n* λ). Detailed information on tuning Lion can be found in Appendix M. Additionally, the advantage\nof Lion over AdamW enlarges as the batch size increases, which fits the common practice of scaling\nup model training through data parallelism (Appendix L).\nMemory and runtime benefits Lion only saves the momentum thus has smaller memory footprint\nthan popular adaptive optimizers like AdamW, which is beneficial when training large models and / or\nusing a large batch size. As an example, AdamW needs at least 16 TPU V4 chips to train a ViT-B/16\nwith image resolution 224 and batch size 4,096, while Lion only needs 8 (both with bfloat16\nmomentum). Another practical benefit is that Lion has faster runtime (steps / sec) in our experiments\n6\n",
  "7": "due to its simplicity, usually 2-15% speedup compared to AdamW and Adafactor depending on the\ntask, codebase, and hardware.\nRelation to existing optimizers The sign operation has been explored in previous optimizers [7, 83].\nThe closest to ours is the handcrafted optimizer signSGD [7] (and its momentum variant) that also\nutilizes the sign operation to calculate the update but has a different momentum update rule from\nLion. Their focus is to mitigate communication costs between agents in distributed training, and they\nobserve inferior performance when training ConvNets on image classification tasks. On the other\nhand, NAdam [27] combines the updated first moment and the gradient to compute the update, but\nLion decouples the momentum tracking and how it is applied to the update through β2. A comparison\nof Lion with related optimizers can be found in Appendix K.\n4\nEvaluation of Lion\nIn this section, we present evaluations of Lion, on various benchmarks. We mainly compare it to\nAdamW (or Adafactor when memory is a bottleneck) as it is exceedingly popular and the de facto\nstandard optimizer on a majority of learning tasks. The result of momentum SGD is only included for\nResNet since it performs worse than AdamW elsewhere. We also benchmark other popular optimizers\nin Appendix K, including handcrafted and automatically discovered ones. We make sure that every\noptimizer is well-tuned for each task (see Appendix M for tuning details). By default, the learning\nrate schedule is cosine decay with 10K steps warmup, and the momentum is saved as bfloat16 to\nreduce the memory footprint. Ablations studies are performed in Appendix L.\n4.1\nImage Classification\nWe perform experiments including various datasets and architectures on the image classification task\n(see Appendix B for dataset details). Apart from training from scratch on ImageNet, we also pre-train\non two larger well-established datasets, ImageNet-21K and JFT [89]. The image size is 2242 by\ndefault otherwise specified by the subscript.\nTrain from scratch on ImageNet Following previous works [26, 37], we train ResNet-50 for 90\nepochs with a batch size of 1,024, and other models for 300 epochs with a batch size of 4,096. As\nshown in Table 8 (in the Appendix), Lion significantly outperforms AdamW on various architectures.\nEmpirically, the improvement is more substantial on models with larger capacity, with accuracy\nincreases of 1.96% and 0.58% for ViT-B/16 and ViT-S/16, respectively. The performance gaps also\ntend to enlarger with fewer inductive biases. When strong augmentations are applied, the gain of\nLion over AdamW shrinks, but it still outperforms AdamW by 0.42% on CoAtNet-3, despite the\nstrong regularization during training [21].\nPre-train on ImageNet-21K We pre-train ViT-B/16 and ViT-L/16 on ImageNet-21K for 90 epochs\nwith a batch size of 4,096. Table 8 shows that Lion surpasses AdamW when the training set is\nenlarged for 10x. The gaps on larger models are bigger, with +0.52% vs. +0.33% (ImageNet),\n+0.57% vs. +0.23% (ReaL), and +0.74% vs. +0.25% (V2) for ViT-L/16 and ViT-B/16, respectively.\nPre-train on JFT To push the limit, we conduct extensive experiments on JFT. We follow the settings\nof Dosovitskiy et al. [26] and Zhai et al. [102] for both pre-training and fine-tuning. Figure 1 (Left)\nand 3 present the accuracy of three ViT models (ViT-B/16, ViT-L/16, and ViT-H/14) under different\npre-training budgets on JFT-300M. Lion enables the ViT-L/16 to match the performance of ViT-H/14\ntrained by AdamW on ImageNet and ImageNet V2 but with 3x less pre-training cost. On ImageNet\nReaL, the compute saving further becomes 5x. Another evidence is that even when a ViT-L/16 is\ntrained by AdamW for 4M steps by Zhai et al. [102], its performance still lags behind the same model\ntrained by Lion for 1M steps.\nTable 5 in the Appendix shows the fine-tuning results, with higher resolution and EMA. Our ViT-L/16\nmatches the previous ViT-H/14 results trained by AdamW, while being 2x smaller. The advantage\nis larger on more challenging benchmarks, such as +1.33% (V2), +6.08% (A), +5.54% (R) for ViT-\nL/16. When pretrained on JFT-3B, the ViT-g/14 trained by Lion outperforms the previous ViT-G/14\nresults [102], with 1.8x fewer parameters. Our ViT-G/14 achieves a 90.71% accuracy on ImageNet.\n7\n",
  "8": "Figure 3: ImageNet ReaL (Left) and ImageNet V2\n(Right) accuracy after we pre-train ViT models on\nJFT-300M then fine-tune on ImageNet. See Table 4\n(in the Appendix) for the detailed numbers.\nTable 2: Zero-shot accuracy of LiTs on Im-\nageNet, CIFAR-100, and Oxford-IIIT Pet.\nAs a reference, the zero-shot accuracy of\nCLIP [75] on ImageNet is 76.2%.\nModel\nOptimizer\nImageNet\nC100\nPet\nLiT-B/32-B\nAdamW\n68.78\n71.41\n86.62\nLion\n69.88\n71.78\n87.36\nLiT-B/16-B\nAdamW\n74.26\n72.25\n89.83\nLion\n75.39\n72.49\n91.20\nLiT-g/14288-L\nAdamW\n83.43\n80.93\n94.88\nLion\n84.09\n81.43\n95.86\nTable 3: One-shot evaluation averaged over three NLG and 21 NLU tasks. The results of GPT-3 [12]\nand PaLM [17] are included for reference. The LLMs trained by Lion have better in-context learning\nability. See Table 11 (in the Appendix) for detailed results on all tasks.\nTask\n1.1B\n2.1B\n7.5B\n6.7B\nGPT-3\n8B\nPaLM\nAdafactor\nLion\nAdafactor\nLion\nAdafactor\nLion\n#Tokens\n300B\n300B\n780B\nAvg NLG\n11.1\n12.1\n15.6\n16.5\n24.1\n24.7\n23.1\n23.9\nAvg NLU\n53.2\n53.9\n56.8\n57.4\n61.3\n61.7\n58.5\n59.4\n4.2\nVision-Language Contrastive Learning\nThis section focuses on the vision-language contrastive training [75]. We compare Lion with AdamW\n(Adafactor) on zero-shot image classification and image-text retrieval benchmarks. We initialize the\nimage encoder with a strong pre-trained model as it is suggested to be more efficient [103].\nLocked-image text Tuning (LiT) We perform a comparison between Lion and AdamW on LiT [103]\nby training the text encoder [103] in a contrastive manner using the same frozen pre-trained ViT. All\nmodels are trained for 1B image-text pairs with a batch size of 16,384. Table 2 shows the zero-shot\nimage classification results on three model scales, with the name specifies the size, e.g., LiT-B/16-B\ndenotes a ViT-B/16 and a base size Transformer as the text encoder. Our method, Lion, demonstrates\nconsistent improvement over AdamW with gains of +1.10%, +1.13%, and +0.66% on zero-shot\nImageNet accuracy for LiT-B/32-B, LiT-B/16-B, and LiT-g/14288-L, respectively. Figure 7 (Left)\nin the Appendix depicts an example zero-shot learning curve of LiT-B/16-B. Similar results are\nobtained on the other two datasets. The zero-shot image-text retrieval results on MSCOCO [56] and\nFlickr30K [74] can be found in Figure 6 (in the Appendix). The evaluation metric is Recall@K,\ncalculated based on if the ground truth label of the query appears in the top-K retrieved examples.\nLion outperforms AdamW on both datasets, with a larger gain in Recall@1 than Recall@10 on\nFlicker30K, implying more accurate retrieval results: +1.70% vs. +0.60% for image →text and\n+2.14% vs. +0.20% for text →image.\nBASIC Pham et al. [72] propose to scale up batch size, dataset, and model size simultaneously,\nachieving drastic improvements over CLIP. It uses a sophisticated CoAtNet [21] pre-trained on\nJFT-5B as the image encoder. Furthermore, the contrastive training is performed on 6.6B image-text\npairs with a larger 65,536 batch size. To push the limit, we only experiment on the largest BASIC-L,\nand use Lion on both image encoder pre-training and contrastive learning stages. As illustrated in\nTable 1, we achieve a significant 2.6% gain over the baseline, striking a 88.3% accuracy on zero-shot\nImageNet classification. Note that this result is 2.0% higher than the previous best result [100]. The\nperformance gain is consistent on five other robustness benchmarks. After fine-tuning the image\nencoder (CoAtNet-7) in BASIC-L obtained by Lion, we further achieve a 91.1% top-1 accuracy on\nImageNet, which is 0.1% better than the previous SOTA.\n4.3\nDiffusion Model\nRecently, diffusion models achieve a huge success on image generation [24, 40, 41, 84, 88], so we test\nthe performance of Lion on unconditional image synthesis and multimodal text-to-image generation.\n8\n",
  "9": "Figure 4: Imagen text-to-image 642 (Left) and\nthe 642 →2562 diffusion models (Right).\nFigure 5: Log perplexity on Wiki-40B (Left)\nand PG-19 (Right). The speedup brought by\nLion tends to increase with the model scale. The\nlargest model on Wiki-40B is omitted as we ob-\nserve severe overfitting.\nImage synthesis on ImageNet We utilize the improved U-Net architecture [24] and perform 64 × 64,\n128 × 128, and 256 × 256 image generation on ImageNet. The batch size is 2,048 and the learning\nrate remains constant throughout training. For decoding, we apply DDPM [41] for 1K sampling\nsteps without classifier-free guidance. The evaluation metric is the standard FID score. Illustrated by\nFigure 1 (Right) and Figure 7 (Middle and Right) in the Appendix, Lion enables both better quality\nand faster convergence on the FID score. The gap between Lion and AdamW increases with the\nimage resolution, where the generation becomes more challenging. When generating 256 × 256\nimages, Lion achieves the final performance of AdamW with 2.3x fewer steps. The final FID scores\nare 4.1 (Lion) vs. 4.7 (AdamW). For reference, the FID of ADM [24] is 10.94.\nText-to-image generation We follow the Imagen [84] setup to train a base 64 × 64 text-to-image\nmodel and a 64 × 64 →256 × 256 super-resolution model. All models are trained on a high-\nquality internal image-text dataset with a batch size of 2,048 and a constant learning rate. Due to\ncomputational constraints, our base U-Net has a width of 192 compared to 512 in the original 2B\nmodel, while the 600M super-resolution model is identical to the original Imagen setup. Along with\nthe training, 2K images are sampled from the MSCOCO [56] validation set for real-time evaluation.\nWe use the CLIP score to measure image-text alignment and the zero-shot FID-30K to measure image\nfidelity. Classifier-free guidance [40] with a weight of 5.0 is applied as it has been shown to improve\nimage-text alignment. Figure 4 depicts the learning curve. While there is no clear improvement on\nthe base 64 × 64 model, Lion outperforms AdamW on the text-conditional super-resolution model. It\nachieves a higher CLIP score and has a less noisy FID metric compared to AdamW.\n4.4\nLanguage Modeling and Fine-tuning\nThis section focuses on language modeling and fine-tuning. On language-only tasks, we find that\ntuning β1 and β2 can improve the quality for both AdamW and Lion. See Appendix M for tuning\ndetails. The masked language modeling and fine-tuning results are shown in Appendix J.\nAutoregressive language modeling We first experiment on two smaller-scale academic datasets\nWiki-40B [34] and PG-19 [76]. The employed Transformer spans three scales: small (110M),\nmedium (336M), and large (731M). The architecture details can be found in Appendix E. All models\nare trained with 218 tokens per batch for 125K steps, with a learning rate schedule of 10K steps\nwarmup followed by linear decay. The context length is set to 512 for Wiki-40B and 1,024 for PG-19.\nFigure 5 illustrates the token-level perplexity for Wiki-40B and word-level perplexity for PG-19. Lion\nconsistently achieves lower validation perplexity than AdamW. It achieves 1.6x and 1.5x speedup\nwhen training the medium size model on Wiki-40B and PG-19, respectively. When the model is\nincreased to the large size, the speedup on PG-19 further increases to 2x.\nWe then conduct larger experiments with the pre-training dataset similar to GLaM [28]. Following\nGPT-3 [12], we train three models, from 1.1B to 7.5B parameters, on 300B tokens with a batch size\nof 3M tokens and a context length of 1K. We evaluate them on three natural language generative\n(NLG) and 21 natural language understanding (NLU) tasks (see Appendix C for details). We observe\nno difference in perplexity throughout training. Nevertheless, Lion outperforms Adafactor on the\naverage in-context learning ability, as shown in Table 3. Our 7.5B baseline model, trained for 300B\ntokens, outperforms the 8B PaLM, trained for 780B tokens, demonstrating the strength of our setup.\nLion outperforms Adafactor on both NLG and NLU tasks, particularly on the NLG tasks, with an\nexact match improvement of +1.0, +0.9, and +0.6 for the 1.1B, 2.1B, and 7.5B models, respectively.\n9\n",
  "10": "5\nRelated Work\nOur work lies in the area of AutoML and meta-learning that includes learning to learn [1, 5, 63, 64,\n78, 96, 97], neural architecture search [14, 15, 57, 71, 79, 87, 93, 94, 98, 106] and hyperparameter\noptimization [25, 43, 44, 55], etc. There is also a long history of using evolutionary methods to\nsearch for programs, i.e., genetic programming [11, 42, 49]. Our approach builds upon a symbolic\nsearch space similar to AutoML-Zero [70, 80]. However, instead of discovering programs with fixed\ndimensional matrices, vector, and scalars for toy tasks, our goal is to develop programs that operate\non n-dimensional arrays and can generalize to state-of-the-art tasks. Other related works include\nnumerous handcrafted optimizers [2, 7, 27, 29, 35, 48, 58, 61, 82, 83, 86, 105].\n6\nConclusion\nThis paper aims at discovering optimization algorithms via program search. Despite the challenges\nfrom an infinite and sparse space, and large generalization gap between the proxy and target tasks,\nour method discovers a simple and effective optimizer, Lion, that is memory-efficient and achieves\nstrong generalization across architectures, datasets and tasks.\nAcknowledgements\nWe would like to thank (in alphabetical order) Angel Yu, Boqing Gong, Chen Cheng, Chitwan\nSaharia, Daiyi Peng, David So, Hanxiao Liu, Hanzhao Lin, Jeff Lund, Jiahui Yu, Jingru Xu, Julian\nGrady, Junyang Shen, Kevin Regan, Li Sheng, Liu Yang, Martin Wicke, Mingxing Tan, Mohammad\nNorouzi, Qiqi Yan, Rakesh Shivanna, Rohan Anil, Ruiqi Gao, Steve Li, Vlad Feinberg, Wenbo Zhang,\nWilliam Chan, Xiao Wang, Xiaohua Zhai, Yaguang Li, Yang Li, Zhuoshu Li, Zihang Dai, Zirui\nWang for helpful discussions, and the Google Brain team at large for providing a supportive research\nenvironment.\nReferences\n[1] Marcin Andrychowicz, Misha Denil, Sergio Gómez, Matthew W Hoffman, David Pfau, Tom\nSchaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent\nby gradient descent. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.,\n2016.\n[2] Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Scalable second\norder optimization for deep learning, 2020.\n[3] Lukas Balles and Philipp Hennig. Dissecting adam: The sign, magnitude and variance\nof stochastic gradients. In Jennifer Dy and Andreas Krause, editors, Proceedings of the\n35th International Conference on Machine Learning, volume 80 of Proceedings of Machine\nLearning Research, pages 404–413. PMLR, 10–15 Jul 2018.\n[4] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund,\nJosh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing\nthe limits of object recognition models. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc., 2019.\n[5] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V. Le. Neural optimizer search with\nreinforcement learning. In Proceedings of the 34th International Conference on Machine\nLearning - Volume 70, ICML’17, page 459–468. JMLR.org, 2017.\n[6] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase\nfrom question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in\nNatural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.\nAssociation for Computational Linguistics.\n10\n",
  "11": "[7] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.\nsignSGD: Compressed optimisation for non-convex problems. In Jennifer Dy and Andreas\nKrause, editors, Proceedings of the 35th International Conference on Machine Learning,\nvolume 80 of Proceedings of Machine Learning Research, pages 560–569. PMLR, 10–15 Jul\n2018.\n[8] Lucas Beyer, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord.\nAre we done with imagenet?, 2020.\n[9] Yonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning\nabout physical commonsense in natural language. Proceedings of the AAAI Conference on\nArtificial Intelligence, 34(05):7432–7439, Apr. 2020. doi: 10.1609/aaai.v34i05.6239.\n[10] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and\nQiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL\nhttp://github.com/google/jax.\n[11] Markus Brameier, Wolfgang Banzhaf, and Wolfgang Banzhaf. Linear genetic programming,\nvolume 1. Springer, 2007.\n[12] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot\nlearners, 2020.\n[13] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn.\nOne billion word benchmark for measuring progress in statistical language modeling. CoRR,\nabs/1312.3005, 2013.\n[14] Xiangning Chen and Cho-Jui Hsieh.\nStabilizing differentiable architecture search via\nperturbation-based regularization. In Hal Daumé III and Aarti Singh, editors, Proceedings\nof the 37th International Conference on Machine Learning, volume 119 of Proceedings of\nMachine Learning Research, pages 1554–1565. PMLR, 13–18 Jul 2020.\n[15] Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, and Cho-Jui Hsieh. DrNAS:\nDirichlet neural architecture search. In International Conference on Learning Representations,\n2021.\n[16] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform\nresnets without pre-training or strong data augmentations. In International Conference on\nLearning Representations, 2022.\n[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,\nReiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju\nDuke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie\nPellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm:\nScaling language modeling with pathways, 2022.\n[18] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions.\n11\n",
  "12": "In Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 2924–2936, Minneapolis, Minnesota, June 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1300.\n[19] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge, 2018.\n[20] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment\nchallenge. In Joaquin Quiñonero-Candela, Ido Dagan, Bernardo Magnini, and Florence\nd’Alché Buc, editors, Machine Learning Challenges. Evaluating Predictive Uncertainty,\nVisual Object Classification, and Recognising Tectual Entailment, pages 177–190, Berlin,\nHeidelberg, 2006. Springer Berlin Heidelberg.\n[21] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution\nand attention for all data sizes. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,\npages 3965–3977. Curran Associates, Inc., 2021.\n[22] Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank:\nInvestigating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung,\n23(2):107–124, Jul. 2019. doi: 10.18148/sub/2019.v23i2.601.\n[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–\n4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1423.\n[24] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems, volume 34, pages 8780–8794. Curran\nAssociates, Inc., 2021.\n[25] Xuanyi Dong, Mingxing Tan, Adams Wei Yu, Daiyi Peng, Bogdan Gabrys, and Quoc V Le.\nAutoHAS: Efficient hyperparameter and architecture search. In 2nd Workshop on Neural\nArchitecture Search at ICLR, 2021.\n[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on Learning Representations, 2021.\n[27] Timothy Dozat. Incorporating Nesterov Momentum into Adam. In Proceedings of the 4th\nInternational Conference on Learning Representations, pages 1–4, 2016.\n[28] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,\nMaxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P\nBosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson,\nKathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu,\nZhifeng Chen, and Claire Cui. GLaM: Efficient scaling of language models with mixture-of-\nexperts. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and\nSivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning,\nvolume 162 of Proceedings of Machine Learning Research, pages 5547–5569. PMLR, 17–23\nJul 2022.\n[29] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine Learning Research, 12(61):2121–2159, 2011.\n[30] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware\nminimization for efficiently improving generalization. In International Conference on Learning\nRepresentations, 2021.\n12\n",
  "13": "[31] Ryan Gillard, Stephen Jonany, Yingjie Miao, Michael Munn, Connal de Souza, Jonathan\nDungay, Chen Liang, David R. So, Quoc V. Le, and Esteban Real. Unified functional hashing\nin automatic machine learning, 2023.\n[32] David E Goldberg and Kalyanmoy Deb. A comparative analysis of selection schemes used in\ngenetic algorithms. FOGA, 1991.\n[33] Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of\nplausible alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The\nFirst Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of\nthe main conference and the shared task, and Volume 2: Proceedings of the Sixth International\nWorkshop on Semantic Evaluation (SemEval 2012), pages 394–398, Montréal, Canada, 7-8\nJune 2012. Association for Computational Linguistics.\n[34] Mandy Guo, Zihang Dai, Denny Vrandeˇci´c, and Rami Al-Rfou. Wiki-40B: Multilingual\nlanguage model dataset. In Proceedings of the Twelfth Language Resources and Evaluation\nConference, pages 2440–2452, Marseille, France, May 2020. European Language Resources\nAssociation. ISBN 979-10-95546-34-4.\n[35] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor\noptimization. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International\nConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,\npages 1842–1850. PMLR, 10–15 Jul 2018.\n[36] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen,\nDavid Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert\nKern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane,\nJaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin\nSheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E.\nOliphant. Array programming with NumPy. Nature, 585(7825):357–362, September 2020.\ndoi: 10.1038/s41586-020-2649-2.\n[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 770–778, 2016. doi: 10.1109/CVPR.2016.90.\n[38] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,\nRahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin\nGilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages\n8340–8349, October 2021.\n[39] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural\nadversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 15262–15271, June 2021.\n[40] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022.\n[41] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural\nInformation Processing Systems, volume 33, pages 6840–6851. Curran Associates, Inc., 2020.\n[42] John H Holland. Adaptation in natural and artificial systems: an introductory analysis with\napplications to biology, control, and artificial intelligence. MIT press, 1992.\n[43] Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization\nfor general algorithm configuration. In Carlos A. Coello Coello, editor, Learning and Intelligent\nOptimization, pages 507–523, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.\n[44] Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparam-\neter optimization. In Arthur Gretton and Christian C. Robert, editors, Proceedings of the 19th\nInternational Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of\nMachine Learning Research, pages 240–248, Cadiz, Spain, 09–11 May 2016. PMLR.\n13\n",
  "14": "[45] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale\ndistantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\ndoi: 10.18653/v1/P17-1147.\n[46] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping\nTak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp\nminima. In International Conference on Learning Representations, 2017.\n[47] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Look-\ning beyond the surface: A challenge set for reading comprehension over multiple sentences.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages\n252–262, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:\n10.18653/v1/N18-1023.\n[48] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014.\n[49] John R Koza. Genetic programming as a means for programming computers by natural\nselection. Statistics and computing, 4:87–112, 1994.\n[50] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,\n2009.\n[51] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova,\nLlion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc\nLe, and Slav Petrov. Natural questions: A benchmark for question answering research.\nTransactions of the Association for Computational Linguistics, 7:452–466, 2019.\n[52] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale\nReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing, pages 785–794, Copenhagen, Denmark,\nSeptember 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082.\n[53] Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.\nIn Proceedings of the Thirteenth International Conference on Principles of Knowledge Repre-\nsentation and Reasoning, KR’12, page 552–561. AAAI Press, 2012. ISBN 9781577355601.\n[54] Ke Li and Jitendra Malik. Learning to optimize. In International Conference on Learning\nRepresentations, 2017.\n[55] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar.\nHyperband: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn.\nRes., 18(1):6765–6816, jan 2017. ISSN 1532-4435.\n[56] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David\nFleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision – ECCV\n2014, pages 740–755, Cham, 2014. Springer International Publishing.\n[57] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search.\nIn International Conference on Learning Representations, 2019.\n[58] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao,\nand Jiawei Han. On the variance of the adaptive learning rate and beyond. In International\nConference on Learning Representations, 2020.\n[59] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert\npretraining approach, 2019.\n14\n",
  "15": "[60] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\nConference on Learning Representations, 2019.\n[61] Jerry Ma and Denis Yarats. Quasi-hyperbolic momentum and adam for deep learning. In\nInternational Conference on Learning Representations, 2019.\n[62] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored\napproximate curvature. In International conference on machine learning, pages 2408–2417.\nPMLR, 2015.\n[63] Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-\nDickstein. Understanding and correcting pathologies in the training of learned optimizers. In\nKamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International\nConference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,\npages 4556–4565. PMLR, 09–15 Jun 2019.\n[64] Luke Metz, James Harrison, C. Daniel Freeman, Amil Merchant, Lucas Beyer, James Bradbury,\nNaman Agrawal, Ben Poole, Igor Mordatch, Adam Roberts, and Jascha Sohl-Dickstein. Velo:\nTraining versatile learned optimizers by scaling up, 2022.\n[65] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor\nconduct electricity? a new dataset for open book question answering. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, pages 2381–2391,\nBrussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1260.\n[66] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy\nVanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper\nunderstanding of commonsense stories. In Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, pages 839–849, San Diego, California, June 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/N16-1098.\n[67] Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Lukasz Kaiser, Karol Kurach, Ilya Sutskever,\nand James Martens. Adding gradient noise improves learning for very deep networks, 2017.\n[68] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\nAdversarial NLI: A new benchmark for natural language understanding. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885–\n4901, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-main.441.\n[69] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE Conference\non Computer Vision and Pattern Recognition, 2012.\n[70] Daiyi Peng, Xuanyi Dong, Esteban Real, Mingxing Tan, Yifeng Lu, Gabriel Bender, Hanxiao\nLiu, Adam Kraft, Chen Liang, and Quoc Le. Pyglove: Symbolic programming for automated\nmachine learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors,\nAdvances in Neural Information Processing Systems, volume 33, pages 96–108. Curran\nAssociates, Inc., 2020.\n[71] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture\nsearch via parameters sharing. In Jennifer Dy and Andreas Krause, editors, Proceedings of the\n35th International Conference on Machine Learning, volume 80 of Proceedings of Machine\nLearning Research, pages 4095–4104. PMLR, 10–15 Jul 2018.\n[72] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu,\nJiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, and Quoc V. Le.\nCombined scaling for open-vocabulary image classification, 2021.\n[73] Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for\nevaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference\n15\n",
  "16": "of the North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Papers), pages 1267–1273, Minneapolis, Min-\nnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1128.\n[74] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier,\nand Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for\nricher image-to-sentence models. In 2015 IEEE International Conference on Computer Vision\n(ICCV), pages 2641–2649, 2015. doi: 10.1109/ICCV.2015.303.\n[75] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and\nIlya Sutskever. Learning transferable visual models from natural language supervision. In\nMarina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages\n8748–8763. PMLR, 18–24 Jul 2021.\n[76] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.\nCompressive transformers for long-range sequence modelling. In International Conference on\nLearning Representations, 2020.\n[77] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\n[78] Sachin Ravi and Hugo Larochelle.\nOptimization as a model for few-shot learning.\nIn\nInternational Conference on Learning Representations, 2017.\n[79] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution\nfor image classifier architecture search. Proceedings of the AAAI Conference on Artificial\nIntelligence, 33(01):4780–4789, Jul. 2019. doi: 10.1609/aaai.v33i01.33014780.\n[80] Esteban Real, Chen Liang, David So, and Quoc Le. Automl-zero: Evolving machine learning\nalgorithms from scratch. In International Conference on Machine Learning, pages 8007–8019.\nPMLR, 2020.\n[81] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet\nclassifiers generalize to ImageNet?\nIn Kamalika Chaudhuri and Ruslan Salakhutdinov,\neditors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of\nProceedings of Machine Learning Research, pages 5389–5400. PMLR, 09–15 Jun 2019.\n[82] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond.\nIn International Conference on Learning Representations, 2018.\n[83] M. Riedmiller and H. Braun. A direct adaptive method for faster backpropagation learning:\nthe rprop algorithm. In IEEE International Conference on Neural Networks, pages 586–591\nvol.1, 1993. doi: 10.1109/ICNN.1993.298623.\n[84] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans,\nJonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion\nmodels with deep language understanding. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n[85] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Proceedings of the AAAI Conference on\nArtificial Intelligence, 34(05):8732–8740, Apr. 2020. doi: 10.1609/aaai.v34i05.6399.\n[86] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International\nConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,\npages 4596–4604. PMLR, 10–15 Jul 2018.\n[87] David So, Quoc Le, and Chen Liang. The evolved transformer. In International Conference\non Machine Learning, pages 5877–5886. PMLR, 2019.\n16\n",
  "17": "[88] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In\nInternational Conference on Learning Representations, 2021.\n[89] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable\neffectiveness of data in deep learning era. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), Oct 2017.\n[90] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[91] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations, 2019.\n[92] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global repre-\nsentations by penalizing local predictive power. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc., 2019.\n[93] Ruochen Wang, Xiangning Chen, Minhao Cheng, Xiaocheng Tang, and Cho-Jui Hsieh. Rank-\nnosh: Efficient predictor-based architecture search via non-uniform successive halving. In\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages\n10377–10386, October 2021.\n[94] Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, and Cho-Jui Hsieh. Re-\nthinking architecture selection in differentiable NAS. In International Conference on Learning\nRepresentations, 2021.\n[95] Ruochen Wang, Yuanhao Xiong, Minhao Cheng, and Cho-Jui Hsieh. Efficient non-parametric\noptimizer search for diverse tasks. arXiv preprint arXiv:2209.13575, 2022.\n[96] Olga Wichrowska, Niru Maheswaranathan, Matthew W. Hoffman, Sergio Gómez Colmenarejo,\nMisha Denil, Nando de Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale\nand generalize. In Proceedings of the 34th International Conference on Machine Learning -\nVolume 70, ICML’17, page 3751–3760. JMLR.org, 2017.\n[97] Yuanhao Xiong, Li-Cheng Lan, Xiangning Chen, Ruochen Wang, and Cho-Jui Hsieh. Learning\nto schedule learning rate with graph neural networks. In International Conference on Learning\nRepresentations, 2022.\n[98] Chengrun Yang, Gabriel Bender, Hanxiao Liu, Pieter-Jan Kindermans, Madeleine Udell,\nYifeng Lu, Quoc V Le, and Da Huang. TabNAS: Rejection sampling for neural architecture\nsearch on tabular datasets. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun\nCho, editors, Advances in Neural Information Processing Systems, 2022.\n[99] Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen, Kevin Murphy, and Frank Hutter.\nNas-bench-101: Towards reproducible neural architecture search. ICML, 2019.\n[100] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui\nWu. Coca: Contrastive captioners are image-text foundation models. Transactions on Machine\nLearning Research, 2022.\n[101] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can\na machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages 4791–4800, Florence, Italy, July 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/P19-1472.\n[102] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision trans-\nformers, 2021.\n[103] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander\nKolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 18123–18133, June 2022.\n17\n",
  "18": "[104] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\nRecord: Bridging the gap between human and machine commonsense reading comprehension,\n2018.\n[105] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon\nPapademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in\nobserved gradients. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors,\nAdvances in Neural Information Processing Systems, volume 33, pages 18795–18806. Curran\nAssociates, Inc., 2020.\n[106] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In\nInternational Conference on Learning Representations (ICLR), 2017.\nAcknowledgments and Disclosure of Funding\nUse unnumbered first level headings for the acknowledgments. All acknowledgments go at the\nend of the paper before the list of references. Moreover, you are required to declare funding\n(financial activities supporting the submitted work) and competing interests (related financial activities\noutside the submitted work). More information about this disclosure can be found at: https:\n//neurips.cc/Conferences/2023/PaperInformation/FundingDisclosure.\nDo not include this section in the anonymized submission, only in the final paper. You can use\nthe ack environment provided in the style file to autmoatically hide this section in the anonymized\nsubmission.\n18\n",
  "19": "Table 4: Model performance when pre-trained on JFT-300M then fine-tuned on ImageNet. Those\nnumbers correspond to Figure 1 (Left) and Figure 3. The fine-tuning resolution is 3842 for ViT-B/16\nand ViT-L/16, and 3922 for ViT-H/14. Following Dosovitskiy et al. [26], Polyak averaging is not\napplied here.\nModel\n#Params\nEpochs / Steps\nOptimizer\nImageNet\nReaL\nV2\nA\nR\nViT-B/16384\n86.86M\n7 / 517,791\nAdamW\n84.24\n89.04\n74.89\n27.39\n53.71\nLion\n84.72\n89.14\n75.83\n29.65\n55.86\nViT-L/16384\n304.72M\n7 / 517,791\nAdamW\n86.69\n89.95\n78.03\n40.55\n64.47\nLion\n87.32\n90.43\n79.29\n47.13\n68.49\n14 / 1,035,583\nAdamW\n87.29\n90.11\n78.91\n42.56\n64.34\nLion\n88.09\n90.62\n80.48\n51.55\n70.72\nViT-H/14392\n632.72M\n14 / 1,035,583\nAdamW\n88.02\n90.27\n80.10\n53.14\n69.48\nLion\n88.78\n90.68\n81.41\n58.21\n73.09\nTable 5: Model performance when pre-trained on JFT then fine-tuned on ImageNet. Two giant ViT\nmodels are pre-trained on JFT-3B while smaller ones are pre-trained on JFT-300M. The ViT-G/14\nresults are directly from Zhai et al. [102].\nModel\nViT-L/16512\nViT-H/14518\nViT-g/14518\nViT-G/14518\n#Params\n305.18M\n633.47M\n1.04B\n1.88B\nOptimizer\nAdamW\nLion\nAdamW\nLion\nAdafactor\nLion\nAdafactor\nLion\nImageNet\n87.72\n88.50\n88.55\n89.09\n90.25\n90.52\n90.45\n90.71 / 90.71⋆\nReaL\n90.46\n90.91\n90.62\n91.02\n90.84\n91.11\n90.81\n91.06 / 91.25⋆\nV2\n79.80\n81.13\n81.12\n82.24\n83.10\n83.39\n83.33\n83.54 / 83.83⋆\nA\n52.72\n58.80\n60.64\n63.78\n-\n-\n-\n-\nR\n66.95\n72.49\n72.30\n75.07\n-\n-\n-\n-\n⋆We observe overfitting in fine-tuning, therefore report both the last and oracle results.\nA\nPseudocode for AdamW and Lion\nAlgorithm 1 AdamW Optimizer\ngiven β1, β2, ϵ, λ, η, f\ninitialize θ0, m0 ←0, v0 ←0, t ←0\nwhile θt not converged do\nt ←t + 1\ngt ←∇θf(θt−1)\nupdate EMA of gt and g2\nt\nmt ←β1mt−1 + (1 −β1)gt\nvt ←β2vt−1 + (1 −β2)g2\nt\nbias correction\nˆmt ←mt/(1 −βt\n1)\nˆvt ←vt/(1 −βt\n2)\nupdate model parameters\nθt ←θt−1 −ηt( ˆmt/(√ˆvt + ϵ) + λθt−1)\nend while\nreturn θt\nAlgorithm 2 Lion Optimizer (ours)\ngiven β1, β2, λ, η, f\ninitialize θ0, m0 ←0\nwhile θt not converged do\ngt ←∇θf(θt−1)\nupdate model parameters\nct ←β1mt−1 + (1 −β1)gt\nθt ←θt−1 −ηt(sign(ct) + λθt−1)\nupdate EMA of gt\nmt ←β2mt−1 + (1 −β2)gt\nend while\nreturn θt\nB\nImage Classification Tasks\nOur evaluation covers various benchmarks: ImageNet, ImageNet ReaL [8], ImageNet V2 [81],\nImageNet A [39], ImageNet R [38], ImageNet Sketch [92], ObjectNet [4], CIFAR-100 [50], and\nOxford-IIIT Pet [69].\n19\n",
  "20": "Figure 6: Zero-shot image-text retrieval results on MSCOCO (Top) and Flickr30K (Bottom) for\nLiT-B/16-B. Recall@K is calculated based on if the ground truth label of the query appears in the\ntop-K retrieved examples.\nC\nNLP Tasks\nThis section shows all the natural language generation (NLG) and natural language understanding\n(NLU) tasks where we evaluate the large-scale language models in Section 4.4. Those tasks include\nOpen-Domain Question Answering, Cloze and Completion Tasks, Winograd-Style Tasks, Common\nSense Reasoning, In-Context Reading Comprehension, SuperGLUE, and Natural Language Inference.\n• NLG: TriviaQA [45], Natural Questions [51], Web Questions [6].\n• NLU: HellaSwag [101], StoryCloze [66], Winograd [53], Winogrande [85], RACE [52],\nPIQA [9], ARC [19], OpenbookQA [65], BoolQ [18], Copa [33], RTE [20], WiC [73],\nMultirc [47], WSC [53], ReCoRD [104], CB [22], Adversarial NLI [68].\nProgram 5: Algorithm with\na better regularization. It dy-\nnamically calculates the dot\nproduct between the weight\nand gradient, before comput-\ning the weight decay.\ndef train(w, g, m, v, lr):\nm = interp(m, g, 0.16)\ng2 = square(g)\nv = interpolate(v, g2, 0.001)\nv753 = dot(g, w)\nsqrt_v = sqrt(v)\nupdate = m / sqrt_v\nwd = v753 * w\nupdate = sin(update)\nupdate = update + wd\nlr = lr * 0.0216\nupdate = update * lr\nv = sin(v)\nreturn update, m, v\nProgram 6:\nAlgorithm\nthat tracks the second mo-\nment without EMA de-\ncay, which is the same as\nAdaGrad.\ndef train(w, g, m, v, lr):\nm = interp(m, g, 0.1)\ng2 = square(g)\ng2 = v + g2\nv = interp(v, g2, 0.0015)\nsqrt_v = sqrt(v)\nupdate = m / sqrt_v\nv70 = get_pi()\nv = min(v, v70)\nupdate = sinh(update)\nlr = lr * 0.0606\nupdate = update * lr\nreturn update, m, v\nProgram 7:\nAlgorithm\nuses the difference be-\ntween gradient and mo-\nmentum to track the sec-\nond moment, resembling\nAdaBelief.\ndef train(w, g, m, v, lr):\nm = interp(m, g, 0.1)\ng = g - m\ng2 = square(g)\nv = interp(v, g2, 0.001)\nsqrt_v = sqrt(v)\nupdate = m / sqrt_v\nwd = w * 0.0238\nupdate = update + wd\nlr = lr * 0.03721\nupdate = update * lr\nreturn update, m, v\nD\nOther Discovered Programs\nBy varying the task setting, different types of algorithms can be discovered. For example, if we\nreduce the amount of data in the proxy task, we are more likely to discover algorithms with better\nregularization (Program 5), and if we reduce the search progress, we are likely to find simple\nvariants of AdamW (Program 6 and 7). Future work can explore this potential to discover optimizers\nspecialized for different tasks.\n20\n",
  "21": "Figure 7: The zero-shot ImageNet accuracy curve of LiT-B/16-B (Left). FID comparison on 64 × 64\n(Middle) and 128 × 128 (Right) image generation when training diffusion models.\nTable 6: Architecture details for language modeling.\nModel\n#Params\nnlayers\ndmodel\nnheads\ndhead\nSmall-scale\nSmall\n110M\n12\n768\n12\n64\nMedium\n336M\n24\n1024\n16\n64\nLarge\n731M\n24\n1536\n16\n96\nLarge-scale\n1.1B\n1.07B\n24\n1536\n16\n96\n2.1B\n2.14B\n32\n2048\n16\n128\n7.5B\n7.49B\n32\n4096\n32\n128\nTable 7: Training error Ltrain and\nlandscape flatness LN\ntrain of ViT-B/16\ntrained from scratch on ImageNet.\nOptimizer\nAdamW\nLion\nImageNet\n75.48\n77.44\nReaL\n80.64\n82.57\nV2\n61.87\n64.81\nLtrain\n0.61\n0.75\nLN\ntrain\n3.74\n1.37\nE\nArchitecture Details for Language Modeling\nTable 6 shows the Transformer architecture details for language modeling (Section 4.4). The\ndimension of the feed-forward layer is 4 × dmodel. We use vocabulary size 32K for small-scale and\n256K for large-scale models.\nF\nDetails of Proxy Tasks\nFor vision tasks, we train a ViT with three layers, 96 hidden units and three heads, on 10% ImageNet\nfor 30k steps with batch size 64. The image size is 64 × 64 and the patch size is 16. For language\ntasks, we train a Transformer with two layers, 128 hidden units and two heads on LM1B [13] for 20K\nsteps with batch size 64, sequence length 32 and vocabulary size 3K. The evaluation time may vary\nfor different programs, but typically a evaluation can be done on one TPU V2 chip within 20min.\nThe validation accuracy or perplexity is used as the fitness.\nFigure 8: Learning curve of ViT-S/16 (Left) and ViT-B/16 (Right) associated with Table 10. The\ncurves of the five adaptive optimizers are similar to each other.\n21\n",
  "22": "Figure 9: Left: Validation perplexity when we perform masked language modeling on the C4 dataset.\nRight: Training loss of ViT-B/16 on ImageNet.\nG\nAnalysis of Loss Landscape\nIn this section, we try to understand why our Lion optimizer achieves better generalization than\nAdamW from the lens of loss geometry. The convergence to a smooth landscape has been shown\nto benefit the generalization of deep neural networks [14, 16, 30, 46]. Following Chen et al. [16],\nwe measure the landscape flatness at convergence by LN\ntrain = Eϵ∼N [Ltrain(w + ϵ)] (average over\n1K random noises) in Table 7. We observe that the ViT-B/16 trained by AdamW enjoys a smaller\ntraining error Ltrain. However, Lion can enable ViT to converge to flatter regions, as it helps the\nmodel retain comparably lower error against Gaussian perturbations.\nH\nAvailable Functions\nProgram 8: Raw program of Lion be-\nfore removing redundent statements.\ndef train(w, g, m, v, lr):\ng = clip(g, lr)\nm = clip(m, lr)\nv845 = sqrt(0.6270633339881897)\nv968 = sign(v)\nv968 = v - v\ng = arcsin(g)\nm = interp(g, v, 0.8999999761581421)\nv1 = m * m\nv = interp(g, m, 1.109133005142212)\nv845 = tanh(v845)\nlr = lr * 0.0002171761734643951\nupdate = m * lr\nv1 = sqrt(v1)\nupdate = update / v1\nwd = lr * 0.4601978361606598\nv1 = square(v1)\nwd = wd * w\nm = cosh(update)\nlr = tan(1.4572199583053589)\nupdate = update + wd\nlr = cos(v845)\nreturn update, m, v\nWe include 43 available functions that can be used in the\nprogram during search. Note that the input of the functions\ncan be one n-dimensional array, dictionaries or lists of arrays,\nsimilar to the pytrees in JAX.\nBasic math functions from NumPy / JAX This includes\nunary functions like abs, cos, sin, tan, arcsin, arccos,\narctan, exp, log, sinh, cosh, tanh, arcsinh, arccosh,\narctanh, sign, exp2, exp10, expm1, log10, log2, log1p,\nsquare, sqrt, cube, cbrt, sign, reciprocal and binary\nfunctions like +, -, *, /, power, maximum, minimum with the\nsame semantic as the corresponding function in NumPy / JAX.\nLinear algebra functions commonly used in first-order\noptimization algorithms This includes: (1) unary function\nnorm that computes the norm of each arrays in the input; (2)\nunary function global_norm that computes the global norm\nby treating all the numbers in the input as one vector; (3) binary\nfunction dot that treats the two inputs as two vectors and\ncomputes their dot product; (4) binary function cosine_sim\nthat treats the two inputs as two vectors and computes their\ncosine similarity; (5) binary clip_by_global_norm (clip)\nthat clips the global norm of the first input to the value of the\nsecond input that is required to be a scalar; (6) ternary function interpolate (interp) that uses the\nthird argument a, required to be a scalar, to compute a linear interpolation of the first two arguments\nx and y with (1 - a) * x + a * y.\nFunctions producing commonly used constants This includes get_pi, get_e, get_eps that\ngenerates π, e and ϵ = 10−8 respectively.\n22\n",
  "23": "Figure 10: Left: Ablation for the effect of batch size. Lion prefers a larger batch than AdamW.\nImageNet accuracy of ViT-B/16 trained from scratch when we vary lr and λ for AdamW (Middle)\nand Lion (Right). Lion is more robust to different hyperparameter choices.\nFigure 11: Left: The meta-validation (defined in Section 2.3) curves of two search runs measured\non a ∼500x larger meta-validation task compared to the proxy. The blue one meta-overfits at ∼15%\nof the search progress, while the orange one meta-overfits at ∼90% and achieves a better metric.\nRight: Histogram of the search progress when meta-overfitting happens based on 50 runs. Half of\nthe runs meta-overfit early but a long tail of runs meta-overfit much later. Blue cross depicts the best\nmeta-validation metric averaged within each bin, indicating that meta-overfitting happening later\nleads to programs that generalize better.\nI\nAbstract Execution\nWe propose to prune the large search space with abstract execution. Our approach is motivated by\nthe fact that a large number of programs are invalid, functionally equivalent, or contain redundant\nstatements that waste compute during evaluation. To address this, we introduce an abstract execution\nstep that checks the type and shape of each variable, and computes a hash for each unique computation\nfrom inputs to outputs to detect redundant statements. The abstract execution can be seen as a static\nanalysis of the program, achieved by replacing functions and inputs with customized values. We\noutline the specifics of the customized values and abstract execution procedure for three use cases\nbelow. The cost of the abstract execution is usually negligible compared to the actual execution.\nDetecting errors with type / shape inference To detect programs containing errors, we infer the\ntype and shape of each variable in the program through the following steps: (1) replace each input\nwith an abstract object that only contains type and shape information, and replace each statement\nwith a type and shape inference function; (2) iterate through all statements. Instead of executing the\nFigure 12: Log perplexity of the small (Left), medium (Middle), and large (Right) size Transformer\non PG-19. Since β1 = 0.95, β2 = 0.98 in Lion when performing language modeling, we compare\nto Ablation0.95 and Ablation0.98 with β = 0.95 and β = 0.98, respectively (see Section L for the\ndefinition). Lion is still the best-performing one.\n23\n",
  "24": "Table 8: Accuracy on ImageNet, ImageNet ReaL, and ImageNet V2. Numbers in (·) are from Dai\net al. [21], Dosovitskiy et al. [26]. Results are averaged from three runs.\nModel\n#Params\nOptimizer\nRandAug\n+ Mixup\nImageNet\nReaL\nV2\nTrain from scratch on ImageNet\nResNet-50\n25.56M\nSGD\n✗\n76.22\n82.39\n63.93\nAdamW\n76.34\n82.72\n64.24\nLion\n76.45\n82.72\n64.02\nMixer-S/16\n18.53M\nAdamW\n✗\n69.26\n75.71\n55.01\nLion\n69.92\n76.19\n55.75\nMixer-B/16\n59.88M\nAdamW\n✗\n68.12\n73.92\n53.37\nLion\n70.11\n76.60\n55.94\nViT-S/16\n22.05M\nAdamW\n✗\n76.12\n81.94\n63.09\nLion\n76.70\n82.64\n64.14\nAdamW\n✓\n78.89\n84.61\n66.73\nLion\n79.46\n85.25\n67.68\nViT-B/16\n86.57M\nAdamW\n✗\n75.48\n80.64\n61.87\nLion\n77.44\n82.57\n64.81\nAdamW\n✓\n80.12\n85.46\n68.14\nLion\n80.77\n86.15\n69.19\nCoAtNet-1\n42.23M\nAdamW\n✓\n83.36 (83.3)\n-\n-\nLion\n84.07\n-\n-\nCoAtNet-3\n166.97M\nAdamW\n✓\n84.45 (84.5)\n-\n-\nLion\n84.87\n-\n-\nPre-train on ImageNet-21K then fine-tune on ImageNet\nViT-B/16384\n86.86M\nAdamW\n✗\n84.12 (83.97)\n88.61 (88.35)\n73.81\nLion\n84.45\n88.84\n74.06\nViT-L/16384\n304.72M\nAdamW\n✗\n85.07 (85.15)\n88.78 (88.40)\n75.10\nLion\n85.59\n89.35\n75.84\noriginal statement, we validate a function call by checking the function signature and type and shape\ninformation of its arguments. If valid, we compute the type and shape information of the output and\nassign it to the new variable; (3) verify the validity of the derived type and shape of the output. This\nprocess essentially performs a static analysis of the program, exposing errors caused by type and\nshape mismatch. Note that there are still run-time errors, such as division by zero, that cannot be\ndetected in this manner. Without such filtering of invalid programs, the search would be overwhelmed\nwith invalid programs, making it difficult to achieve meaningful progress.\nDeduplicating with functional hash Among the valid programs that execute without errors, there\nare still lots of duplicates due to functionally equivalent programs that have different surface forms\nbut the same underlying functionality. To address this issue, we calculate a functional hash value\nfor every unique computation from the inputs to the outputs as follows: (1) a unique hash value is\nassigned to each input and function; (2) iterate through all statements, calculating the hash value of\nthe outputs by combining the hash values of the functions and arguments; (3) compute the hash value\nof program by combining the hash values of all outputs. We then build a hash table that maps each\nunique functional hash value to the fitness of the corresponding program. When a new program is\ngenerated, we first look up its hash value and only perform evaluation if it is not found or if we want\nto evaluate it multiple times to reduce measurement noise. In our experiments, this technique reduces\nthe search cost by ∼10x, as depicted in Figure 2 (Right).\nIdentifying redundant statements by tracking dependencies In program evolution, redundant\nstatements are included to enable combining multiple mutations to make larger program changes.\nHowever, these redundant statements increase the evaluation cost and make program analysis more\nchallenging. To identify redundant statements, we need to determine the set of statements that the\noutputs depend on, which can be computed in a recursive manner using the following steps: (1)\n24\n",
  "25": "Table 9: Fine-tuning performance of the T5 Base, Large, and 11B on the GLUE dev set. Results\nreported are the peak validation scores per task.\nModel\nOptimizer\nCoLA\nSST-2\nMRPC\nSTS-B\nQQP\nMNLI\n-m\nMNLI\n-mm\nQNLI\nRTE\nAvg\nBase\nAdamW\n60.87\n95.18\n92.39 / 89.22\n90.70 / 90.51\n89.23 / 92.00\n86.77\n86.91\n93.70\n81.59\n87.42\nLion\n61.07\n95.18\n92.52 / 89.46\n90.61 / 90.40\n89.52 / 92.20\n87.27\n87.25\n93.85\n85.56\n87.91\nLarge\nAdamW\n63.89\n96.10\n93.50 / 90.93\n91.69 / 91.56\n90.08 / 92.57\n89.69\n89.92\n94.45\n89.17\n89.46\nLion\n65.12\n96.22\n94.06 / 91.67\n91.79 / 91.60\n90.23 / 92.67\n89.85\n89.94\n94.89\n90.25\n89.86\n11B\nAdamW\n69.50\n97.02\n93.75 / 91.18\n92.57 / 92.61\n90.45 / 92.85\n92.17\n91.99\n96.41\n92.42\n91.08\nLion\n71.31\n97.13\n94.58 / 92.65\n93.04 / 93.04\n90.57 / 92.95\n91.88\n91.65\n96.56\n93.86\n91.60\nreplace the value of each input with an empty set, as they do not depend on any statement; (2) iterate\nthrough each statement. Note that each statement is an assignment that calls a function and assigns the\nresult to a variable, which in turn depends on the current statement and all the depending statements\nof the function arguments. Therefore we replace the value of the variable with its dependency, i.e., a\nset of all depending statements; (3) compute the union of all statements that each output depends\non, which contains all non-redundant statements. By filtering out redundant statements, we obtain a\nsimplified version of the program that is cheaper to execute and easier to analyze. In our experiments,\nthis reduces the program length by ∼3x on average, as shown in Figure 2 (Right).\nJ\nMasked Language Modeling and Fine-tuning\nMasked language modeling We also perform BERT training on the C4 dataset [77]. It requires\nthe language models to reconstruct randomly masked out tokens in the input sequence. We use the\nsame architectures and training setups as the smaller-scale autoregressive experiments. Lion performs\nslightly better than AdamW regarding the validation perplexity: 4.18 vs. 4.25 (small), 3.42 vs. 3.54\n(medium), and 3.18 vs. 3.25 (large). See Figure 9 (Left) in the Appendix for the learning curves.\nFine-tuning We fine-tune Base (220M), Large (770M), and the largest 11B T5 [77] on the GLUE\nbenchmark [91]. Every model is fine-tuned for 500K steps with a batch size of 128 and a constant\nlearning rate. Table 9 shows the results on the GLUE dev set. For MRPC and QQP, we report the\nF1 / Accuracy scores, for STS-B, we report the Pearson / Spearman correlation, and for the other\ndatasets, we report their default metric. On average, Lion beats AdamW across all three model scales.\nIt achieves 10, 12, and 10 wins out of 12 scores for T5 Base, Large, and 11B models, respectively.\nK\nComparison with Other Popular Optimizers\nWe also employ four popular handcrafted optimizers: RAdam [58], NAdam [27], AdaBelief [105],\nAMSGrad [82] and two optimizers discovered by AutoML: PowerSign [5] and AddSign [5] to train\nViT-S/16 and ViT-B/16 on ImageNet (with RandAug and Mixup). We thoroughly tune the peak\nlearning rate lr and decoupled weight decay λ [60] of every optimizer, while other hyperparameters\nare set as the default values in Optax.2 As shown in Table 10, Lion is still the best performing one. We\nnotice that there is no clear winner amongst the baselines. AMSGrad performs the best on ViT-S/16\nbut the worst on ViT-B/16. The inferior performance of PowerSign and AddSign compared to other\noptimizers is consistent with previous observations that automatically discovered optimizers have\ndifficulty generalizing to real-world learning tasks. Figure 8 further shows that the learning curves of\nthe five adaptive optimizers are pretty similar, whereas Lion has a unique one that learns faster.\nL\nAblations\nMomentum tracking To ablate the effects of both β1 and β2, we compare to a simple update\nrule: m = interp(g, m, β); update = sign(m). Two optimizers, Ablation0.9 and Ablation0.99,\nare created with β values of 0.9 and 0.99 respectively. Illustrated by Table 10, the two ablated\noptimization algorithms perform worse than all five compared baselines, let alone our Lion. Further\n2https://github.com/deepmind/optax\n25\n",
  "26": "Table 10: The performance of various optimizers to train ViT-S/16 and ViT-B/16 on ImageNet (with\nRandAug and Mixup). Lion is still the best performing one.\nModel\nTask\nAdamW\nRAdam\nNAdam\nAda-\nBelief\nAMSGrad\nPower-\nSign\nAdd-\nSign\nAblation0.9\nAblation0.99\nLion\nViT-S/16\nImageNet\n78.89\n78.59\n78.91\n78.71\n79.01\n77.36\n77.37\n78.23\n78.19\n79.46\nReaL\n84.61\n84.47\n84.62\n84.56\n85.01\n83.39\n83.36\n84.28\n84.17\n85.25\nV2\n66.73\n66.39\n66.02\n66.35\n66.82\n65.17\n64.52\n66.13\n65.96\n67.68\nViT-B/16\nImageNet\n80.12\n80.26\n80.32\n80.29\n79.85\n78.95\n78.50\n79.54\n79.90\n80.77\nReaL\n85.46\n85.45\n85.44\n85.48\n85.16\n84.76\n84.49\n85.10\n85.36\n86.15\nV2\n68.14\n67.76\n68.46\n68.19\n68.48\n67.46\n65.95\n68.07\n68.20\n69.19\nablation studies on the language modeling task (as depicted in Figure 12 in the Appendix) yield similar\nconclusions. Those results validate the effectiveness and necessity of using two linear interpolation\nfunctions, letting Lion to remember longer gradient history meanwhile assign a higher weight to the\ncurrent gradient.\nEffect of batch size Some may question whether Lion requires a large batch size to accurately\ndetermine the direction due to the added noise from the sign operation. To address this concern, we\ntrain a ViT-B/16 model on ImageNet using various batch sizes while maintaining the total training\nepoch as 300, and incorporating RandAug and Mixup techniques. As shown in Figure 10 (Left), the\noptimal batch size for AdamW is 256, while for Lion is 4,096. This indicates that Lion indeed prefers\na larger batch size, but its performance remains robust even with a small 64 batch size. Furthermore,\nwhen the batch size enlarges to 32K, leading to only 11K training steps, Lion achieves a significant\n2.5% accuracy gain over AdamW (77.9% vs. 75.4%), demonstrating its effectiveness in the large\nbatch training setting.\nM\nHyperparameter Tuning\nTo ensure a fair comparison, we tune the peak learning rate lr and decoupled weight decay λ for both\nAdamW (Adafactor) and our Lion using a logarithmic scale. The default values for β1 and β2 in\nAdamW are set as 0.9 and 0.999, respectively, with an ϵ of 1e −8, while in Lion, the default values\nfor β1 and β2 are discovered through the program search process and set as 0.9 and 0.99, respectively.\nWe only tune those hyperparameters in Section 4.4, where β1 = 0.9, β2 = 0.99 in AdamW, and\nβ1 = 0.95, β2 = 0.98 in Lion. In our experience, reducing β2 results in shorter memorization of\nhistorical information and enhanced training stability. Additionally, the ϵ in AdamW is set as 1e −6\ninstead of the default 1e −8 as it improves stability in our experiments, similar to the observations in\nRoBERTa [59].\nThe update generated by Lion is an element-wise binary ±1, as a result of the sign operation,\ntherefore it has a larger norm than those generated by other optimizers. Based on our experience, a\nsuitable learning rate for Lion is typically 3-10x smaller than that for AdamW. Note that the initial\nvalue, peak value, and end value of the learning rate should be changed simultaneously with the\nsame ratio compared to AdamW. We do not modify other training settings such as the learning rate\nschedule, gradient and update clipping. Since the effective weight decay is lr * λ: update += w\n* λ; update *= lr, the value of λ used for Lion is 3-10x larger than that for AdamW in order to\nmaintain a similar strength. For instance,\n• lr = 1e−4, λ = 10.0 in Lion and lr = 1e−3, λ = 1.0 in AdamW when training ViT-B/16\non ImageNet with strong augmentations,\n• lr = 3e −5, λ = 0.1 in Lion and lr = 3e −4, λ = 0.01 in AdamW for diffusion models,\n• lr = 1e −4, λ = 0.01 in Lion and lr = 1e −3, λ = 0.001 in Adafactor for the 7.5B\nlanguage modeling.\nPlease see Table 12 (in the Appendix) for all hyperparameters.\nApart from the peak performance, the sensitivity to hyperparameters and the difficulty in tuning them\nare also critical for the adoption of an optimizer in practice. In Figure 10 (Middle and Right), we\nalter both lr and λ when training ViT-B/16 from scratch on ImageNet. Suggested by the heatmaps,\nLion is more robust to different hyperparameter choices compared to AdamW.\n26\n",
  "27": "N\nLimitations\nLimitations of search Despite the efforts to make the search space less restrictive, it remains inspired\nby the popular first-order optimization algorithms, leading to a bias towards similar algorithms. It\nalso lacks the functions required to construct advanced second-order algorithms [2, 35, 62]. The\nsearch cost is still quite large and the algorithm simplification requires manual intervention. Further\nreducing the bias in the search space to discover more novel algorithms and improving the search\nefficiency are important future directions. The current program structure is quite simplistic, as we do\nnot find a good usage of more advanced program constructs such as conditional, loop statements, and\ndefining new functions. Exploring how to incorporate these elements has the potential to unlock new\npossibilities.\nLimitations of Lion While we endeavour to evaluate Lion on as many tasks as possible, the\nassessment is limited to the chosen tasks. On vision tasks, the discrepancies between Lion, AdamW,\nand momentum SGD are pretty small on ResNets, likely due to the fact that ConvNets are easier\nto optimize compared to Transformers. The performance gain brought by Lion decreases when\nstrong augmentations are utilized. There are also several tasks where Lion performs similarly to\nAdamW, including: (1) the Imagen text-to-image base model, (2) the perplexity of autoregressive\nlanguage model trained on the large-scale internal dataset, which is arguably a more reliable metric\nthe in-context learning benchmarks, and (3) masked language modeling on C4. These tasks have a\ncommon characteristic in that the datasets are massive and of high quality, which results in a reduced\ndifference between optimizers. Another potential limitation is the batch size. Though people often\nscale up the batch size to enable more parallelism, it is likely that Lion performs no better than\nAdamW if the batch size is small (<64). Additional, Lion still requires momentum tracking in\nbfloat16, which can be expensive for training giant models. One potential solution is to factorize\nthe momentum to save memory.\n27\n",
  "28": "Table 11: One-shot evaluation on English NLP tasks. TriviaQA, NQs, and WebQs are NLG tasks and\nthe rest are NLU tasks. This corresponds to Table 3 in the main text.\nTask\n1.1B\n2.1B\n7.5B\n6.7B\nGPT-3\n8B\nPaLM\nAdafactor\nLion\nAdafactor\nLion\nAdafactor\nLion\n#Tokens\n300B\n300B\n780B\nTriviaQA (EM)\n21.5\n25.1\n32.0\n33.4\n47.9\n48.8\n44.4\n48.5\nNQs (EM)\n4.3\n4.8\n6.3\n7.3\n12.3\n12.1\n9.8\n10.6\nWebQs (EM)\n7.5\n6.3\n8.4\n8.7\n12.1\n13.3\n15.1\n12.6\nHellaSwag\n50.7\n50.3\n59.4\n59.3\n68.2\n68.3\n66.5\n68.2\nStoryCloze\n74.8\n74.4\n78.2\n78.3\n81.2\n81.5\n78.7\n78.7\nWinograd\n75.1\n80.2\n81.3\n82.1\n85.3\n84.2\n84.6\n85.3\nWinogrande\n59.7\n60.5\n64.8\n65.7\n71.4\n71.0\n65.8\n68.3\nRACE-m\n52.0\n50.8\n55.1\n53.8\n59.1\n61.3\n54.7\n57.7\nRACE-h\n36.8\n35.4\n40.3\n40.7\n44.5\n43.9\n44.3\n41.6\nPIQA\n69.4\n69.9\n71.3\n72.1\n75.5\n74.5\n76.3\n76.1\nARC-e\n64.3\n62.0\n69.5\n68.9\n72.4\n72.7\n62.6\n71.3\nARC-c\n31.2\n32.9\n37.3\n38.0\n43.3\n42.6\n41.5\n42.3\nOpenbookQA\n44.8\n48.0\n48.4\n49.0\n51.4\n52.4\n53.0\n47.4\nBoolQ\n54.3\n56.7\n64.1\n62.9\n73.5\n73.9\n68.7\n64.7\nCopa\n75.0\n78.0\n83.0\n84.0\n85.0\n87.0\n82.0\n82.0\nRTE\n55.6\n52.4\n49.8\n59.2\n63.9\n62.5\n54.9\n57.8\nWiC\n47.6\n47.3\n46.1\n48.1\n50.9\n48.1\n50.3\n47.3\nMultirc (F1a)\n35.9\n44.3\n45.0\n48.8\n44.7\n59.2\n64.5\n50.6\nWSC\n76.5\n75.4\n79.6\n79.3\n86.7\n85.6\n60.6\n81.4\nReCoRD\n73.4\n73.7\n77.8\n77.7\n81.0\n81.1\n88.0\n87.8\nCB\n46.4\n44.6\n48.2\n44.6\n51.8\n46.4\n33.9\n41.1\nANLI R1\n33.3\n30.1\n32.4\n31.2\n31.5\n34.0\n31.6\n32.4\nANLI R2\n29.8\n31.8\n29.8\n30.6\n32.4\n31.9\n33.9\n31.4\nANLI R3\n29.8\n31.8\n31.4\n31.9\n33.6\n34.2\n33.1\n34.5\nAvg NLG\n11.1\n12.1\n15.6\n16.5\n24.1\n24.7\n23.1\n23.9\nAvg NLU\n53.2\n53.9\n56.8\n57.4\n61.3\n61.7\n58.5\n59.4\n28\n",
  "29": "Table 12: Hyperparameters for all the experiments.\nModel\nDropout\nStoch\nDepth\nAugmentations\nOptimizer\nβ1\nβ2\nlr\nλ\nTrain from scratch on ImageNet\nResNet-50\n-\n-\n-\nAdamW\n0.9\n0.999\n3e −3\n0.1\nLion\n0.9\n0.99\n3e −4\n1.0\nMixer-S/16\n-\n0.1\n-\nAdamW\n0.9\n0.999\n1e −2\n0.3\nLion\n0.9\n0.99\n3e −3\n1.0\nMixer-B/16\n-\n0.1\n-\nAdamW\n0.9\n0.999\n1e −2\n0.3\nLion\n0.9\n0.99\n3e −3\n3.0\nViT-S/16\n0.1\n0.1\n-\nAdamW\n0.9\n0.999\n1e −2\n0.1\nLion\n0.9\n0.99\n1e −3\n1.0\n-\n-\nRandAug: 2, 15\nMixup: 0.5\nAdamW\n0.9\n0.999\n3e −3\n0.1\nLion\n0.9\n0.99\n3e −4\n1.0\nViT-B/16\n0.1\n0.1\n-\nAdamW\n0.9\n0.999\n3e −3\n0.3\nLion\n0.9\n0.99\n1e −3\n1.0\n-\n-\nRandAug: 2, 15\nMixup: 0.5\nAdamW\n0.9\n0.999\n1e −3\n1.0\nLion\n0.9\n0.99\n1e −4\n10.0\nCoAtNet-1\n-\n0.3\nRandAug: 2, 15\nMixup: 0.8\nAdamW\n0.9\n0.999\n1e −3\n0.05\nLion\n0.9\n0.99\n2e −4\n1.0\nCoAtNet-3\n-\n0.7\nRandAug: 2, 15\nMixup: 0.8\nAdamW\n0.9\n0.999\n1e −3\n0.05\nLion\n0.9\n0.99\n2e −4\n1.0\nPre-train on ImageNet-21K\nViT-B/16\n0.1\n0.1\n-\nAdamW\n0.9\n0.999\n1e −3\n0.1\nLion\n0.9\n0.99\n1e −4\n0.3\nViT-L/16\n0.1\n0.1\n-\nAdamW\n0.9\n0.999\n1e −3\n0.3\nLion\n0.9\n0.99\n1e −4\n1.0\nPre-train on JFT\nViT-B/16\n-\n-\n-\nAdamW\n0.9\n0.999\n6e −4\n0.1\nLion\n0.9\n0.99\n1e −4\n0.3\nViT-L/16\n-\n-\n-\nAdamW\n0.9\n0.999\n3e −4\n0.1\nLion\n0.9\n0.99\n1e −4\n0.3\nViT-H/14\n-\n-\n-\nAdamW\n0.9\n0.999\n3e −4\n0.1\nLion\n0.9\n0.99\n3e −5\n0.3\nViT-g/14 & ViT-G/14\n-\n-\n-\nAdafactor\n0.9\n0.999\n8e −4\n0.03\nLion\n0.9\n0.99\n3e −5\n0.3\nVision-language contrastive learning\nLiT-B/∗-B\n-\n-\n-\nAdamW\n0.9\n0.999\n1e −3\n-\nLion\n0.9\n0.99\n3e −4\nLiT-g/14-L\n-\n-\n-\nAdamW\n0.9\n0.999\n1e −3\n0.1\nLion\n0.9\n0.99\n2e −4\n0.5\nBASIC-L\n-\n-\n-\nAdafactor\n0.9\n0.999\n5e −4\n0.01\nLion\n0.9\n0.99\n2e −4\n0.1\nDiffusion model\nImagen base & super-resolution\n-\n-\n-\nAdamW\n0.9\n0.999\n1e −3\n-\nLion\n0.9\n0.99\n1e −4\nImage generation on ImageNet\n64 × 64: 0.1\n128 × 128 & 256 × 256: 0.2\n-\n-\nAdamW\n0.9\n0.999\n3e −4\n0.01\nLion\n0.9\n0.99\n3e −5\n0.1\nAutoregressive & masked language modeling\nSmall & Medium (PG-19, C4) & Large\n-\n-\n-\nAdamW\n0.9\n0.99\n3e −3\n-\nLion\n0.95\n0.98\n3e −4\nMedium (Wiki-40B)\n-\n-\n-\nAdamW\n0.9\n0.99\n3e −3\n0.001\nLion\n0.95\n0.98\n3e −4\n0.01\n1.1B & 2.1B\n-\n-\n-\nAdafactor\n0.9\n0.99\n2e −3\n0.0005\nLion\n0.95\n0.98\n2e −4\n0.005\n7.5B\n-\n-\n-\nAdafactor\n0.9\n0.99\n1e −3\n0.001\nLion\n0.95\n0.98\n1e −4\n0.01\nLanguage model fine-tuning\nT5-Base & Large & 11B\n0.1\n-\n-\nAdamW\n0.9\n0.99\n3e −5\n-\nLion\n0.95\n0.98\n3e −6\n29\n"
}