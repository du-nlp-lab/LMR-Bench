{
  "1": "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 604–614\nNovember 15, 2024 ©2024 Association for Computational Linguistics\nProbing Language Models on Their Knowledge Source\nZineddine Tighidet1, 2, Andrea Mogini1, Jiali Mei1, Benjamin Piwowarski2,\nPatrick Gallinari2, 3\n1BNP Paribas, Paris, France\n2Sorbonne Université, CNRS, ISIR, F-75005 Paris, France\n3Criteo AI Lab, Paris, France\nfirstname.lastname@{isir.upmc.fr, bnpparibas.com}\nAbstract\nLarge Language Models (LLMs) often en-\ncounter conflicts between their learned, in-\nternal (parametric knowledge, PK) and exter-\nnal knowledge provided during inference (con-\ntextual knowledge, CK). Understanding how\nLLMs models prioritize one knowledge source\nover the other remains a challenge. In this pa-\nper, we propose a novel probing framework\nto explore the mechanisms governing the se-\nlection between PK and CK in LLMs. Us-\ning controlled prompts designed to contradict\nthe model’s PK, we demonstrate that specific\nmodel activations are indicative of the knowl-\nedge source employed. We evaluate this frame-\nwork on various LLMs of different sizes and\ndemonstrate that mid-layer activations, particu-\nlarly those related to relations in the input, are\ncrucial in predicting knowledge source selec-\ntion, paving the way for more reliable models\ncapable of handling knowledge conflicts effec-\ntively.\n1\nIntroduction\nLarge Language Models (LLMs) have demon-\nstrated remarkable proficiency in memorizing and\nretrieving massive amounts of information. De-\nspite these strengths, LLMs often struggle when\nexposed to novel information not seen during train-\ning (Ovadia et al., 2019) or when there is a conflict\nbetween their parametric knowledge (PK) and\nthe context knowledge (CK) provided at infer-\nence (Xie et al., 2024). Such discrepancies can\nlead to erroneous outputs, a phenomenon that re-\nmains a significant challenge in LLMs applications\n(Ji et al., 2023). While several approaches, such\nas reinforcement learning and retrieval-augmented\ngeneration, have been proposed to mitigate these\nissues (Ziegler et al., 2020; Lewis et al., 2021), the\nmechanisms by which LLMs select and prioritize\nknowledge sources are not well understood, sug-\ngesting a gap in current research methodologies.\nSouth\nAmerica\nEurope\nactivations\nParametric\nKnowledge (PK)\nExternal Context\nKnowledge (CK)\nPredictive Model\nCK\nPK\nBrazil is located in Europe.\nBrazil is located in _\nKnowledge Source\nFigure 1: Illustration of our method for probing knowl-\nedge sources in LLMs. We present the model with\na prompt containing contradictory information to its\nlearned knowledge to test whether it uses parametric\nknowledge (PK) or contextual knowledge (CK). The\nresulting activations are used to train a classifier to dis-\ntinguish between PK and CK.\nThis paper explores the internal dynamics of\nLLMs, and more precisely decoder-only layers, fo-\ncusing on their decision-making processes regard-\ning the use of CK versus PK. By prompting the\nLLM in a way that contradicts its PK, we probe the\nmodel’s knowledge-sourcing behaviors. By train-\ning a linear classifier on model activations, our ex-\nperiments reveal that certain activations correlate\nwith determining whether context or parametric\nknowledge predominates in the generated outputs.\nIn this paper, we make the following key find-\nings and contributions:\n• We define a framework that characterizes the\nsource of knowledge used by the model to\ngenerate its outputs – Sections 3 and 4. To\nfacilitate further research and validation of\nour findings, we make our framework publicly\n604\n",
  "2": "available on GitHub1.\n• Specific activations are indicative of the\nknowledge source: by applying our frame-\nwork to LLMs of different sizes, we estab-\nlish that specific activations correlate with\nthe model’s use of contextual or parametric\nknowledge.\n2\nRelated Work\nThe understanding of the mechanisms and knowl-\nedge localization within transformers has pro-\ngressed through various studies. On the one hand,\nsome work investigated the PK-based outputs (fac-\ntual setting) (Meng et al., 2023; Geva et al., 2021,\n2023; AlKhamissi et al., 2022; Heinzerling and\nInui, 2021). These works hypothesized that LLMs\nstore parametric information in the Multi-Layer\nPerceptron (MLP), which acts as a key-value mem-\nory, subsequently accessed by the Multi-Head Self-\nAttention (MHSA) mechanisms.\nOn the other\nhand, other studies focused on the CK-based out-\nputs and concluded that processing CK, as opposed\nto PK, is not specifically localized in the LLM’s\nparameters (Monea et al., 2024).\nYu et al. (2023) employed an attribution method\n(Wang et al., 2022; Belrose et al., 2023) to identify\nthe most influential attention heads responsible for\ngenerating PK and CK outputs, and subsequently\nadjusted the weights of these heads to modify the\nsource of knowledge. Their work however focuses\nexclusively on knowledge specific to capital cities\nand relies on causal tracing, which is costly to\ncompute.\nIn contrast, our work utilizes a probing approach\nthat uses a classifier on the LLM activations to\nidentify the source of knowledge, leveraging the\ninsights from previous research on the respective\nroles of MLPs and MHSAs in the inference process.\nWe extend the scope of Yu et al. (2023) by incorpo-\nrating a dataset with a broader range of knowledge\ncategories (ParaRel (Elazar et al., 2021)), moving\nbeyond just capital cities.\n3\nMethodology\nWe aim to show that specific activations correlate\nwith the used knowledge source, parametric or con-\ntext knowledge. In order to probe LLMs, we con-\nstruct prompts that are composed of inputs rep-\n1Link to the code and dataset: https://github.com/\nZineddine-Tighidet/knowledge-probing-framework\nresenting information about a subject s that con-\ntradicts what the model learned during training,\nfollowed by a query about the same subject (see\nFigure 1). If the model answers according to the\nprompt, then it uses context knowledge. On the\nother hand, if the model answers according to what\nit learned, then it is using its parametric knowledge.\nIn the following two sections, we define more for-\nmally PK and CK.\n3.1\nParametric Knowledge (PK)\nWe consider the parametric knowledge (PK) to be\nthe information that the model learned during train-\ning. More specifically, we restrict this PK by using\na knowledge base KB = {(s, r, o)}, i.e. a set of\n(subject, relation, object) triplets from the ParaRel\ndataset (Elazar et al., 2021). We then define PK to\nbe the set of objects that are generated by a LLM:\nPK = {(s, r, o\n′) | ∃o s.t. (s, r, o) ∈KB\n∧o\n′ = Gθ(q(s, r))}\n(1)\nwhere Gθ is an LLM; q(s, r) is a prompt in natural\nlanguage corresponding to a subject-relation pair\n(s, r); o\n′ is the output of Gθ given the query prompt\n(e.g. \"Brazil is located in the continent of _\").\nNote that we use this method to define PK be-\ncause we do not have access to the training data\nof LLMs in general, and, more importantly, we\nare interested in what the LLM infers by itself. If\no = Gθ(q(s, r)), that is, the object o was gener-\nated by the model after providing an input query\nq(s, r), we can conclude that the model learned to\nassociate the object o with the subject s with the\nrelation r during training. Note also that, unlike\nprevious work (Meng et al., 2023; Yu et al., 2023),\neven when o is factually incorrect (e.g. \"Paris is the\ncapital of Italy\"), we still consider it in our study\nas our only interest is the parametric knowledge\nand not the external world factual truth2.\n3.1.1\nKnowledge Base (ParaRel)\nWe extend the ParaRel dataset (Elazar et al., 2021)\nfor constructing a parametric knowledge base.\nParaRel dataset consists of triplets, each composed\nof a subject, a relation, and an object. Table 1\nillustrates a sample of the raw ParaRel dataset.\nWhile the majority of the triplets adhere to\nthe subject-relation-object structure, some deviate\n2This behavior happens when the subjects are unpopular\nand the LLM was not trained on enough examples. We discuss\nthis further in Section 6.\n605\n",
  "3": "Complete the following statement directly and concisely with the\nname of the place where the headquarters of Microsoft are located.\nDo not try to answer with a detailed explanation, just answer in a\nfew words without being specific. Do not use any specific formatting\nand end the output with a point.\nHere is an example: BNP Paribas is headquartered in Paris.\nMicrosoft is headquartered in\nRedmond.\nFigure 2: Example of the template used to generate the parametric knowledge dataset. The blue text is proper to the\nrelation and the orange is specific to a subject-relation example in the ParaRel dataset (Elazar et al., 2021).\nfrom this format. To ensure consistency, a pre-\nprocessing step was applied on the raw ParaRel\ndataset using Mistral-Large3. Specifically, the goal\nwas to transform triplets where the subject precedes\nthe relation (e.g., \"The official language of France\nis French.\") into triplets where the subject is placed\ndirectly before the relation (e.g., \"France’s official\nlanguage is French.\"). We selected Mistral-Large\nbecause it is open-weight, enabling reproducibil-\nity, and its capabilities are very close to those of\nGPT-4.\n3.1.2\nParametric Knowledge Query Format\nTo guide the studied LLMs towards generating\nparametric knowledge objects that are coherent\nwith the relation and to help specifying the type of\nobject that is expected when there are multiple pos-\nsible answers (for example in \"Napoleon passed\naway in\" the LLM can generate the place of death\n\"Longwood\" or the year of death \"1821\") we pro-\npose to use a template prompt that is illustrated\nin Figure 2. The prompt specifies the requested\ntype of object with a brief description as well as\nan example (one-shot learning) to help the LLM\nunderstand the kind of object that is intended (il-\nlustrated in blue in Figure 2). The description and\nexample were manually created for each relation.\nThe prompt also tries to guide the LLM towards\ngenerating a concise output as these models tend\nto give a long explanation that is irrelevant in our\nstudy (e.g. Amazon is headquartered in the city of\nSeattle where Starbucks is also headquartered...).\n3.1.3\nSubject/Object Bias\nThe subject can sometimes provide relevant infor-\nmation about the object which can bias our def-\ninition of parametric knowledge (e.g. Princeton\n3https://mistral.ai/news/mistral-large/\nUniversity Press is located in Princeton. or Niger\nshares the border with Nigeria). To avoid this, we\nremoved examples where the subject is similar to\nthe object, utilizing the Jaro-Winkler string dis-\ntance (Jaro–Winkler) with a threshold empirically\nfixed at 0.8. This method is advantageous for our\ndataset, as it assigns closer distances to subjects\nwith the same prefix as the objects, which is com-\nmon in cases like \"Croatia’s official language is\nCroatian\" where \"Croatia\" and \"Croatian\" have\nthe same prefix.\n3.2\nContext Knowledge (CK)\nIn our framework, we perturb the LLM by pro-\nviding a CK that contradicts the PK, which we\nname counter-PK and denote PK. It is challeng-\ning to test what the model does not know (Yin\net al., 2023). One way to build these inputs is to\ncontradict what the model learned during training\nby taking (s, r, o) ∈PK and replacing o with an-\nother object ¯o ∈Or that shares the same relation r\nto keep semantic consistency (e.g. \"Elvis Presley\nis a citizen of Japan\", here we replaced \"the USA\"\nwith a country name: \"Japan\"). More specifically,\nthe set of tuples PK that represents the counter-PK\nis defined as follows:\nPK =\n[\n(s,r,o)∈PK\nCounter-PKk(s, r, o)\n(2)\nwhere:\nCounter-PKk(s, r, o) =\n{(s, r, o, ¯o) | ¯o ∈Or∧\n¯o ̸= o ∧rankθ(¯o | s, r) ≤k}\n(3)\nwhere k is the number of counter-knowledge\ntriplets per triplet (s, r, o) in PK; rankθ(o | s, r)\nis the rank of ¯o among the Or ordered by the in-\ncreasing probability p(ˆo | q(s, r)) of the LLM to\n606\n",
  "4": "subject\nrel-lemma\nobject\nquery\nNewport County A.F.C.\nis-headquarter\nNewport\nNewport County A.F.C. is headquartered in\nNorway\ncapital-city-of\nOslo\nNorway’s capital city,\nWWE\nis-headquarter\nStamford\nWWE is headquartered in\nPrinceton University Press\nis-headquarter\nPrinceton\nPrinceton University Press is headquartered in\nInternet censorship\nis-subclass\ncensorship\nInternet censorship is a subclass of\nMcMurdo Station\npart-of-continent\nAntarctica\nMcMurdo Station is a part of the continent of\nWindows Update\nproduct-manufacture-by\nMicrosoft\nWindows Update, a product manufactured by\nNintendo\nlocated-in\nKyoto\nThe headquarter of Nintendo is located in\nMicrosoft Windows SDK\nproduct-manufacture-by\nMicrosoft\nMicrosoft Windows SDK, a product manufactured by\nHarare\ncapital-of\nZimbabwe\nHarare, the capital of\nTable 1: A sample of the raw ParaRel dataset (Elazar et al., 2021)\nquery = \"Virginia's official language is\"\nVirginia's official language is Croatian.\nVirginia's official language is Serbian.\nVirginia's official language is Swedish.\nP(\"English\" | query) = 0.7\nP(\"French\" | query) = 0.15\n.\n.\n.\nP(\"Croatian\" | query) = 0.06\nP(\"Serbian\" | query) = 0.03\nP(\"Swedish\" | query) = 0.02\nFigure 3: Example of 3 counter-knowledge objects that\nwere associated to a parametric knowledge element.\nThe probability distribution is ranked in an descendant\norder and we selected the objects with the lowerst prob-\nabilities.\ngenerate an object ˆo ∈Or given the prompt q(s, r).\nWe also make sure that the model has not learned\nthe (s, r, ¯o) association by considering the objects\nˆo with the k lowest ranks (rankθ ≤k) – indicating\nthat the LLM is very unlikely to use its parametric\nknowledge to generate ¯o.\nFigure 3 illustrates the counter-knowledge ob-\njects that were generated by Phi-1.5 for a paramet-\nric knowledge example.\n3.3\nModels\nWe consider decoder-only Transformer models.\nBetween layer l and l −1, the hidden state X(l−1)\nis updated by:\nX(l) = γ(X(l−1) + A(l))) + M(l)\n(4)\nwhere A(l) and M(l) are the outputs of the MHSA\nand MLP modules respectively, and γ is a non-\nlinearity.\nThe MLP module is a two-layer neural network\nparameterized by matrices W (l)\nmlp ∈Rd×dmlp and\nW (l)\nproj ∈Rdmlp×d:\nM(l) = σ(X(l)\nmlpW (l)\nmlp)W (l)\nproj ∈Rn×d\n(5)\nwhere σ is a non-linearity function (e.g. GeLU)\nand X(l)\nmlp is the input of the MLP. We refer the\nreader to Vaswani et al. (2017) for more details on\nthe architecture.\nIn our probing set-up (Section 4), we use the\nfollowing activations: σ(X(l)\nmlpW (l)\nmlp) the first layer\nof the MLP (referred as MLP-L1 in this paper),\nσ(X(l)\nmlpW (l)\nmlp)W (l)\nproj the output of the MLP (i.e.\nsecond layer, referred as MLP in this paper), and\nA(l) the output of the MHSA. We consider the first\nand second MLP layers activations, based on Geva\net al. (2021) work, and also the MHSA activations\nas the attentions play a crucial role in informa-\ntion selection from the MLP memory (Geva et al.,\n2023).\nWe evaluate our method on several LLMs with\ndifferent sizes: Phi-1.5 with 1.3B parameters (Li\net al., 2023), Pythia-1.4B with 1.4B parameters\n(Biderman et al., 2023), Mistral-7B with 7B pa-\nrameters (Jiang et al., 2023), and Llama3-8B with\n8B parameters (AI@Meta, 2024). Table 2 gives\ncharacteristics about the LLMs’ modules dimen-\nsions.\nModel\nMLP\nMLP-L1\nMHSA\nPhi-1.5\n2048\n8192\n2048\nPythia-1.4B\n2048\n8192\n2048\nLlama3-8B\n4096\n14336\n4096\nMistral-7B\n4096\n14336\n4096\nTable 2: Activation dimensions for Phi-1.5, Pythia-1.4B,\nLlama3-8B and Mistral-7B for the different considered mod-\nules (MLP, MLP-L1 and MHSA)\nDecoding strategy As the generated sequences\nare short, we use a greedy decoding strategy and\nlimit the number of generated tokens to 10.\n4\nProbing Set-up\nTo build our probing dataset,\nwe associate\neach tuple (s, r, o, ¯o)\n∈\nPK with a prompt\n607\n",
  "5": "Relation Group ID\nRelations\n#Examples\ngeographic-geopolitic-language\nis-headquarter, located-in, headquarters-in, locate, share-border, is-twin-\ncity-of, located, border-with, is-located, work-in-area, which-is-located,\ncapital-city-of, part-of-continent, capital-of, headquarter, belong-to-\ncontinent, based-in, is-citizen-of, that-originate-in, originate-in, is-in,\nfound-in, share-common-border, is-native-to, is-originally-from, pass-\naway-in, born-in, hold-citizenship-of, have-citizenship-of, citizen-of,\nstart-in, formulate-in, legal-term, tie-diplomatic-relations, maintains-\ndiplomatic-relations, have-diplomatic-relations, native, mother-tongue,\noriginal-language-is, the-official-language, communicate\n2815\ncorporate-products-employment\nproduct-manufacture-by, develop-by, owned-by, product-develope-by,\nproduct-release-by, create-by, product-of, produce-by, owner, is-product-\nof, is-part-of, who-works-for, employed-by, who-employed-by, works-for,\nwork-in-field, profession-is, found-employment\n1217\nmedia\npremiere-on, to-debut-on, air-on-originally, debut-on\n128\nreligion\nofficial-religion\n249\nhierarchy\nis-subclass\n183\nnaming-reference\nis-call-after, is-name-after, is-name-for\n6\noccupy-position\nplay-in-position, who-holds\n77\nplay-instrument\nplay-the\n13\nTable 3: All the relation groups with their corresponding relations and number of examples.\nprompt(s, r, ¯o) that corresponds to a natural\nlanguage statement of (s, r, ¯o) followed by\nq(s, r) (see Figure 1).\nEach prompt is asso-\nciated with a label among CK, PK, and ND,\nwhere CK if Gθ(prompt(s, r, ¯o)) = ¯o, PK if\nGθ(prompt(s, r, ¯o)) = o, and with ND (Not De-\nfined) otherwise. In this work, we discard tuples\nassociated with ND.\nWe specifically probe the activations ¯o of the\nobject, sq of the subject in the query, and rq the\nrelation in the query. As each of these elements\nmay have multiple tokens, we use their last tokens\nas their representative (e.g. for \"Washington\" →\n[\"Wash\", \"inghton\"], we consider the activations\nof the token \"inghton\"). The fact that this token\nrepresentation summarizes the entity is intuitively\ntrue for decoder-only models and has been exper-\nimentally validated in (Meng et al., 2023; Geva\net al., 2023).\nNote that our first probe targets ¯o as this is\nwhere the knowledge conflict starts (e.g.\nBill\nGates is the founder of Apple(¯o). Bill Gates(sq)\nis the founder of(rq) _).\n4.1\nControl experiments\nWe also probe the activations of the first token to\nmeasure how much of the prediction can be at-\ntributed to the subject representation itself. Since\nthe knowledge perturbation starts with the first ob-\nject token, the first token activations should not\nindicate the knowledge source. For instance, in\nParis is located in Italy the representation of the\nfirst token (Paris) should not contain information\nabout the knowledge source as the perturbation\nstarts at Italy.\n4.2\nRelation Groups\nTo avoid syntactic and semantic biases related to\nthe type of relation when training a classifier, we\ngrouped the relations that are similar into relation\ngroups. The relation groups are illustrated in Ta-\nble 3.\n4.3\nEvaluation\nWe use each relation group as a test set and train\non the rest of the relation groups. We make sure\nthat the train and test sets do not share similar\nsubjects and objects to avoid biases related to the\nsyntax or the nature of the relation and subject. We\nensure the train set is balanced (equal number of\nCK and PK), as current LLMs are more likely to\nuse context information (CK) than their parametric\nknowledge Xie et al. (2024). This is illustrated by\nFigure 4 (and Figure 7 in appendix for a breakdown\nby relation), where we can see that the considered\nLLMs mostly generate CK-based outputs.\nWe also ensure that the test set is balanced so\nwe can use the success rate (accuracy) as the main\nmetric –– with 50% being the performance of a ran-\ndom classifier. We compute the success rate pi for\n608\n",
  "6": "each group of relations. As pi follows a binomial\ndistribution, we used a binomial proportion confi-\ndence interval to compute the weighted standard\nerror (WSE – see formula 6) around the average\nsuccess rate (see formula 9) with a 95% confidence\ninterval to assess the significance of the resulting\nclassification scores for each layer and token. We\nused the following formula in order to propagate\nthe errors across the relation groups:\nWSE =\nv\nu\nu\nt\nG\nX\ni=1\n\u0010ni\nN × SEi\n\u00112\n(6)\nWhere SEi is the standard error for the ith rela-\ntion group, defined as:\nSEi =\ns\npi × (1 −pi)\nni\n(7)\nG = 8 is the number of relation groups; ni is the\nnumber of test examples for the ith relation group;\nN is the total number of test examples across all\nthe relation groups.\nThe error bars are finally computed using a z-\nscore of 1.96 for a confidence interval of 95%:\nError Bars = [P −1.96 × WSE, P + 1.96 × WSE]\n(8)\nWhere P is the average success rate across all\nthe relation groups:\nP =\nPG\ni=1 ni × pi\nN\n(9)\nFigure 5 presents the success rates for classifiers\ntrained on activations from object, subject, and re-\nlation tokens, with the first token used as a control\n(see Section 4.1 for more details on the control\nexperiment.) Results are reported for Mistral-7B,\nPhi-1.5, Llama3-8B, and Pythia-1.4B. Solid lines\nrepresent the average success rates across relation\ngroups, while shaded areas denote the weighted\nstandard error with a 95% confidence interval.\n5\nResults and Discussion\nIn Figure 5, we can first observe that the features\nlinked to ¯o, the subject sq and the relation rq ex-\nhibit a correlation with the used knowledge source\nfor MLP and MLP-L1 activations. The most pre-\ndictive features are those of rq, i.e. the relation\ntoken in the query. Starting from the mid-layers\nPythia-1.4B Llama3-8B\nPhi-1.5\nMistral-7B\nModel\n0\n2000\n4000\n6000\n8000\nCount of Used Knowledge Source\nKnowledge Source\nCK\nND\nPK\nFigure 4: Count of used knowledge sources by each\nmodel (CK, PK, and ND). ND refers to outputs where\nthe knowledge source is not defined.\nof the relation token, the success rate increases\nsignificantly, reaching about 80%. This finding\nis consistent with prior research, which indicates\nthat LLMs primarily store knowledge in the MLPs\n(Meng et al., 2023; Geva et al., 2021). Moreover, it\nsupports Geva et al. (2023)’s insights on the infor-\nmation extraction process, where the relation token\nretrieves attributes from sq (a process referred to\nas Attribute Extraction).\nAdditionally, it is noteworthy that the knowledge\nsource can be detected directly starting from the\nperturbing object ¯o. This shows that detecting a\npotentially harmful conflict knowledge statement\nis possible early in the LLM inference process.\nMHSA activations are less connected to the used\nknowledge source than MLP and MLP-L1 activa-\ntions.\nThe results of the control experiments conducted\non the first token of the input indicate that the\nlearned patterns in the object, subject, and relation\nare not arbitrary. The success rates of most LLMs\nfor the first token appear to be random (about 0.5),\nwith the exception of Pythia-1.4B, where the first\ntoken provides a slight indication of the knowl-\nedge source, although no significant fluctuations\nare observed.\nFinally, compared to (Yu et al., 2023), we show\nin this work that it is possible to predict the knowl-\nedge source based on the sole activations of an\nLLM, and, even more importantly, that we predict\nthis for multiple relations rather than being limited\nto a single relation.\n6\nSubject frequency Vs. Knowledge\nSource\nTo understand what makes an LLM select the CK\nobject over the PK object, we observed the subject\n609\n",
  "7": "1\n3\n6\n9\n12\n15\n18\n21\n24\n0.4\n0.5\n0.6\n0.7\n0.8\nSuccess Rate\nFirst Token (control)\n1\n3\n6\n9\n12\n15\n18\n21\n24\n                                                                     Phi-1.5\nObject\n1\n3\n6\n9\n12\n15\n18\n21\n24\nSubject\n1\n3\n6\n9\n12\n15\n18\n21\n24\nRelation\n1\n3\n6\n9\n12\n15\n18\n21\n24\n28\n32\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSuccess Rate\n1\n3\n6\n9\n12\n15\n18\n21\n24\n28\n32\n                                                                     Llama3-8B\n1\n3\n6\n9\n12\n15\n18\n21\n24\n28\n32\n1\n3\n6\n9\n12\n15\n18\n21\n24\n28\n32\n1\n3\n6\n9\n12\n15\n18\n21\n24\n28\n32\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSuccess Rate\n1\n3\n6\n9\n12\n15\n18\n21\n24\n28\n32\n                                                                     Mistral-7B\n1\n3\n6\n9\n12\n15\n18\n21\n24\n28\n32\n1\n3\n6\n9\n12\n15\n18\n21\n24\n28\n32\n1\n3\n6\n9\n12\n15\n18\n21\n24\nLayer\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSuccess Rate\n1\n3\n6\n9\n12\n15\n18\n21\n24\nLayer\n                                                                     Pythia-1.4B\n1\n3\n6\n9\n12\n15\n18\n21\n24\nLayer\n1\n3\n6\n9\n12\n15\n18\n21\n24\nLayer\nMLP\nMHSA\nMLP-L1\nFigure 5: Performance of the linear classifier in identifying knowledge sources across different layers and modules\n(MLP, MLP-L1, MHSA). The plots show success rates for classifiers trained on activations from object, subject, and\nrelation tokens, with the first token used as a control (see Section 4.1 for more details on the control experiment.)\nResults are reported for the Mistral-7B, Phi-1.5, Llama3-8B, and Pythia-1.4B models. Solid lines represent the\naverage success rates across relation groups, while shaded areas denote the weighted standard error with a 95%\nconfidence interval. See Section 4.3 for further details on the evaluation methodology.\n610\n",
  "8": "Input Prompt\nKnowledge Source\nPK Object\nModel\nHarney County has its capital city in Taiwan. Harney County has its\ncapital city in Burns.\nND\nOregon\nLlama3-8B\nLisa Appignanesi has citizenship of Finland. Lisa Appignanesi has\ncitizenship of France.\nND\nthe UK\nLlama3-8B\nCraiova is located in the continent of India. Craiova is located in the\ncontinent of Romania.\nND\nEurope\nPythia-1.4B\nThe Kingdom of Hungary had its capital as Connecticut. The Kingdom\nof Hungary had its capital as Connecticut.\nCK\nBudapest\nMistral-7B\nThe Wii U system software is a product that was manufactured by\nSquare. The Wii U system software is a product that was manufactured\nby Square.\nCK\nNintendo\nLlama3-8B\nThe Centers for Disease Control and Prevention is headquartered in\nLyon. The Centers for Disease Control and Prevention is headquartered\nin Lyon.\nCK\nAtlanta\nLlama3-8B\nHarare is the capital city of Florida. Harare is the capital city of\nZimbabwe.\nPK\nZimbabwe\nPythia-1.4B\nGoodreads is owned by Microsoft. Goodreads is owned by Amazon.\nPK\nAmazon\nPhi-1.5\nOneDrive is owned by Toyota. OneDrive is owned by Microsoft.\nPK\nMicrosoft\nMistral-7B\nTable 4: Examples of final probing prompts, including their knowledge source, the LLM, and the corresponding parametric\nknowledge (PK) object. Bold text indicates the generated object, while underlined text represents the counter-knowledge object.\nMeta-Llama-3-8B\nMistral-7B-v0.1\nEleutherAI_pythia-1.4b\nPhi-1_5\nModel\n101\n102\n103\n104\n105\n106\n107\nSubject Count in The Pile (Log Scale)\nKnowledge Source\nCK\nND\nPK\nFigure 6: Subject frequency in the training dataset (The\nPile) for CK, PK, and ND outputs. We use The Pile as\nan approximation of what the LLMs might have learned\nexcept for Pythia-1.4B for which it is the actual training\ndata.\nfrequency in The Pile corpus (Gao et al., 2020) for\nCK, PK, and ND outputs as illustrated in Figure 6\n– We use The Pile as an approximation of what the\nLLMs might have learned except for Pythia-1.4B\nfor which it is the training data. We used the infini-\ngram API made available by Liu et al. (2024) in\norder to get the frequencies. A Mann-Whitney U\ntest reveals that the subject frequency distribution\nfor PK outputs is significantly higher than for CK\nand ND outputs, except in the case of Pythia-1.4B,\nwhere PK is only higher than CK but not ND. This\nsuggests that as a model gains more knowledge\nabout a subject, it becomes more likely to select\nPK over CK objects.\n7\nProbing Dataset Examples\nTable 4 illustrates some examples of the final prob-\ning prompts with their knowledge source, the LLM,\nand the corresponding PK object.\n8\nConclusion\nIn this study, we introduced a novel probing frame-\nwork to investigate if we can detect when LLMs\nswitch from PK to CK. Our findings reveal that spe-\ncific model activations are significantly correlated\nwith the used knowledge source. This opens the\ndoor for future work investigating the mechanism\nat play when such a switch occurs, and finally to\nbuilding models that can better control this behav-\nior.\n9\nLimitations\nOur current framework is designed to probe LLMs\nby introducing contradictions to their learned\nknowledge, effectively identifying the source of\nknowledge. However, this controlled experimental\nsetting does not account for many other situations,\ne.g. where the knowledge remains unperturbed.\nFuture work should extend the framework to han-\ndle cases where both the parametric knowledge\n(PK) and the contextual knowledge (CK) are con-\nsistent or not related, providing a more comprehen-\nsive understanding of LLM behavior. Additionally,\n611\n",
  "9": "our study primarily measures the correlation be-\ntween specific activations and the use of PK or CK,\nwhich, while providing valuable insights, does not\nestablish an explanation of the underlying process.\nFurther research is needed to uncover the under-\nlying mechanisms that govern knowledge source\nselection in LLMs, possibly through experimental\ndesigns that manipulate specific model parame-\nters or activations to observe resulting behavioral\nchanges.\nIt might also be interesting to employ a variety\nof prompt structures to mitigate biases associated\nwith the conventional subject-relation-object for-\nmat. Exploring alternative combinations, such as\nrelation-subject-object (e.g., The official language\nof Italy is Italian), could yield valuable insights.\n10\nEthical Considerations\nOur probing framework of LLMs for their\nknowledge-sourcing behaviors only uses publicly\navailable, non-personal datasets to ensure privacy\nand security. We recognize the potential for mis-\nuse of our findings. The insights derived from our\nresearch could be exploited to generate misleading\ninformation or make the models more susceptible\nto adversarial attacks. Therefore, we emphasize\nthe importance of the ethical application of our\nwork. Researchers and practitioners must imple-\nment robust safeguards to prevent the misuse of\nthese technologies and ensure they are used to ben-\nefit society. Developing and deploying robust secu-\nrity measures is essential to protect against these\nvulnerabilities and maintain the integrity of infor-\nmation generated by LLMs. While we recognize\ninherent biases in LLMs, our commitment to trans-\nparency is demonstrated through the public release\nof our framework, facilitating reproducibility and\nfurther research.\n11\nAcknowledgements\nWe would like to thank BNP Paribas and the French\nNational Association for Research and Technology\n(ANRT) who founded this project under the CIFRE\nprogram (2023/1673).\nReferences\nAI@Meta. 2024. Llama 3 model card.\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona\nDiab, and Marjan Ghazvininejad. 2022. A review\non language models as knowledge bases. Preprint,\narXiv:2204.06031.\nNora Belrose, Zach Furman, Logan Smith, Danny Ha-\nlawi, Igor Ostrovsky, Lev McKinney, Stella Bider-\nman, and Jacob Steinhardt. 2023. Eliciting latent\npredictions from transformers with the tuned lens.\nPreprint, arXiv:2303.08112.\nStella Biderman, Hailey Schoelkopf, Quentin An-\nthony, Herbie Bradley, Kyle O’Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Puro-\nhit, USVSN Sai Prashanth, Edward Raff, Aviya\nSkowron, Lintang Sutawika, and Oskar van der\nWal. 2023. Pythia: A suite for analyzing large lan-\nguage models across training and scaling. Preprint,\narXiv:2304.01373.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha\nRavichander, Eduard Hovy, Hinrich Schütze, and\nYoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Preprint,\narXiv:2102.01017.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The pile: An\n800gb dataset of diverse text for language modeling.\nPreprint, arXiv:2101.00027.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual associ-\nations in auto-regressive language models. Preprint,\narXiv:2304.14767.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are\nkey-value memories. Preprint, arXiv:2012.14913.\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\nguage models as knowledge bases: On entity repre-\nsentations, storage capacity, and paraphrased queries.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 1772–1791, Online.\nAssociation for Computational Linguistics.\nJaro–Winkler. Jaro–winkler distance — Wikipedia, the\nfree encyclopedia.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of hallu-\ncination in natural language generation. ACM Com-\nputing Surveys, 55(12):1–38.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Tim-\nothée Lacroix, and William El Sayed. 2023. Mistral\n7b. Preprint, arXiv:2310.06825.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2021.\n612\n",
  "10": "Retrieval-augmented generation for knowledge-\nintensive nlp tasks. Preprint, arXiv:2005.11401.\nYuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del\nGiorno, Suriya Gunasekar, and Yin Tat Lee. 2023.\nTextbooks are all you need ii: phi-1.5 technical report.\nPreprint, arXiv:2309.05463.\nJiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin\nChoi, and Hannaneh Hajishirzi. 2024. Infini-gram:\nScaling unbounded n-gram language models to a\ntrillion tokens. arXiv preprint arXiv:2401.17377.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2023. Locating and editing factual associ-\nations in gpt. Preprint, arXiv:2202.05262.\nGiovanni Monea, Maxime Peyrard, Martin Josifoski,\nVishrav Chaudhary, Jason Eisner, Emre Kıcıman,\nHamid Palangi, Barun Patra, and Robert West. 2024.\nA glitch in the matrix? locating and detecting lan-\nguage model grounding with fakepedia. Preprint,\narXiv:2312.02073.\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado,\nD Sculley, Sebastian Nowozin, Joshua V. Dillon,\nBalaji Lakshminarayanan, and Jasper Snoek. 2019.\nCan you trust your model’s uncertainty? evaluating\npredictive uncertainty under dataset shift. Preprint,\narXiv:1906.02530.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-\njie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. Preprint, arXiv:1912.01703.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nKevin Wang, Alexandre Variengien, Arthur Conmy,\nBuck Shlegeris, and Jacob Steinhardt. 2022.\nIn-\nterpretability in the wild:\na circuit for indirect\nobject identification in gpt-2 small.\nPreprint,\narXiv:2211.00593.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020. Hug-\ngingface’s transformers: State-of-the-art natural lan-\nguage processing. Preprint, arXiv:1910.03771.\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and\nYu Su. 2024. Adaptive chameleon or stubborn sloth:\nRevealing the behavior of large language models in\nknowledge conflicts. Preprint, arXiv:2305.13300.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,\nXipeng Qiu, and Xuanjing Huang. 2023. Do large\nlanguage models know what they don’t know? In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, pages 8653–8665, Toronto,\nCanada. Association for Computational Linguistics.\nQinan Yu, Jack Merullo, and Ellie Pavlick. 2023. Char-\nacterizing mechanisms for factual recall in language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 9924–9959, Singapore. Association for\nComputational Linguistics.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2020. Fine-tuning lan-\nguage models from human preferences. Preprint,\narXiv:1909.08593.\nA\nData Characteristics\nThe ParaRel (Elazar et al., 2021) dataset includes\n5313 unique subject-relation pairs, leading to the\nformation of the same number of PK triplets. After\nremoving the examples where the subject is sim-\nilar to the parametric object (see Section 3.1.3)\nwe are left with approximately 3600 examples\ndepending on the LLMs’ parametric knowledge.\nWe take k = 3 for Counter-PKk which gives ap-\nproximately counter-PK 10k triplets. After under-\nsampling, we are left with approximately 3000\nbalanced prompts depending on the LLM.\nB\nHardware and Software\nText generation tasks were performed using A100\nGPUs, each equipped with 80 GB of memory. The\nprocess of generating the outputs spanned around\n100 GPU hours. Our framework was constructed\nutilizing PyTorch (Paszke et al., 2019) and the Hug-\ngingFace Transformers library (Wolf et al., 2020).\nC\nLicense\nModel weights. Llama3-8B weights are released\nunder the license available at https://llama.\nmeta.com/llama3/license/.\nMistral-7B and\nPythia-1.4B weights are released under an Apache\n2.0 license. Mistral-Large weights are released\nunder the licence available at https://mistral.\nai/licenses/MRL-0.1.md. Phi-1.5 weights are\nreleased under a MIT license.\nData. The ParaRel dataset we used is released\nunder a MIT License.\n613\n",
  "11": "0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nCount\nmother-tongue\nthe-official-language\npart-of-continent\nofficial-religion\nbelong-to-continent\ncapital-of\nis-headquarter\ncommunicate\ncapital-city-of\nis-citizen-of\nnative\nhave-citizenship-of\nproduct-manufacture-by\nair-on-originally\nshare-border\nplay-in-position\nwork-in-field\nis-subclass\nproduct-develope-by\nheadquarters-in\nproduct-release-by\nowned-by\ndevelop-by\ndebut-on\npremiere-on\nborder-with\nhold-citizenship-of\nlocate\nheadquarter\nthat-originate-in\nborn-in\nlegal-term\ncitizen-of\nlocated-in\nto-debut-on\noriginate-in\noriginal-language-is\nshare-common-border\npass-away-in\nproduce-by\ncreate-by\nprofession-is\nplay-the\nwho-works-for\nemployed-by\nbased-in\nworks-for\nwho-holds\nis-originally-from\ntie-diplomatic-relations\nwho-employed-by\nis-in\nis-native-to\nhave-diplomatic-relations\nwhich-is-located\nis-located\nRelation\nKnowledge Source\nPK\nCK\nND\nFigure 7: All the considered relations with the number of outputs that used CK (orange), PK (green), and ND (blue)\nsources (the counts include all the considered LLMs).\n614\n"
}