{
  "1": "Published as a conference paper at ICLR 2023\nADALORA: ADAPTIVE BUDGET ALLOCATION FOR\nPARAMETER-EFFICIENT FINE-TUNING\nQingru Zhang†∗, Minshuo Chen‡, Alexander Bukharin†, Nikos Karampatziakis⋄,\nPengcheng He⋄, Yu Cheng⋄, Weizhu Chen⋄and Tuo Zhao†\n†Georgia Institute of Technology\n‡Princeton University\n⋄Microsoft Azure AI\n{qingru.zhang,abukharin3,tourzhao}@gatech.edu\nmc0750@princeton.edu\n{nikosk,penhe,yu.cheng,wzchen}@microsoft.com\nABSTRACT\nFine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large number\nof downstream tasks are present. Therefore, many fine-tuning methods are proposed\nto learn incremental updates of pre-trained weights in a parameter efficient way,\ne.g., low-rank increments. These methods often evenly distribute the budget\nof incremental updates across all pre-trained weight matrices, and overlook the\nvarying importance of different weight parameters. As a consequence, the fine-\ntuning performance is suboptimal. To bridge this gap, we propose AdaLoRA,\nwhich adaptively allocates the parameter budget among weight matrices according\nto their importance score. In particular, AdaLoRA parameterizes the incremental\nupdates in the form of singular value decomposition. Such a novel approach\nallows us to effectively prune the singular values of unimportant updates, which\nis essentially to reduce their parameter budget but circumvent intensive exact\nSVD computations. We conduct extensive experiments with several pre-trained\nmodels on natural language processing, question answering, and natural language\ngeneration to validate the effectiveness of AdaLoRA. Results demonstrate that\nAdaLoRA manifests notable improvement over baselines, especially in the low\nbudget settings. Our code is publicly available at https://github.com/\nQingruZhang/AdaLoRA.\n1\nINTRODUCTION\nPre-trained language models (PLMs) have manifested superior performance in various natural\nlanguage processing tasks (Devlin et al., 2019; Liu et al., 2019; He et al., 2021b; Radford et al.,\n2019; Brown et al., 2020). The most common way to adapt pre-trained models to down-stream\ntasks is to fine-tune all the parameters (full fine-tuning, Qiu et al. (2020); Raffel et al. (2020)).\nHowever, pre-trained models typically incurs large memory footprint. For example, BERT model\n(Devlin et al., 2019) consists up to 300 million parameters; T5 (Raffel et al., 2020) comprises up\nto 11 billion parameters and GPT-3 (Brown et al., 2020) contains up to 175 billion parameters.\nWhen building a NLP system upon these pre-trained models, we usually handle multiple tasks\nthat arrive simultaneously (Radford et al., 2019). Given a large number of down-stream tasks, full\nfine-tuning requires that each task maintains a separated copy of large models. The resulting memory\nconsumption is prohibitively expensive.\nTo address this issue, researchers have proposed two main lines of research to reduce the fine-tuning\nparameters, while maintaining or even improving the performance of PLMs. Specifically, one line\nof research focuses on adding small neural modules to PLMs and fine-tune only these modules for\neach task – the base model is kept frozen and shared across tasks. In this way, only a small number\nof task-specific parameters are introduced and updated, greatly enhancing the practicality of large\nmodels. For example, adapter tuning (Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2020;\n∗Work was done during Qingru Zhang’s internship at Microsoft Azure AI.\n1\narXiv:2303.10512v2  [cs.CL]  20 Dec 2023\n",
  "2": "Published as a conference paper at ICLR 2023\nWq\nWk\nWv\nWo\nWf1\nWf2\n88.50\n88.75\n89.00\n89.25\n89.50\n89.75\n90.00\nMNLI Matched Acc\n88.58\n88.98\n89.36 89.28\n89.91 89.99\n(a) Selected weight matrix\n1,2,3\n4,5,6\n7,8,9\n10,11,12\n78\n80\n82\n84\n86\n88\nMNLI Matched Acc\n77.87\n85.82\n88.15\n88.6\n(b) Selected layers\nFigure 1: Given the total trainable parameters as 0.28M, we apply LoRA only to selected weight matrices (left)\nor selected layers (right) of DeBERTaV3-base and compare the fine-tuning performance on MNLI-m. Figure 1a:\nwe only fine-tune a selected type of weight matrix of every transformer layer, including query/key/value\nprojection (Wq, Wk, Wv), output projection (Wo) in the self-attention, and two weight matrices (Wf1, Wf2) in\ntwo-layer FFNs. In Figure 1b, we apply LoRA to every weight matrix of the selected layers.\nHe et al., 2022) inserts small neural modules called adapters between the layers of the base model.\nPrefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) attach additional trainable\nprefix tokens to the input or hidden layers of the base model. These methods have shown to achieve\ncomparable performance to full fine-tuning, while only updating less than 1% of the original model\nparameters, significantly releasing the memory consumption.\nAnother line of research proposes to model the incremental update of the pre-trained weights in a\nparameter-efficient way, without modifying the model architecture (Zaken et al., 2021; Guo et al.,\n2020; Hu et al., 2022). Given a pre-trained weight matrix1 W (0), for example, diff pruning (Guo et al.,\n2020) models its incremental update ∆as a sparse matrix. Diff pruning initializes ∆as the same\ndimension as W (0) and then prunes ∆element-wise based on the magnitude of the entries. As such,\ndiff pruning can increase the parameter efficiency substantially by adaptively retaining important\nupdates and pruning unimportant ones. Nonetheless, diff pruning has several limitations. First, it\nrelies on low-level implementation to speed up the computation of unstructured sparse matrices,\nwhich is not well supported by existing deep learning frameworks. Therefore, we have to store ∆as\na dense matrix during training. Second, it needs to update every entry of ∆with their gradients and\nthen prune them. This results in similar computational cost as full fine-tuning (Guo et al., 2020).\nTo overcome these drawbacks, Hu et al. (2022) propose a method named LoRA, which parameterizes\n∆as a low-rank matrix by the product of two much smaller matrices:\nW = W (0) + ∆= W (0) + BA,\n(1)\nwhere W (0), ∆∈Rd1×d2, A ∈Rr×d2 and B ∈Rd1×r with r ≪{d1, d2}. During fine-tuning, only\nA and B are updated. The rank r is chosen to be much smaller than the dimension of W (e.g., r = 8\nwhen d1 = d2 = 1024). With less than 0.5% additional trainable parameters, the training overhead\ncan be reduced up to 70%, compared to full fine-tuning. However, LoRA achieves comparable or\neven better performance than full fine-tuning (Hu et al., 2022). Meanwhile, the product of two samll\nmatrices is more friendly to implement and deploy than unstructured sparse matrices in diff pruning.\nLoRA still has limitations as it prespecifies the rank r of each incremental matrix ∆identical. This\nignores the fact that the importance of weight matrices varies significantly across modules and layers\nwhen fine-tuning pre-trained models. To illustrate this point, we present an concrete example in\nFigure 1. We compare the performance of LoRA when fine-tuning specific modules or layers with\nthe same number of trainable parameters. Figure 1a shows that fine-tuning feed-forward networks\n(FFN) achieves better performance than self-attention modules. In addition, Figure 1b demonstrates\nthat weight matrices in top layers are more important than those in bottom layers.\nAdding more trainable parameters to the critical weight matrices can lead to better model performance.\nIn contrast, adding more parameters to those less important weight matrices yields very marginal\ngains or even hurt model performance. Given the parameter budget, i.e., the number of total trainable\nparameters, we always prefer to allocate more parameters to those important modules. Distributing\nthe budget evenly to all weight matrices/layers, like LoRA and other methods (e.g., adapter and prefix\ntuning), often gives suboptimal performance. To this end, a natural question is:\nHow can we allocate the parameter budget adaptively according to importance\nof modules to improve the performance of parameter-efficient fine-tuning?\n1Unless specified otherwise, we use W (0) to denote any pre-trained weight matrix.\n2\n",
  "3": "Published as a conference paper at ICLR 2023\nTo answer this question, we propose a new method – AdaLoRA (Adaptive Low-Rank Adaptation),\nwhich dynamically allocates the parameter budget among weight matrices during LoRA-alike fine-\ntuning. Specifically, AdaLoRA adjusts the rank of incremental matrices to control their budget.\nCritical incremental matrices are assigned with high rank such that they can capture more fine-grained\nand task-specific information. Less importance ones are pruned to have lower rank to prevent\noverfitting and save the computational budget. There are some methods to control the rank of matrices\nin the existing literature of matrix approximation (Cai et al., 2010; Koltchinskii et al., 2011; Toh &\nYun, 2010). Most of them directly compute singular value decomposition (SVD) of a matrix and\nthen truncate the smallest singular values. Such an operation can manipulate the rank explicitly\nand, more importantly, minimize the difference between the resulting matrix and the original matrix.\nHowever, for fine-tuning large models, it becomes prohibitively expensive to iteratively apply SVD\nfor a large number of high-dimensional weight matrices. Therefore, instead of computing SVD\nexactly, we parameterize ∆as ∆= PΛQ to mimic SVD. The diagonal matrix Λ contains singular\nvalues while the orthogonal matrices P and Q represent left/right singular vectors of ∆. To regularize\nthe orthogonality of P and Q, an additional penalty is added to training loss. Such a parameterization\navoids the intensive computations of SVD. Besides, another advantage is that we only need to drop the\nunimportant singular values while the singular vectors are maintained. This preserves the possibility\nof future recovery and stabilizes the training. See a detailed comparison to LoRA in Section 3.\nBased on our SVD parameterization, AdaLoRA dynamically adjusts the rank of ∆= PΛQ by\nimportance scoring. Specifically, we divide the incremental matrix PΛQ into triplets, where each\ntriplet Gi contains the i-th singular value and the corresponding singular vectors. To quantify the\nimportance of triplets, we propose a novel importance metric, which takes account of the contribution\nof every entry in Gi to the model performance (Sanh et al., 2020; Liang et al., 2021; Zhang et al.,\n2022). Triplets with low importance scores are granted low priority and hence the singular values are\nzeroed out. Triplets with high importance are retained for fine-tuning. Moreover, we also propose\na global budget scheduler to facilitate the training. In particular, we start from an initial parameter\nbudget, which is slightly higher than the final budget, and then gradually reduce it until matching\nthe target. Such a scheduler can improve the training stability and model performance. Please see\nSection 3 for a detailed description of our importance metric and budget scheduler.\nWe conduct extensive experiments on a wide range of tasks and models to demonstrate the effec-\ntiveness of AdaLoRA. Specifically, we evaluate the performance using DeBERTaV3-base (He et al.,\n2021a) on natural language understanding (GLUE, Wang et al. (2019)) and question answering\n(SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)) datasets. We also apply\nour methods to BART-large (Lewis et al., 2019) and evaluate the performance on natural language\ngeneration (XSum, Narayan et al. (2018) and CNN/DailyMail, Hermann et al. (2015)) tasks. We\nshow AdaLoRA consistently outperforms the baseline, especially under low budget settings. For\nexample, with less than 0.1% trainable parameters of full fine-tuning, AdaLoRA achieves a 1.2% F1\nimprovement on the SQuAD2.0 dataset compared with state-of-the-art approaches.\n2\nBACKGROUND\nTransformer-based Models. A typical transformer model consists of L stacked blocks, where each\nblock contains two submodules: a multi-head attention (MHA) and a fully connected FFN. Given the\ninput sequence X ∈Rn×d, MHA performs the attention function in parallel h heads:\nMHA (X) = Concat(head1, ..., headh)Wo,\nheadi = Softmax\n\u0010\nXWqi(XWki)⊤/\np\ndh\n\u0011\nXWvi,\nwhere Wo ∈Rd×d is an output projection and Wqi, Wki, Wvi ∈Rd×dh are query, key and value\nprojections of head i. dh is typically set to d/h. The other important module is a FFN which consists\nof two linear transformations with a ReLU activation in between: FFN(X) = ReLU(XWf1 +\nb1)Wf2 + b2, where Wf1 ∈Rd×dm and Wf2 ∈Rdm×d. Finally, a residual connection is used\nfollowed by a layer normalization (Ba et al., 2016).\nLow Rank Adaptation. LoRA (Hu et al., 2022) models the incremental update of the pre-trained\nweights by the product of two small matrices. For h = W (0)x, the modified forward pass is:\nh = W (0)x + ∆x = W (0)x + BAx,\n(2)\nwhere W (0), ∆∈Rd1×d2, A ∈Rr×d2 and B ∈Rd1×r with r ≪{d1, d2}. A typically adopts a\nrandom Gaussion initialization while B is initialized with zero to have ∆= 0 at the beginning of\n3\n",
  "4": "Published as a conference paper at ICLR 2023\ntraining. We further denote Ai∗as the i-th row of A, B∗i as the i-th column of B, and Gi = {Ai∗, B∗i}\nas the i-th doublet. Hu et al. (2022) only apply LoRA to query and value projections (i.e, Wq and\nWv) in the MHAs. He et al. (2022) extend it to weight matrices of FFNs (i.e, Wf1 and Wf2), leading\nto the performance improvement . Meanwhile, they propose a unified view of various efficient tuning\nmethods including adapter tuning, prefix tuning and LoRA.\n3\nADALORA METHOD\nOur method contains two important components: (i) SVD-based adaptation, which formulates\nthe incremental matrices in the form of singular value decomposition; (ii) Importance-aware rank\nallocation, which prunes redundant singular values based on our newly-designed importance metric.\n3.1\nSVD-BASED ADAPTATION\nAs mentioned in Section 1, we propose to parameterize the incremental updates of the pre-trained\nweight matrices in the form of singular value decomposition:\nW = W (0) + ∆= W (0) + PΛQ,\n(3)\nwhere P ∈Rd1×r and Q ∈Rr×d2 represent the left/right singular vectors of ∆and the diagonal\nmatrix Λ ∈Rr×r contains the singular values {λi}1≤i≤r with r ≪min(d1, d2). We further denote\nGi = {P∗i, λi, Qi∗} as the triplet containing the i-th singular value and vectors. In practice, since\nΛ is diagonal, we only need to save it as a vector in Rr. Λ is initialized with zero while P and Q\nadopt a random Gaussian initialization to ensure ∆= 0 at the beginning of training. To enforce the\northogonality of P and Q, i.e., P ⊤P = QQ⊤= I, we utilize the following regularizer2:\nR(P, Q) = ∥P ⊤P −I∥2\nF + ∥QQ⊤−I∥2\nF.\n(4)\nIn our method, Λ is iteratively pruned to adjust the rank after each gradient decent step. As mentioned\nin Section 1, one can directly compute SVD for every ∆to manipulate singular values. The\ncomputational complexity, however, is O(min(d1, d2)d1d2). It becomes extremely expensive to\niteratively apply SVD for a large number of high-dimensional incremental matrices. In contrast, our\nparameterization avoids intensive SVD computation, greatly releasing the computational overhead.\nWe remark that one can also apply structured pruning to LoRA to control the rank (i.e., prune BA\ndoublet-wise in (1)), whereas it has the following disadvantages. First, when a doublet is measured as\nunimportant, we have to prune all of its elements. It makes scarcely possible to reactivate the pruned\ndoublets as their entries are all zeroed out and not trained. In contrast, AdaLoRA only masks out\nthe singular values based on (3) while the singular vectors are always maintained. It preserves the\npotential of future recovery for the triplets dropped by mistake. Second, A and B of LoRA are not\northogonal, meaning the doublets can be dependent with each other. Discarding the doublets can\nincur larger variation from the original matrix than truncating the smallest singular values. Therefore,\nthe incremental matrices are often altered dramatically after each step of rank allocation, which\ncauses training instability and even hurts generalization. To demonstrate this point, we present an\nablation study in Section 4.4, which compares AdaLoRA with structured pruning for LoRA.\n3.2\nIMPORTANCE-AWARE RANK ALLOCATION\nWe apply the SVD-based adaptation (3) to every weight matrix including Wq, Wk, Wv, Wf1 and\nWf2 of each transformer layer. In order to control the budget, we iteratively prune singular values\nin correspondence to their importance score during the training. For clear reference, we use k to\nindex the incremental matrix, i.e., ∆k = PkΛkQk for k = 1, . . . , n, where n is the number of\nadapted weight matrices. We denote the i-th triplet of ∆k as Gk,i = {Pk,∗i, λk,i, Qk,i∗} and its\nimportance score as Sk,i. We further denote the parameter sets P = {Pk}n\nk=1, E = {Λk}n\nk=1,\nQ = {Qk}n\nk=1 and training cost as C(P, E, Q). With the regularization (4), the training objective\nis given by L(P, E, Q) = C(P, E, Q) + γ Pn\nk=1 R(Pk, Qk), where γ > 0 is the regularization\ncoefficient. At the t-th step, we first take a stochastic gradient step to update P (t)\nk , Λ(t)\nk\nand Q(t)\nk\nfor\nk = 1, . . . , n. Specifically, for Λ(t)\nk\n˜Λ(t)\nk\n= Λ(t)\nk −η∇ΛkL(P(t), E(t), Q(t)),\n(5)\n2We present the experiments in Appendix G to verify the effectiveness of the regularization.\n4\n",
  "5": "Published as a conference paper at ICLR 2023\nwhere η > 0 is learning rate. Then, given importance score S(t)\nk , the singular values are pruned\nfollowing\nΛ(t+1)\nk\n= T (˜Λ(t)\nk , S(t)\nk ), with T (˜Λ(t)\nk , S(t)\nk )ii =\n\u001a ˜Λ(t)\nk,ii\nS(t)\nk,i is in the top-b(t) of S(t),\n0\notherwise,\n(6)\nwhere S(t) = {S(t)\nk,i}1≤k≤n,1≤i≤r contains the importance score of all triplets. Here b(t) is the budget\nof remaining singular values at the t-th step, which we explain more in Section 3.3. In this way, we\nleave more budget to the incremental matrices of higher priority by pruning the singular values of\nless important ones. In the sequel, we introduce several options to design the importance score.\nMagnitude of singular values is the most straightforward way to quantify the importance of every\ntriplet, i.e., Sk,i = |λk,i|. In this way, only the least significant singular values are discarded. It\nminimizes the deviation from the original matrix and further stabilizes the training. Many existing\nmethods use this criterion to control the rank of matrix (Cai et al., 2010; Koltchinskii et al., 2011;\nToh & Yun, 2010). However, we remark that such a simple metric cannot properly quantify the\ncontribution of parameters to model performance.\nSensitivity-based importance is another option for importance scoring, which quantifies the sensi-\ntivity of parameters to the training loss (Molchanov et al., 2019; Sanh et al., 2020; Liang et al., 2021;\nZhang et al., 2022). The prior work, however, leverages the sensitivity to quantify the importance of\nsingle entries and applies it for unstructured pruning that prunes weights element-wise. When it turns\nto our case, we have to design a new metric as the triplets are discarded group-wise. Every entry’s\nsensitivity ought to be considered and properly combined to quantify the overall contribution of the\ntriplet to model performance. Therefore, we propose a newly-designed importance metric in account\nof both the singular value and vectors in triplet Gk,i:\nSk,i = s(λk,i) + 1\nd1\nd1\nX\nj=1\ns(Pk,ji) + 1\nd2\nd2\nX\nj=1\ns(Qk,ij),\n(7)\nwhere we calculate the mean importance of Pk,∗i and Qk,i∗such that Sk,i does not scale with the\nnumber of parameters in Gk,i. Here s(·) is a specific importance function for single entries. We can\nadopt the sensitivity for s(·), which is defined as the magnitude of the gradient-weight product:\nI(wij) = |wij∇wijL|,\n(8)\nwhere wij is any trainable parameter. (8) essentially approximates the change in loss when a parameter\nis zeroed out. If the removal of a parameter has a large influence, then the model is sensitive to it and\nwe should retain it (Molchanov et al., 2019; Liang et al., 2021; Zhang et al., 2022).\nHowever, Zhang et al. (2022) point out that the sensitivity in (8) is not yet a reliable importance indi-\ncator. Such a score is estimated on the sampled mini batch. The stochastic sampling and complicated\ntraining dynamics incur high variability and large uncertainty for estimating the sensitivity with (8).\nTherefore, Zhang et al. (2022) propose to resolve this issue by sensitivity smoothing and uncertainty\nquantification:\nI\n(t)(wij) =β1I\n(t−1)(wij) + (1 −β1)I(t)(wij)\n(9)\nU\n(t)(wij) =β2U\n(t−1)(wij) + (1 −β2)\n\f\f\fI(t)(wij) −I\n(t)(wij)\n\f\f\f,\n(10)\nwhere 0 < β1, β2 < 1. I\n(t) is the smoothed sensitivity by exponential moving average and U\n(t) is\nthe uncertainty term quantified by the local variation between I(t) and I\n(t). Then they define the\nimportance as the product between I\n(t) and U\n(t), which can be another option for s(·):\ns(t)(wij) = I\n(t)(wij) · U\n(t)(wij).\n(11)\nWe present a detailed ablation study in Section 4.4 to compare the performance of different importance\nmetrics. We find the proposed metric (7) based on the sensitivity variant (11) generally performs best.\nWe summarize the detailed algorithm in Algorithm 1.\n5\n",
  "6": "Published as a conference paper at ICLR 2023\nAlgorithm 1 AdaLoRA\n1: Input: Dataset D; total iterations T; budget schedule {b(t)}T\nt=0; hyperparameters η, γ, β1, β2.\n2: for t = 1, . . . , T do\n3:\nSample a mini-batch from D and compute the gradient ∇L(P, E, Q);\n4:\nCompute the sensitivity I(t) in (8) for every parameter in {P, E, Q};\n5:\nUpdate I\n(t) as (9) and U\n(t) as (10) for every parameter in {P, E, Q};\n6:\nCompute S(t)\nk,i by (7), for k = 1, . . . , n and i = 1, . . . , r ;\n7:\nUpdate P (t+1)\nk\n= P (t)\nk\n−η∇PkL(P, E, Q) and Q(t+1)\nk\n= Q(t)\nk −η∇QkL(P, E, Q);\n8:\nUpdate Λ(t+1)\nk\n= T (Λ(t)\nk −η∇ΛkL(P, E, Q), S(t)\nk ) given the budget b(t).\n9: end for\n10: Output: The fine-tuned parameters {P(T ), E(T ), Q(T )}.\n3.3\nGLOBAL BUDGET SCHEDULER\nAs mentioned in Section 1, adjusting the rank is naturally to control the parameter budget in the\ncontext of low-rank adaptation. Hence we define the budget b(t) as the total rank of all incremental\nmatrices, i.e., the number of total singular values. Recall that the budget allocation is iteratively\nconducted during the fine-tuning. To facilitate the training, we propose a global budget scheduler.\nSpecifically, we start from an initial budget b(0) that is slightly higher than the target budget b(T ) (e.g.,\n1.5 times of b(T )). We set the initial rank of each incremental matrix as r = b(0)/n. We warm up\nthe training for ti steps, and then follow a cubic schedule to decrease the budget b(t) until it reaches\nb(T ). Finally, we fix the resulting budget distribution and fine-tune the model for tf steps. The exact\nequation for the budget schedule is presented in Appendix A. This allows AdaLoRA to explore the\nparameter space first and then focus on the most important weights later.\n4\nEXPERIMENTS\nWe implement AdaLoRA for fine-tuning DeBERTaV3-base (He et al., 2021a) and BART-large\n(Lewis et al., 2019). We evaluate the effectiveness of the proposed algorithm on natural language\nunderstanding (GLUE, Wang et al. (2019)), question answering (SQuADv1, Rajpurkar et al. (2016)\nand SQuADv2, Rajpurkar et al. (2018)), and natural language generation (XSum, Narayan et al.\n(2018) and CNN/DailyMail Hermann et al. (2015)). All the gains have passed significant tests with\np < 0.05.\nImplementation Details. We use PyTorch (Paszke et al., 2019) to implement all the algorithms. Our\nimplementation is based on the publicly available Huggingface Transformers3 (Wolf et al., 2019)\ncode-base. All the experiments are conducted on NVIDIA V100 GPUs.\nLoRA scales ∆x by α/r where α is a constant in r. As a result, the magnitude of output can be\nconsistent given different r. It reduces the efforts of retuning learning rate when varying r. Typically\nα is set as 16 or 32 and never tuned (Hu et al., 2022; Yang & Hu, 2020). Following LoRA, we add\nthe same scaling for (3) and fix α as LoRA. Besides, in Algorithm 1, we prune singular values every\n∆T steps (e.g., ∆T = 100) such that the pruned triplets can still get updated within these intervals\nand possibly reactivated in future iterations.\nBaselines. We compare AdaLoRA with the following methods:\n• Full fine-tuning is the most common approach for adaptation. During fine-tuning, the model is\ninitialized with pre-trained weights and biases, and all model parameters undergo gradient updates.\n• Bitfit (Zaken et al., 2021) is an effective parameter-efficient fine-tuning method. The method only\nfine-tunes bias vectors in the pre-trained model.\n• Adapter tuning (Houlsby et al., 2019; Pfeiffer et al., 2020) inserts two-layer adapters between\ntransformer blocks. We compare with two types of adapter. Houlsby adapter as proposed in Houlsby\net al. (2019) is inserted between the self-attention module and the FFN module followed by a\nsubsequent residual connection. Recently, Pfeiffer et al. (2020) propose a more efficient design with\nadapters only applied after FFN modules and LayerNorm modules (Ba et al., 2016), which we call\n3https://github.com/huggingface/transformers\n6\n",
  "7": "Published as a conference paper at ICLR 2023\nTable 1: Results with DeBERTaV3-base on GLUE development set. The best results on each dataset are shown\nin bold. We report the average correlation for STS-B. Full FT, HAdapter and PAdapter represent full fine-tuning,\nHoulsby adapter, and Pfeiffer adapter respectively. We report mean of 5 runs using different random seeds.\nMethod\n# Params\nMNLI\nSST-2 CoLA\nQQP\nQNLI\nRTE\nMRPC STS-B\nAll\nm/mm\nAcc\nMcc\nAcc/F1\nAcc\nAcc\nAcc\nCorr\nAve.\nFull FT\n184M\n89.90/90.12\n95.63\n69.19\n92.40/89.80\n94.03\n83.75\n89.46\n91.60\n88.09\nBitFit\n0.1M\n89.37/89.91\n94.84\n66.96\n88.41/84.95\n92.24\n78.70\n87.75\n91.35\n86.02\nHAdapter\n1.22M\n90.13/90.17\n95.53\n68.64\n91.91/89.27\n94.11\n84.48\n89.95\n91.48\n88.12\nPAdapter\n1.18M\n90.33/90.39\n95.61\n68.77\n92.04/89.40\n94.29\n85.20\n89.46\n91.54\n88.24\nLoRAr=8\n1.33M\n90.65/90.69\n94.95\n69.82\n91.99/89.38\n93.87\n85.20\n89.95\n91.60\n88.34\nAdaLoRA\n1.27M\n90.76/90.79\n96.10\n71.45\n92.23/89.74\n94.55\n88.09\n90.69\n91.84\n89.31\nHAdapter\n0.61M\n90.12/90.23\n95.30\n67.87\n91.65/88.95\n93.76\n85.56\n89.22\n91.30\n87.93\nPAdapter\n0.60M\n90.15/90.28\n95.53\n69.48\n91.62/88.86\n93.98\n84.12\n89.22\n91.52\n88.04\nHAdapter\n0.31M\n90.10/90.02\n95.41\n67.65\n91.54/88.81\n93.52\n83.39\n89.25\n91.31\n87.60\nPAdapter\n0.30M\n89.89/90.06\n94.72\n69.06\n91.40/88.62\n93.87\n84.48\n89.71\n91.38\n87.90\nLoRAr=2\n0.33M\n90.30/90.38\n94.95\n68.71\n91.61/88.91\n94.03\n85.56\n89.71\n91.68\n88.15\nAdaLoRA\n0.32M\n90.66/90.70\n95.80\n70.04\n91.78/89.16\n94.49\n87.36\n90.44\n91.63\n88.86\nPfeiffer adapter. The number of trainable parameters is determined by the number of layers, the\nhidden dimension of adapters and the dimension of their inputs.\n• LoRA (Hu et al., 2022) is a state-of-the-art method for parameter-efficient fine-tuning. The method\nparameterizes incremental updates by two small matrices and only fine-tune them. The number of\ntrainable parameter is controlled by the rank r and the number of adapted weight matrices n. Hu et al.\n(2022) apply LoRA to query and value projections only. In empirical, we find that applying LoRA\nto all weight matrices, i.e., Wq, Wk, Wv, Wf1 and Wf2, can further improve its performance (Please\nsee Appendix F). Hence, we compare with this generalized LoRA to maximize its performance. We\nuse publicly available implementation 4 to run all the baselines. Please refer to Hu et al. (2022) and\nreference therein for details.\n4.1\nNATURAL LANGUAGE UNDERSTANDING\nModels and Datasets. We evaluate the fine-tuning performance of DeBERTaV3-base (He et al.,\n2021a) using the proposed algorithm. We conduct experiments on the General Language Understand-\ning Evaluation (GLUE, Wang et al. 2019) benchmark. The benchmark includes two single-sentence\nclassification tasks, three similarity and paraphrase tasks and four natural language inference tasks.\nDataset details are summarized in Appendix B.\nImplementation Details. DeBERTaV3-base consists of 183 millions parameters. We compare\nAdaLoRA with the baselines under different budget levels, for example, given the total trainable\nparameters as 0.3/0.6/1.2 million. In order to match the parameter budget, we select the hidden\ndimensions of adapters from {8, 16, 32, 64}, set the rank r of LoRA as {2, 4, 8}, and choose the\nfinal budget b(T ) of AdaLoRA from {144, 288, 576}. Then we set b(0) as 1.5 times of b(T ) for\nAdaLoRA and select the regularization coefficient γ from {0.1, 0.3, 0.5}. We set the exponential\nmoving average parameters β1 and β2 as their default value 0.85. We select the learning rate from\n{5 × 10−5, 8 × 10−5, 1 × 10−4, 2 × 10−4}. More details are presented in Appendix C.\nMain results. We compare AdaLoRA with the baseline methods under different budget settings.\nTable 1 shows experimental results on the GLUE development set. We see that AdaLoRA achieves\nbetter or on par performance compared with existing approaches on all datasets under all budget\nlevels. For example, when the parameter budget is 0.3M, AdaLoRA achieves 87.36% accuracy on\nRTE, which is 1.8% higher than the best-performing baseline. Besides, AdaLoRA with extreme\nlow budget can often perform better than the baselines with higher budget. For example, AdaLoRA\nachieve 70.04% Mcc. score on CoLA with 0.3M fine-tuning parameters, which is higher than all\nbaseline methods with lager budget (e.g., 0.6M and 1.2M).\n4.2\nQUESTION ANSWERING\nModels and Datasets. We evaluate performance of the proposed algorithm on two question answering\n(QA) datasets: SQuAD v1.1 (Rajpurkar et al., 2016) and SQuADv2.0 (Rajpurkar et al., 2018), where\n4https://github.com/microsoft/LoRA\n7\n",
  "8": "Published as a conference paper at ICLR 2023\nTable 2: Results with DeBERTaV3-base on SQuAD v1.1 and SQuADv2.0. Here # Params is the\nnumber of trainable parameters relative to that in full fine-tuning. We report EM/F1. The best results\nin each setting are shown in bold.\nSQuADv1.1\nSQuADv2.0\nFull FT\n86.0 / 92.7\n85.4 / 88.4\n# Params\n0.08%\n0.16%\n0.32%\n0.65%\n0.08%\n0.16%\n0.32%\n0.65%\nHAdapter\n84.4/91.5 85.3/92.1 86.1/92.7 86.7/92.9 83.4/86.6 84.3/87.3 84.9/87.9 85.4/88.3\nPAdapter\n84.4/91.7 85.9/92.5 86.2/92.8 86.6/93.0 84.2/87.2 84.5/87.6 84.9/87.8 84.5/87.5\nLoRA\n86.4/92.8 86.6/92.9 86.7/93.1 86.7/93.1 84.7/87.5 83.6/86.7 84.5/87.4 85.0/88.0\nAdaLoRA 87.2/93.4 87.5/93.6 87.5/93.7 87.6/93.7 85.6/88.7 85.7/88.8 85.5/88.6 86.0/88.9\nwe use AdaLoRA to fine-tune DeBERTaV3-base. These tasks are treated as a sequence labeling\nproblem, where we predict the probability of each token being the start and end of the answer span.\nDataset details can be found in Appendix D.\nImplementation Details. We compare AdaLoRA with the baseline methods under different parameter\nbudgets. That is we have the number of trainable parameters as 0.08%/0.16%/0.32%/0.65% of\ntotal pre-trained parameters. To match the budget requirements, we select the hidden dimensions\nof adapters from {4, 8, 16, 32, 64}, set the rank r of LoRA as {1, 2, 4, 8} and choose the final total\nrank b(T ) of AdaLoRA from {72, 144, 288, 576}. We set the batch size as 16. We use AdamW\n(Loshchilov & Hutter, 2019) as the optimizer and we set the learning rate as 1 × 10−3 for AdaLoRA.\nPlease refer to Appendix D for more details.\nMain Results. Table 2 summarizes experimental results when we fine-tune DeBERTaV3-base under\n4 different budget settings: 0.08%, 0.16%, 0.32% and 0.65% of total pre-trained parameters. From the\nresult, we see that AdaLoRA consistently outperforms existing approaches under all the budget levels\nin term of two evaluation metrics: exact match (EM) and F1. Notice that the performance of Houlsby\nadapter and Pfeiffer adapter are notably decreased when we reduce the parameter budget. In contrast,\nour method shows the consistent performance under different budget levels. For example, AdaLoRA\nachieves 88.7% F1 on SQuADv2.0 with the smallest budget 0.08%. It is close to its performance\nunder the high budget and it is also 1.2% higher than the best-performing baseline.\n4.3\nNATURAL LANGUAGE GENERATION\nTable 3: Results with BART-large on XSum and CNN/DailyMail. Here # Params is the number of trainable\nparameters relative to that in full fine-tuning. We report R-1/2/L. The best results are shown in bold.\n# Params\nMethod\nXSum\nCNN/DailyMail\n100%\nFull FT\n45.49 / 22.33 / 37.26\n44.16 / 21.28 / 40.90\n2.20%\nLoRA\n43.95 / 20.72 / 35.68\n45.03 / 21.84 / 42.15\nAdaLoRA\n44.72 / 21.46 / 36.46\n45.00 / 21.89 / 42.16\n1.10%\nLoRA\n43.40 / 20.20 / 35.20\n44.72 / 21.58 / 41.84\nAdaLoRA\n44.35 / 21.13 / 36.13\n44.96 / 21.77 / 42.09\n0.26%\nLoRA\n43.18 / 19.89 / 34.92\n43.95 / 20.91 / 40.98\nAdaLoRA\n43.55 / 20.17 / 35.20\n44.39 / 21.28 / 41.50\n0.13%\nLoRA\n42.81 / 19.68 / 34.73\n43.68 / 20.63 / 40.71\nAdaLoRA\n43.29 / 19.95 / 35.04\n43.94 / 20.83 / 40.96\nModels and Datasets. To provide a comparison with the state-of-the-art in natural language gener-\nation (NLG) tasks, we apply AdaLoRA to fine-tune a BART-large model (Lewis et al., 2019). We\nevaluate model performance on two datasets: XSum (Narayan et al., 2018) and CNN/DailyMail\n(Hermann et al., 2015).\nImplementation Details. Similarly as DeBERTav3-base, we apply low-rank/SVD-based adaptation\nto every weight matrix of both encoder and decoder layers. We report ROUGE 1/2/L scores (R-1/2/L,\nLin (2004)). We set the training epochs as 15. For XSum, we set the beam length as 8 and batch size\n8\n",
  "9": "Published as a conference paper at ICLR 2023\nas 64. For CNN/DailyMail, we set the beam length as 4 and batch size as 32. Please see Appendix E\nfor the detailed configuration.\nMain Results. Experimental results are summarized in Table 3, where we compare the fine-tuning\nperformance under four budget levels: the number of trainable parameters is 0.13%, 0.26%, 1.10% and\n2.20% of total pre-trained parameters. We see that AdaLoRA achieves better or on par performance\ncompared with the baseline on both datasets (XSum and CNN/DailyMail) under all the budget levels.\nFor example, AdaLoRA achieves 21.13 R-2 score when budget level is 1.10%, compared with 19.89\nfor LoRA.\n4.4\nANALYSIS\nDifferent budget levels. Figure 2 illustrates experimental results of fine-tuning DeBERTaV3-base\nunder different budget levels. We see that on all the three datasets (MNLI-m, SQuADv2.0 and XSum),\nAdaLoRA achieves consistent performance improvement under all the budget levels compared with\nthe baseline. The performance gain is more significant when increasing the budget for the XSum\ntask, suggesting a high budget can help NLG tasks. Note that on the MNLI and SQuADv2.0 datasets,\nthe performance of AdaLoRA under low budget levels (≤1%) can match the results of high budget\nsettings. For example, AdaLoRA achieves 88.78% F1 on SQuADv2.0 when the budget is 0.16%. It\nis close to the performance (88.89% F1) of the highest budget (4.65%) with a more significant gain\nover the baseline.\n0.08 0.16 0.32 0.65 0.96 1.30 1.95 2.88\n# Params (%)\n90.2\n90.3\n90.4\n90.5\n90.6\n90.7\nAcc (MNLI-m)\nLoRA \nAdaLoRA\n(a) MNLI\n0.08\n0.16\n0.32\n0.65\n1.30\n2.70\n4.65\n# Params (%)\n87.0\n87.5\n88.0\n88.5\n89.0\nF1\n(b) SQuADv2.0\n0.13\n0.26\n1.1\n2.2\n4.5\n7.9\n12.5\n# Params (%)\n20.0\n20.5\n21.0\n21.5\nROUGE-2\n(c) XSum\nFigure 2: Fine-tuning performance under different budget levels. We compare AdaLoRA with the\ngeneralized LoRA that applies to every weight matrix.\nComparison to low-rank parameterization. As mentioned in Section 3.1, one can alternatively\nprune LoRA doublet-wise to conduct the rank allocation. In this case, the doublets are zeroed\nout entirely, raising the barrier to reactivate them. It can cause training instability and hurt the\ngeneralization when some crucial doublets are pruned by mistake. In Table 4, we compare AdaLoRA\nwith pruning LoRA on three datasets (SST-2, RTE, and CoLA) to illustrate this point. We apply the\nsame importance score, budget scheduler and training setups as Section 4.1 for pruning LoRA. We\ncan see that AdaLoRA outperforms pruning LoRA on all the datasets under all the budget levels.\nTable 4: We present two ablation studies in this table: (i) Comparison between AdaLoRA and\nstructured pruning on LoRA. (ii) Comparison of different importance metrics for AdaLoRA.\nSST-2\nRTE\nCoLA\n# Params\n0.08%\n0.16%\n0.65%\n0.08%\n0.16%\n0.65%\n0.08%\n0.16%\n0.65%\nPrune LoRA\n94.84\n94.50\n94.95\n86.28\n86.15\n87.00\n66.71\n69.29\n69.57\nAdaLoRA\n95.52\n95.80\n96.10\n87.36\n87.73\n88.09\n70.21\n70.04\n71.45\ns(·) = I(·)\n94.61\n95.30\n95.64\n87.36\n87.71\n88.10\n66.71\n68.83\n70.19\nSi = |λi|\n95.41\n95.41\n95.87\n87.00\n86.28\n88.00\n67.67\n68.44\n70.38\nVariants of the importance score. Recall that in AdaLoRA, the importance score is defined by the\nsensitivity and uncertainty of every entry in the triplet (7). In Table 4, we examine two variants of the\nimportance score: (i) changing s(·) in (7) to sensitivity-only; (ii) directly defining Si as |λi|. From\nthe results, we can see that the proposed importance score generally performs best. The other two\nvariants can degenerate the model performance up to 0.9%.\nThe role of two components. We remark that both two components of our method - SVD adaptation\nand adaptive budget allocation, play vital roles for the performance gain. To demonstrate it, we\n9\n",
  "10": "Published as a conference paper at ICLR 2023\ncompare AdaLoRA with the following variants: (i) SVD-LoRA: fine-tuning only with the proposed\nSVD-based adaptation in (3) and (4); (ii) LoRAregu: LoRA with orthogonal regularization (4) on\nA and B; (iii) AdaLoRAγ = 0: AdaLoRA without orthogonal regularization (4). Table 5 present\nthe results when fine-tuning DeBERTaVe-base on SST-2 and MNLI. We can see that fine-tuning\nonly with SVD adaptation shows an improvement over LoRA but cannot match the performance of\nAdaLoRA. Meanwhile, without SVD orthogonal regularization, the performance of AdaLoRA can\ndegenerate. These results validate that both components contribute to the model performance.\nTable 5: We present ablation studies about SVD-based adaptation, orthogonal regularization, and\nbudget allocation in this table. For MNLI, we report the average score of m/mm acc.\nSST-2\nMNLI\n# Params\n0.08%\n0.16%\n0.32%\n0.65%\n0.08%\n0.16%\n0.32%\n0.65%\nLoRA\n94.38\n94.95\n-\n94.95\n90.19\n90.34\n-\n90.57\nLoRAregu\n-\n94.61\n94.72\n94.61\n-\n90.30\n90.40\n90.66\nSVD-LoRA\n95.33\n95.18\n95.07\n95.53\n90.28\n90.25\n90.52\n90.62\nAdaLoRAγ = 0\n95.41\n95.10\n95.30\n95.10\n90.37\n90.34\n90.56\n90.43\nAdaLoRA\n95.64\n95.80\n96.10\n96.10\n90.65\n90.68\n90.66\n90.77\nThe resulting budget distribution. Figure 3 shows the resulting rank of each incremental matrix of\nDeBERTaV3-base fine-tuned with AdaLoRA. We find that AdaLoRA always prefers to allocating\nmore budget to FFNs and top layers. Such behavior aligns with our empirical conclusions presented\nin Figure 1 that weight matrices of FFN moduels and top layers are more important for model\nperformance. Hence, it validates that our proposed importance metric can guide AdaLoRA to focus\non crucial modules. Meanwhile, the rank distribution generated by AdaLoRA is consistent across\ndifferent budget levels, tasks and models. It means the number of remaining parameters is linearly\nscaled with b(T ) and hence we can tune b(T ) to control the remaining parameters.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nLayer\nWf2\nWf1\nWo\nWv\nWk\nWq\n4\n1\n5\n2\n3\n5\n5\n6\n10\n5\n5\n0\n6\n9\n9\n9\n12\n11\n12\n12\n12\n12\n12\n2\n7\n3\n5\n8\n8\n10\n12\n12\n12\n12\n12\n5\n6\n6\n10\n6\n10\n11\n11\n11\n12\n12\n11\n9\n5\n4\n5\n5\n10\n9\n9\n11\n12\n12\n12\n12\n3\n2\n5\n4\n7\n7\n7\n10\n11\n11\n10\n3\n0\n2\n4\n6\n8\n10\n12\nThe ﬁnal rank\nFigure 3: The resulting rank of each incremental matrix when fine-tuning DeBERTaV3-base on\nMNLI with AdaLoRA. Here the x-axis is the layer index and the y-axis represents different types of\nadapted weight matrices.\n5\nCONCLUSION\nWe propose a parameter-efficient fine-tuning method – AdaLoRA that adaptively allocates the\nparameter budget according to importance scoring. In AdaLoRA, we parameterize the incremental\nupdates of weight matrices in the form of singular value decomposition. Then, we dynamically\nallocate the parameter budget among incremental matrices by manipulating the singular values based\non a new importance metric. Such an a pproach effectively improves the model performance and\nparameter efficiency. We conduct extensive experiments on natural language processing, question\nanswering and natural language generation tasks. Results show that AdaLoRA outperforms existing\napproaches.\n10\n",
  "11": "Published as a conference paper at ICLR 2023\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle,\nMarc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances\nin Neural Information Processing Systems 33: Annual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nJian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for\nmatrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/N19-1423.\nDemi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning.\narXiv preprint arXiv:2012.07463, 2020.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\na unified view of parameter-efficient transfer learning. In International Conference on Learning\nRepresentations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style\npre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543,\n2021a.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention. In International Conference on Learning Representations, 2021b.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural\ninformation processing systems, 28, 2015.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\nnlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International\nConference on Learning Representations, 2022. URL https://openreview.net/forum?\nid=nZeVKeeFYf9.\nVladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization and\noptimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39(5):2302–2329,\n2011.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https:\n//aclanthology.org/2021.emnlp-main.243.\n11\n",
  "12": "Published as a conference paper at ICLR 2023\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461,\n2019.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.\nIn Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers),\nVirtual Event, August 1-6, 2021, pp. 4582–4597. Association for Computational Linguistics,\n2021. doi: 10.18653/v1/2021.acl-long.353. URL https://doi.org/10.18653/v1/2021.\nacl-long.353.\nChen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao,\nand Weizhu Chen. Super tickets in pre-trained language models: From model compression to\nimproving generalization. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pp. 6524–6538, Online, 2021. Association for Computational\nLinguistics. doi: 10.18653/v1/2021.acl-long.510.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74–81, 2004.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net, 2019.\nPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for\nneural network pruning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019, pp. 11264–11272. Computer Vision Foundation /\nIEEE, 2019. doi: 10.1109/CVPR.2019.01152.\nShashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the sum-\nmary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint\narXiv:1808.08745, 2018.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep\nlearning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-\nBuc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\nfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\n2020.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained\nmodels for natural language processing: A survey. Science China Technological Sciences, 63(10):\n1872–1897, 2020.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\n12\n",
  "13": "Published as a conference paper at ICLR 2023\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, pp. 2383–2392, Austin, Texas, 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/D16-1264.\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\nfor SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pp. 784–789, Melbourne, Australia, 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/P18-2124.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\nresidual adapters. Advances in neural information processing systems, 30, 2017.\nVictor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by\nfine-tuning. 2020.\nKim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear norm\nregularized linear least squares problems. Pacific Journal of optimization, 6(615-640):15, 2010.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net, 2019.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv preprint, abs/1910.03771, 2019.\nGreg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint\narXiv:2011.14522, 2020.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.\nQingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, and\nTuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight\nimportance. In International Conference on Machine Learning, pp. 26809–26823. PMLR, 2022.\n13\n",
  "14": "Published as a conference paper at ICLR 2023\nA\nGLOBAL BUDGET SCHEDULE\nAs mentioned in Section 3.3, we propose a global budget scheduler to gradually decrease the budget\nb(t) following a cubic schedule. The detailed equation is given as follows:\nb(t) =\n\n\n\n\n\nb(0)\n0 ≤t < ti\nb(T ) +\n\u0000b(0) −b(T )\u0001 \u0010\n1 −t−ti−tf\nT −ti−tf\n\u00113\nti ≤t < T −tf\nb(T )\no.w.\n.\n(12)\nB\nGLUE DATASET STATISTICS\nWe present the dataset statistics of GLUE (Wang et al., 2019) in the following table.\nTable 6: Summary of the GLUE benchmark.\nCorpus\nTask\n#Train\n#Dev\n#Test\n#Label\nMetrics\nSingle-Sentence Classification (GLUE)\nCoLA\nAcceptability\n8.5k\n1k\n1k\n2\nMatthews corr\nSST\nSentiment\n67k\n872\n1.8k\n2\nAccuracy\nPairwise Text Classification (GLUE)\nMNLI\nNLI\n393k\n20k\n20k\n3\nAccuracy\nRTE\nNLI\n2.5k\n276\n3k\n2\nAccuracy\nQQP\nParaphrase\n364k\n40k\n391k\n2\nAccuracy/F1\nMRPC\nParaphrase\n3.7k\n408\n1.7k\n2\nAccuracy/F1\nQNLI\nQA/NLI\n108k\n5.7k\n5.7k\n2\nAccuracy\nText Similarity (GLUE)\nSTS-B\nSimilarity\n7k\n1.5k\n1.4k\n1\nPearson/Spearman corr\nC\nNATURAL LANGUAGE UNDERSTANDING\nC.1\nBUDGET CONFIGURATION\nFor each budget level, we tune the final budget b(T ) for AdaLoRA, the rank r for LoRA, the hidden\ndimension d for two adapters to match the budget requirements.\nTable 7: Detailed budget setup for GLUE benchmark.\n# Params\nHoulsby Adapter (d)\nPfeiffer Adapter (d)\nLoRA (r)\nAdaLoRA (b(T ))\n1.2M\n32\n64\n8\n576\n0.6M\n16\n32\n4\n288\n0.3M\n8\n16\n2\n144\nAlternatively, we can also set the final average rank ¯r(T ) = b(T )/n for AdaLoRA to control the\nbudget, which is set as 2, 4, and 8 given the final budget as 144, 288, and 576 respectively. Then we\nselect the initial rank r from {4, 6, 12} for the final average rank {2, 4, 8} respectively.\nC.2\nTRAINING DETAILS\nWe tune the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 ×\n10−4, 1 × 10−3} and pick the best learning rate for every method. For each dataset, the batch size is\nset as identical for every method.\n14\n",
  "15": "Published as a conference paper at ICLR 2023\nTable 8: Hyper-parameter setup of AdaLoRA for GLUE benchmark.\nDataset\nlearning rate\nbatch size\n# epochs\nγ\nti\n∆T\ntf\nMNLI\n5 × 10−4\n32\n7\n0.1\n8000\n100\n50000\nRTE\n1.2 × 10−3\n32\n50\n0.3\n600\n1\n1800\nQNLI\n1.2 × 10−3\n32\n5\n0.1\n2000\n100\n8000\nMRPC\n1 × 10−3\n32\n30\n0.1\n600\n1\n1800\nQQP\n5 × 10−4\n32\n5\n0.1\n8000\n100\n25000\nSST-2\n8 × 10−4\n32\n24\n0.1\n6000\n100\n22000\nCoLA\n5 × 10−4\n32\n25\n0.5\n800\n10\n3500\nSTS-B\n2.2 × 10−3\n32\n25\n0.1\n800\n10\n2000\nD\nQUESTION ANSWERING\nD.1\nBUDGET CONFIGURATION\nGiven the budget, we control the trainable parameters for each method as the following table.\nTable 9: Detailed budget setup for question answering.\n# Params\nHoulsby Adapter\nPfeiffer Adapter\nLoRA\nAdaLoRA\nd\nd\nr\nb(T )/¯r(T )/r\n0.65%\n32\n64\n8\n576 / 8 / 12\n0.32%\n16\n32\n4\n288 / 4 / 6\n0.16%\n8\n16\n2\n144 / 2 / 4\n0.08%\n4\n8\n1\n72 / 1 / 2\nD.2\nTRAINING DETAILS\nWe set the batch size as 16. We select the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 ×\n10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best-performing learning rate for every\nmethod. The configuration of AdaLoRA is listed in the following table.\nTable 10: Hyper-parameter setup of AdaLoRA for question answering tasks.\nDataset\nlearning rate\nbatch size\n# epochs\nγ\nti\n∆T\ntf\nSQuADv1.1\n1 × 10−3\n16\n10\n0.1\n5000\n100\n25000\nSQuADv2.0\n1 × 10−3\n16\n12\n0.1\n5000\n100\n50000\nD.3\nDATASET\nThe statistics of question answering datasets are summarized in Table 11.\nTable 11: Statistics of the SQuAD dataset.\n# Train\n# Validation\nSQuAD v1.1\n87,599\n10,570\nSQuAD v2.0\n130,319\n11,873\nE\nNATURAL LANGUAGE GENERATION\nE.1\nBUDGET CONFIGURATION\nGiven the budget, we control the trainable parameters for each method as the following table.\n15\n",
  "16": "Published as a conference paper at ICLR 2023\nTable 12: Detailed budget setup for summarization tasks.\n# Params\nHoulsby Adapter\nPfeiffer Adapter\nLoRA\nAdaLoRA\nd\nd\nr\nb(T )/¯r(T )/r\n0.65%\n32\n64\n8\n576 / 8 / 12\n0.32%\n16\n32\n4\n288 / 4 / 6\n0.16%\n8\n16\n2\n144 / 2 / 4\n0.08%\n4\n8\n1\n72 / 1 / 2\nE.2\nTRAINING DETAILS\nWe set the batch size as 16. We select the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 ×\n10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best-performing learning rate for every\nmethod. The configuration of AdaLoRA is listed in the following table.\nTable 13: Hyper-parameter setup of AdaLoRA for summarization tasks.\nDataset\nlearning rate\nbatch size\n# epochs\nγ\nti\n∆T\ntf\nXSum\n5 × 10−4\n64\n25\n0.1\n6000\n100\n50000\nCNN/DailyMail\n5 × 10−4\n32\n15\n0.1\n5000\n100\n85000\nF\nABLATION STUDY FOR LORA\nAs mentioned in Section 4, we find that the performance of LoRA can be further improved when\napplying it to every weight matrix, compared to fine-tuning Wq and Wv only (Hu et al., 2022). This\nobservation aligns with the empirical results of He et al. (2022). In Table 14, we follow the same\ntraining configuration as Section 4.1 and present an ablation study to illustrate this point.\nTable 14: We compare the fine-tuning performance when apply LoRA to every weight matrix or\nWq, Wv only. The parameter budget is fixed as 0.3M. We report accuracy for QQP and MRPC,\naccuracy(m) for MNLI, and average correlation for STS-B.\nMNLI\nQQP\nCoLA\nRTE\nQNLI\nSST-2\nMRPC\nSTS-B\nLoRA (Wq, Wk)\n89.80\n90.48\n67.04\n83.75\n93.69\n94.84\n90.20\n91.05\nLoRA (all)\n90.30\n91.61\n68.71\n85.56\n94.31\n94.95\n90.44\n91.68\nG\nORTHOGONAL REGULARIZATION\nTo verify the effectiveness of (4), we plot ∥P ⊤P −I∥2\nF and ∥QQ⊤−I∥2\nF to show whether P and Q\nare regularized to be orthogonal. We fine-tune a DeBERTaV3-base model on SST-2 with AdaLoRA\nand follow the same training configuration as Section 4.1. We set γ as 0.1 and plot the two terms\nalong the training horizon. From Figure 4, we can see that two regularization terms can be optimized\nto a very small value (e.g., 0.001) at the beginning of training. Therefore, both P and Q can be\nenforced to be orthogonal quickly during the initial warm-up of AdaLoRA. It ensures that the triplets\nare not dependent with each other.\nH\nCOMPARISON OF TRAINING COST\nWe compare the training cost between AdaLoRA and LoRA in the following table. We use two\nmethods to fine-tune DeBERTaV3-base on a single NVIDIA V100 GPU. We do training only and set\nhyperparameters, e.g., batch size and training epochs, the same as in Section 4.\nTable 15 shows that AdaLoRA incurs 11% additional training time on MNLI and 16% on SQuADv2\nunder different budgets. The memory footprint of two methods are quite close. Such results\ndemonstrate that AdaLoRA does not incur significant training overheads. The reason behind is that\n16\n",
  "17": "Published as a conference paper at ICLR 2023\n0\n20000\n40000\nIterations\n10−4\n10−3\n10−2\n10−1\n100\n||P⊤P−I||2\nF\n(a) P of Wo at the first layer.\n0\n20000\n40000\nIterations\n10−4\n10−3\n10−2\n10−1\n100\n||QQ⊤−I||2\nF\n(b) Q of Wo at the first layer.\n0\n20000\n40000\nIterations\n10−4\n10−3\n10−2\n10−1\n100\n||P⊤P−I||2\nF\n(c) P of Wf2 at the first layer.\n0\n20000\n40000\nIterations\n10−4\n10−3\n10−2\n10−1\n||QQ⊤−I||2\nF\n(d) Q of Wf2 at the first layer\nFigure 4: We plot the ∥P ⊤P −I∥2\nF and ∥QQ⊤−I∥2\nF when fine-tuning DeBERTaV3-base on SST-2.\nTable 15: Comparison of practical training cost between AdaLoRA and LoRA.\nDataset\n# Param\nMethod\nGPU Mem\nTime/epoch\nMNLI\n0.08%\nLoRA\n11.094 GB\n105 min\nAdaLoRA\n11.104 GB\n116 min\n0.16%\nLoRA\n11.098 GB\n105 min\nAdaLoRA\n11.110 GB\n117 min\n0.65%\nLoRA\n11.128 GB\n105 min\nAdaLoRA\n11.188 GB\n117 min\nSST-2\n0.08%\nLoRA\n13.138 GB\n60 min\nAdaLoRA\n13.148 GB\n71 min\n0.16%\nLoRA\n13.142 GB\n61 min\nAdaLoRA\n13.164 GB\n71 min\n0.65%\nLoRA\n13.170 GB\n61 min\nAdaLoRA\n13.226 GB\n71 min\nwe only evaluate the importance score for small incremental matrices PΛQ. Their total number of\nparameters is usually less than 1% of pre-trained weights. Therefore, it does not lead to significant\ncomputational cost to update the importance scores of these well-structured small matrices, compared\nto forward-backward pass of full model.\n17\n"
}