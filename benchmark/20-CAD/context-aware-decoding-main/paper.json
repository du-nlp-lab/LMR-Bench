{
  "1": "Trusting Your Evidence:\nHallucinate Less with Context-aware Decoding\nWeijia Shi 1 ∗\nXiaochuang Han 1 ∗\nMike Lewis 2\nYulia Tsvetkov 1\nLuke Zettlemoyer 1\nScott Yih 2\n1 University of Washington, Seattle, WA, 2 Meta AI\n{swj0419, xhan77}@cs.washington.edu\nAbstract\nLanguage models (LMs) often struggle to pay\nenough attention to the input context, and gen-\nerate texts that are unfaithful or contain hallu-\ncinations. To mitigate this issue, we present\ncontext-aware decoding (CAD), which follows\na contrastive output distribution that amplifies\nthe difference between the output probabilities\nwhen a model is used with and without con-\ntext. Our experiments show that CAD, without\nadditional training, significantly improves the\nfaithfulness of different LM families, including\nOPT, GPT, LLaMA and FLAN-T5 for summa-\nrization tasks (e.g., 14.3% gain for LLaMA in\nfactuality metrics). Furthermore, CAD is par-\nticularly effective in overriding a model’s prior\nknowledge when it contradicts the provided\ncontext, leading to substantial improvements in\ntasks where resolving the knowledge conflict is\nessential.\n1\nIntroduction\nLanguage models (LMs) are remarkably effective\nin generating coherent and fluent continuations of\na prompt or document prefix. During generation,\nthey mostly rely on two sources of knowledge: (1)\nprior knowledge, which is learned during pretrain-\ning and stored implicitly within the model parame-\nters; (2) context knowledge, which is passed as in-\nputs in the prefix context (Chan et al., 2022). How-\never, it remains an open question how a pretrained\nLM, particularly a vanilla LM without task-specific\nfinetuning, balances these two knowledge sources\nduring generation.\nPrevious research shows that LMs can fail to pay\nenough attention to new information introduced\nin the context knowledge. This can lead to hallu-\ncination in summarization (Maynez et al., 2020;\nPagnoni et al., 2021), where the generated sum-\nmaries include facts not present in the input doc-\nument. Insufficient attention to context is espe-\ncially problematic when the context knowledge\n∗Equal contribution. Order randomly determined.\nFigure 1: An illustration of context-aware decoding.\ncontradicts with the prior knowledge (Longpre\net al., 2021; Zhou et al., 2023). For instance, when\nLLaMA (Touvron et al., 2023) is presented with a\nlatest document “Argentina won the FIFA World\nCups in 1978,1986 and 2022 ...” in its context (Fig-\nure 1), it still predicts “Two” in response to the\nquestion “How many World Cups have Argentina\nwon?”, due in part to the outdated training data.\nIn this work, we present a simple context-aware\ndecoding (CAD) method to encourage the LM to\nattend to its context during generation. As shown\nin Figure 1, CAD samples from a new output dis-\ntribution, which amplifies the difference between\noutput probabilities with and without the context\ndocument. This provides a new form of contrastive\ndecoding (Li et al., 2022), which effectively down-\nweights the prior knowledge when more relevant\ncontextual information is provided. CAD can be\nused with off-the-shelf pretrained language models\nwithout any additional training.\nExperimental results from summarization tasks\nshow that context-aware decoding significantly\nenhances the generation faithfulness of various\nvanilla LMs including OPT (Zhang et al., 2022),\nGPT-Neo (Black et al., 2021), LLaMA (Touvron\net al., 2023) and instruction-finetuned LMs such as\nFLAN (Chung et al., 2022). For instance, when ap-\nplied to LLaMA-30B in CNN-DM, CAD leads to\nsubstantial improvement in both ROUGE-L (21%)\narXiv:2305.14739v1  [cs.CL]  24 May 2023\n",
  "2": "and summary factuality evaluation metrics (14.3%).\nMore notably, CAD is especially beneficial for\nknowledge conflicting tasks, where the context\ncontains information contradictory to the model’s\nprior knowledge. CAD brings a 2.9x improve-\nment to LLaMA-30B on a knowledge conflicts QA\ndataset (Longpre et al., 2021). Furthermore, we\nobserve that this gain brought by CAD increases\nas the model size grows in knowledge conflicts\ntasks. These results demonstrate the potential of\nCAD in mitigating hallucinations in text generation\nand overriding prior knowledge with reliable and\ntrusted information.\n2\nMethod\n2.1\nBackground\nGiven a language model θ, an input query x, and\na context c that contains some external knowledge\nunfamiliar or in conflict to the model’s prior knowl-\nedge, we ask our model θ to generate a response\ny given the the query and context. The response\ncan be directly sampled (autoregressively) from the\nprobability distribution conditioned on query x and\ncontext c:\nyt ∼pθ(yt ∣c, x, y<t)\n∝exp logitθ(yt ∣c, x, y<t)\nHowever, in cases where the context c contains\nknowledge that is out-of-distribution with respect\nto θ, we hypothesize that the model can struggle\nto effectively attend to c and overly rely on the\nprior knowledge encoded in θ. For instance, as\nillustrated in Figure 1, when the context c states\n“Argentina won the FIFA World Cups in 1978, 1986\nand 2022 ...”, it contradicts the LM’s outdated prior\nknowledge that Argentina has won the World Cup\ntwice. The language model may still incorrectly\npredict “Two” even when presented with the con-\ntext c and the query x.\n2.2\nContext-aware Decoding\nTo mitigate such issues, we factor out the prior\nknowledge from the model’s original output dis-\ntribution contrastively. Here, we model the prior\nknowledge as pθ(yt\n∣x, y<t) and adjust the\nmodel’s original output probability distribution us-\ning the pointwise mutual information (PMI) be-\ntween the context c and the generation yt, condi-\ntioned on x, y<t. Formally, we have:\nyt ∼˜pθ(yt ∣c, x, y<t)\n∝pθ(yt ∣c, x, y<t) (pθ(yt ∣c, x, y<t)\npθ(yt ∣x, y<t) )\nα\nwhere the output probability is a product-of-experts\nof the original output probability and PMI weighted\nby α. Essentially, outputs that become much more\nlikely when the context is included are preferred\n(Figure 1).\nThis expression is not a valid probability distribu-\ntion and needs to be normalized across all possible\nvalues of yt. By rearranging the terms, we obtain\nthe final form:\nyt ∼softmax[(1 + α) logitθ(yt ∣c, x, y<t)\n−α logitθ(yt ∣x, y<t)]\nLarger α means more weight on our adjustment\n(α = 0 reduces to regular decoding).1 We refer\nto this simple method as context-aware decoding.\nFrom the adjusted output distribution ˜p, we can\napply various sampling strategies, such as nucleus\nsampling (Holtzman et al., 2019).\nEssentially, context-aware decoding is just a con-\ntrastive ensemble between the logits of pθ(yt ∣\nc, x, y<t) and pθ(yt ∣x, y<t).\nA similar con-\ntrastive objective is universal in image genera-\ntion, where classifier-free diffusion models (Ho\nand Salimans, 2022) predict diffusion noise with\n(1+α)ϵθ(x, c)−αϵθ(x), with c being a control to\nthe image. In text generation, Malkin et al. (2021)\npropose coherence boosting with the same intu-\nition, with a focus on contrasting the full input and\na short premise-free input, promoting coherence\nw.r.t. the long context. Instead of using a single\nmodel θ in this work, different models can also\nbe used in the distribution adjustments to demote\nunwanted model behaviors or distill expert model’s\ncapability (Liu et al., 2021; Li et al., 2022).\n3\nExperimental Setup\nWe perform evaluation on tasks that require LMs to\nread and reason over contexts and produce outputs\nthat are faithful to the contexts. Following prior\nwork (Zhang et al., 2023; Zhou et al., 2023), we\nevaluate the models using prompting.\n1If we identify an external knowledge c conditionally in-\ndependent to the generation, pθ(yt ∣c, x, y<t) = pθ(yt ∣\nx, y<t), even a non-zero α would not have an impact to the\noriginal output distribution.\n",
  "3": "3.1\nDatasets and Metrics\nSummarization\nWe conduct summarization ex-\nperiments on two news datasets: CNN-DM (See\net al., 2017) and XSUM (Narayan et al., 2018). We\nuse ROUGE-L (Lin, 2004) to evaluate summariza-\ntion quality. To measure the factual consistency\nof summaries, we adopt BERT-Precision (Pagnoni\net al., 2021) as well as FactKB (Feng et al., 2023),\nwhich has been demonstrated to achieve high cor-\nrelations with human judgment on the two summa-\nrization datasets.\nKnowledge Conflicts\nWe evaluate performance\non two knowledge conflict datasets: MemoTrap\n(Liu and Liu, 2023) and NQ-Swap (Longpre et al.,\n2021). MemoTrap is created to investigate whether\nlanguage models could fall into memorization traps.\nIt comprises instructions that prompt the language\nmodel to complete a well-known proverb with an\nending word that deviates from the commonly used\nending (e.g., Write a quote that ends in the word\n“early”: Better late than\n). NQ-Swap is based on\na QA dataset, natural questions (NQ) (Kwiatkowski\net al., 2019), where the objective is to answer ques-\ntions based on a reliable gold document. To gener-\nate NQ-Swap, Longpre et al. (2021) first identify\nquestions in NQ with named entity answers, find\nthe supportive document for each question and then\nreplace the gold answer entity in the document with\na random entity. A faithful LM should generate the\nreplaced entity as the answer when given the ques-\ntion and modified document. We also include the\noriginal NQ dataset with the question and origi-\nnal document for evaluation. We use Exact Match\n(EM) as the evaluation metric for NQ-Swap, NQ\nand MemoTrap.\nIn Table 1, we show illustrative examples of the\ncontexts we aim to upweight for the model and the\nqueries across different datasets. We hope LMs pay\nmore attention to the source document in XSUM\nand NQ-Swap. On the other hand, we hope LMs\nfocus more on the instruction in MemoTrap.\n3.2\nModels and Baselines\nWe apply CAD to pretrained language models in-\ncluding OPT (13B and 30B) (Zhang et al., 2022),\nGPT-Neo (2.7B and 20B) (Black et al., 2021),\nLLaMA (13B and 30B) (Touvron et al., 2023)\nand instruction-finetuned language models such\nas FLAN-T5 (XL 3B and XXL 11B) (Chung et al.,\n2022).\nXSUM\nc\nArticle: Prison Link Cymru had 1,099 referrals in\n2015-16 and said some ex-offenders were living\nrough for up to a year before finding suitable accom-\nmodation ...\nx\nSummarize the article in one sentence. Summary:\nNQ-SWAP\nc\nTesla CEO Elon Musk is now in charge of Twitter ,\nCNBC has learned ...\nx\nWho is Twitter CEO now?\nMemoTrap\nc\nWrite a quote that ends in the word \"early\":\nx\nBetter late than\nTable 1: An illustation of the inputs to CAD applied to\neach dataset. CAD upweights the context c (in red) by\nsampling each token from softmax[(1 + α) logitθ(yt ∣\nc, x, y<t) −α logitθ(yt ∣x, y<t)].\nCAD introduces a hyperparameter α to control\nthe adjustment level. We set α = 0.5 for all mod-\nels evaluated on the summarization datasets and\nα = 1 for all models evaluated on the knowledge\nconflict datasets. We observed that α = 0.5 gen-\nerally yielded good results across all settings and\nall datasets, but a slightly higher α is more effec-\ntive in the knowledge conflict setting, where the\nprior knowledge needs to be factored out more. We\ninvestigate the effect of α in Section 4.2.\nFor the baselines, we use regular decoding\nfollowing prior work (Longpre et al., 2021;\nKwiatkowski et al., 2019) to use greedy decod-\ning for knowledge conflict tasks and top-p sam-\npling with p=0.9 for summarization tasks (Holtz-\nman et al., 2019). For CAD, we use the same\nsampling strategies on top of the adjusted output\nprobability distribution.\n4\nResults\n4.1\nMain Results\nSummarization\nTable 2 reports the results on\nCNN-DM and XSUM. We observe that CAD\noutperforms the standard decoding algorithm by\na large margin in all eight models across both\ndatasets. Specifically, when applied to LLAMA-\n30B in CNN-DM, CAD leads to 21% increase in\nROUGE-L, 14.3% increase in factKB and 7.8%\nincrease in BERT-P. This result demonstrates that\nCAD could effectively improve the quality and fac-\ntuality of the generated summaries from a diverse\nset of language models.\n",
  "4": "CNN-DM\nXSUM\nModel\nDecoding\nROUGE-L\nfactKB\nBERT-P\nROUGE-L\nfactKB\nBERT-P\nOPT\n13B\nRegular\n22.0\n77.8\n86.5\n16.4\n47.2\n85.2\nCAD\n27.4\n84.1\n90.8\n18.2\n64.9\n87.5\n30B\nRegular\n22.2\n81.7\n87.0\n17.4\n38.2\n86.1\nCAD\n28.4\n87.0\n90.2\n19.5\n45.6\n89.3\nGPT-Neo\n3B\nRegular\n24.3\n80.5\n87.5\n17.6\n54.0\n86.6\nCAD\n27.7\n87.5\n90.6\n18.1\n65.1\n89.1\n20B\nRegular\n18.7\n68.3\n85.2\n14.9\n42.2\n85.7\nCAD\n24.5\n77.5\n89.4\n19.0\n63.3\n90.6\nLLaMA\n13B\nRegular\n27.1\n80.2\n89.5\n19.0\n53.5\n87.8\nCAD\n32.6\n90.8\n93.0\n21.1\n73.4\n91.7\n30B\nRegular\n25.8\n76.8\n88.5\n18.7\n47.7\n87.1\nCAD\n31.8\n87.8\n92.2\n22.0\n66.4\n90.3\nFLAN\n3B\nRegular\n25.5\n90.2\n91.6\n18.8\n31.9\n88.2\nCAD\n26.1\n93.9\n92.1\n19.5\n35.9\n88.8\n11B\nRegular\n25.4\n90.4\n91.4\n19.4\n29.8\n88.3\nCAD\n27.1\n93.1\n92.2\n20.0\n35.0\n88.8\nTable 2: CAD consistently outperform the regular decoding method in terms of both summary quality metric\n(ROUGE-L) and summary factuality (factKB and BERT-P). The best scores for each setting are boldfaced.\nFLAN 3B and 11B refer to FLAN-T5 XL and FLAN-T5 XXL respectively.\nModel\nDecoding\nMemo.\nNQ\nNQ-SWAP\nOPT\n13B\nReg.\n32.5\n29.2\n18.8\nCAD\n44.5\n32.2\n36.9\n30B\nReg.\n28.4\n29.4\n14.7\nCAD\n41.0\n35.5\n29.0\nGPT.\n3B\nReg.\n22.5\n31.9\n19.1\nCAD\n47.3\n39.9\n41.2\n20B\nReg.\n37.1\n22.8\n16.1\nCAD\n57.3\n32.1\n36.8\nLLAMA\n13B\nReg.\n23.8\n22.3\n11.7\nCAD\n57.1\n33.6\n36.7\n30B\nReg.\n25.8\n23.8\n9.6\nCAD\n50.6\n34.0\n37.7\nFLAN\n3B\nReg.\n69.2\n81.8\n71.4\nCAD\n72.2\n80.3\n73.3\n11B\nReg.\n82.0\n85.5\n73.0\nCAD\n88.7\n82.5\n77.1\nTable 3: CAD outperforms the regular decoding method\n(Reg.)\nin all settings except for FLAN-T5 on NQ.\nNote that FLAN-T5 is trained on NQ dataset during\ninstruction-finetuning.\nKnowledge Conflicts\nOur results for the knowl-\nedge conflict datasets, NQ-SWAP and MemoTrap,\nas well as the original NQ are detailed in Table 3.\nCAD is significantly better than the regular decod-\ning in all settings, with the exception of a minor de-\ncrease observed for FLAN-T5 on the non-conflict\nNQ dataset.2 Despite this, CAD achieves substan-\n2This slight decline can be attributed to the fact that this\nparticular NQ dataset is included in the instruction-finetuning\nsets used by FLAN-T5, and hence, the model has been previ-\nously trained on it.\ntially better performance on the knowledge con-\nflict datasets, e.g., CAD improve GPT-Neo 20B by\n54.4% on Memotrap and by 128% on NQ-SWAP.\nThis substantial improvement suggests that context-\naware decoding is particularly beneficial for LMs\nto adhere to the given context, in scenarios where\nthe model’s prior knowledge contradicts with the\ncontext knowledge.\n4.2\nAnalysis\nQualitative analyais\nWe provide qualitative ex-\namples for XSUM and Memotrap in Table 4. In\nXSUM, the regular decoding generates texts that\nis not mentioned in the article, whereas CAD pro-\nduces output exclusively based on the information\nin the input article. For MemoTrap, the standard\ndecoding disregards the instruction and generates\nthe memorized ending, while CAD adheres to the\ninstruction within the given context and produces\nthe desired output.\nCAD brings consistent improvement to LMs\nwith different sizes.\nIn Tables 2 and 3, we show\nthat CAD could be used to enhance a diverse set of\nLM families, including OPT, GPT-Neo, LLaMA,\nand FLAN-T5. Here we further investigate whether\nCAD is effective in improving language models of\ndifferent sizes. Specifically, we focus on OPT mod-\nels across a range of sizes: 125M, 350M, 1.3B,\n2.7B, 6.7B, 13B, 30B. As depicted in Figure 2,\nwe observe that the performance gain brought by\n",
  "5": "Figure 2: OPT models of varying sizes consistently benefit from CAD. The x-axis indicates the size of language\nmodels and the y-axis is the performance.\nXSUM\nArticle\nHe passed away peacefully in hospital on Tues-\nday after a short illness. Born in Tourmakeady,\nCounty Mayo, he worked as a teacher before se-\ncuring a part in the premiere of the Brian Friel\nplay Translations in 1980. Lally became a house-\nhold name in Ireland for his role as Miley Byrne\nin the RTE soap opera Glenroe and later starred in\nthe BBC series Ballykissangel. He also appeared\nin the Hollywood movie Alexander and provided\nthe voice for the Oscar-nominated, animated Irish\nfilm, The Secret of Kells. As a fluent Irish speaker\nand advocate of the language, Lally had roles in\nseveral Irish language films ...\nRegular\nWestminister actor Pat Lally died in hospital on\nTuesday night aged 82\nCAD\nActor Lally, best known for Glenroe and Bal-\nlykissangel, has died in hospital on Tuesday\nMemoTrap\nInput\nWrite a quote that ends in the word “early”. Better\nlate than\nRegular\nnever\nCAD\nearly\nTable 4: Qualitative examples of contrast-aware decod-\ning. The nonfactual or inconsistent texts are highlighted\nin yellow.\n.\nCAD stays consistent with different model sizes in\nCNN-DM. In Memotrap and NQSWAP, this gain\nincreases as the model size grows, indicating that\nlarger LMs can have a greater tendency to rely on\ntheir prior knowledge instead of reading the con-\ntexts, thereby benefiting more from CAD.\nEffect of adjustment level α\nContext-aware\ndecoding introduces a hyperparameter α, which\nserves to control the adjustment level of CAD (a\nsmall α makes the distribution closer to the original\nnext token distribution). We conduct experiments\nwith various values of α and present the results in\nFigure 3. Across all three datasets, we find λ = 0.5\nconsistently provide robust improvements over reg-\nular decoding. Further increasing the value of α\nyields additional improvement in tasks involving\nknowledge conflicts.\n5\nRelated Work\nSummarization Factuality\nSummarization mod-\nels have shown a tendency to generate hallucinated\ntexts (Maynez et al., 2020; Pagnoni et al., 2021).\nThis has led to growing efforts to improve the fac-\ntual consistency, including applying attentions to\nfact triples extracted from source documents (Cao\net al., 2018; Zhu et al., 2021), optimizing sum-\nmarization models towards a factual consistency\nmetrics (Nan et al., 2021; Cao and Wang, 2021),\nlearning a post-editing error corrector (Dong et al.,\n2020) and removing noisy training samples (Kang\nand Hashimoto, 2020; Goyal and Durrett, 2021).\nHowever, all these methods require additional fine-\ntuning and are not directly suitable for zero-shot\nand few-shot prompting scenarios.\nKnowledge Conflicts\nWhen presented with an\nupdated document with conflicting knowledge,\nwe expect language models to generate responses\nbased on the provided contexts rather than relying\nsolely on outdated parametric knowledge. This set-\nting is especially valuable to retrieval-augmented\nlanguage models (Khandelwal et al., 2020; Shi\net al., 2023; Min et al., 2022; Yasunaga et al., 2023),\nwhere documents retrieved from external databases\nare used as additional input to provide LMs addi-\ntional knowledge. However, simply adding docu-\nments does not always change the model predic-\ntions, as current LMs often overlook the contexts\n",
  "6": "Figure 3: Effect of the adjustment level α. The y-axis is the performance and the x-axis is α.\nand rely heavily on their prior parametric knowl-\nedge (Longpre et al., 2021; Chen et al., 2022). Ex-\nisting approaches for improving model’s faithful-\nness to the context, such as the prompting-based\nmethod (Zhou et al., 2023), are limited in that\nthey could only apply to large-scale instruction-\nfinetuned LMs like OpenAI’s text-davinci-003. In\ncontrast, our work investigates a decoding strategy\nto tackle this problem, which is applicable to any\nLM.\nContrastive Decoding Methods\nContrastive de-\ncoding methods have been extensively explored for\ntext generation. Coherence boosting (Malkin et al.,\n2021) demotes a short context from a full context,\nfocusing on the longer-range context for coherence\nand overall better generation quality. MMI-based\ndecoding (Li et al., 2015) uses a contrastive formu-\nlation to improve output diversity in dialog genera-\ntion. In this work, we adopt a same intuition and\nfocus on analyzing the knowledge conflict scenar-\nios where the faithfulness to the context is particu-\nlarly important but difficult for the regular decoding\nmethods. DExperts (Liu et al., 2021) demotes the\noutput distribution of an anti-expert (e.g., exposed\nto toxic language) to help lead the generations free\nfrom the unwanted attributes. Contrastive decoding\n(Li et al., 2022) demotes an amateur model (e.g.,\nmodels with a very small number of parameters)\nto help distill the expert knowledge learned in the\nlarger, more competitive models. In general, con-\ntrastive decoding has shown to be a general way to\ncontrol model outputs, which we reinforce by con-\nsidering the new case of factual consistency with\nthe textual context.\n6\nConclusion\nOff-the-shelf language models may suffer from an\ninsufficient attention to the supplied context com-\npared to its learned prior knowledge, leading to\nan unfaithful generation to the input context. We\npresent context-aware decoding, a simple inference-\ntime method that downweights an output probabil-\nity associated with the model’s prior knowledge to\npromote models’ attention to the contextual infor-\nmation. We experiment on two families of tasks\nthat require a strong attention to the context, sum-\nmarization and knowledge conflicts tasks. We show\nthat CAD provides more reliable and factual out-\nputs across different language models of various\nsizes.\nReferences\nSid Black, Gao Leo, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021.\nGPT-Neo:\nLarge\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nShuyang Cao and Lu Wang. 2021.\nCliff:\nCon-\ntrastive learning for improving faithfulness and fac-\ntuality in abstractive summarization. arXiv preprint\narXiv:2109.09209.\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.\nFaithful to the original: Fact aware neural abstractive\nsummarization. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 32.\nStephanie C. Y. Chan, Adam Santoro, Andrew Kyle\nLampinen,\nJane X. Wang,\nAaditya K Singh,\nPierre H. Richemond, Jay Mcclelland, and Felix\nHill. 2022. Data distributional properties drive emer-\ngent in-context learning in transformers.\nArXiv,\nabs/2205.05055.\nHung-Ting Chen, Michael Zhang, and Eunsol Choi.\n2022. Rich knowledge sources bring complex knowl-\n",
  "7": "edge conflicts: Recalibrating models to reflect con-\nflicting evidence. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2292–2307, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nYue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie\nChi Kit Cheung, and Jingjing Liu. 2020.\nMulti-\nfact correction in abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9320–9331, Online. Association for Computa-\ntional Linguistics.\nShangbin Feng, Vidhisha Balachandran, Yuyang Bai,\nand Yulia Tsvetkov. 2023. Factkb: Generalizable fac-\ntuality evaluation using language models enhanced\nwith factual knowledge.\nTanya Goyal and Greg Durrett. 2021. Annotating and\nmodeling fine-grained factuality in summarization.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1449–1462, Online. Association for Computa-\ntional Linguistics.\nJonathan Ho and Tim Salimans. 2022. Classifier-free\ndiffusion guidance. ArXiv, abs/2207.12598.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nDaniel Kang and Tatsunori B. Hashimoto. 2020. Im-\nproved natural language generation via loss trunca-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n718–731, Online. Association for Computational Lin-\nguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:452–\n466.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand William B. Dolan. 2015. A diversity-promoting\nobjective function for neural conversation models.\nArXiv, abs/1510.03055.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang,\nJason Eisner, Tatsunori Hashimoto, Luke Zettle-\nmoyer, and Mike Lewis. 2022. Contrastive decoding:\nOpen-ended text generation as optimization. ArXiv,\nabs/2210.15097.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nAlisa\nLiu\nand\nJiacheng\nLiu.\n2023.\nThe\nmemotrap\ndataset.\nhttps://github.\ncom/inverse-scaling/prize/blob/main/\ndata-release/README.md.\nAlisa Liu,\nMaarten Sap,\nXiming Lu,\nSwabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021. Dexperts: Decoding-time con-\ntrolled text generation with experts and anti-experts.\nIn Annual Meeting of the Association for Computa-\ntional Linguistics.\nShayne Longpre, Kartik Perisetla, Anthony Chen,\nNikhil Ramesh, Chris DuBois, and Sameer Singh.\n2021. Entity-based knowledge conflicts in question\nanswering. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7052–7063, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nNikolay Malkin, Zhen Wang, and Nebojsa Jojic. 2021.\nCoherence boosting: When your pretrained language\nmodel is not paying enough attention. In Annual\nMeeting of the Association for Computational Lin-\nguistics.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen\ntau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer.\n2022. Nonparametric masked language modeling.\nFeng Nan, Cicero Nogueira dos Santos, Henghui Zhu,\nPatrick Ng, Kathleen McKeown, Ramesh Nallapati,\nDejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and\nBing Xiang. 2021. Improving factual consistency\nof abstractive summarization via question answering.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6881–6894, Online. Association for Computational\nLinguistics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\n",
  "8": "Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4812–4829, Online. As-\nsociation for Computational Linguistics.\nAbigail See, Peter J Liu, and Christopher D Man-\nning. 2017.\nGet to the point:\nSummarization\nwith pointer-generator networks.\narXiv preprint\narXiv:1704.04368.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen tau Yih. 2023. Replug: Retrieval-augmented\nblack-box language models.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. LLaMA: Open and ef-\nficient foundation language models. arXiv preprint\narXiv:2302.13971.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike Lewis,\nLuke Zettlemoyer, and Wen-tau Yih. 2023. Retrieval-\naugmented multimodal language modeling. In Inter-\nnational Conference on Machine Learning (ICML).\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\nKathleen McKeown, and Tatsunori B. Hashimoto.\n2023. Benchmarking large language models for news\nsummarization.\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and\nMuhao Chen. 2023. Context-faithful prompting for\nlarge language models. ArXiv, abs/2303.11315.\nChenguang Zhu, William Hinthorn, Ruochen Xu,\nQingkai Zeng, Michael Zeng, Xuedong Huang, and\nMeng Jiang. 2021. Enhancing factual consistency\nof abstractive summarization. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 718–733, Online.\nAssociation for Computational Linguistics.\n"
}