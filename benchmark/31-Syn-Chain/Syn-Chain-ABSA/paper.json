{
  "1": "Proceedings of the 31st International Conference on Computational Linguistics, pages 3123–3137\nJanuary 19–24, 2025. ©2025 Association for Computational Linguistics\n3123\nAspect-Based Sentiment Analysis with Syntax-Opinion-Sentiment\nReasoning Chain\nRui Fan1,2,3,†, Shu Li1,2,4,†, Tingting He1,2,4,∗, Yu Liu1,2,3\n1Hubei Provincial Key Laboratory of Artiﬁcial Intelligence and Smart Learning,\n2National Language Resources Monitor and Research Center for Network Media,\n3Faculty of Artiﬁcial Intelligence in Education,\n4School of Computer, Central China Normal University, Wuhan, China\n{fanrui, lishu-keep, yuliu}@mails.ccnu.edu.cn, tthe@mail.ccnu.edu.cn\nAbstract\nDespite the impressive capabilities of large lan-\nguage models (LLMs) in aspect-based senti-\nment analysis (ABSA), the role of syntactic\ninformation remains underexplored in LLMs.\nSyntactic structures are known to be crucial for\ncapturing aspect-opinion relationships. To ex-\nplore whether LLMs can effectively leverage\nsyntactic information to improve ABSA perfor-\nmance, we propose a novel multi-step reason-\ning framework, the Syntax-Opinion-Sentiment\nReasoning Chain (Syn-Chain). Syn-Chain se-\nquentially analyzes syntactic dependencies, ex-\ntracts opinions, and classiﬁes sentiment. We\nintroduce Syn-Chain into LLMs via zero-shot\nprompting, and results show that Syn-Chain\nsigniﬁcantly enhances ABSA performance,\nthough smaller LLMs1 exhibit weaker per-\nformance. Furthermore, we enhance smaller\nLLMs via distillation using GPT-3.5-generated\nSyn-Chain responses, achieving state-of-the-art\nABSA performance. Our ﬁndings highlight the\nimportance of syntactic information for improv-\ning LLMs in ABSA and offer valuable insights\nfor future research2.\n1\nIntroduction\nAspect-based sentiment analysis (ABSA) is a ﬁne-\ngrained sentiment analysis task which aims to an-\nalyze the sentiment polarity associated with spe-\nciﬁc aspects in a sentence (Pontiki et al., 2014).\nFor example, in the sentence \"Great food but the\nservice was dreadful!\", \"food\" and \"service\" are\naspect terms, exhibiting positive and negative sen-\ntiments, respectively. The potential to analyze user\nsentiments at such a granular level has garnered sig-\nniﬁcant attention from both industry and academia.\nIdentifying an aspect’s sentiment depends on\ncapturing the associated opinion, which typically\n∗Corresponding author\n†Equal contribution\n1In this study, smaller LLMs refer to language models with\nfewer than 10 billion(B) parameters.\n2https://github.com/rf-x/Syn-Chain-ABSA\nreveals the sentiment orientation (Kandhro et al.,\n2024). Attention mechanisms have been employed\nto capture words linked to the aspect term (Yang\net al., 2017; Liu and Zhang, 2017; Wang et al.,\n2016), but these methods often struggle with lin-\nguistic complexities (Sun et al., 2019). Conse-\nquently, incorporating syntactic information has\ngained widespread acceptance (Zhang et al., 2019;\nNazir et al., 2022). Some studies constructed con-\nnections between words based on syntactic depen-\ndency trees and utilized graph neural networks to\ncapture latent relationships among sentiment ele-\nments (Wang et al., 2020; Wu et al., 2021; Chen\net al., 2022; Yang et al., 2023; Zhong et al., 2023).\nLeveraging additional syntactic information along\nwith word dependencies signiﬁcantly enhances\nABSA performance.\nRecently, large language models (LLMs) have\ndemonstrated exceptional generalization and con-\ntextual understanding capabilities. They also ex-\nhibit impressive sentiment comprehension abilities\n(Wang et al., 2024b). The incorporation of instruc-\ntions and demonstrations within prompts, follow-\ning the in-context learning (ICL) paradigm, has\nfurther boosted LLMs performance (Zhang et al.,\n2023). Yang et al. (2024) improved LLMs perfor-\nmance in cross-domain ABSA tasks by providing\ndiverse examples. Wang et al. (2024a) trained a\nretriever that selects examples based on semantic\nrelevance, syntactic structure, and aspect-sentiment\nalignment, but LLMs have not directly leveraged\nsyntactic information.\nGiven the success of syntactic information in\nprior ABSA studies, and the ﬁndings by Roy et al.\n(2023), which show that LLMs possess a certain de-\ngree of syntactic and semantic parsing abilities, we\nare motivated to explore whether LLMs can utilize\nsyntactic information to further enhance their per-\nformance in ABSA. Drawing inspiration from the\nmulti-hop reasoning Chain-of-Thought (CoT) ap-\nproach (Fei et al., 2023) for ABSA , we propose a\n",
  "2": "3124\nSyntax-Opinion-Sentiment Reasoning Chain (Syn-\nChain): (1) analyzing syntactic dependency infor-\nmation, (2) capturing aspect-related opinions using\ndependency relations, and (3) performing compre-\nhensive sentiment analysis.\nOur experiments on several ABSA benchmark\ndatasets demonstrate that Syn-Chain signiﬁcantly\nimproves LLM performance in ABSA, although\nsmaller LLMs with limited syntactic understanding\nbeneﬁt less. Additionally, we leverage GPT-3.5\nto automatically generate Syn-Chain reasoning in-\nformation for supervised training. We ﬁne-tune\nT5 (Raffel et al., 2020) and Flan-T5 (Chung et al.,\n2024) using full-parameter tuning, and Llama-2-7B\n(Touvron et al., 2023) and Llama-3-8B (AI@Meta,\n2024) using parameter-efﬁcient tuning, both achiev-\ning results that surpass baseline models. These\nﬁndings highlight that even in LLM-based research,\nsyntactic information continues to offer valuable\ninsights for ABSA.\nThe main contributions of this work include:\n• We propose Syn-Chain, a multi-step reason-\ning framework that integrates syntactic infor-\nmation into LLMs to enhance ABSA perfor-\nmance. To the best of our knowledge, we are\nthe ﬁrst to explore the direct use of syntactic\ninformation in LLMs for ABSA.\n• We construct Syn-Chain ABSA datasets for\nsupervised training, enabling smaller LLMs\nto effectively utilize syntactic information.\n• Extensive experiments on ABSA benchmarks\nvalidate the effectiveness of Syn-Chain and\nprovide insights into integrating syntactic in-\nformation within LLMs.\n2\nRelated Work\nAs a subtask of sentiment analysis, ABSA poses\na more intricate challenge, garnering substantial\nattention in recent years. A key point of ABSA is\ncapturing information related to aspect terms. At-\ntention mechanisms have been widely adopted to\ncapture contextual relationships with target words\n(Tang et al., 2016; Wang et al., 2016; Cheng et al.,\n2017; Ma et al., 2017; Li et al., 2018; Gu et al.,\n2018; Fan et al., 2018). However, these mecha-\nnisms often underperform when opinion words are\ndistant from their corresponding aspects. To ad-\ndress this, researchers have explored leveraging\nsyntactic information to identify relevant words\nfor aspects (Huang and Carley, 2019; Zhang et al.,\n2019). Speciﬁcally, syntactic dependency trees and\ngraph neural networks have been constructed to\nmodel the intricate connections between words and\ntarget aspects (Huang and Carley, 2019; Sun et al.,\n2019; Wang et al., 2020; Zhang et al., 2022; Liang\net al., 2022).\nMore recently, the advent of pretrained language\nmodels such as BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019) has revolutionized\nABSA, yielding impressive results (Song et al.,\n2019; Jiang et al., 2019).\nFor instance, Wang\net al. (2020) combined BERT with graph neural\nnetworks to encode syntactic information (Wang\net al., 2020), and (Phan and Ogunbona, 2020) ex-\nplored grammatical and syntactic features based on\nBERT to enhance ABSA performance.\nLLMs have made signiﬁcant advancements in\nsentiment analysis, especially in zero-shot settings\n(Zhang et al., 2023; Wang et al., 2024b). Their\nemergence has facilitated novel techniques like\nCoT reasoning (Wei et al., 2022) and ICL (Dong\net al., 2024). For instance, Fei et al. (2023) lever-\naged multi-step CoT for comprehending implicit\nsentiments and opinions. Yang et al. (2024) investi-\ngated multi-domain ABSA under the ICL paradigm.\nAdditionally, Wang et al. (2024a) trained a retriever\nto select demonstrations for ICL.\nDespite these advancements, current LLM-based\nABSA research largely overlooks syntactic infor-\nmation, a crucial component in traditional ABSA\nmethods. Thus, this study aims to explore if incor-\nporating syntactic knowledge can further enhance\ntheir ABSA performance.\n3\nMethodology\n3.1\nTask Deﬁnition\nGiven a sentence X with an aspect term A ∈X,\nthe goal of ABSA is to predict the sentiment polar-\nity y ∈{Positive, Negative, Neutral} towards\nA. LLMs tackle this task by processing a prompt\nthat includes X and A, subsequently generating a\nsentiment prediction, that is P(y|X, A).\n3.2\nSyn-Chain Prompting\nAs shown in Fig. 1, Syn-Chain consists of three\nsequential steps: syntactic parsing, opinion extrac-\ntion, and sentiment classiﬁcation. We provide the\nfollowing notation: S represents the syntactic de-\npendency sequence; Q1, Q2, and Q3 denote the\nanalysis targets of the three steps, respectively; and\n",
  "3": "3125\nStep 3: Sentiment Classification\nR 2 :  The adjectives \"fresh\" and \"hot\" are \ngenerally associated with positive dining \nexperiences, further  emphasized by the \nconcluding phrase \" ready to eat!\" which \nconveys enthusiasm and satisfaction.\nQ3: Based on the common sense and such \nspeaker's opinion, what is the sentiment \npolarity towards \"Food\"?\nStep 1: Syntactic Parsing\nX: Food is always fresh and hot-ready to eat!\nQ 1: Based on the syntactic dependency of \nthe sentence, analyze information related \nto the \"Food\" in the sentence.\n4\n3\n2\n1\nID\nfresh\nalways\nis\nFood\nText\n...\n...\n...\n...\n...\n...\n...\n...\n2\n2\n2\n2\nHead\nacomp\nadvmod\nroot\nnsubj\nDepRel\n...\n...\nS :\nE: Each line in the sequence represents a \nword in the sentence, and each element \nwithin a line ... (Describe the structure of \nsyntactic dependency sequences).\nStep 2: Opinion Extraction\nR1: \"Food\" (ID 1) is the subject (DEPREL: \nnsubj) of the verb \"is\" (ID 2). This means it \nis the main topic of the sentence and the \nadjective phrase \"fresh and hot-ready to eat\" \ndescribes its state.\nQ2: Considering the context and information \nrelated to \"Food\", what is the user's opinion \ntowards \"Food\"?\nX: Food is always fresh and hot-ready to eat!\nX: Food is always fresh and hot-ready to eat!\nR3: The sentiment towards \"Food\" in the given sentence is positive. Because the sentence \nemploys positive adjectives like \"fresh\" and \"hot\" to describe the food. These words are \ngenerally associated with pleasant dining experiences. Additionally, the phrase \"ready to eat!\" \nimplies enthusiasm and satisfaction, further reinforcing the positive sentiment towards the food.\nFigure 1: Overview of the Syn-Chain Prompting Process. The response from the previous step is utilized in the\nsubsequent step.\nR1, R2, and R3 represent the corresponding out-\nputs of LLMs.\nStep 1: Syntactic Parsing. We begin by using\nspaCy3 to generate a dependency parse for each\nsentence, which is converted into the CoNLL-U for-\nmat, a tabular representation for LLM processing.\nEach line in the CoNLL-U format corresponds to a\nword and its associated attributes, such as part-of-\nspeech and dependency relations (see Appendix A\nfor details).\nGiven the discrepancy between syntactic depen-\ndency information and natural language expres-\nsions, we add the descriptions of each attribute’s\nmeaning, denoted as E. By querying the LLMs\nabout aspect-related information, we obtain natural\nand readable description of the syntactic structure,\ndenoted as R1, This process can be formalized as\nP(R1|X, S, E, Q1).\nStep 2: Opinion Extraction. Building upon the\nsyntactic analysis provided by R1, we ask the LLM\nto analyze user opinions related to speciﬁc aspects.\nThe resulting response, R2, is generated in the\nformat P(R2|X, R1, Q2).\nStep 3: Sentiment Classiﬁcation. Leveraging the\naspect opinions from R2, the LLM classiﬁes the\nsentiment polarity of the aspect term, producing a\nreasoned judgment denoted as R3. This process\nis represented as P(R3|X, R2, Q3). By extracting\npolarity words from R3, we derive the predicted\nsentiment label ˆy.\n3.3\nSyn-Chain Supervised Learning\nThe understanding and reasoning capabilities of\nsmaller models are limited, particularly when deal-\ning with syntactic dependency information that is\n3https://spacy.io/\nexpressed differently in natural language. Leverag-\ning reasoning information from powerful LLMs as\nsupervisory signals and distilling this knowledge\ninto smaller models can effectively enhance their\nperformance on speciﬁc tasks (Li et al., 2022; Ho\net al., 2023; Fan et al., 2024). To this end, we\nconstruct Syn-Chain data for ﬁne-tuning smaller\nmodels.\nData Construction. As shown in Fig. 1, the re-\nsponses R1, R2, and R3 generated by the LLM\nare directly used as target responses for model\ntraining. However, incorrect sentiment predictions\nby the LLM can result in unreasonable responses\nthat mislead the model during training. To ensure\nconsistency between each step’s response and the\nﬁnal sentiment label, we obtain reasoning by in-\nputting prompts conditioned on the true sentiment\nlabel y. This process is formalized as P( ˜\nRj|Cj, y),\nwhere Cj represents the input for each step (where\nj ∈[1, 2, 3]), and ˜\nRj is the corresponding LLM-\ngenerated response.\nModel Training. Following the construction of the\nSyn-Chain dataset, we employ supervised learn-\ning to train the model. Technically, Syn-Chain\nsupervised learning is a multi-task learning frame-\nwork that addresses three primary tasks: syntactic\nparsing, opinion extraction, and sentiment classiﬁ-\ncation. The model’s inputs and outputs align with\nthe steps outlined in Section 3.2, where the goal\nof outputs is to generate ˜\nRj. The uniﬁed training\nobjective for all three tasks can be formulated as a\ngenerative loss:\nLG = −1\nN\nN\nX\ni\nT\nX\nt=1\nlog P(gi,t|ˆgi,<t, C)\n(1)\nwhere N represents the total number of samples,\nand T represents the length of the sequence for\n",
  "4": "3126\nDataset\nSplit\nPos.\nNeu.\nNeg.\nTotal\nLen of S\nLen of R1\nLen of R2\nLen of R3\nRest14\nTrain\n2164\n633\n805\n3602\n17.54\n108.05\n59.55\n38.28\nTest\n728\n196\n196\n1120\n16.31\n108.72\n58.67\n38.15\nLap14\nTrain\n987\n460\n866\n2313\n19.28\n108.84\n62.03\n39.42\nTest\n341\n169\n128\n638\n15.99\n111.98\n61.89\n39.34\nRest15\nTrain\n912\n36\n256\n1204\n16.98\n110.00\n58.83\n37.89\nTest\n326\n34\n182\n542\n18.17\n109.63\n59.69\n37.38\nRest16\nTrain\n1240\n69\n439\n1748\n17.33\n110.20\n59.38\n37.83\nTest\n469\n30\n117\n616\n19.71\n110.86\n59.20\n37.68\nTable 1: Statistics of the ABSA datasets. “Len\" represents the average length of the corresponding sequence.\neach sample, gi,t denotes the true token at position\nt, ˆgi,<t denotes the generated sequence up to po-\nsition t, and P(gi,t|ˆgi,<t, C) is the probability of\ngenerating token gi,t given ˆgi,<t and input C.\nError Propagation. In executing Step 2 and Step\n3 of Syn-Chain, the model depends on the output\nfrom the prior step. During training, ground-truth\nresponses are used, but during inference, the model\nrelies on its generated outputs, leading to poten-\ntial error propagation. Errors in earlier steps can\naccumulate, affecting sentiment predictions. To\nmitigate this issue, we propose a training strategy\nto \"break the chain\", which prevents error propaga-\ntion by ensuring that the response from the previous\nstep is not used as input for the subsequent step. For\nexample, the notation 1 ⊕2 ⊘3 indicates that Step 1\nand Step 2 are linked, and the connection between\nStep 2 and Step 3 is deliberately severed. In other\nwords, R2 from Step 2 is not used as input to Step\n3, which is formally represented as P(R3|X, Q3).\nThis ensures that any errors from the ﬁrst two steps\ndo not affect predictions in the third step.\nThe training of Step 1 and Step 2 is designed\nto equip the model with the ability to comprehend\nsyntactic information and capture aspect-related\nopinions.\nEven when the chain is broken, the\nmodel retains these capabilities within the multi-\ntask learning framework, while minimizing the risk\nof error accumulation. Notably, our breaking strat-\negy is based on independent Step 3, and the model\nis trained to focus exclusively on sentiment pre-\ndiction from the ﬁrst sentence of ˜\nR3, excluding\nsubsequent reasoning content.\n4\nExperimental Setup\n4.1\nDatasets\nWe utilize four ABSA datasets in our experiments:\nRest14 and Lap14, sourced from (Pontiki et al.,\n2014), Rest15 from (Pontiki et al., 2015), and\nRest16 from (Pontiki et al., 2016). To balance\nexperimental rigor with resource constraints, we\nfocus on Rest14 and Lap14 for zero-shot exper-\niments due to the high cost of LLM API usage.\nThis allows us to conduct in-depth ablation studies.\nFine-tuning experiments are conducted on local\nGPUs. This setup enabled us to compare a broader\nrange of methods using four common datasets.\nDetailed statistics of these datasets are shown in\nTable 1. Instances with multiple aspects within a\nsingle sentence are split into multiple single-aspect\nsamples. The supplementary Syn-Chain data sub-\nstantially enrich the informational content of the\noriginal sentences.\n4.2\nImplement Details and Metrics\nWe evaluate Syn-Chain prompting on several\nLLMs including GPT-3.5, GPT-4o4, Gemini-1.55,\nLlama-2-7B (Touvron et al., 2023), and Llama-\n3-8B (AI@Meta, 2024). Details of the prompts\ncan be found in Appendix A. For supervised ﬁne-\ntuning, we ensure a fair comparison with previous\nmethods by performing full-parameter ﬁne-tuning\non T5 and Flan-T5. Additionally, we employ LoRA\nﬁne-tuning (Hu et al., 2022) for Llama-2-7B and\nLlama-3-8B. All model parameters are obtained\nfrom the Transformers library6. For full ﬁne-tuning,\nwe set the learning rate to 5e-5, batch size to 4, and\ntrain for 10 epochs. For LoRA ﬁne-tuning, we use\na learning rate of 1e-4, batch size of 2, and train for\n10 epochs with α = 16 and r = 8. Both setups use\nthe AdamW optimizer.\nThe experiments are implemented in PyTorch\nand execute on two A5000 GPUs. We employ\nAccuracy (Acc) and Macro-F1 score (F1) for eval-\nuation. For ﬁne-tuned models, results are reported\nas the average of three independent runs.\n4OpenAI API: https://openai.com/api/\n5Google Gemini API: https://ai.google.dev/\n6https://huggingface.co/\n",
  "5": "3127\nMethod\nRest14\nLap14\nAcc\nF1\nAcc\nF1\nGPT-3.5\n82.50\n71.27\n79.62\n74.63\n+ Syn-Chain\n85.08\n78.61\n80.09\n76.45\nGPT-4o\n86.96\n76.36\n82.13\n76.40\n+ Syn-Chain\n90.26\n83.85\n82.60\n78.01\nGemini-1.5\n87.41\n80.22\n78.52\n71.66\n+ Syn-Chain\n87.58\n81.06\n79.31\n74.54\nLlama-2-7B\n74.73\n65.02\n73.04\n69.10\n+ Syn-Chain\n77.58\n64.10\n71.00\n64.56\nLlama-3-8B\n82.23\n67.83\n78.36\n71.73\n+ Syn-Chain\n81.42\n71.73\n75.23\n70.97\nT5\n87.50\n81.44\n80.87\n76.69\nTable 2: Experimental results of Syn-Chain Zero-shot\nPrompting on ABSA. T5 is ﬁne-tuned on ABSA and\nprovided for comparison.\nFormat\nSyn-Chain1⊕3\nSyn-Chain\nAcc\nF1\nAcc\nF1\nCoNLL-U\n77.56\n71.68\n80.09\n76.45\nXML\n76.33\n68.29\n81.50\n77.21\nJSON\n76.80\n68.87\n81.50\n77.15\nHTML\n76.48\n68.76\n80.87\n77.11\nTable 3: Performance comparison of GPT-3.5 on Lap14\nusing different syntactic representations. 1 ⊕3 indicates\nthat only Steps 1 and 3 of the Syn-Chain are used.\n5\nExperimental Results and Analysis\n5.1\nSyn-Chain Zero-shot Prompting\nMain Results. As shown in Table 2, we com-\npare the performance of various LLMs before and\nafter applying the Syn-Chain strategy. For more\npowerful models, such as GPT-3.5 and GPT-4o,\nthe performance improves signiﬁcantly after us-\ning Syn-Chain. Speciﬁcally, on the Res14 dataset,\nthe F1 scores of GPT-3.5 and GPT-4o increase by\n7.34% and 7.49%, respectively. GPT-4o achieves\nthe best performance on both datasets, even surpass-\ning ﬁne-tuned models. However, smaller LLMs,\nLlama-2-7B and Llama-3-8B, generally experience\na decline in performance. Their limited capacity\nmakes it difﬁcult to beneﬁt from the complex rea-\nsoning process of Syn-Chain.\nEffectiveness of Syn-Chain. As shown in Table 3,\na comparison between removing Step 2 (1 ⊕3) and\nthe full Syn-Chain strategy highlights the impor-\ntance of opinion extraction (Step 2). The perfor-\nmance of 1 ⊕3 is lower than that of direct sentiment\nprediction on aspect terms using LLMs. This result\n0\n20\n40\n60\n80\n100\nGPT-4o\nLlama-3-8B\nStep 1 Errors\nStep 2 Errors\nStep 3 Errors\nFigure 2: Proportions of different error types in Syn-\nChain sentiment prediction errors.\nindicates that Syn-Chain effectively leverages syn-\ntactic information to enhance LLM performance.\nIn Syn-Chain, syntactic information supports\nopinion extraction, which in turn strengthens senti-\nment analysis. Directly incorporating syntactic in-\nformation for sentiment analysis introduces weaker\ntask interdependencies, making the model’s reason-\ning more complex and limiting performance im-\nprovements. By decomposing the task into smaller,\nmore manageable steps, Syn-Chain more effec-\ntively harnesses the model’s reasoning and com-\nprehension capabilities.\nSyntactic Information Format Analysis.\nAs\nshown in Table 3, we compare the effects of differ-\nent data formats. We devise a tabular representa-\ntion using newlines and tabs to incorporate syntac-\ntic dependency information effectively into LLMs.\nThis approach replaces traditional tree structures\nto better align with the LLMs’ sequential process-\ning nature. Additionally, we try other structured\nformats such as XML, JSON, and HTML. When\nemploying Step 1 and Step 3 (1 ⊕3), the CoNLL-U\nformat achieves the best performance. However,\nwith the full Syn-Chain strategy, XML format out-\nperforms the others. This suggests that LLMs can\nunderstand and reason with various syntactic for-\nmats. In subsequent ﬁne-tuning experiments, we\nuse the CoNLL-U format due to its simplicity and\nminimal introduction of special symbols, reducing\nthe model’s learning complexity.\nError Analysis. To gain deeper insights into the\nmodels’ understanding of syntactic information,\nwe conduct a manual analysis of Syn-Chain results\nfrom GPT-4o and Llama-3-8B the best-performing\nmodels among the large and small models, respec-\ntively. Speciﬁcally, we randomly select 100 sam-\nples from the sentiment prediction errors: 50 from\nRest14 and 50 from Lap14. Through manual evalu-\nation, we identify at which step in Syn-Chain errors\noccur. We categorize the errors into three types:\n1) Step 1 errors, where the syntactic description\ndoes not align with the provided syntactic informa-\n",
  "6": "3128\ntion, 2) Step 2 errors, where the opinion descrip-\ntion is inconsistent with the original sentence, and\n3) Step 3 errors, where the ﬁnal sentiment judg-\nment is incorrect. Since the process follows a chain\nstructure, if an earlier step is incorrect, subsequent\nsteps are not evaluated (e.g., if Step 1 is incorrect,\nStep 2 and Step 3 are not considered). The evalua-\ntion is conducted independently by three master’s\nstudents with expertise in English and natural lan-\nguage processing (NLP). To mitigate the risk of\noverlooking errors in the lengthy model responses,\na consensus-based approach is adopted to resolve\nany discrepancies among the evaluators.\nAs shown in Fig. 2, GPT-4o rarely makes mis-\ntakes in Step 1, demonstrating a solid understand-\ning of the provided syntactic information. In con-\ntrast, Llama-3-8B exhibits more errors in syntac-\ntic comprehension. Additionally, we observe that\nGPT-4o’s outputs are more structured and easier\nto read (see Fig. 5 in the Appendix). Both models\nshow higher error rates in Step 2 than in Step 3,\nas the sentiment inclination is often conveyed in\nStep 2, making it account for a higher proportion\nof errors in these sentiment misclassiﬁcation cases.\nThis also highlights the importance of generating\nreasoning information conditioned on sentiment la-\nbels when constructing Syn-Chain ﬁne-tuning data,\nwhich avoid inconsistencies between reasoning and\nsentiment labels.\n5.2\nSyn-Chain Fine-tuning\nBaseline. We select BERT-SPC (Song et al., 2019)\nand BERT-PT (Xu et al., 2019), both of which\nare BERT-based models. Additionally, we con-\nsider several works that combine BERT with graph\nneural networks to leverage syntactic dependency\ntrees. BERT-RGAT (Wang et al., 2020) employs\nrelational graph attention to capture syntactic de-\npendencies. BERT-DualGCN (Li et al., 2021) in-\ntegrates both syntactic and semantic knowledge.\nBERT-TGCN (Tian et al., 2021) incorporates de-\npendency types into the graph convolutional net-\nwork. BERT-SenticGCN (Liang et al., 2022) fur-\nther enriches the dependency graph with affective\nknowledge. Moreover, we ﬁne-tune T5, Flan-T5,\nLlama-2-7B, and Llama-3-8B on ABSA to provide\nadditional baselines.\nMain Results. As shown in Table 4, generative\nmodels such as T5 and Flan-T5 outperform BERT-\nbased methods. Speciﬁcally, T5-base and Flan-\nT5-base excess BERT-SPC by 4.44% and 5.24%\nin average F1 scores.\nWhile BERT combined\nwith graph neural networks to incorporate syntac-\ntic information yields signiﬁcant performance im-\nprovements, our Syn-Chain1⊕2⊘3 training strategy\nachieves the best results with T5-base and Flan-\nT5-base, surpassing the previously top-performing\nBERT-SenticGCN across all four datasets.\nAs model size increases, Flan-T5-large demon-\nstrates noticeably better performance than Flan-T5-\nbase. Furthermore, LoRA ﬁne-tuning on larger\nmodels such as Llama-2-7B and Llama-3-8B leads\nto further improvements in ABSA performance.\nEffectiveness of Breaking Chain. We evaluate\nthe full Syn-Chain and three variations of the bro-\nken Syn-Chain: 1 ⊘3, 1 ⊘2 ⊘3, and 1 ⊕2 ⊘3. The\nnumbers 1, 2, and 3 correspond to the three sequen-\ntial steps in the Syn-Chain process, while ⊕and\n⊘represent linked and broken steps, respectively.\nThe broken versions are designed with an indepen-\ndent Step 3 to prevent the ﬁnal sentiment prediction\nfrom being inﬂuenced by erroneous information.\nAs shown in Table 4, the best performance across\ndifferent models is consistently achieved by Syn-\nChain1⊕2⊘3, which breaks the connection between\nStep 2 and Step 3. This ensures sufﬁcient learning\nof syntactic information while avoiding the propa-\ngation of errors to the ﬁnal sentiment classiﬁcation.\nIn contrast, the full Syn-Chain1⊕2⊕3 generally per-\nforms worse than the baseline. Smaller models\nhave limited reasoning capabilities, and even with\nsupervised learning, they may struggle to maintain\naccuracy in multi-step reasoning.\nWhen\ncomparing\nSyn-Chain1⊘3\nand\nSyn-\nChain1⊘2⊘3, the latter demonstrates superior per-\nformance, with higher average F1 scores. This\nsuggests that learning from more related tasks ef-\nfectively enhances the model’s performance.\nImpact of Error Propagation. To investigate the\nimpact of error propagation, we conduct two com-\nparative experiments on Llama-3-8B trained with\nSyn-Chain1⊕2⊕3. In one experiment, the ground-\ntruth response from Step 1 is used in Step 2 during\ninference, ensuring that no errors are introduced in\nthe ﬁrst step. In the other experiment, ground-truth\nresponses of previous steps are used as inputs for\nboth Step 2 and Step 3, guaranteeing that the ﬁrst\ntwo steps are error-free. These experiments allow\nus to evaluate the impact of errors introduced at\ndifferent stages of Syn-Chain.\nAs shown in Table 5, when only ˜\nR1 is used,\nthere is a slight performance improvement. How-\n",
  "7": "3129\nMethod\nRest14\nLap14\nRest15\nRest16\nAVG\nAcc\nF1\nAcc\nF1\nAcc\nF1\nAcc\nF1\nF1\nFull Fine-Tuning\nBERT-SPC\n84.11\n76.68\n77.59\n73.28\n83.48\n66.18\n90.10\n74.16\n72.58\nBERT-PT\n84.95\n76.96\n78.07\n75.08\n-\n-\n-\n-\n-\nBERT-RGAT\n85.18\n78.38\n78.21\n73.27\n82.84\n69.33\n90.91\n75.76\n74.19\nBERT-DualGCN\n87.13\n81.16\n81.80\n78.10\n-\n-\n-\n-\n-\nBERT-TGCN\n86.16\n79.95\n80.88\n77.03\n85.26\n71.69\n92.32\n77.29\n76.49\nBERT-SenticGCN\n86.92\n81.03\n82.12\n79.05\n85.32\n71.28\n91.07\n79.56\n77.73\nT5-base\n87.50\n81.44\n80.87\n76.69\n86.34\n72.03\n93.83\n77.92\n77.02\n+ Syn-Chain1⊕2⊕3\n81.07\n72.43\n78.19\n73.68\n78.59\n65.52\n90.58\n69.28\n70.02\n+ Syn-Chain1⊘3\n87.76\n81.20\n81.50\n78.03\n86.34\n73.13\n92.85\n77.54\n77.48\n+ Syn-Chain1⊘2⊘3\n88.12\n82.05\n81.22\n78.71\n86.34\n73.76\n93.50\n79.98\n78.63\n+ Syn-Chain1⊕2⊘3\n88.30\n82.47\n81.81\n79.12\n86.71\n74.17\n94.37\n80.15\n78.98\nFlan-T5-base\n87.58\n81.36\n81.66\n78.42\n86.90\n74.20\n93.34\n77.30\n77.82\n+ Syn-Chain1⊕2⊕3\n81.78\n72.23\n80.56\n76.09\n85.97\n67.47\n93.66\n74.26\n72.51\n+ Syn-Chain1⊘3\n86.69\n79.91\n82.28\n79.00\n85.05\n72.84\n92.50\n78.86\n77.65\n+ Syn-Chain1⊘2⊘3\n87.75\n82.47\n81.97\n78.72\n87.26\n76.04\n93.18\n76.77\n78.50\n+ Syn-Chain1⊕2⊘3\n88.39\n82.79\n83.22\n80.04\n87.82\n76.86\n93.50\n79.25\n79.73\nFlan-T5-large\n89.62\n84.19\n84.16\n81.04\n89.22\n78.08\n94.61\n84.62\n81.98\n+ Syn-Chain1⊕2⊕3\n85.98\n76.19\n80.09\n74.84\n88.00\n73.70\n94.80\n80.25\n76.25\n+ Syn-Chain1⊘3\n89.08\n84.15\n84.01\n81.44\n87.82\n78.54\n92.12\n83.31\n81.86\n+ Syn-Chain1⊘2⊘3\n89.71\n84.81\n84.63\n82.61\n89.81\n78.91\n95.12\n84.61\n82.74\n+ Syn-Chain1⊕2⊘3\n90.35\n85.30\n85.10\n82.72\n90.22\n79.04\n95.29\n85.03\n83.62\nLoRA Fine-Tuning\nLlama-2-7B\n89.73\n84.86\n84.79\n81.85\n90.22\n78.52\n94.48\n82.26\n81.87\n+ Syn-Chain1⊕2⊕3\n86.87\n79.61\n82.75\n79.43\n90.03\n69.43\n93.18\n72.86\n75.33\n+ Syn-Chain1⊘3\n89.91\n84.67\n84.63\n82.41\n89.66\n81.17\n94.80\n81.94\n82.55\n+ Syn-Chain1⊘2⊘3\n90.44\n85.14\n85.57\n83.07\n91.88\n82.31\n94.96\n84.43\n83.74\n+ Syn-Chain1⊕2⊘3\n90.71\n86.10\n85.89\n83.28\n91.88\n82.71\n94.96\n84.78\n84.22\nLlama-3-8B\n90.62\n85.10\n84.48\n81.33\n91.69\n81.39\n94.96\n83.46\n82.82\n+ Syn-Chain1⊕2⊕3\n87.23\n81.67\n82.60\n79.27\n89.37\n72.30\n93.99\n77.70\n77.74\n+ Syn-Chain1⊘3\n91.25\n86.94\n86.05\n83.45\n92.61\n82.80\n95.29\n83.41\n84.15\n+ Syn-Chain1⊘2⊘3\n90.71\n86.25\n86.52\n84.05\n91.51\n81.36\n95.45\n84.89\n84.14\n+ Syn-Chain1⊕2⊘3\n91.87\n87.61\n86.36\n84.21\n91.88\n83.65\n95.94\n85.50\n85.24\nTable 4: Experimental results of ﬁne-tuned models on ABSA. AVG is the average performance of the model across\nfour data sets. The superscript numbers 1, 2, and 3 correspond to the three sequential steps in the Syn-Chain process.\n⊕and ⊘denote link and break, respectively. Different combinations represent various training strategies.\nInput\nRest14\nLap14\nRest15\nRest16\nAcc\nF1\nAcc\nF1\nAcc\nF1\nAcc\nF1\nSyn-Chain1⊕2⊕3\n87.23\n81.67\n82.60\n79.27\n89.37\n72.30\n93.99\n77.70\nw/ ˜\nR1\n88.14\n82.57\n83.54\n80.35\n89.72\n73.70\n94.50\n78.95\nw/ ˜\nR1, ˜\nR2\n97.94\n97.00\n97.02\n96.49\n94.49\n94.22\n95.19\n92.53\nTable 5: Performance of Syn-Chain ﬁne-tuned Llama-3-8B using ground-truth ˜\nR1 or ˜\nR2 during inference, illustrating\nthe impact of error propagation.\never, when both ˜\nR1 and ˜\nR2 are utilized, the model\nachieves a signiﬁcant boost in performance. The\nmajority of errors tend to originate from Step 2, as\nthe response in this step typically conveys senti-\nment polarity, which strongly inﬂuences the ﬁnal\noutcome. Therefore, the use of the ground-truth ˜\nR2\nresults in a substantial performance improvement\nby minimizing errors propagated from Step 2.\n",
  "8": "3130\nInput\nRest14\nLap14\nAcc\nF1\nAcc\nF1\nX, A\n87.50\n81.44\n80.87\n76.69\nX, A, ˜\nR1\n87.32\n81.17\n81.81\n77.98\nX, A, ˜\nR2\n98.12\n97.29\n97.49\n97.07\nTable 6: Automatic evaluation results of ˜\nR1 and ˜\nR2.\nPerform sentiment classiﬁcation on different inputs us-\ning the ﬁne-tuned T5.\n5.3\nSyn-Chain Data Quality Analysis\nWe use GPT-3.5 to automatically generate Syn-\nChain reasoning information for supervised learn-\ning. To better understand the quality of the gener-\nated data, we conduct data quality analysis through\nboth automatic and manual evaluation. In the third\nstep of training, we exclude the reasoning informa-\ntion. Therefore the training data of Step 3 can be\nguaranteed to be consistent with sentiment labels.\nWe do not assess its quality.\nAutomatic Evaluation. We train two specialized\naspect-level sentiment classiﬁers based on T5 to\nassess the inﬂuence of ˜\nR1 and ˜\nR2. These clas-\nsiﬁers take X and A as inputs, with ˜\nR1 or ˜\nR2\nadded respectively, formulated as P(y|X, A, ˜\nR1)\nand P(y|X, A, ˜\nR2). We train and test the models\non the Rest14 and Lap14 datasets. The results,\nshown in Tabel 6, indicate that ˜\nR1 has no signiﬁ-\ncant impact on sentiment classiﬁcation, while ˜\nR2\nachieves near-perfect performance. This is likely\nbecause ˜\nR2 typically contains words that explicitly\nexpress sentiment polarity, indicating a high consis-\ntency between ˜\nR2 and sentiment labels. ˜\nR1 is less\ndirectly useful for sentiment classiﬁcation, making\nits quality difﬁcult to assess automatically.\nThis raises the question of whether the improve-\nment in Syn-Chain is driven by syntactic parsing\nor opinion extraction. In other words, does solely\nrelying on opinion extraction lead to enhanced sen-\ntiment classiﬁcation performance? We explore in\nSection 5.4.\nManual Evaluation. We randomly selected 100\nsamples, 50 from the Lap14 dataset and 50 from\nthe Rest14 dataset, to assess the correctness of the\ngenerated ˜\nR1 and ˜\nR2. Three master’s students pro-\nﬁcient in English and NLP independently evaluated\nthe correctness of these outputs. The evaluation\nof ˜\nR1 focus on its alignment with the provided\nsyntactic information, while the evaluation of ˜\nR2\ncenter on its consistency with the original sentence\nMethod\nRest14\nLap14\nAcc\nF1\nAcc\nF1\nGPT-3.5\n82.50\n71.27\n79.62\n74.63\n+ Syn-Chain1⊕3\n82.05\n69.01\n77.56\n71.68\n+ Syn-Chain2⊕3\n83.83\n69.38\n77.27\n67.15\n+ Syn-Chain1⊕2⊕3\n85.08\n78.61\n80.09\n76.45\nLlama-3-8B\n90.62\n85.10\n84.48\n81.33\n+ Syn-Chain1⊘3\n91.25\n86.94\n86.05\n83.45\n+ Syn-Chain2⊘3\n90.35\n85.28\n86.05\n83.86\n+ Syn-Chain1⊕2⊘3\n91.87\n87.61\n86.36\n84.21\nTable 7: Performance comparison of models under zero-\nshot and ﬁne-tuned settings using syntactic information,\nopinion extraction, and both.\nand sentiment label. In cases of disagreement, a\nconsensus is reached through discussion. GPT-3.5\nexhibits a low error rate of only 5% for both ˜\nR1 and\n˜\nR2, demonstrating strong comprehension capabili-\nties. The few errors observed in ˜\nR1 are primarily\ndue to incorrect dependency relation descriptions,\nwhereas those in ˜\nR2 are mainly attributed to incon-\nsistencies between the generated content and the\nsentiment polarity.\n5.4\nRole of Syntax and Opinion\nAs shown in Table 7, we conduct experiments un-\nder both zero-shot GPT-3.5 and ﬁne-tuned Llama-3-\n8B, comparing models that rely solely on syntactic\ninformation, those that rely solely on opinion ex-\ntraction, and those that utilize both. Experimental\nresults demonstrate that models relying on a single\ninformation source experience a performance de-\ncline. Speciﬁcally, for GPT-3.5, only by combining\nthe syntactic analysis step and the opinion extrac-\ntion step can we signiﬁcantly surpass the baseline.\nLlama-3-8B under ﬁne-tuned settings also validates\nthis synergistic effect. These ﬁndings indicate that\nsyntactic information and opinion extraction play\ncomplementary roles in sentiment classiﬁcation,\njointly promoting a deeper understanding of text\nby the model. The Syn-Chain design effectively\nenhances model performance by organically com-\nbining these two information sources, validating its\neffectiveness in ABSA.\n6\nConclusion\nIn this paper, we propose Syn-Chain, a novel frame-\nwork that integrates syntactic information to im-\nprove the performance of LLMs in ABSA. By\ndecomposing ABSA into three sequential steps:\nsyntactic parsing, opinion extraction, and senti-\n",
  "9": "3131\nment classiﬁcation, Syn-Chain enables more ef-\nfective utilization of syntactic dependencies and\nenhances the model’s reasoning capabilities. Fur-\nthermore, we ﬁne-tune smaller LLMs and intro-\nduce a breaking-chain strategy that mitigates error\npropagation, ensuring accurate sentiment classiﬁ-\ncation even when earlier steps contain errors. Our\nexperimental results demonstrate that Syn-Chain\nsigniﬁcantly improves LLMs’ zero-shot ability in\nABSA and out ﬁne-tuned performance signiﬁcantly\noutperforms previous models. This work highlights\nthe potential of syntactic information to enhance\nLLM performance in ABSA tasks.\nLimitations\nWhile our proposed Syn-Chain framework demon-\nstrates notable improvements in ABSA, there are\nseveral limitations to our approach. First, ABSA\nconsists of various sub-tasks beyond sentiment clas-\nsiﬁcation, such as aspect term and opinion extrac-\ntion. LLMs generally struggle with extraction tasks,\nparticularly in identifying the correct boundaries of\naspect terms and opinions, leading to inconsisten-\ncies with the golden labels. This limitation arises\nfrom the inherent difﬁculty LLMs face in aligning\nextracted spans with precise human annotations.\nAlthough our method incorporates syntactic infor-\nmation to improve sentiment classiﬁcation, its ef-\nfectiveness in signiﬁcantly enhancing extraction\ntasks remains uncertain.\nFurthermore, although Syn-Chain demonstrates\nthat LLMs can effectively understand and utilize\nsyntactic information, the datasets used in our ex-\nperiments are primarily derived from user reviews,\nwhich generally feature relatively simple syntactic\nstructures. This simplicity may not fully capture\nthe complexity of more diverse sentence structures\nfound in other domains, such as news articles or\nacademic texts. As a result, the applicability and\nrobustness of Syn-Chain in handling more complex\nsyntactic dependencies remain to be fully evalu-\nated. Future work should explore its effectiveness\nacross a wider range of text types with more intri-\ncate syntactic characteristics to determine whether\nthe framework can generalize to more complex lin-\nguistic environments.\nEthics Statement\nIn this research, we utilize publicly available\ndatasets, which have been widely used in the aca-\ndemic community. As such, there are no concerns\nregarding copyright infringement or proprietary\ndata. The use of these datasets adheres to the terms\nof use speciﬁed by the dataset providers.\nFor the manual evaluation, we employed three\nmaster’s students, each tasked with analyzing 300\nsamples. Each evaluator were compensated $100\nfor their time and effort. This payment rate reﬂects\na fair and competitive compensation for the amount\nof work involved. Additionally, all evaluators par-\nticipated voluntarily and were informed of the tasks\nand compensation prior to the study.\nAcknowledgements\nThis work was supported by the National Language\nand Character Research Base.\nReferences\nAI@Meta. 2024. Llama 3 model card.\nHao Chen, Zepeng Zhai, Fangxiang Feng, Ruifan Li,\nand Xiaojie Wang. 2022. Enhanced multi-channel\ngraph convolutional network for aspect sentiment\ntriplet extraction. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2974–2985,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nJiajun Cheng, Shenglin Zhao, Jiani Zhang, Irwin King,\nXin Zhang, and Hui Wang. 2017. Aspect-level senti-\nment classiﬁcation with heat (hierarchical attention)\nnetwork. In Proceedings of the 2017 ACM on Con-\nference on Information and Knowledge Management,\npage 97–106, New York, USA. Association for Com-\nputing Machinery.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2024. Scaling instruction-ﬁnetuned\nlanguage models. Journal of Machine Learning Re-\nsearch, 25(70):1–53.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n",
  "10": "3132\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan\nMa, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu,\nBaobao Chang, Xu Sun, Lei Li, and Zhifang Sui.\n2024. A survey on in-context learning. Preprint,\narXiv:2301.00234.\nFeifan Fan, Yansong Feng, and Dongyan Zhao. 2018.\nMulti-grained attention network for aspect-level sen-\ntiment classiﬁcation. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3433–3442, Brussels, Belgium.\nAssociation for Computational Linguistics.\nRui Fan, Tingting He, Menghan Chen, Mengyuan\nZhang, Xinhui Tu, and Ming Dong. 2024.\nDual\ncauses generation assisted model for multimodal\naspect-based sentiment classiﬁcation. IEEE Trans-\nactions on Neural Networks and Learning Systems,\npages 1–15.\nHao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, and\nTat-Seng Chua. 2023. Reasoning implicit sentiment\nwith chain-of-thought prompting. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 1171–1182, Toronto, Canada. Association for\nComputational Linguistics.\nShuqin Gu, Lipeng Zhang, Yuexian Hou, and Yin Song.\n2018. A position-aware bidirectional attention net-\nwork for aspect-level sentiment analysis. In Pro-\nceedings of the 27th International Conference on\nComputational Linguistics, pages 774–784, Santa Fe,\nNew Mexico, USA. Association for Computational\nLinguistics.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2023.\nLarge language models are reasoning teachers. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 14852–14882, Toronto, Canada.\nAssociation for Computational Linguistics.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nBinxuan Huang and Kathleen Carley. 2019. Syntax-\naware aspect level sentiment classiﬁcation with graph\nattention networks. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 5469–5477, Hong Kong, China. As-\nsociation for Computational Linguistics.\nQingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, and\nMin Yang. 2019.\nA challenge dataset and effec-\ntive models for aspect-based sentiment analysis. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 6280–\n6285, Hong Kong, China. Association for Computa-\ntional Linguistics.\nIrfan Ali Kandhro, Fayyaz Ali, Mueen Uddin, Asadullah\nKehar, and Selvakumar Manickam. 2024. Exploring\naspect-based sentiment analysis: an in-depth review\nof current methods and prospects for advancement.\nKnowledge and Information Systems, pages 1–31.\nLishuang Li, Yang Liu, and AnQiao Zhou. 2018. Hier-\narchical attention based position-aware network for\naspect-level sentiment analysis. In Proceedings of\nthe 22nd Conference on Computational Natural Lan-\nguage Learning, pages 181–189, Brussels, Belgium.\nAssociation for Computational Linguistics.\nRuifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xi-\naojie Wang, and Eduard Hovy. 2021. Dual graph\nconvolutional networks for aspect-based sentiment\nanalysis. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6319–6329, Online. Association for Computa-\ntional Linguistics.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,\nXinlu Zhang, Zekun Li, Hong Wang, Jing Qian,\nBaolin Peng, Yi Mao, Wenhu Chen, and Xifeng\nYan. 2022.\nExplanations from large language\nmodels make small reasoners better.\nPreprint,\narXiv:2210.06726.\nBin Liang, Hang Su, Lin Gui, Erik Cambria, and\nRuifeng Xu. 2022.\nAspect-based sentiment anal-\nysis via affective knowledge enhanced graph con-\nvolutional networks.\nKnowledge-Based Systems,\n235:107643.\nJiangming Liu and Yue Zhang. 2017. Attention model-\ning for targeted sentiment. In Proceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 2, Short\nPapers, pages 572–577, Valencia, Spain. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. Preprint, arXiv:1907.11692.\nDehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng\nWang. 2017.\nInteractive attention networks for\naspect-level sentiment classiﬁcation. In Proceedings\nof the Twenty-Sixth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI-17, pages 4068–4074.\nAmbreen Nazir, Yuan Rao, Lianwei Wu, and Ling Sun.\n2022. Issues and challenges of aspect-based senti-\nment analysis: A comprehensive survey. IEEE Trans-\nactions on Affective Computing, 13(2):845–863.\n",
  "11": "3133\nMinh Hieu Phan and Philip O. Ogunbona. 2020. Mod-\nelling context and syntactical features for aspect-\nbased sentiment analysis. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3211–3220, Online. Association\nfor Computational Linguistics.\nMaria Pontiki, Dimitris Galanis, Haris Papageorgiou,\nIon Androutsopoulos, Suresh Manandhar, Moham-\nmad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan\nZhao, Bing Qin, Orphée De Clercq, Véronique\nHoste, Marianna Apidianaki, Xavier Tannier, Na-\ntalia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel,\nSalud María Jiménez-Zafra, and Gül¸sen Eryi˘git.\n2016. SemEval-2016 task 5: Aspect based sentiment\nanalysis. In Proceedings of the 10th International\nWorkshop on Semantic Evaluation (SemEval-2016),\npages 19–30, San Diego, California. Association for\nComputational Linguistics.\nMaria Pontiki, Dimitris Galanis, Haris Papageorgiou,\nSuresh Manandhar, and Ion Androutsopoulos. 2015.\nSemEval-2015 task 12: Aspect based sentiment anal-\nysis. In Proceedings of the 9th International Work-\nshop on Semantic Evaluation (SemEval 2015), pages\n486–495, Denver, Colorado. Association for Compu-\ntational Linguistics.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-\nris Papageorgiou, Ion Androutsopoulos, and Suresh\nManandhar. 2014. SemEval-2014 task 4: Aspect\nbased sentiment analysis. In Proceedings of the 8th\nInternational Workshop on Semantic Evaluation (Se-\nmEval 2014), pages 27–35, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nSubhro Roy, Samuel Thomson, Tongfei Chen, Richard\nShin, Adam Pauls, Jason Eisner, and Benjamin\nVan Durme. 2023. Benchclamp: A benchmark for\nevaluating language models on syntactic and seman-\ntic parsing. In Advances in Neural Information Pro-\ncessing Systems, volume 36, pages 49814–49829.\nCurran Associates, Inc.\nYouwei Song, Jiahai Wang, Tao Jiang, Zhiyue Liu,\nand Yanghui Rao. 2019. Targeted sentiment clas-\nsiﬁcation with attentional encoder network. Preprint,\narXiv:1902.09314.\nKai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao,\nand Xudong Liu. 2019. Aspect-level sentiment analy-\nsis via convolution over dependency tree. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5679–5688, Hong\nKong, China. Association for Computational Linguis-\ntics.\nDuyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.\n2016. Effective LSTMs for target-dependent sen-\ntiment classiﬁcation. In Proceedings of COLING\n2016, the 26th International Conference on Compu-\ntational Linguistics: Technical Papers, pages 3298–\n3307, Osaka, Japan. The COLING 2016 Organizing\nCommittee.\nYuanhe Tian, Guimin Chen, and Yan Song. 2021.\nAspect-based sentiment analysis with type-aware\ngraph convolutional networks and layer ensemble.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2910–2922, Online. Association for Computa-\ntional Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and ﬁne-\ntuned chat models. Preprint, arXiv:2307.09288.\nKai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan,\nand Rui Wang. 2020. Relational graph attention net-\nwork for aspect-based sentiment analysis. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3229–\n3238, Online. Association for Computational Lin-\nguistics.\nQianlong Wang, Hongling Xu, Keyang Ding, Bin Liang,\nand Ruifeng Xu. 2024a. In-context example retrieval\nfrom multi-perspectives for few-shot aspect-based\nsentiment analysis. In Proceedings of the 2024 Joint\nInternational Conference on Computational Linguis-\ntics, Language Resources and Evaluation (LREC-\nCOLING 2024), pages 8975–8985, Torino, Italia.\nELRA and ICCL.\nYequan Wang, Minlie Huang, Xiaoyan Zhu, and\nLi Zhao. 2016. Attention-based LSTM for aspect-\nlevel sentiment classiﬁcation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 606–615, Austin, Texas.\nAssociation for Computational Linguistics.\n",
  "12": "3134\nZengzhi Wang, Qiming Xie, Yi Feng, Zixiang Ding,\nZinong Yang, and Rui Xia. 2024b. Is chatgpt a good\nsentiment analyzer? a preliminary study. Preprint,\narXiv:2304.04339.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research. Survey Certiﬁca-\ntion.\nShengqiong Wu, Hao Fei, Yafeng Ren, Donghong Ji,\nand Jingye Li. 2021. Learn from syntax: Improving\npair-wise aspect and opinion terms extractionwith\nrich syntactic knowledge. In International Joint Con-\nference on Artiﬁcial Intelligence.\nHu Xu, Bing Liu, Lei Shu, and Philip Yu. 2019. BERT\npost-training for review reading comprehension and\naspect-based sentiment analysis. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2324–2335, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMin Yang, Wenting Tu, Jingxuan Wang, Fei Xu, and\nXiaojun Chen. 2017. Attention based lstm for target\ndependent sentiment classiﬁcation. Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, 31(1).\nSonghua Yang, Xinke Jiang, Hanjie Zhao, Wenxuan\nZeng, Hongde Liu, and Yuxiang Jia. 2024. FaiMA:\nFeature-aware in-context learning for multi-domain\naspect-based sentiment analysis. In Proceedings of\nthe 2024 Joint International Conference on Compu-\ntational Linguistics, Language Resources and Eval-\nuation (LREC-COLING 2024), pages 7089–7100,\nTorino, Italia. ELRA and ICCL.\nSonghua Yang, Tengxun Zhang, Hongfei Xu, and Yuxi-\nang Jia. 2023. Improving aspect sentiment triplet ex-\ntraction with perturbed masking and edge-enhanced\nsentiment graph attention network. In 2023 Interna-\ntional Joint Conference on Neural Networks (IJCNN),\npages 1–8.\nChen Zhang, Qiuchi Li, and Dawei Song. 2019.\nSyntax-aware aspect-level sentiment classiﬁcation\nwith proximity-weighted convolution network. Pro-\nceedings of the 42nd International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval.\nWenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan,\nand Lidong Bing. 2023. Sentiment analysis in the era\nof large language models: A reality check. Preprint,\narXiv:2305.15005.\nZheng Zhang, Zili Zhou, and Yanna Wang. 2022.\nSSEGCN: Syntactic and semantic enhanced graph\nconvolutional network for aspect-based sentiment\nanalysis. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4916–4925, Seattle, United States.\nAssociation for Computational Linguistics.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Hua\nJin, and Dacheng Tao. 2023. Knowledge graph aug-\nmented network towards multiview representation\nlearning for aspect-based sentiment analysis. IEEE\nTransactions on Knowledge and Data Engineering,\n35(10):10098–10111.\n",
  "13": "3135\nAppendix\nA\nPrompt Details\nThe prompt consists of two parts: the system\nprompt and the user prompt. The system prompt\ninforms the LLMs of their role, task, and basic\nrequirements, ensuring consistent and coherent re-\nsponses. For the three steps in Sys-Chain, we de-\nsign speciﬁc system prompts as illustrated in Fig. 3.\nThe system prompts for Step 1 and Step 2 primarily\nfocus on limiting the response length, while the\nsystem prompt for Step 3 guides the format of the\ngenerated content to facilitate accurate extraction\nand classiﬁcation of sentiment polarity. The user\nprompt, shown in Fig. 4, provides an example of\nSyn-Chain completing ABSA. In Step 1, we pro-\nvide the syntactic information of the sentence and\nthe corresponding meaning. Fig. 5 presents the re-\nsults of GPT-4o’s syntactic parsing, demonstrating\na more structured and readable output.\nYou are an AI assistant that helps people find \ninformation.  Please refine your reply and \nensure its accuracy. The reply length is \nlimited to 200 words.\nStep 1: Syntactic Parsing\nYou are an AI assistant that helps people find \ninformation.  Please refine your reply and \nensure its accuracy. The reply length is \nlimited to 120 words.\nStep 2: Opinion Extraction\nYou are an sentiment analysis expert. I will \nprovide you with a sentence and a certain \naspect mentioned in the sentence. Please \nanalyze the sentiment polarity of that aspect \nin a given sentence. \nOutput: The sentiment towards {Aspect} in \nthe given sentence is {positive, negative or \nneutral}. Because...\nStep 3: Sentiment Classification\nFigure 3: System content prompts in Syn-Chain\n",
  "14": "3136\nStep 1: Syntactic Parsing\n\"Meatball parm\" is a compound noun phrase in this sentence, acting as the subject of the verb phrase \"is \ngood\".  The dependency parse shows both words are nouns (\"NN\") and \"meatball\" modifies \" parm\" \n(compound relationship). This tells us the sentence expresses a positive opinion about a dish called \n\"meatball parm\".\nSentence: \" I think the meatball parm is good.\"\n4\n3\n2\n1\nID\n5\n7\n6\n8\nmeatball\nthe\nthink\nI\nTEXT\nparm\ngood\nis\n.\nmeatball\nthe\nthink\nI\nLEMMA\nparm\ngood\nbe\n.\nNOUN\nDET\nVERB\nPRON\nPOS\nNOUN\nADJ\nAUX\nPUNCT\nNN\nDT\nVBP\nPRP\nTAG\nNN\nJJ\nVBZ\n.\n-\n-\n-\n-\nFEATS\n-\n-\n-\n-\n5\n5\n2\n2\nHEAD\n6\n6\n2\n2\ncompound\ndet\nroot\nnsubj\nDEPREL\nnsubj\nacomp\nccomp\npunct\n-\n-\n-\n-\nDEPS\n-\n-\n-\n-\n-\n-\n-\n-\nMISC\n-\n-\n-\n-\nThe following is the CoNLL-U format for the syntactic dependency relationship of this sentence:\nEach row in the table represents a word in the sentence, and each column represents some specific \nproperties of the word, including ID (Word position in the sentence, starting from 1), TEXT (word itself), \nLemma (the base form of the word), POS (the simple universal part-of-speech tag), TAG (the detailed \npart-of-speech tag), FEATS (other grammatical features, empty here), HEAD (the header word of the \ndependency relationship of the current word), DEPREL (dependency label, describing the relationship \nbetween the current word and the header word), DEPS (word dependency, empty here), MISC (other \nadditional information, empty here).\nBased on the syntactic dependency information of the sentence, analyze information related to \"Meatball \nparm\" in the sentence.\nStep 2: Opinion Extraction\nThe speaker believes that the \"meatball parm\" is good. This is clear from the sentence's structure where \n\"meatball parm,\" a compound noun referring to a food dish, is the subject, and \"is good\" is the predicate \nattributing a positive quality to the subject.\nSentence: \" I think the meatball parm is good.\"\n\"Meatball parm\" is a compound noun phrase in this sentence, acting as the subject of the verb phrase \"is \ngood\".  The dependency parse shows both words are nouns (\"NN\") and \"meatball\" modifies \" parm\" \n(compound relationship). This tells us the sentence expresses a positive opinion about a dish called \n\"meatball parm\".\nConsidering the context and information related to \"meatball parm\", what is the speaker's opinion towards \n\"meatball parm\"?\nSentence: \" I think the meatball parm is good.\"\nThe speaker believes that the \"meatball parm\" is good. This is clear from the sentence's structure where \n\"meatball parm,\" a compound noun referring to a food dish, is the subject, and \"is good\" is the predicate \nattributing a positive quality to the subject.\nBased on the common sense and such speaker's opinion, what is the sentiment polarity towards \"meatball \nparm\"?\nStep 3: Sentiment Classfication\nThe sentiment towards meatball parm in the given sentence is positive. Because the sentence directly \nstates \"the meatball parm is good,\" using the positive adjective \"good\" to describe it.\nFigure 4: A zero-shot Syn-Chain prompt for GPT-3.5.\n",
  "15": "3137\nStep 1: Syntactic Parsing\nSentence: \" I think the meatball parm is good.\"\n4\n3\n2\n1\nID\n5\n7\n6\n8\nmeatball\nthe\nthink\nI\nTEXT\nparm\ngood\nis\n.\nmeatball\nthe\nthink\nI\nLEMMA\nparm\ngood\nbe\n.\nNOUN\nDET\nVERB\nPRON\nPOS\nNOUN\nADJ\nAUX\nPUNCT\nNN\nDT\nVBP\nPRP\nTAG\nNN\nJJ\nVBZ\n.\n-\n-\n-\n-\nFEATS\n-\n-\n-\n-\n5\n5\n2\n2\nHEAD\n6\n6\n2\n2\ncompound\ndet\nroot\nnsubj\nDEPREL\nnsubj\nacomp\nccomp\npunct\n-\n-\n-\n-\nDEPS\n-\n-\n-\n-\n-\n-\n-\n-\nMISC\n-\n-\n-\n-\nThe following is the CoNLL-U format for the syntactic dependency relationship of this sentence:\nEach row in the table represents a word in the sentence, and each column represents some specific \nproperties of the word, including ID (Word position in the sentence, starting from 1), TEXT (word itself), \nLemma (the base form of the word), POS (the simple universal part-of-speech tag), TAG (the detailed \npart-of-speech tag), FEATS (other grammatical features, empty here), HEAD (the header word of the \ndependency relationship of the current word), DEPREL (dependency label, describing the relationship \nbetween the current word and the header word), DEPS (word dependency, empty here), MISC (other \nadditional information, empty here).\nBased on the syntactic dependency information of the sentence, analyze information related to \"Meatball \nparm\" in the sentence.\nIn the sentence \"I think the meatball parm is good,\" the phrase \"meatball parm\" consists of two words: \n\"meatball\" and \"parm.\" Here is the analysis of these words based on the CoNLL-U format provided:\n1. meatball (ID 4)\n - Text: meatball\n - Lemma: meatball\n - POS: NOUN\n - Tag: NN\n - FEATS: _\n - HEAD: 5 (parm)\n - DEPREL: compound\n - DEPS: _\n - MISC: _\n2. parm (ID 5)\n - Text: parm\n - Lemma: parm\n - POS: NOUN\n - Tag: NN\n - FEATS: _\n - HEAD: 6 (is)\n - DEPREL: nsubj\n - DEPS: _\n - MISC: _\nAnalysis:\n-\n \"meatball\" is a noun that functions as a compound modifier of \"parm.\" This means \"meatball\" is \nproviding additional descriptive information about \"parm.\"\n- \"parm\" is a noun that serves as the nominal subject (nsubj) of the verb \"is.\" This indicates that \n\"parm\" is the main subject of the clause \"parm is good.\"\nIn summary, \"meatball parm\" is a compound noun phrase where \"meatball\" modifies \"parm,\" and \n\"parm\" is the subject of the clause describing it as \"good.\"\nFigure 5: The result of GPT-4o output in syntactic parsing step.\n"
}