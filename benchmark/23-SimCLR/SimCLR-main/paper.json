{
  "1": "A Simple Framework for Contrastive Learning of Visual Representations\nTing Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton 1\nAbstract\nThis paper presents SimCLR: a simple framework\nfor contrastive learning of visual representations.\nWe simplify recently proposed contrastive self-\nsupervised learning algorithms without requiring\nspecialized architectures or a memory bank. In\norder to understand what enables the contrastive\nprediction tasks to learn useful representations,\nwe systematically study the major components of\nour framework. We show that (1) composition of\ndata augmentations plays a critical role in deﬁning\neffective predictive tasks, (2) introducing a learn-\nable nonlinear transformation between the repre-\nsentation and the contrastive loss substantially im-\nproves the quality of the learned representations,\nand (3) contrastive learning beneﬁts from larger\nbatch sizes and more training steps compared to\nsupervised learning. By combining these ﬁndings,\nwe are able to considerably outperform previous\nmethods for self-supervised and semi-supervised\nlearning on ImageNet. A linear classiﬁer trained\non self-supervised representations learned by Sim-\nCLR achieves 76.5% top-1 accuracy, which is a\n7% relative improvement over previous state-of-\nthe-art, matching the performance of a supervised\nResNet-50. When ﬁne-tuned on only 1% of the\nlabels, we achieve 85.8% top-5 accuracy, outper-\nforming AlexNet with 100× fewer labels. 1\n1. Introduction\nLearning effective visual representations without human\nsupervision is a long-standing problem. Most mainstream\napproaches fall into one of two classes: generative or dis-\ncriminative. Generative approaches learn to generate or\notherwise model pixels in the input space (Hinton et al.,\n2006; Kingma & Welling, 2013; Goodfellow et al., 2014).\n1Google Research, Brain Team. Correspondence to: Ting Chen\n<iamtingchen@google.com>.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\n1Code available at https://github.com/google-research/simclr.\n25\n50\n100\n200\n400\n626\nNumber of Parameters (Millions)\n55\n60\n65\n70\n75\nImageNet Top-1 Accuracy (%)\nInstDisc\nRotation\nBigBiGAN\nLA\nCPCv2\nCPCv2-L\nCMC\nAMDIM\nMoCo\nMoCo (2x)\nMoCo (4x)\nPIRL\nPIRL-ens.\nPIRL-c2x\nSimCLR\nSimCLR (2x)\nSimCLR (4x)\nSupervised\nFigure 1. ImageNet Top-1 accuracy of linear classiﬁers trained\non representations learned with different self-supervised meth-\nods (pretrained on ImageNet). Gray cross indicates supervised\nResNet-50. Our method, SimCLR, is shown in bold.\nHowever, pixel-level generation is computationally expen-\nsive and may not be necessary for representation learning.\nDiscriminative approaches learn representations using objec-\ntive functions similar to those used for supervised learning,\nbut train networks to perform pretext tasks where both the in-\nputs and labels are derived from an unlabeled dataset. Many\nsuch approaches have relied on heuristics to design pretext\ntasks (Doersch et al., 2015; Zhang et al., 2016; Noroozi &\nFavaro, 2016; Gidaris et al., 2018), which could limit the\ngenerality of the learned representations. Discriminative\napproaches based on contrastive learning in the latent space\nhave recently shown great promise, achieving state-of-the-\nart results (Hadsell et al., 2006; Dosovitskiy et al., 2014;\nOord et al., 2018; Bachman et al., 2019).\nIn this work, we introduce a simple framework for con-\ntrastive learning of visual representations, which we call\nSimCLR. Not only does SimCLR outperform previous work\n(Figure 1), but it is also simpler, requiring neither special-\nized architectures (Bachman et al., 2019; Hénaff et al., 2019)\nnor a memory bank (Wu et al., 2018; Tian et al., 2019; He\net al., 2019; Misra & van der Maaten, 2019).\nIn order to understand what enables good contrastive repre-\nsentation learning, we systematically study the major com-\nponents of our framework and show that:\narXiv:2002.05709v3  [cs.LG]  1 Jul 2020\n",
  "2": "A Simple Framework for Contrastive Learning of Visual Representations\n• Composition of multiple data augmentation operations\nis crucial in deﬁning the contrastive prediction tasks that\nyield effective representations. In addition, unsupervised\ncontrastive learning beneﬁts from stronger data augmen-\ntation than supervised learning.\n• Introducing a learnable nonlinear transformation be-\ntween the representation and the contrastive loss substan-\ntially improves the quality of the learned representations.\n• Representation learning with contrastive cross entropy\nloss beneﬁts from normalized embeddings and an appro-\npriately adjusted temperature parameter.\n• Contrastive learning beneﬁts from larger batch sizes and\nlonger training compared to its supervised counterpart.\nLike supervised learning, contrastive learning beneﬁts\nfrom deeper and wider networks.\nWe combine these ﬁndings to achieve a new state-of-the-art\nin self-supervised and semi-supervised learning on Ima-\ngeNet ILSVRC-2012 (Russakovsky et al., 2015). Under the\nlinear evaluation protocol, SimCLR achieves 76.5% top-1\naccuracy, which is a 7% relative improvement over previous\nstate-of-the-art (Hénaff et al., 2019). When ﬁne-tuned with\nonly 1% of the ImageNet labels, SimCLR achieves 85.8%\ntop-5 accuracy, a relative improvement of 10% (Hénaff et al.,\n2019). When ﬁne-tuned on other natural image classiﬁca-\ntion datasets, SimCLR performs on par with or better than\na strong supervised baseline (Kornblith et al., 2019) on 10\nout of 12 datasets.\n2. Method\n2.1. The Contrastive Learning Framework\nInspired by recent contrastive learning algorithms (see Sec-\ntion 7 for an overview), SimCLR learns representations\nby maximizing agreement between differently augmented\nviews of the same data example via a contrastive loss in\nthe latent space. As illustrated in Figure 2, this framework\ncomprises the following four major components.\n• A stochastic data augmentation module that transforms\nany given data example randomly resulting in two cor-\nrelated views of the same example, denoted ˜xi and ˜xj,\nwhich we consider as a positive pair. In this work, we\nsequentially apply three simple augmentations: random\ncropping followed by resize back to the original size, ran-\ndom color distortions, and random Gaussian blur. As\nshown in Section 3, the combination of random crop and\ncolor distortion is crucial to achieve a good performance.\n• A neural network base encoder f(·) that extracts repre-\nsentation vectors from augmented data examples. Our\nframework allows various choices of the network archi-\ntecture without any constraints. We opt for simplicity\nand adopt the commonly used ResNet (He et al., 2016)\n←−Representation −→\nx\n˜xi\n˜xj\nhi\nhj\nzi\nzj\nt ∼T\nt′ ∼T\nf(·)\nf(·)\ng(·)\ng(·)\nMaximize agreement\nFigure 2. A simple framework for contrastive learning of visual\nrepresentations. Two separate data augmentation operators are\nsampled from the same family of augmentations (t ∼T and\nt′ ∼T ) and applied to each data example to obtain two correlated\nviews. A base encoder network f(·) and a projection head g(·)\nare trained to maximize agreement using a contrastive loss. After\ntraining is completed, we throw away the projection head g(·) and\nuse encoder f(·) and representation h for downstream tasks.\nto obtain hi = f(˜xi) = ResNet(˜xi) where hi ∈Rd is\nthe output after the average pooling layer.\n• A small neural network projection head g(·) that maps\nrepresentations to the space where contrastive loss is\napplied. We use a MLP with one hidden layer to obtain\nzi = g(hi) = W (2)σ(W (1)hi) where σ is a ReLU non-\nlinearity. As shown in section 4, we ﬁnd it beneﬁcial to\ndeﬁne the contrastive loss on zi’s rather than hi’s.\n• A contrastive loss function deﬁned for a contrastive pre-\ndiction task. Given a set {˜xk} including a positive pair\nof examples ˜xi and ˜xj, the contrastive prediction task\naims to identify ˜xj in {˜xk}k̸=i for a given ˜xi.\nWe randomly sample a minibatch of N examples and deﬁne\nthe contrastive prediction task on pairs of augmented exam-\nples derived from the minibatch, resulting in 2N data points.\nWe do not sample negative examples explicitly. Instead,\ngiven a positive pair, similar to (Chen et al., 2017), we treat\nthe other 2(N −1) augmented examples within a minibatch\nas negative examples. Let sim(u, v) = u⊤v/∥u∥∥v∥de-\nnote the dot product between ℓ2 normalized u and v (i.e.\ncosine similarity). Then the loss function for a positive pair\nof examples (i, j) is deﬁned as\nℓi,j = −log\nexp(sim(zi, zj)/τ)\nP2N\nk=1 1[k̸=i] exp(sim(zi, zk)/τ)\n,\n(1)\nwhere 1[k̸=i] ∈{0, 1} is an indicator function evaluating to\n1 iff k ̸= i and τ denotes a temperature parameter. The ﬁ-\nnal loss is computed across all positive pairs, both (i, j)\nand (j, i), in a mini-batch. This loss has been used in\nprevious work (Sohn, 2016; Wu et al., 2018; Oord et al.,\n2018); for convenience, we term it NT-Xent (the normalized\ntemperature-scaled cross entropy loss).\n",
  "3": "A Simple Framework for Contrastive Learning of Visual Representations\nAlgorithm 1 SimCLR’s main learning algorithm.\ninput: batch size N, constant τ, structure of f, g, T .\nfor sampled minibatch {xk}N\nk=1 do\nfor all k ∈{1, . . . , N} do\ndraw two augmentation functions t∼T , t′ ∼T\n# the ﬁrst augmentation\n˜x2k−1 = t(xk)\nh2k−1 = f(˜x2k−1)\n# representation\nz2k−1 = g(h2k−1)\n# projection\n# the second augmentation\n˜x2k = t′(xk)\nh2k = f(˜x2k)\n# representation\nz2k = g(h2k)\n# projection\nend for\nfor all i ∈{1, . . . , 2N} and j ∈{1, . . . , 2N} do\nsi,j = z⊤\ni zj/(∥zi∥∥zj∥)\n# pairwise similarity\nend for\ndeﬁne ℓ(i, j) as ℓ(i, j)=−log\nexp(si,j/τ)\nP2N\nk=1 1[k̸=i] exp(si,k/τ)\nL =\n1\n2N\nPN\nk=1 [ℓ(2k−1, 2k) + ℓ(2k, 2k−1)]\nupdate networks f and g to minimize L\nend for\nreturn encoder network f(·), and throw away g(·)\nAlgorithm 1 summarizes the proposed method.\n2.2. Training with Large Batch Size\nTo keep it simple, we do not train the model with a memory\nbank (Wu et al., 2018; He et al., 2019). Instead, we vary\nthe training batch size N from 256 to 8192. A batch size\nof 8192 gives us 16382 negative examples per positive pair\nfrom both augmentation views. Training with large batch\nsize may be unstable when using standard SGD/Momentum\nwith linear learning rate scaling (Goyal et al., 2017). To\nstabilize the training, we use the LARS optimizer (You et al.,\n2017) for all batch sizes. We train our model with Cloud\nTPUs, using 32 to 128 cores depending on the batch size.2\nGlobal BN. Standard ResNets use batch normaliza-\ntion (Ioffe & Szegedy, 2015). In distributed training with\ndata parallelism, the BN mean and variance are typically\naggregated locally per device. In our contrastive learning,\nas positive pairs are computed in the same device, the model\ncan exploit the local information leakage to improve pre-\ndiction accuracy without improving representations. We ad-\ndress this issue by aggregating BN mean and variance over\nall devices during the training. Other approaches include\nshufﬂing data examples across devices (He et al., 2019), or\nreplacing BN with layer norm (Hénaff et al., 2019).\n2With 128 TPU v3 cores, it takes ∼1.5 hours to train our\nResNet-50 with a batch size of 4096 for 100 epochs.\nA\nB\n(a) Global and local views.\nC\nD\n(b) Adjacent views.\nFigure 3. Solid rectangles are images, dashed rectangles are ran-\ndom crops. By randomly cropping images, we sample contrastive\nprediction tasks that include global to local view (B →A) or\nadjacent view (D →C) prediction.\n2.3. Evaluation Protocol\nHere we lay out the protocol for our empirical studies, which\naim to understand different design choices in our framework.\nDataset and Metrics. Most of our study for unsupervised\npretraining (learning encoder network f without labels)\nis done using the ImageNet ILSVRC-2012 dataset (Rus-\nsakovsky et al., 2015). Some additional pretraining experi-\nments on CIFAR-10 (Krizhevsky & Hinton, 2009) can be\nfound in Appendix B.9. We also test the pretrained results\non a wide range of datasets for transfer learning. To evalu-\nate the learned representations, we follow the widely used\nlinear evaluation protocol (Zhang et al., 2016; Oord et al.,\n2018; Bachman et al., 2019; Kolesnikov et al., 2019), where\na linear classiﬁer is trained on top of the frozen base net-\nwork, and test accuracy is used as a proxy for representation\nquality. Beyond linear evaluation, we also compare against\nstate-of-the-art on semi-supervised and transfer learning.\nDefault setting. Unless otherwise speciﬁed, for data aug-\nmentation we use random crop and resize (with random\nﬂip), color distortions, and Gaussian blur (for details, see\nAppendix A). We use ResNet-50 as the base encoder net-\nwork, and a 2-layer MLP projection head to project the\nrepresentation to a 128-dimensional latent space. As the\nloss, we use NT-Xent, optimized using LARS with learning\nrate of 4.8 (= 0.3 × BatchSize/256) and weight decay of\n10−6. We train at batch size 4096 for 100 epochs.3 Fur-\nthermore, we use linear warmup for the ﬁrst 10 epochs,\nand decay the learning rate with the cosine decay schedule\nwithout restarts (Loshchilov & Hutter, 2016).\n3. Data Augmentation for Contrastive\nRepresentation Learning\nData augmentation deﬁnes predictive tasks. While data\naugmentation has been widely used in both supervised and\nunsupervised representation learning (Krizhevsky et al.,\n3Although max performance is not reached in 100 epochs, rea-\nsonable results are achieved, allowing fair and efﬁcient ablations.\n",
  "4": "A Simple Framework for Contrastive Learning of Visual Representations\n(a) Original\n(b) Crop and resize\n(c) Crop, resize (and ﬂip) (d) Color distort. (drop) (e) Color distort. (jitter)\n(f) Rotate {90◦, 180◦, 270◦}\n(g) Cutout\n(h) Gaussian noise\n(i) Gaussian blur\n(j) Sobel ﬁltering\nFigure 4. Illustrations of the studied data augmentation operators. Each augmentation can transform data stochastically with some internal\nparameters (e.g. rotation degree, noise level). Note that we only test these operators in ablation, the augmentation policy used to train our\nmodels only includes random crop (with ﬂip and resize), color distortion, and Gaussian blur. (Original image cc-by: Von.grzanka)\n2012; Hénaff et al., 2019; Bachman et al., 2019), it has\nnot been considered as a systematic way to deﬁne the con-\ntrastive prediction task. Many existing approaches deﬁne\ncontrastive prediction tasks by changing the architecture.\nFor example, Hjelm et al. (2018); Bachman et al. (2019)\nachieve global-to-local view prediction via constraining the\nreceptive ﬁeld in the network architecture, whereas Oord\net al. (2018); Hénaff et al. (2019) achieve neighboring view\nprediction via a ﬁxed image splitting procedure and a con-\ntext aggregation network. We show that this complexity can\nbe avoided by performing simple random cropping (with\nresizing) of target images, which creates a family of predic-\ntive tasks subsuming the above mentioned two, as shown in\nFigure 3. This simple design choice conveniently decouples\nthe predictive task from other components such as the neural\nnetwork architecture. Broader contrastive prediction tasks\ncan be deﬁned by extending the family of augmentations\nand composing them stochastically.\n3.1. Composition of data augmentation operations is\ncrucial for learning good representations\nTo systematically study the impact of data augmentation,\nwe consider several common augmentations here. One type\nof augmentation involves spatial/geometric transformation\nof data, such as cropping and resizing (with horizontal\nﬂipping), rotation (Gidaris et al., 2018) and cutout (De-\nVries & Taylor, 2017). The other type of augmentation\ninvolves appearance transformation, such as color distortion\n(including color dropping, brightness, contrast, saturation,\nhue) (Howard, 2013; Szegedy et al., 2015), Gaussian blur,\nand Sobel ﬁltering. Figure 4 visualizes the augmentations\nthat we study in this work.\nCrop\nCutout\nColor\nSobel\nNoise\nBlur\nRotate\nAverage\n2nd transformation\nCrop\nCutout\nColor\nSobel\nNoise\nBlur\nRotate\n1st transformation\n33.1\n33.9\n56.3\n46.0\n39.9\n35.0\n30.2\n39.2\n32.2\n25.6\n33.9\n40.0\n26.5\n25.2\n22.4\n29.4\n55.8\n35.5\n18.8\n21.0\n11.4\n16.5\n20.8\n25.7\n46.2\n40.6\n20.9\n4.0\n9.3\n6.2\n4.2\n18.8\n38.8\n25.8\n7.5\n7.6\n9.8\n9.8\n9.6\n15.5\n35.1\n25.2\n16.6\n5.8\n9.7\n2.6\n6.7\n14.5\n30.0\n22.5\n20.7\n4.3\n9.7\n6.5\n2.6\n13.8\n10\n20\n30\n40\n50\nFigure 5. Linear evaluation (ImageNet top-1 accuracy) under in-\ndividual or composition of data augmentations, applied only to\none branch. For all columns but the last, diagonal entries corre-\nspond to single transformation, and off-diagonals correspond to\ncomposition of two transformations (applied sequentially). The\nlast column reﬂects the average over the row.\nTo understand the effects of individual data augmentations\nand the importance of augmentation composition, we in-\nvestigate the performance of our framework when applying\naugmentations individually or in pairs. Since ImageNet\nimages are of different sizes, we always apply crop and re-\nsize images (Krizhevsky et al., 2012; Szegedy et al., 2015),\nwhich makes it difﬁcult to study other augmentations in\nthe absence of cropping. To eliminate this confound, we\nconsider an asymmetric data transformation setting for this\nablation. Speciﬁcally, we always ﬁrst randomly crop im-\nages and resize them to the same resolution, and we then\napply the targeted transformation(s) only to one branch of\nthe framework in Figure 2, while leaving the other branch\nas the identity (i.e. t(xi) = xi). Note that this asymmet-\n",
  "5": "A Simple Framework for Contrastive Learning of Visual Representations\n(a) Without color distortion.\n(b) With color distortion.\nFigure 6. Histograms of pixel intensities (over all channels) for\ndifferent crops of two different images (i.e. two rows). The image\nfor the ﬁrst row is from Figure 4. All axes have the same range.\nColor distortion strength\nMethods\n1/8\n1/4\n1/2\n1\n1 (+Blur)\nAutoAug\nSimCLR\n59.6\n61.0\n62.6\n63.2\n64.5\n61.1\nSupervised\n77.0\n76.7\n76.5\n75.7\n75.4\n77.1\nTable 1. Top-1 accuracy of unsupervised ResNet-50 using linear\nevaluation and supervised ResNet-505, under varied color distor-\ntion strength (see Appendix A) and other data transformations.\nStrength 1 (+Blur) is our default data augmentation policy.\nric data augmentation hurts the performance. Nonetheless,\nthis setup should not substantively change the impact of\nindividual data augmentations or their compositions.\nFigure 5 shows linear evaluation results under individual\nand composition of transformations. We observe that no\nsingle transformation sufﬁces to learn good representations,\neven though the model can almost perfectly identify the\npositive pairs in the contrastive task. When composing aug-\nmentations, the contrastive prediction task becomes harder,\nbut the quality of representation improves dramatically. Ap-\npendix B.2 provides a further study on composing broader\nset of augmentations.\nOne composition of augmentations stands out: random crop-\nping and random color distortion. We conjecture that one\nserious issue when using only random cropping as data\naugmentation is that most patches from an image share a\nsimilar color distribution. Figure 6 shows that color his-\ntograms alone sufﬁce to distinguish images. Neural nets\nmay exploit this shortcut to solve the predictive task. There-\nfore, it is critical to compose cropping with color distortion\nin order to learn generalizable features.\n3.2. Contrastive learning needs stronger data\naugmentation than supervised learning\nTo further demonstrate the importance of the color aug-\nmentation, we adjust the strength of color augmentation as\n5Supervised models are trained for 90 epochs; longer training\nimproves performance of stronger augmentation by ∼0.5%.\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\nNumber of Parameters (Millions)\n50\n55\n60\n65\n70\n75\n80\nTop 1\nR101\nR101(2x)\nR152\nR152(2x)\nR18\nR18(2x)\nR18(4x)\nR34\nR34(2x)\nR34(4x)\nR50\nR50(2x)\nR50(4x)\nSup. R50\nSup. R50(2x)\nSup. R50(4x)\nR50*\nR50(2x)*\nR50(4x)*\nFigure 7. Linear evaluation of models with varied depth and width.\nModels in blue dots are ours trained for 100 epochs, models in red\nstars are ours trained for 1000 epochs, and models in green crosses\nare supervised ResNets trained for 90 epochs7 (He et al., 2016).\nshown in Table 1. Stronger color augmentation substan-\ntially improves the linear evaluation of the learned unsuper-\nvised models. In this context, AutoAugment (Cubuk et al.,\n2019), a sophisticated augmentation policy found using su-\npervised learning, does not work better than simple cropping\n+ (stronger) color distortion. When training supervised mod-\nels with the same set of augmentations, we observe that\nstronger color augmentation does not improve or even hurts\ntheir performance. Thus, our experiments show that unsu-\npervised contrastive learning beneﬁts from stronger (color)\ndata augmentation than supervised learning. Although pre-\nvious work has reported that data augmentation is useful\nfor self-supervised learning (Doersch et al., 2015; Bachman\net al., 2019; Hénaff et al., 2019; Asano et al., 2019), we\nshow that data augmentation that does not yield accuracy\nbeneﬁts for supervised learning can still help considerably\nwith contrastive learning.\n4. Architectures for Encoder and Head\n4.1. Unsupervised contrastive learning beneﬁts (more)\nfrom bigger models\nFigure 7 shows, perhaps unsurprisingly, that increasing\ndepth and width both improve performance. While similar\nﬁndings hold for supervised learning (He et al., 2016), we\nﬁnd the gap between supervised models and linear classiﬁers\ntrained on unsupervised models shrinks as the model size\nincreases, suggesting that unsupervised learning beneﬁts\nmore from bigger models than its supervised counterpart.\n7Training longer does not improve supervised ResNets (see\nAppendix B.3).\n",
  "6": "A Simple Framework for Contrastive Learning of Visual Representations\nName\nNegative loss function\nGradient w.r.t. u\nNT-Xent\nuT v+/τ −log P\nv∈{v+,v−} exp(uT v/τ)\n(1 −exp(uT v+/τ)\nZ(u)\n)/τv+ −P\nv−\nexp(uT v−/τ)\nZ(u)\n/τv−\nNT-Logistic\nlog σ(uT v+/τ) + log σ(−uT v−/τ)\n(σ(−uT v+/τ))/τv+ −σ(uT v−/τ)/τv−\nMargin Triplet\n−max(uT v−−uT v+ + m, 0)\nv+ −v−if uT v+ −uT v−< m else 0\nTable 2. Negative loss functions and their gradients. All input vectors, i.e. u, v+, v−, are ℓ2 normalized. NT-Xent is an abbreviation for\n“Normalized Temperature-scaled Cross Entropy”. Different loss functions impose different weightings of positive and negative examples.\n32\n64\n128\n256\n512\n1024\n2048\nProjection output dimensionality\n30\n40\n50\n60\n70\nTop 1\nProjection\nLinear\nNon-linear\nNone\nFigure 8. Linear evaluation of representations with different pro-\njection heads g(·) and various dimensions of z = g(h). The\nrepresentation h (before projection) is 2048-dimensional here.\n4.2. A nonlinear projection head improves the\nrepresentation quality of the layer before it\nWe then study the importance of including a projection\nhead, i.e. g(h). Figure 8 shows linear evaluation results\nusing three different architecture for the head: (1) identity\nmapping; (2) linear projection, as used by several previous\napproaches (Wu et al., 2018); and (3) the default nonlinear\nprojection with one additional hidden layer (and ReLU acti-\nvation), similar to Bachman et al. (2019). We observe that a\nnonlinear projection is better than a linear projection (+3%),\nand much better than no projection (>10%). When a pro-\njection head is used, similar results are observed regardless\nof output dimension. Furthermore, even when nonlinear\nprojection is used, the layer before the projection head, h,\nis still much better (>10%) than the layer after, z = g(h),\nwhich shows that the hidden layer before the projection\nhead is a better representation than the layer after.\nWe conjecture that the importance of using the representa-\ntion before the nonlinear projection is due to loss of informa-\ntion induced by the contrastive loss. In particular, z = g(h)\nis trained to be invariant to data transformation. Thus, g can\nremove information that may be useful for the downstream\ntask, such as the color or orientation of objects. By leverag-\ning the nonlinear transformation g(·), more information can\nbe formed and maintained in h. To verify this hypothesis,\nwe conduct experiments that use either h or g(h) to learn\nto predict the transformation applied during the pretraining.\nHere we set g(h) = W (2)σ(W (1)h), with the same input\nand output dimensionality (i.e. 2048). Table 3 shows h\ncontains much more information about the transformation\napplied, while g(h) loses information. Further analysis can\nWhat to predict?\nRandom guess\nRepresentation\nh\ng(h)\nColor vs grayscale\n80\n99.3\n97.4\nRotation\n25\n67.6\n25.6\nOrig. vs corrupted\n50\n99.5\n59.6\nOrig. vs Sobel ﬁltered\n50\n96.6\n56.3\nTable 3. Accuracy of training additional MLPs on different repre-\nsentations to predict the transformation applied. Other than crop\nand color augmentation, we additionally and independently add\nrotation (one of {0◦, 90◦, 180◦, 270◦}), Gaussian noise, and So-\nbel ﬁltering transformation during the pretraining for the last three\nrows. Both h and g(h) are of the same dimensionality, i.e. 2048.\nbe found in Appendix B.4.\n5. Loss Functions and Batch Size\n5.1. Normalized cross entropy loss with adjustable\ntemperature works better than alternatives\nWe compare the NT-Xent loss against other commonly used\ncontrastive loss functions, such as logistic loss (Mikolov\net al., 2013), and margin loss (Schroff et al., 2015). Table\n2 shows the objective function as well as the gradient to\nthe input of the loss function. Looking at the gradient, we\nobserve 1) ℓ2 normalization (i.e. cosine similarity) along\nwith temperature effectively weights different examples, and\nan appropriate temperature can help the model learn from\nhard negatives; and 2) unlike cross-entropy, other objec-\ntive functions do not weigh the negatives by their relative\nhardness. As a result, one must apply semi-hard negative\nmining (Schroff et al., 2015) for these loss functions: in-\nstead of computing the gradient over all loss terms, one can\ncompute the gradient using semi-hard negative terms (i.e.,\nthose that are within the loss margin and closest in distance,\nbut farther than positive examples).\nTo make the comparisons fair, we use the same ℓ2 normaliza-\ntion for all loss functions, and we tune the hyperparameters,\nand report their best results.8 Table 4 shows that, while\n(semi-hard) negative mining helps, the best result is still\nmuch worse than our default NT-Xent loss.\n8Details can be found in Appendix B.10. For simplicity, we\nonly consider the negatives from one augmentation view.\n",
  "7": "A Simple Framework for Contrastive Learning of Visual Representations\nMargin\nNT-Logi.\nMargin (sh)\nNT-Logi.(sh)\nNT-Xent\n50.9\n51.6\n57.5\n57.9\n63.9\nTable 4. Linear evaluation (top-1) for models trained with different\nloss functions. “sh” means using semi-hard negative mining.\nℓ2 norm?\nτ\nEntropy\nContrastive acc.\nTop 1\nYes\n0.05\n1.0\n90.5\n59.7\n0.1\n4.5\n87.8\n64.4\n0.5\n8.2\n68.2\n60.7\n1\n8.3\n59.1\n58.0\nNo\n10\n0.5\n91.7\n57.2\n100\n0.5\n92.1\n57.0\nTable 5. Linear evaluation for models trained with different choices\nof ℓ2 norm and temperature τ for NT-Xent loss. The contrastive\ndistribution is over 4096 examples.\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTraining epochs\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\nTop 1\nBatch size\n256\n512\n1024\n2048\n4096\n8192\nFigure 9. Linear evaluation models (ResNet-50) trained with differ-\nent batch size and epochs. Each bar is a single run from scratch.10\nWe next test the importance of the ℓ2 normalization (i.e.\ncosine similarity vs dot product) and temperature τ in our\ndefault NT-Xent loss. Table 5 shows that without normal-\nization and proper temperature scaling, performance is sig-\nniﬁcantly worse. Without ℓ2 normalization, the contrastive\ntask accuracy is higher, but the resulting representation is\nworse under linear evaluation.\n5.2. Contrastive learning beneﬁts (more) from larger\nbatch sizes and longer training\nFigure 9 shows the impact of batch size when models are\ntrained for different numbers of epochs. We ﬁnd that, when\nthe number of training epochs is small (e.g. 100 epochs),\nlarger batch sizes have a signiﬁcant advantage over the\nsmaller ones. With more training steps/epochs, the gaps\nbetween different batch sizes decrease or disappear, pro-\nvided the batches are randomly resampled. In contrast to\n10A linear learning rate scaling is used here. Figure B.1 shows\nusing a square root learning rate scaling can improve performance\nof ones with small batch sizes.\nMethod\nArchitecture\nParam (M)\nTop 1\nTop 5\nMethods using ResNet-50:\nLocal Agg.\nResNet-50\n24\n60.2\n-\nMoCo\nResNet-50\n24\n60.6\n-\nPIRL\nResNet-50\n24\n63.6\n-\nCPC v2\nResNet-50\n24\n63.8\n85.3\nSimCLR (ours)\nResNet-50\n24\n69.3\n89.0\nMethods using other architectures:\nRotation\nRevNet-50 (4×)\n86\n55.4\n-\nBigBiGAN\nRevNet-50 (4×)\n86\n61.3\n81.9\nAMDIM\nCustom-ResNet\n626\n68.1\n-\nCMC\nResNet-50 (2×)\n188\n68.4\n88.2\nMoCo\nResNet-50 (4×)\n375\n68.6\n-\nCPC v2\nResNet-161 (∗)\n305\n71.5\n90.1\nSimCLR (ours)\nResNet-50 (2×)\n94\n74.2\n92.0\nSimCLR (ours)\nResNet-50 (4×)\n375\n76.5\n93.2\nTable 6. ImageNet accuracies of linear classiﬁers trained on repre-\nsentations learned with different self-supervised methods.\nMethod\nArchitecture\nLabel fraction\n1%\n10%\nTop 5\nSupervised baseline\nResNet-50\n48.4\n80.4\nMethods using other label-propagation:\nPseudo-label\nResNet-50\n51.6\n82.4\nVAT+Entropy Min.\nResNet-50\n47.0\n83.4\nUDA (w. RandAug)\nResNet-50\n-\n88.5\nFixMatch (w. RandAug)\nResNet-50\n-\n89.1\nS4L (Rot+VAT+En. M.)\nResNet-50 (4×)\n-\n91.2\nMethods using representation learning only:\nInstDisc\nResNet-50\n39.2\n77.4\nBigBiGAN\nRevNet-50 (4×)\n55.2\n78.8\nPIRL\nResNet-50\n57.2\n83.8\nCPC v2\nResNet-161(∗)\n77.9\n91.2\nSimCLR (ours)\nResNet-50\n75.5\n87.8\nSimCLR (ours)\nResNet-50 (2×)\n83.0\n91.2\nSimCLR (ours)\nResNet-50 (4×)\n85.8\n92.6\nTable 7. ImageNet accuracy of models trained with few labels.\nsupervised learning (Goyal et al., 2017), in contrastive learn-\ning, larger batch sizes provide more negative examples,\nfacilitating convergence (i.e. taking fewer epochs and steps\nfor a given accuracy). Training longer also provides more\nnegative examples, improving the results. In Appendix B.1,\nresults with even longer training steps are provided.\n6. Comparison with State-of-the-art\nIn this subsection, similar to Kolesnikov et al. (2019); He\net al. (2019), we use ResNet-50 in 3 different hidden layer\nwidths (width multipliers of 1×, 2×, and 4×). For better\nconvergence, our models here are trained for 1000 epochs.\nLinear evaluation. Table 6 compares our results with previ-\nous approaches (Zhuang et al., 2019; He et al., 2019; Misra\n& van der Maaten, 2019; Hénaff et al., 2019; Kolesnikov\net al., 2019; Donahue & Simonyan, 2019; Bachman et al.,\n",
  "8": "A Simple Framework for Contrastive Learning of Visual Representations\nFood CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers\nLinear evaluation:\nSimCLR (ours)\n76.9\n95.3\n80.2\n48.4\n65.9\n60.0\n61.2\n84.2\n78.9\n89.2\n93.9\n95.0\nSupervised\n75.2\n95.7\n81.2\n56.4\n64.9\n68.8\n63.8\n83.8\n78.7\n92.3\n94.1\n94.2\nFine-tuned:\nSimCLR (ours)\n89.4\n98.6\n89.0\n78.2\n68.1\n92.1\n87.0\n86.6\n77.8\n92.1\n94.1\n97.6\nSupervised\n88.7\n98.3\n88.7\n77.8\n67.0\n91.4\n88.0\n86.5\n78.8\n93.2\n94.2\n98.0\nRandom init\n88.3\n96.0\n81.9\n77.0\n53.7\n91.3\n84.8\n69.4\n64.1\n82.7\n72.5\n92.5\nTable 8. Comparison of transfer learning performance of our self-supervised approach with supervised baselines across 12 natural image\nclassiﬁcation datasets, for ResNet-50 (4×) models pretrained on ImageNet. Results not signiﬁcantly worse than the best (p > 0.05,\npermutation test) are shown in bold. See Appendix B.8 for experimental details and results with standard ResNet-50.\n2019; Tian et al., 2019) in the linear evaluation setting (see\nAppendix B.6). Table 1 shows more numerical compar-\nisons among different methods. We are able to use standard\nnetworks to obtain substantially better results compared to\nprevious methods that require speciﬁcally designed archi-\ntectures. The best result obtained with our ResNet-50 (4×)\ncan match the supervised pretrained ResNet-50.\nSemi-supervised learning. We follow Zhai et al. (2019)\nand sample 1% or 10% of the labeled ILSVRC-12 training\ndatasets in a class-balanced way (∼12.8 and ∼128 images\nper class respectively). 11 We simply ﬁne-tune the whole\nbase network on the labeled data without regularization\n(see Appendix B.5). Table 7 shows the comparisons of\nour results against recent methods (Zhai et al., 2019; Xie\net al., 2019; Sohn et al., 2020; Wu et al., 2018; Donahue &\nSimonyan, 2019; Misra & van der Maaten, 2019; Hénaff\net al., 2019). The supervised baseline from (Zhai et al.,\n2019) is strong due to intensive search of hyper-parameters\n(including augmentation). Again, our approach signiﬁcantly\nimproves over state-of-the-art with both 1% and 10% of the\nlabels. Interestingly, ﬁne-tuning our pretrained ResNet-50\n(2×, 4×) on full ImageNet are also signiﬁcantly better then\ntraining from scratch (up to 2%, see Appendix B.2).\nTransfer learning. We evaluate transfer learning perfor-\nmance across 12 natural image datasets in both linear evalu-\nation (ﬁxed feature extractor) and ﬁne-tuning settings. Fol-\nlowing Kornblith et al. (2019), we perform hyperparameter\ntuning for each model-dataset combination and select the\nbest hyperparameters on a validation set. Table 8 shows\nresults with the ResNet-50 (4×) model. When ﬁne-tuned,\nour self-supervised model signiﬁcantly outperforms the su-\npervised baseline on 5 datasets, whereas the supervised\nbaseline is superior on only 2 (i.e. Pets and Flowers). On\nthe remaining 5 datasets, the models are statistically tied.\nFull experimental details as well as results with the standard\nResNet-50 architecture are provided in Appendix B.8.\n11The details of sampling and exact subsets can be found in\nhttps://www.tensorﬂow.org/datasets/catalog/imagenet2012_subset.\n7. Related Work\nThe idea of making representations of an image agree with\neach other under small transformations dates back to Becker\n& Hinton (1992). We extend it by leveraging recent ad-\nvances in data augmentation, network architecture and con-\ntrastive loss. A similar consistency idea, but for class label\nprediction, has been explored in other contexts such as semi-\nsupervised learning (Xie et al., 2019; Berthelot et al., 2019).\nHandcrafted pretext tasks. The recent renaissance of self-\nsupervised learning began with artiﬁcially designed pretext\ntasks, such as relative patch prediction (Doersch et al., 2015),\nsolving jigsaw puzzles (Noroozi & Favaro, 2016), coloriza-\ntion (Zhang et al., 2016) and rotation prediction (Gidaris\net al., 2018; Chen et al., 2019). Although good results\ncan be obtained with bigger networks and longer train-\ning (Kolesnikov et al., 2019), these pretext tasks rely on\nsomewhat ad-hoc heuristics, which limits the generality of\nlearned representations.\nContrastive visual representation learning. Dating back\nto Hadsell et al. (2006), these approaches learn represen-\ntations by contrasting positive pairs against negative pairs.\nAlong these lines, Dosovitskiy et al. (2014) proposes to\ntreat each instance as a class represented by a feature vector\n(in a parametric form). Wu et al. (2018) proposes to use\na memory bank to store the instance class representation\nvector, an approach adopted and extended in several recent\npapers (Zhuang et al., 2019; Tian et al., 2019; He et al.,\n2019; Misra & van der Maaten, 2019). Other work explores\nthe use of in-batch samples for negative sampling instead\nof a memory bank (Doersch & Zisserman, 2017; Ye et al.,\n2019; Ji et al., 2019).\nRecent literature has attempted to relate the success of their\nmethods to maximization of mutual information between\nlatent representations (Oord et al., 2018; Hénaff et al., 2019;\nHjelm et al., 2018; Bachman et al., 2019). However, it is not\nclear if the success of contrastive approaches is determined\nby the mutual information, or by the speciﬁc form of the\ncontrastive loss (Tschannen et al., 2019).\n",
  "9": "A Simple Framework for Contrastive Learning of Visual Representations\nWe note that almost all individual components of our frame-\nwork have appeared in previous work, although the speciﬁc\ninstantiations may be different. The superiority of our frame-\nwork relative to previous work is not explained by any single\ndesign choice, but by their composition. We provide a com-\nprehensive comparison of our design choices with those of\nprevious work in Appendix C.\n8. Conclusion\nIn this work, we present a simple framework and its in-\nstantiation for contrastive visual representation learning.\nWe carefully study its components, and show the effects\nof different design choices. By combining our ﬁndings,\nwe improve considerably over previous methods for self-\nsupervised, semi-supervised, and transfer learning.\nOur approach differs from standard supervised learning on\nImageNet only in the choice of data augmentation, the use of\na nonlinear head at the end of the network, and the loss func-\ntion. The strength of this simple framework suggests that,\ndespite a recent surge in interest, self-supervised learning\nremains undervalued.\nAcknowledgements\nWe would like to thank Xiaohua Zhai, Rafael Müller and\nYani Ioannou for their feedback on the draft. We are also\ngrateful for general support from Google Research teams in\nToronto and elsewhere.\nReferences\nAsano, Y. M., Rupprecht, C., and Vedaldi, A. A critical analysis\nof self-supervision, or what we can learn from a single image.\narXiv preprint arXiv:1904.13132, 2019.\nBachman, P., Hjelm, R. D., and Buchwalter, W. Learning rep-\nresentations by maximizing mutual information across views.\nIn Advances in Neural Information Processing Systems, pp.\n15509–15519, 2019.\nBecker, S. and Hinton, G. E. Self-organizing neural network that\ndiscovers surfaces in random-dot stereograms. Nature, 355\n(6356):161–163, 1992.\nBerg, T., Liu, J., Lee, S. W., Alexander, M. L., Jacobs, D. W.,\nand Belhumeur, P. N. Birdsnap: Large-scale ﬁne-grained visual\ncategorization of birds. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 2019–2026. IEEE, 2014.\nBerthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver,\nA., and Raffel, C. A. Mixmatch: A holistic approach to semi-\nsupervised learning. In Advances in Neural Information Pro-\ncessing Systems, pp. 5050–5060, 2019.\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101–mining\ndiscriminative components with random forests. In European\nconference on computer vision, pp. 446–461. Springer, 2014.\nChen, T., Sun, Y., Shi, Y., and Hong, L. On sampling strategies\nfor neural network-based collaborative ﬁltering. In Proceed-\nings of the 23rd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pp. 767–776, 2017.\nChen, T., Zhai, X., Ritter, M., Lucic, M., and Houlsby, N. Self-\nsupervised gans via auxiliary rotation loss. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\npp. 12154–12163, 2019.\nCimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi,\nA. Describing textures in the wild. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 3606–\n3613. IEEE, 2014.\nCubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V.\nAutoaugment: Learning augmentation strategies from data. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pp. 113–123, 2019.\nDeVries, T. and Taylor, G. W.\nImproved regularization of\nconvolutional neural networks with cutout.\narXiv preprint\narXiv:1708.04552, 2017.\nDoersch, C. and Zisserman, A. Multi-task self-supervised visual\nlearning. In Proceedings of the IEEE International Conference\non Computer Vision, pp. 2051–2060, 2017.\nDoersch, C., Gupta, A., and Efros, A. A. Unsupervised visual\nrepresentation learning by context prediction. In Proceedings\nof the IEEE International Conference on Computer Vision, pp.\n1422–1430, 2015.\nDonahue, J. and Simonyan, K. Large scale adversarial representa-\ntion learning. In Advances in Neural Information Processing\nSystems, pp. 10541–10551, 2019.\nDonahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E.,\nand Darrell, T. Decaf: A deep convolutional activation feature\nfor generic visual recognition. In International Conference on\nMachine Learning, pp. 647–655, 2014.\nDosovitskiy, A., Springenberg, J. T., Riedmiller, M., and Brox, T.\nDiscriminative unsupervised feature learning with convolutional\nneural networks. In Advances in neural information processing\nsystems, pp. 766–774, 2014.\nEveringham, M., Van Gool, L., Williams, C. K., Winn, J., and\nZisserman, A. The pascal visual object classes (voc) challenge.\nInternational Journal of Computer Vision, 88(2):303–338, 2010.\nFei-Fei, L., Fergus, R., and Perona, P. Learning generative visual\nmodels from few training examples: An incremental bayesian\napproach tested on 101 object categories. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) Workshop\non Generative-Model Based Vision, 2004.\nGidaris, S., Singh, P., and Komodakis, N. Unsupervised represen-\ntation learning by predicting image rotations. arXiv preprint\narXiv:1803.07728, 2018.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-\nFarley, D., Ozair, S., Courville, A., and Bengio, Y. Generative\nadversarial nets. In Advances in neural information processing\nsystems, pp. 2672–2680, 2014.\n",
  "10": "A Simple Framework for Contrastive Learning of Visual Representations\nGoyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L.,\nKyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large\nminibatch sgd: Training imagenet in 1 hour. arXiv preprint\narXiv:1706.02677, 2017.\nHadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction\nby learning an invariant mapping. In 2006 IEEE Computer So-\nciety Conference on Computer Vision and Pattern Recognition\n(CVPR’06), volume 2, pp. 1735–1742. IEEE, 2006.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for\nimage recognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 770–778, 2016.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum\ncontrast for unsupervised visual representation learning. arXiv\npreprint arXiv:1911.05722, 2019.\nHénaff, O. J., Razavi, A., Doersch, C., Eslami, S., and Oord, A.\nv. d. Data-efﬁcient image recognition with contrastive predictive\ncoding. arXiv preprint arXiv:1905.09272, 2019.\nHinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning al-\ngorithm for deep belief nets. Neural computation, 18(7):1527–\n1554, 2006.\nHjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K.,\nBachman, P., Trischler, A., and Bengio, Y. Learning deep repre-\nsentations by mutual information estimation and maximization.\narXiv preprint arXiv:1808.06670, 2018.\nHoward, A. G.\nSome improvements on deep convolutional\nneural network based image classiﬁcation.\narXiv preprint\narXiv:1312.5402, 2013.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. arXiv\npreprint arXiv:1502.03167, 2015.\nJi, X., Henriques, J. F., and Vedaldi, A. Invariant information\nclustering for unsupervised image classiﬁcation and segmenta-\ntion. In Proceedings of the IEEE International Conference on\nComputer Vision, pp. 9865–9874, 2019.\nKingma, D. P. and Welling, M. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114, 2013.\nKolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-supervised\nvisual representation learning. In Proceedings of the IEEE\nconference on Computer Vision and Pattern Recognition, pp.\n1920–1929, 2019.\nKornblith, S., Shlens, J., and Le, Q. V. Do better ImageNet models\ntransfer better?\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 2661–2671, 2019.\nKrause, J., Deng, J., Stark, M., and Fei-Fei, L.\nCollecting a\nlarge-scale dataset of ﬁne-grained cars. In Second Workshop on\nFine-Grained Visual Categorization, 2013.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of features\nfrom tiny images.\nTechnical report, University of Toronto,\n2009. URL https://www.cs.toronto.edu/~kriz/\nlearning-features-2009-TR.pdf.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁ-\ncation with deep convolutional neural networks. In Advances in\nneural information processing systems, pp. 1097–1105, 2012.\nLoshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent\nwith warm restarts. arXiv preprint arXiv:1608.03983, 2016.\nMaaten, L. v. d. and Hinton, G. Visualizing data using t-sne. Jour-\nnal of machine learning research, 9(Nov):2579–2605, 2008.\nMaji, S., Kannala, J., Rahtu, E., Blaschko, M., and Vedaldi, A.\nFine-grained visual classiﬁcation of aircraft. Technical report,\n2013.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁcient esti-\nmation of word representations in vector space. arXiv preprint\narXiv:1301.3781, 2013.\nMisra, I. and van der Maaten, L.\nSelf-supervised learn-\ning of pretext-invariant representations.\narXiv preprint\narXiv:1912.01991, 2019.\nNilsback, M.-E. and Zisserman, A. Automated ﬂower classiﬁcation\nover a large number of classes. In Computer Vision, Graphics &\nImage Processing, 2008. ICVGIP’08. Sixth Indian Conference\non, pp. 722–729. IEEE, 2008.\nNoroozi, M. and Favaro, P. Unsupervised learning of visual repre-\nsentations by solving jigsaw puzzles. In European Conference\non Computer Vision, pp. 69–84. Springer, 2016.\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learning with\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748,\n2018.\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. Cats\nand dogs. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 3498–3505. IEEE, 2012.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma,\nS., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.\nImagenet large scale visual recognition challenge. International\njournal of computer vision, 115(3):211–252, 2015.\nSchroff, F., Kalenichenko, D., and Philbin, J. Facenet: A uniﬁed\nembedding for face recognition and clustering. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pp. 815–823, 2015.\nSimonyan, K. and Zisserman, A.\nVery deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\nSohn, K. Improved deep metric learning with multi-class n-pair\nloss objective. In Advances in neural information processing\nsystems, pp. 1857–1865, 2016.\nSohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk,\nE. D., Kurakin, A., Zhang, H., and Raffel, C. Fixmatch: Simpli-\nfying semi-supervised learning with consistency and conﬁdence.\narXiv preprint arXiv:2001.07685, 2020.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D.,\nErhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper\nwith convolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 1–9, 2015.\nTian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding.\narXiv preprint arXiv:1906.05849, 2019.\nTschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S., and Lu-\ncic, M. On mutual information maximization for representation\nlearning. arXiv preprint arXiv:1907.13625, 2019.\n",
  "11": "A Simple Framework for Contrastive Learning of Visual Representations\nWu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised feature\nlearning via non-parametric instance discrimination. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 3733–3742, 2018.\nXiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun\ndatabase: Large-scale scene recognition from abbey to zoo. In\nIEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 3485–3492. IEEE, 2010.\nXie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. Unsu-\npervised data augmentation. arXiv preprint arXiv:1904.12848,\n2019.\nYe, M., Zhang, X., Yuen, P. C., and Chang, S.-F. Unsupervised\nembedding learning via invariant and spreading instance feature.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 6210–6219, 2019.\nYou, Y., Gitman, I., and Ginsburg, B. Large batch training of con-\nvolutional networks. arXiv preprint arXiv:1708.03888, 2017.\nZhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S4l: Self-\nsupervised semi-supervised learning. In The IEEE International\nConference on Computer Vision (ICCV), October 2019.\nZhang, R., Isola, P., and Efros, A. A. Colorful image coloriza-\ntion. In European conference on computer vision, pp. 649–666.\nSpringer, 2016.\nZhuang, C., Zhai, A. L., and Yamins, D. Local aggregation for\nunsupervised learning of visual embeddings. In Proceedings\nof the IEEE International Conference on Computer Vision, pp.\n6002–6012, 2019.\n",
  "12": "A Simple Framework for Contrastive Learning of Visual Representations\nA. Data Augmentation Details\nIn our default pretraining setting (which is used to train our best models), we utilize random crop (with resize and random\nﬂip), random color distortion, and random Gaussian blur as the data augmentations. The details of these three augmentations\nare provided below.\nRandom crop and resize to 224x224\nWe use standard Inception-style random cropping (Szegedy et al., 2015). The\ncrop of random size (uniform from 0.08 to 1.0 in area) of the original size and a random aspect ratio (default: of\n3/4 to 4/3) of the original aspect ratio is made. This crop is ﬁnally resized to the original size. This has been imple-\nmented in Tensorﬂow as “slim.preprocessing.inception_preprocessing.distorted_bounding_box_crop”, or in Pytorch\nas “torchvision.transforms.RandomResizedCrop”. Additionally, the random crop (with resize) is always followed by a\nrandom horizontal/left-to-right ﬂip with 50% probability. This is helpful but not essential. By removing this from our default\naugmentation policy, the top-1 linear evaluation drops from 64.5% to 63.4% for our ResNet-50 model trained in 100 epochs.\nColor distortion\nColor distortion is composed by color jittering and color dropping. We ﬁnd stronger color jittering\nusually helps, so we set a strength parameter.\nA pseudo-code for color distortion using TensorFlow is as follows.\nimport tensorflow as tf\ndef color_distortion(image, s=1.0):\n# image is a tensor with value range in [0, 1].\n# s is the strength of color distortion.\ndef color_jitter(x):\n# one can also shuffle the order of following augmentations\n# each time they are applied.\nx = tf.image.random_brightness(x, max_delta=0.8*s)\nx = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s)\nx = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s)\nx = tf.image.random_hue(x, max_delta=0.2*s)\nx = tf.clip_by_value(x, 0, 1)\nreturn x\ndef color_drop(x):\nimage = tf.image.rgb_to_grayscale(image)\nimage = tf.tile(image, [1, 1, 3])\n# randomly apply transformation with probability p.\nimage = random_apply(color_jitter, image, p=0.8)\nimage = random_apply(color_drop, image, p=0.2)\nreturn image\nA pseudo-code for color distortion using Pytorch is as follows 12.\nfrom torchvision import transforms\ndef get_color_distortion(s=1.0):\n# s is the strength of color distortion.\ncolor_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\nrnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\nrnd_gray = transforms.RandomGrayscale(p=0.2)\ncolor_distort = transforms.Compose([\nrnd_color_jitter,\nrnd_gray])\n12Our code and results are based on Tensorﬂow, the Pytorch code here is a reference.\n",
  "13": "A Simple Framework for Contrastive Learning of Visual Representations\nreturn color_distort\nGaussian blur\nThis augmentation is in our default policy. We ﬁnd it helpful, as it improves our ResNet-50 trained for\n100 epochs from 63.2% to 64.5%. We blur the image 50% of the time using a Gaussian kernel. We randomly sample\nσ ∈[0.1, 2.0], and the kernel size is set to be 10% of the image height/width.\nB. Additional Experimental Results\nB.1. Batch Size and Training Steps\nFigure B.1 shows the top-5 accuracy on linear evaluation when trained with different batch sizes and training epochs. The\nconclusion is very similar to top-1 accuracy shown before, except that the differences between different batch sizes and\ntraining steps seems slightly smaller here.\nIn both Figure 9 and Figure B.1, we use a linear scaling of learning rate similar to (Goyal et al., 2017) when training\nwith different batch sizes. Although linear learning rate scaling is popular with SGD/Momentum optimizer, we ﬁnd a\nsquare root learning rate scaling is more desirable with LARS optimizer. With square root learning rate scaling, we have\nLearningRate = 0.075 ×\n√\nBatchSize, instead of LearningRate = 0.3 × BatchSize/256 in the linear scaling case, but\nthe learning rate is the same under both scaling methods when batch size of 4096 (our default batch size). A comparison is\npresented in Table B.1, where we observe that square root learning rate scaling improves the performance for models trained\nwith small batch sizes and in smaller number of epochs.\nBatch size \\ Epochs\n100\n200\n400\n800\n256\n57.5 / 62.8\n61.9 / 64.3\n64.7 / 65.7\n66.6 / 66.5\n512\n60.7 / 63.8\n64.0 / 65.6\n66.2 / 66.7\n67.8 / 67.4\n1024\n62.8 / 64.3\n65.3 / 66.1\n67.2 / 67.2\n68.5 / 68.3\n2048\n64.0 / 64.7\n66.1 / 66.8\n68.1 / 67.9\n68.9 / 68.8\n4096\n64.6 / 64.5\n66.5 / 66.8\n68.2 / 68.0\n68.9 / 69.1\n8192\n64.8 / 64.8\n66.6 / 67.0\n67.8 / 68.3\n69.0 / 69.1\nTable B.1. Linear evaluation (top-1) under different batch sizes and training epochs. On the left side of slash sign are models trained with\nlinear LR scaling, and on the right are models trained with square root LR scaling. The result is bolded if it is more than 0.5% better.\nSquare root LR scaling works better for smaller batch size trained in fewer epochs (with LARS optimizer).\nWe also train with larger batch size (up to 32K) and longer (up to 3200 epochs), with the square root learning rate scaling. A\nshown in Figure B.2, the performance seems to saturate with a batch size of 8192, while training longer can still signiﬁcantly\nimprove the performance.\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTraining epochs\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\nTop 5\nBatch size\n256\n512\n1024\n2048\n4096\n8192\nFigure B.1. Linear evaluation (top-5) of ResNet-50 trained with\ndifferent batch sizes and epochs. Each bar is a single run from\nscratch. See Figure 9 for top-1 accuracy.\n50\n100\n200\n400\n800\n1600\n3200\nTraining epochs\n60\n62\n64\n66\n68\n70\n72\nTop 1\nBatch size\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\nFigure B.2. Linear evaluation (top-1) of ResNet-50 trained with\ndifferent batch sizes and longer epochs. Here a square root learn-\ning rate, instead of a linear one, is utilized.\n",
  "14": "A Simple Framework for Contrastive Learning of Visual Representations\nB.2. Broader composition of data augmentations further improves performance\nOur best results in the main text (Table 6 and 7) can be further improved when expanding the default augmentation policy to\ninclude the following: (1) Sobel ﬁltering, (2) additional color distortion (equalize, solarize), and (3) motion blur. For linear\nevaluation protocol, the ResNet-50 models (1×, 2×, 4×) trained with broader data augmentations achieve 70.0 (+0.7), 74.4\n(+0.2), 76.8 (+0.3), respectively.\nTable B.2 shows ImageNet accuracy obtained by ﬁne-tuning the SimCLR model (see Appendix B.5 for the details of\nﬁne-tuning procedure). Interestingly, when ﬁne-tuned on full (100%) ImageNet training set, our ResNet (4×) model\nachieves 80.4% top-1 / 95.4% top-5 13, which is signiﬁcantly better than that (78.4% top-1 / 94.2% top-5) of training from\nscratch using the same set of augmentations (i.e. random crop and horizontal ﬂip). For ResNet-50 (2×), ﬁne-tuning our\npre-trained ResNet-50 (2×) is also better than training from scratch (77.8% top-1 / 93.9% top-5). There is no improvement\nfrom ﬁne-tuning for ResNet-50.\nArchitecture\nLabel fraction\n1%\n10%\n100%\nTop 1\nTop 5\nTop 1\nTop 5\nTop 1\nTop 5\nResNet-50\n49.4\n76.6\n66.1\n88.1\n76.0\n93.1\nResNet-50 (2×)\n59.4\n83.7\n71.8\n91.2\n79.1\n94.8\nResNet-50 (4×)\n64.1\n86.6\n74.8\n92.8\n80.4\n95.4\nTable B.2. Classiﬁcation accuracy obtained by ﬁne-tuning the SimCLR (which is pretrained with broader data augmentations) on 1%,\n10% and full of ImageNet. As a reference, our ResNet-50 (4×) trained from scratch on 100% labels achieves 78.4% top-1 / 94.2% top-5.\nB.3. Effects of Longer Training for Supervised Models\nHere we perform experiments to see how training steps and stronger data augmentation affect supervised training. We test\nResNet-50 and ResNet-50 (4×) under the same set of data augmentations (random crops, color distortion, 50% Gaussian\nblur) as used in our unsupervised models. Figure B.3 shows the top-1 accuracy. We observe that there is no signiﬁcant\nbeneﬁt from training supervised models longer on ImageNet. Stronger data augmentation slightly improves the accuracy of\nResNet-50 (4×) but does not help on ResNet-50. When stronger data augmentation is applied, ResNet-50 generally requires\nlonger training (e.g. 500 epochs 14) to obtain the optimal result, while ResNet-50 (4×) does not beneﬁt from longer training.\nModel\nTraining epochs\nTop 1\nCrop\n+Color\n+Color+Blur\nResNet-50\n90\n76.5\n75.6\n75.3\n500\n76.2\n76.5\n76.7\n1000\n75.8\n75.2\n76.4\nResNet-50 (4×)\n90\n78.4\n78.9\n78.7\n500\n78.3\n78.4\n78.5\n1000\n77.9\n78.2\n78.3\nTable B.3. Top-1 accuracy of supervised models trained longer under various data augmentation procedures (from the same set of data\naugmentations for contrastive learning).\nB.4. Understanding The Non-Linear Projection Head\nFigure B.3 shows the eigenvalue distribution of linear projection matrix W ∈R2048×2048 used to compute z = Wh. This\nmatrix has relatively few large eigenvalues, indicating that it is approximately low-rank.\nFigure B.4 shows t-SNE (Maaten & Hinton, 2008) visualizations of h and z = g(h) for randomly selected 10 classes by\nour best ResNet-50 (top-1 linear evaluation 69.3%). Classes represented by h are better separated compared to z.\n13It is 80.1% top-1 / 95.2% top-5 without broader augmentations for pretraining SimCLR.\n14With AutoAugment (Cubuk et al., 2019), optimal test accuracy can be achieved between 900 and 500 epochs.\n",
  "15": "A Simple Framework for Contrastive Learning of Visual Representations\n0\n500\n1000\n1500\n2000\nRanking\n0\n2\n4\n6\n8\n10\n12\n14\n16\nSquared eigenvalue\n(a) Y-axis in uniform scale.\n0\n500\n1000\n1500\n2000\nRanking\n10−11\n10−9\n10−7\n10−5\n10−3\n10−1\n101\nSquared eigenvalue\n(b) Y-axis in log scale.\nFigure B.3. Squared real eigenvalue distribution of linear projection\nmatrix W ∈R2048×2048 used to compute g(h) = Wh.\n(a) h\n(b) z = g(h)\nFigure B.4. t-SNE visualizations of hidden vectors of images from\na randomly selected 10 classes in the validation set.\nB.5. Semi-supervised Learning via Fine-Tuning\nFine-tuning Procedure\nWe ﬁne-tune using the Nesterov momentum optimizer with a batch size of 4096, momentum of\n0.9, and a learning rate of 0.8 (following LearningRate = 0.05×BatchSize/256) without warmup. Only random cropping\n(with random left-to-right ﬂipping and resizing to 224x224) is used for preprocessing. We do not use any regularization\n(including weight decay). For 1% labeled data we ﬁne-tune for 60 epochs, and for 10% labeled data we ﬁne-tune for 30\nepochs. For the inference, we resize the given image to 256x256, and take a single center crop of 224x224.\nTable B.4 shows the comparisons of top-1 accuracy for different methods for semi-supervised learning. Our models\nsigniﬁcantly improve state-of-the-art.\nMethod\nArchitecture\nLabel fraction\n1%\n10%\nTop 1\nSupervised baseline\nResNet-50\n25.4\n56.4\nMethods using label-propagation:\nUDA (w. RandAug)\nResNet-50\n-\n68.8\nFixMatch (w. RandAug)\nResNet-50\n-\n71.5\nS4L (Rot+VAT+Ent. Min.)\nResNet-50 (4×)\n-\n73.2\nMethods using self-supervised representation learning only:\nCPC v2\nResNet-161(∗)\n52.7\n73.1\nSimCLR (ours)\nResNet-50\n48.3\n65.6\nSimCLR (ours)\nResNet-50 (2×)\n58.5\n71.7\nSimCLR (ours)\nResNet-50 (4×)\n63.0\n74.4\nTable B.4. ImageNet top-1 accuracy of models trained with few labels. See Table 7 for top-5 accuracy.\nB.6. Linear Evaluation\nFor linear evaluation, we follow similar procedure as ﬁne-tuning (described in Appendix B.5), except that a larger learning\nrate of 1.6 (following LearningRate = 0.1 × BatchSize/256) and longer training of 90 epochs. Alternatively, using LARS\noptimizer with the pretraining hyper-parameters also yield similar results. Furthermore, we ﬁnd that attaching the linear\nclassiﬁer on top of the base encoder (with a stop_gradient on the input to linear classiﬁer to prevent the label information\nfrom inﬂuencing the encoder) and train them simultaneously during the pretraining achieves similar performance.\nB.7. Correlation Between Linear Evaluation and Fine-Tuning\nHere we study the correlation between linear evaluation and ﬁne-tuning under different settings of training step and network\narchitecture.\nFigure B.5 shows linear evaluation versus ﬁne-tuning when training epochs of a ResNet-50 (using batch size of 4096) are\nvaried from 50 to 3200 as in Figure B.2. While they are almost linearly correlated, it seems ﬁne-tuning on a small fraction\n",
  "16": "A Simple Framework for Contrastive Learning of Visual Representations\nof labels beneﬁts more from training longer.\n62\n64\n66\n68\n70\nLinear eval\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\nFine-tuning on 1%\n62\n64\n66\n68\n70\nLinear eval\n60\n62\n64\n66\nFine-tuning on 10%\nFigure B.5. Top-1 accuracy of models trained in different epochs (from Figure B.2), under linear evaluation and ﬁne-tuning.\nFigure B.6 shows shows linear evaluation versus ﬁne-tuning for different architectures of choice.\n50\n55\n60\n65\n70\nLinear eval\n30\n33\n36\n39\n42\n45\n48\n51\n54\nFinetune (1%)\nWidth\n1x\n2x\n4x\nDepth\n18\n34\n50\n101\n152\n50\n55\n60\n65\n70\nLinear eval\n51\n54\n57\n60\n63\n66\n69\nFinetune (10%)\nWidth\n1x\n2x\n4x\nDepth\n18\n34\n50\n101\n152\nFigure B.6. Top-1 accuracy of different architectures under linear evaluation and ﬁne-tuning.\nB.8. Transfer Learning\nWe evaluated the performance of our self-supervised representation for transfer learning in two settings: linear evaluation,\nwhere a logistic regression classiﬁer is trained to classify a new dataset based on the self-supervised representation learned\non ImageNet, and ﬁne-tuning, where we allow all weights to vary during training. In both cases, we follow the approach\ndescribed by Kornblith et al. (2019), although our preprocessing differs slightly.\nB.8.1. METHODS\nDatasets\nWe investigated transfer learning performance on the Food-101 dataset (Bossard et al., 2014), CIFAR-10\nand CIFAR-100 (Krizhevsky & Hinton, 2009), Birdsnap (Berg et al., 2014), the SUN397 scene dataset (Xiao et al.,\n2010), Stanford Cars (Krause et al., 2013), FGVC Aircraft (Maji et al., 2013), the PASCAL VOC 2007 classiﬁcation\ntask (Everingham et al., 2010), the Describable Textures Dataset (DTD) (Cimpoi et al., 2014), Oxford-IIIT Pets (Parkhi et al.,\n2012), Caltech-101 (Fei-Fei et al., 2004), and Oxford 102 Flowers (Nilsback & Zisserman, 2008). We follow the evaluation\nprotocols in the papers introducing these datasets, i.e., we report top-1 accuracy for Food-101, CIFAR-10, CIFAR-100,\nBirdsnap, SUN397, Stanford Cars, and DTD; mean per-class accuracy for FGVC Aircraft, Oxford-IIIT Pets, Caltech-101,\nand Oxford 102 Flowers; and the 11-point mAP metric as deﬁned in Everingham et al. (2010) for PASCAL VOC 2007. For\nDTD and SUN397, the dataset creators deﬁned multiple train/test splits; we report results only for the ﬁrst split. Caltech-101\ndeﬁnes no train/test split, so we randomly chose 30 images per class and test on the remainder, for fair comparison with\nprevious work (Donahue et al., 2014; Simonyan & Zisserman, 2014).\nWe used the validation sets speciﬁed by the dataset creators to select hyperparameters for FGVC Aircraft, PASCAL VOC\n",
  "17": "A Simple Framework for Contrastive Learning of Visual Representations\n2007, DTD, and Oxford 102 Flowers. For other datasets, we held out a subset of the training set for validation while\nperforming hyperparameter tuning. After selecting the optimal hyperparameters on the validation set, we retrained the\nmodel using the selected parameters using all training and validation images. We report accuracy on the test set.\nTransfer Learning via a Linear Classiﬁer\nWe trained an ℓ2-regularized multinomial logistic regression classiﬁer on\nfeatures extracted from the frozen pretrained network. We used L-BFGS to optimize the softmax cross-entropy objective\nand we did not apply data augmentation. As preprocessing, all images were resized to 224 pixels along the shorter side\nusing bicubic resampling, after which we took a 224 × 224 center crop. We selected the ℓ2 regularization parameter from a\nrange of 45 logarithmically spaced values between 10−6 and 105.\nTransfer Learning via Fine-Tuning\nWe ﬁne-tuned the entire network using the weights of the pretrained network as\ninitialization. We trained for 20,000 steps at a batch size of 256 using SGD with Nesterov momentum with a momentum\nparameter of 0.9. We set the momentum parameter for the batch normalization statistics to max(1 −10/s, 0.9) where s is\nthe number of steps per epoch. As data augmentation during ﬁne-tuning, we performed only random crops with resize and\nﬂips; in contrast to pretraining, we did not perform color augmentation or blurring. At test time, we resized images to 256\npixels along the shorter side and took a 224 × 224 center crop. (Additional accuracy improvements may be possible with\nfurther optimization of data augmentation, particularly on the CIFAR-10 and CIFAR-100 datasets.) We selected the learning\nrate and weight decay, with a grid of 7 logarithmically spaced learning rates between 0.0001 and 0.1 and 7 logarithmically\nspaced values of weight decay between 10−6 and 10−3, as well as no weight decay. We divide these values of weight decay\nby the learning rate.\nTraining from Random Initialization\nWe trained the network from random initialization using the same procedure\nas for ﬁne-tuning, but for longer, and with an altered hyperparameter grid. We chose hyperparameters from a grid of 7\nlogarithmically spaced learning rates between 0.001 and 1.0 and 8 logarithmically spaced values of weight decay between\n10−5 and 10−1.5. Importantly, our random initialization baselines are trained for 40,000 steps, which is sufﬁciently long to\nachieve near-maximal accuracy, as demonstrated in Figure 8 of Kornblith et al. (2019).\nOn Birdsnap, there are no statistically signiﬁcant differences among methods, and on Food-101, Stanford Cars, and FGVC\nAircraft datasets, ﬁne-tuning provides only a small advantage over training from random initialization. However, on the\nremaining 8 datasets, pretraining has clear advantages.\nSupervised Baselines\nWe compare against architecturally identical ResNet models trained on ImageNet with standard\ncross-entropy loss. These models are trained with the same data augmentation as our self-supervised models (crops, strong\ncolor augmentation, and blur) and are also trained for 1000 epochs. We found that, although stronger data augmentation and\nlonger training time do not beneﬁt accuracy on ImageNet, these models performed signiﬁcantly better than a supervised\nbaseline trained for 90 epochs and ordinary data augmentation for linear evaluation on a subset of transfer datasets. The\nsupervised ResNet-50 baseline achieves 76.3% top-1 accuracy on ImageNet, vs. 69.3% for the self-supervised counterpart,\nwhile the ResNet-50 (4×) baseline achieves 78.3%, vs. 76.5% for the self-supervised model.\nStatistical Signiﬁcance Testing\nWe test for the signiﬁcance of differences between model with a permutation test. Given\npredictions of two models, we generate 100,000 samples from the null distribution by randomly exchanging predictions\nfor each example and computing the difference in accuracy after performing this randomization. We then compute the\npercentage of samples from the null distribution that are more extreme than the observed difference in predictions. For top-1\naccuracy, this procedure yields the same result as the exact McNemar test. The assumption of exchangeability under the null\nhypothesis is also valid for mean per-class accuracy, but not when computing average precision curves. Thus, we perform\nsigniﬁcance testing for a difference in accuracy on VOC 2007 rather than a difference in mAP. A caveat of this procedure is\nthat it does not consider run-to-run variability when training the models, only variability arising from using a ﬁnite sample\nof images for evaluation.\nB.8.2. RESULTS WITH STANDARD RESNET\nThe ResNet-50 (4×) results shown in Table 8 of the text show no clear advantage to the supervised or self-supervised models.\nWith the narrower ResNet-50 architecture, however, supervised learning maintains a clear advantage over self-supervised\nlearning. The supervised ResNet-50 model outperforms the self-supervised model on all datasets with linear evaluation,\nand most (10 of 12) datasets with ﬁne-tuning. The weaker performance of the ResNet model compared to the ResNet (4×)\n",
  "18": "A Simple Framework for Contrastive Learning of Visual Representations\nFood CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers\nLinear evaluation:\nSimCLR (ours)\n68.4\n90.6\n71.6\n37.4\n58.8\n50.3\n50.3\n80.5\n74.5\n83.6\n90.3\n91.2\nSupervised\n72.3\n93.6\n78.3\n53.7\n61.9\n66.7\n61.0\n82.8\n74.9\n91.5\n94.5\n94.7\nFine-tuned:\nSimCLR (ours)\n88.2\n97.7\n85.9\n75.9\n63.5\n91.3\n88.1\n84.1\n73.2\n89.2\n92.1\n97.0\nSupervised\n88.3\n97.5\n86.4\n75.8\n64.3\n92.1\n86.0\n85.0\n74.6\n92.1\n93.3\n97.6\nRandom init\n86.9\n95.9\n80.2\n76.1\n53.6\n91.4\n85.9\n67.3\n64.8\n81.5\n72.6\n92.0\nTable B.5. Comparison of transfer learning performance of our self-supervised approach with supervised baselines across 12 natural\nimage datasets, using ImageNet-pretrained ResNet models. See also Figure 8 for results with the ResNet (4×) architecture.\nmodel may relate to the accuracy gap between the supervised and self-supervised models on ImageNet. The self-supervised\nResNet gets 69.3% top-1 accuracy, 6.8% worse than the supervised model in absolute terms, whereas the self-supervised\nResNet (4×) model gets 76.5%, which is only 1.8% worse than the supervised model.\nB.9. CIFAR-10\nWhile we focus on using ImageNet as the main dataset for pretraining our unsupervised model, our method also works with\nother datasets. We demonstrate it by testing on CIFAR-10 as follows.\nSetup\nAs our goal is not to optimize CIFAR-10 performance, but rather to provide further conﬁrmation of our observations\non ImageNet, we use the same architecture (ResNet-50) for CIFAR-10 experiments. Because CIFAR-10 images are much\nsmaller than ImageNet images, we replace the ﬁrst 7x7 Conv of stride 2 with 3x3 Conv of stride 1, and also remove the ﬁrst\nmax pooling operation. For data augmentation, we use the same Inception crop (ﬂip and resize to 32x32) as ImageNet,15 and\ncolor distortion (strength=0.5), leaving out Gaussian blur. We pretrain with learning rate in {0.5, 1.0, 1.5}, temperature in\n{0.1, 0.5, 1.0}, and batch size in {256, 512, 1024, 2048, 4096}. The rest of the settings (including optimizer, weight decay,\netc.) are the same as our ImageNet training.\nOur best model trained with batch size 1024 can achieve a linear evaluation accuracy of 94.0%, compared to 95.1% from the\nsupervised baseline using the same architecture and batch size. The best self-supervised model that reports linear evaluation\nresult on CIFAR-10 is AMDIM (Bachman et al., 2019), which achieves 91.2% with a model 25× larger than ours. We note\nthat our model can be improved by incorporating extra data augmentations as well as using a more suitable base network.\nPerformance under different batch sizes and training steps\nFigure B.7 shows the linear evaluation performance under\ndifferent batch sizes and training steps. The results are consistent with our observations on ImageNet, although the largest\nbatch size of 4096 seems to cause a small degradation in performance on CIFAR-10.\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTraining epochs\n80\n82\n84\n86\n88\n90\n92\n94\nTop 1\nBatch size\n256\n512\n1024\n2048\n4096\nFigure B.7. Linear evaluation of ResNet-50 (with ad-\njusted stem) trained with different batch size and\nepochs on CIFAR-10 dataset. Each bar is averaged\nover 3 runs with different learning rates (0.5, 1.0,\n1.5) and temperature τ = 0.5. Error bar denotes\nstandard deviation.\n15It is worth noting that, although CIFAR-10 images are much smaller than ImageNet images and image size does not differ among\nexamples, cropping with resizing is still a very effective augmentation for contrastive learning.\n",
  "19": "A Simple Framework for Contrastive Learning of Visual Representations\nOptimal temperature under different batch sizes\nFigure B.8 shows the linear evaluation of model trained with three\ndifferent temperatures under various batch sizes. We ﬁnd that when training to convergence (e.g. training epochs > 300), the\noptimal temperature in {0.1, 0.5, 1.0} is 0.5 and seems consistent regardless of the batch sizes. However, the performance\nwith τ = 0.1 improves as batch size increases, which may suggest a small shift of optimal temperature towards 0.1.\n256\n512\n1024\n2048\n4096\nBatch size\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5\n95.0\nTop 1\nTemperature\n0.1\n0.5\n1.0\n(a) Training epochs ≤300\n256\n512\n1024\n2048\n4096\nBatch size\n90\n91\n92\n93\n94\n95\nTop 1\nTemperature\n0.1\n0.5\n1.0\n(b) Training epochs > 300\nFigure B.8. Linear evaluation of the model (ResNet-50) trained with three temperatures on different batch sizes on CIFAR-10. Each bar is\naveraged over multiple runs with different learning rates and total train epochs. Error bar denotes standard deviation.\nB.10. Tuning For Other Loss Functions\nThe learning rate that works best for NT-Xent loss may not be a good learning rate for other loss functions. To ensure a\nfair comparison, we also tune hyperparameters for both margin loss and logistic loss. Speciﬁcally, we tune learning rate\nin {0.01, 0.1, 0.3, 0.5, 1.0} for both loss functions. We further tune the margin in {0, 0.4, 0.8, 1.6} for margin loss, the\ntemperature in {0.1, 0.2, 0.5, 1.0} for logistic loss. For simplicity, we only consider the negatives from one augmentation\nview (instead of both sides), which slightly impairs performance but ensures fair comparison.\nC. Further Comparison to Related Methods\nAs we have noted in the main text, most individual components of SimCLR have appeared in previous work, and the\nimproved performance is a result of a combination of these design choices. Table C.1 provides a high-level comparison of\nthe design choices of our method with those of previous methods. Compared with previous work, our design choices are\ngenerally simpler.\nModel\nData Augmentation\nBase Encoder\nProjection Head\nLoss\nBatch Size\nTrain Epochs\nCPC v2\nCustom\nResNet-161 (modiﬁed)\nPixelCNN\nXent\n512#\n∼200\nAMDIM\nFast AutoAug.\nCustom ResNet\nNon-linear MLP\nXent w/ clip,reg\n1008#\n150\nCMC\nFast AutoAug.\nResNet-50 (2×, L+ab)\nLinear layer\nXent w/ ℓ2, τ\n156∗\n280\nMoCo\nCrop+color\nResNet-50 (4×)\nLinear layer\nXent w/ ℓ2, τ\n256∗\n200\nPIRL\nCrop+color\nResNet-50 (2×)\nLinear layer\nXent w/ ℓ2, τ\n1024∗\n800\nSimCLR\nCrop+color+blur\nResNet-50 (4×)\nNon-linear MLP\nXent w/ ℓ2, τ\n4096\n1000\nTable C.1. A high-level comparison of design choices and training setup (for best result on ImageNet) for each method. Note that\ndescriptions provided here are general; even when they match for two methods, formulations and implementations may differ (e.g. for\ncolor augmentation). Refer to the original papers for more details. #Examples are split into multiple patches, which enlarges the effective\nbatch size. ∗A memory bank is employed.\nIn below, we provide an in-depth comparison of our method to the recently proposed contrastive representation learning\nmethods:\n• DIM/AMDIM (Hjelm et al., 2018; Bachman et al., 2019) achieve global-to-local/local-to-neighbor prediction by\npredicting the middle layer of ConvNet. The ConvNet is a ResNet that has bewen modiﬁed to place signiﬁcant\nconstraints on the receptive ﬁelds of the network (e.g. replacing many 3x3 Convs with 1x1 Convs). In our framework,\nwe decouple the prediction task and encoder architecture, by random cropping (with resizing) and using the ﬁnal\n",
  "20": "A Simple Framework for Contrastive Learning of Visual Representations\nrepresentations of two augmented views for prediction, so we can use standard and more powerful ResNets. Our\nNT-Xent loss function leverages normalization and temperature to restrict the range of similarity scores, whereas they\nuse a tanh function with regularization. We use a simpler data augmentation policy, while they use FastAutoAugment\nfor their best result.\n• CPC v1 and v2 (Oord et al., 2018; Hénaff et al., 2019) deﬁne the context prediction task using a deterministic strategy\nto split examples into patches, and a context aggregation network (a PixelCNN) to aggregate these patches. The base\nencoder network sees only patches, which are considerably smaller than the original image. We decouple the prediction\ntask and the encoder architecture, so we do not require a context aggregation network, and our encoder can look at the\nimages of wider spectrum of resolutions. In addition, we use the NT-Xent loss function, which leverages normalization\nand temperature, whereas they use an unnormalized cross-entropy-based objective. We use simpler data augmentation.\n• InstDisc, MoCo, PIRL (Wu et al., 2018; He et al., 2019; Misra & van der Maaten, 2019) generalize the Exemplar\napproach originally proposed by Dosovitskiy et al. (2014) and leverage an explicit memory bank. We do not use a\nmemory bank; we ﬁnd that, with a larger batch size, in-batch negative example sampling sufﬁces. We also utilize a\nnonlinear projection head, and use the representation before the projection head. Although we use similar types of\naugmentations (e.g., random crop and color distortion), we expect speciﬁc parameters may be different.\n• CMC (Tian et al., 2019) uses a separated network for each view, while we simply use a single network shared for all\nrandomly augmented views. The data augmentation, projection head and loss function are also different. We use larger\nbatch size instead of a memory bank.\n• Whereas Ye et al. (2019) maximize similarity between augmented and unaugmented copies of the same image, we\napply data augmentation symmetrically to both branches of our framework (Figure 2). We also apply a nonlinear\nprojection on the output of base feature network, and use the representation before projection network, whereas Ye\net al. (2019) use the linearly projected ﬁnal hidden vector as the representation. When training with large batch sizes\nusing multiple accelerators, we use global BN to avoid shortcuts that can greatly decrease representation quality.\n"
}