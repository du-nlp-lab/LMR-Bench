{
  "1": "Hyena Hierarchy:\nTowards Larger Convolutional Language Models\nMichael Poli∗,1, Stefano Massaroli∗,2, Eric Nguyen1,∗,\nDaniel Y. Fu1, Tri Dao1, Stephen Baccus1,\nYoshua Bengio2, Stefano Ermon1,†, Christopher Ré1,†\nVersion: submitted draft, Last Compiled: April 21, 2023\nAbstract\nRecent advances in deep learning have relied heavily on the use of large Transformers due to their\nability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits\nquadratic cost in sequence length, limiting the amount of context accessible.\nExisting subquadratic\nmethods based on low-rank and sparse approximations need to be combined with dense attention layers\nto match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic\ndrop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions\nand data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of\nthousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-\nspaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-\nthe-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103\nand The Pile), reaching Transformer quality with a 20% reduction in training compute required at\nsequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length\n8K, and 100× faster at sequence length 64K.\n1\nIntroduction\nLarge Transformers have enabled a number of breakthrough advances in modeling language, vision, audio,\nbiology and numerous other domains (Vaswani et al., 2017), (Dosovitskiy et al., 2020), (Radford et al., 2022),\n(Cramer, 2021). Much of the success of Transformers, powered by the attention operator (Vaswani et al.,\n2017), relies on their scaling properties (Hoﬀmann et al., 2022) and the emergence of in-context learning\n(Garg et al., 2022), which allows them to generalize to unseen data and tasks given context as input. The\nTransformer block is a powerful tool for sequence modeling, but it is not without its limitations. One of the\nmost notable is the computational cost, which grows rapidly as the length of the input sequence increases.\nSpeciﬁcally, the cost scales quadratically with the length L of the sequence, which places a strict limit on the\namount of context that can be considered by the model. Breaking the quadratic barrier is a key step towards\nnew possibilities for deep learning, such as using entire textbooks as context, generating long-form music or\nprocessing gigapixel scale images.\nEﬀorts to reduce the computational cost of attention in models primarily involve the use of linearized,\nlow-rank, and sparse approximations (Child et al., 2019; Wang et al., 2020; Kitaev et al., 2020; Zhai et al.,\n2021; Roy et al., 2021; Schlag et al., 2021; Tu et al., 2022). These approaches introduce a trade-oﬀbetween\nexpressivity and speed, requiring hybridization with standard attention layers to reach Transformer quality\n(Mehta et al., 2022; Dao et al., 2022c).\nA growing amount of evidence suggests that attention mechanisms only utilize a small portion of their\nquadratic capabilities for language processing (Olsson et al., 2022; Dao et al., 2022c), leading us to question\nits role as the gold-standard operator for deep learning at scale. Speciﬁcally, we ask:\nAre there subquadratic operators that can match the quality of attention at scale?\n∗Equal contribution. † Equal senior authorship. 1Stanford University. 2Mila and Université de Montréal.\n1\narXiv:2302.10866v3  [cs.LG]  19 Apr 2023\n",
  "2": "Figure 1.1: The Hyena operator is deﬁned as a recurrence of two eﬃcient subquadratic primitives: an implicit\nlong convolution h (i.e. Hyena ﬁlters parameterized by a feed-forward network) and multiplicative element-\nwise gating of the (projected) input. The depth of the recurrence speciﬁes the size of the operator. Hyena\ncan equivalently be expressed as a multiplication with data-controlled (conditioned by the input u) diagonal\nmatrices Dx and Toeplitz matrices Sh. In addition, Hyena exhibits sublinear parameter scaling (in sequence\nlength) and unrestricted context, similar to attention, while having lower time complexity.\nWe obtain a positive answer based on a composition of eﬃcient subquadratic primitives, such as element-\nwise multiplication (gating) and long convolutions i.e., convolutions with ﬁlter sizes as long as the input. We\nrely on a set of targeted reasoning tasks, grounded in recent work on mechanistic interpretability (Elhage\net al., 2021; Power et al., 2022; Olsson et al., 2022; Zhang et al., 2022) such as recall and induction, to distill\nthree properties of attention correlated with its performance and the quality gap with existing subquadratic\napproaches:\na. Data control: Attention implements an expressive data-controlled (Massaroli et al., 2020) linear operator1,\nencoding an entire family of linear functions in a single block.\nb. Sublinear parameter scaling: Parameter counts of attention layers are decoupled from sequence length,\nallowing Transformers to allocate more parameters elsewhere e.g., the feed-forward neural networks (FFNs)\nbetween attention layers.\nc. Unrestricted context: For a given input, attention has an unrestricted context i.e., it can approximate\ndependencies between any two inputs, without arbitrary restrictions such as locality (except in cases using\nmasking such as autoregressive models).\nThe Hyena hierarchy\nGuided by these ﬁndings, we introduce the Hyena hierarchy, an operator deﬁned by a\nrecurrence of two eﬃcient subquadratic primitives: a long convolution and element-wise multiplicative\ngating (see Figure 1.1). A speciﬁed depth (i.e., number of steps) of the recurrence controls the size of the\noperator.\nFor short recurrences, existing models are recovered as special cases (Mehta et al., 2022; Dao\net al., 2022c). By mapping each step in the Hyena recurrence to its corresponding matrix form, we reveal\nHyena operators to be equivalently deﬁned as a decomposition of a data-controlled matrix i.e., a matrix\nwhose entries are functions of the input.\nFurthermore, we show how Hyena operators can be evaluated\neﬃciently without materializing the full matrix, by leveraging fast convolution algorithms (Selesnick and\nBurrus, 2017). Empirically, Hyena operators are able to signiﬁcantly shrink the quality gap with attention at\nscale, reaching similar perplexity and downstream performance with a smaller computational budget (Section\n4.2) and without hybridization of attention.\nNarrowing the capabilities gap\nThe design of Hyena is motivated by a quality gap between standard\ndense attention and alternative subquadratic operators, which we identify by focusing on reasoning tasks cor-\nrelated with language modeling performance at scale. We extend the suite of basic mechanistic interpretability\nbenchmarks (induction and recall) with additional tasks that probe how quickly model performance degrades\n1Self-attention can be expressed as y = A(k, q)v where A is the attention matrix conditioned by linear projections k, q of the\ninput and multiplied by v, another projection.\n2\n",
  "3": "when task complexity increases (e.g. vocabulary size grows). In addition, we investigate the optimal param-\neterization of long convolutions in Hyena. In the most challenging settings with hundreds of thousands of\ntokens, our implicit parameterization scheme improves over other operators leveraging state spaces (Gu et al.,\n2021), frequency-domain parametrizations (Li et al., 2020), or standard convolutions by over 50% accuracy.\nScaling in language and vision\nNext, we aim to verify whether rankings in our reasoning benchmark\nsuite are predictive of quality at scale. We test Hyena on autoregressive language modeling at the sub-billion\nparameter scale, setting a new state-of-the-art for dense-attention-free architectures in standard datasets\n(WikiText103 and The Pile) and matching Transformer quality. On the The Pile at the 335M parame-\nter scale, we match Transformer perplexity with a 20% reduction in the total count of ﬂoating point operations\n(FLOPs). As an extension, we investigate the generality of Hyena operators by testing on large-scale im-\nage recognition, replacing attention in the Vision Transformer (ViT) (Dosovitskiy et al., 2020). In image\nclassiﬁcation, Hyena is able to match attention in accuracy when training on ImageNet-1k from scratch.\nToward much longer context\nFinally, we benchmark the eﬃciency of Hyena on long sequences. We\nmeasure 5x speedups over dense self-attention at length 8192 – 2x over highly optimized FlashAttention2\n(Dao et al., 2022b) – and 100x speedup over FlashAttention at sequence lengths of 64k, where standard\nattention implementation in PyTorch runs out of memory.\n2\nPreliminaries and Related Work\nA discrete convolution is a function of two arguments: an input u signal of length L and a learnable ﬁlter h.\nThe linear (aperiodic) convolution of a (possibly inﬁnitely long) measurable3 ﬁlter h with a length-L input\nsignal u is deﬁned as\nyt = (h ∗u)t =\nL−1\nX\nn=0\nht−nun.\n(1)\nGenerally, ut ∈RD where D is the width of the signal, or in deep learning parlance, the number of\nchannels. Without loss of generality, we specialize our analysis to single input single output (SISO) layers,\ni.e. with D = 1. The multiple input multiple output (MIMO) case, canonical in standard convolutional layers,\nfollows directly.\nIn this case, the input signal can be represented as a vector u ∈RL and the convolution as a matrix-vector\nproduct between the input and the Toeplitz kernel matrix Sh ∈RL×L induced by the ﬁlter h:\n(h ∗u) =\n\n\nh0\nh−1\n· · ·\nh−L+1\nh1\nh0\n· · ·\nh−L+2\n...\n...\n...\n...\nhL−1\nhL−2\n· · ·\nh0\n\n\n\n\nu0\nu1\n...\nuL−1\n\n\n(2)\n2.1\nExplicit and Implicit Convolutions\nParametrizing and optimizing convolution ﬁlters ht is a standard procedure in deep learning and more broadly\nsignal processing. The classical approach of convolutional neural networks (CNNs) (Fukushima and Miyake,\n1982; LeCun et al., 1998; Ronneberger et al., 2015; He et al., 2016) is to optimize directly the values ht of\nthe ﬁlter’s response at M prescribed steps, a parametrization we call explicit. M is referred to as the ﬁlter\nsize and is typically much shorter than the input sequence length M ≪L. Such ﬁlters are denoted in signal\nprocessing as ﬁnite impulse response (FIR).\nFIR ﬁlters are local and can capture dependencies between inputs separated at most by M steps. Their\nmain advantage is their speed, with complexity O(ML). However, the number of parameters of FIR ﬁlters\nscales linearly with ﬁlter size, which can be computationally prohibitive. To disentangle the parameter count\nfrom the ﬁlter size, we can instead represent the ﬁlter ht as a parametric function of the time step t, i.e.\nht = γθ(t), where θ are the parameters of the function γθ. This parametrization is called implicit. The class\n2FlashAttention is already 2-4x faster than a standard attention implementation in PyTorch.\n3In the L1(Z) sense: P∞\nt=−∞|ht| < ∞\n3\n",
  "4": "of functions γθ is a design choice with a signiﬁcant impact on the expressivity and computational complexity\nof the layer.\nOne choice of implicit parametrization is to select h as the response function of a linear state-space model\n(SSM) (Chen, 1984), described by the ﬁrst-order diﬀerence equation:\nxt+1 = Axt + But\nstate equation\nyt = Cxt + Dut\noutput equation\nHere, the convenient choice of x0 = 0 renders the input-output map to a simple convolution\nyt =\nt\nX\nn=0\n\u0000CAt−nB + Dδt−n\n\u0001\nun\nwhere δt denotes the Kronecker delta. We can then identify the ﬁlter h as\nt 7→ht =\n(\n0\nt < 0\nCAtB + Dδt\nt ≥0\nwhere the entries of A, B, C and D are the learned parameters of the ﬁlter. In terms of layer design, the\ndegrees of freedom of SSMs are the dimension of the state and the structure of the matrices. SSMs are\na canonical example of how long convolutions with sub-linear parameter counts can improve deep learning\nmodels for long sequences (Gu et al., 2020, 2021). Other implicit approaches include parametrizing ﬁlters\nas maps from (a positional encoding of) t to the ﬁlter response i.e. γθ : t 7→ht = γθ(t), for example with\nfeed-forward neural networks (Romero et al., 2021b,a).\nLong convolutions and memory: A crude proxy for memory of a single computational unit is how\nfar in the past it can access information to produce the output at a certain step. This can be roughly\nquantiﬁed by the number of non-zero entries ∂yt/∂ut−n for n = 0, . . . , t.\nThe memory of CNNs\nﬁlters is equivalent to the ﬁlter size M since ∂yt/∂ut−n = hn. The total mnemonic capacity of an all-\nconvolutions CNN therefore scales with the number of model’s parameters. Implicit parametrizations,\non the other hand, allow us to disentangle the memory of each ﬁlter from the parameter count\nand where the length of the ﬁlter is implicitly controlled by the learned parameters. In an SSM,\n∂yt/∂ut−n = CAnB and the memory extent is solely determined by the spectral radius of A and can\nbe ﬁnely tuned by the training processa. On the other hand, the number of parameters controls the\nexpressivity of the memory unit, e.g. the number of basis functions forming ht.\naSee e.g.Gu et al. (2020, 2021)\nFast Methods for Convolutions\nOne of the ﬁrst applications of the Cooley-Tukey fast Fourier transform\n(FFT) algorithm was to implement convolution faster than the direct evaluation of (1). At ﬁrst glance (1)\ncomes with O(L2) an asymptotic time complexity. A common approach to achieve fast long convolutions in\nsubquadratic time is through the FFT algorithm. The method ﬁrst converts the aperiodic convolution into a\ncircular convolution Selesnick and Burrus (2017) by appropriate zero-padding of input and ﬁlter sequences.\nThe resulting kernel ˆSh is a circulant matrix and is diagonalized by the discrete Fourier basis\nˆSh = W−1DHW\nwhere W is the DFT matrix, Wtt′ = z−t, z = ei2πt′/L and H is the DFT of the padded ﬁlter h, H = Wpad(h).\nThus, the calculation of such convolutions is performed as\npad(y) = ˆShpad(u)\n= W−1DHW pad(u)\n= iFFT(DHFFT(pad(u)))\nwhere DH is the matrix with Wh on its diagonal. The above is known as the convolution theorem of DFT\n(Oppenheim et al., 1997). In this FFTConv form the convolution can be performed without materializing\nthe operator Sh with the same asymptotic cost O(L log2 L) of FFT.\n4\n",
  "5": "Figure 2.1: Comparison between data-controlled matrices: SelfAttention and Hyena.\n2.2\nThe Self-Attention Operator\nAt the heart of Transformers is the multi-head attention (MHA) mechanism. Given a length-L sequence\nu ∈RL×D, each head of scaled self-attention (Vaswani et al., 2017) is a map from RL×D to RL×D which\nperforms the following operations\nA(u) = SoftMax\n\u0010\n1\n√\nDuMqM⊤\nk u⊤\u0011\ny = SelfAttention(u)\n= A(u)uMv,\n(3)\nwhere Mq, Mk, Mv ∈RD×D are learnable linear projections and SoftMax is intended to be applied row-wise.\nAttention parametrizes a family of dense linear operators and for an input u, indexes through it via\nprojections of u i.e., A(u). We refer to operators of this type as data-controlled, as they encode a linear\ntransformation u 7→y, that is, however, nonlinearly deﬁned by u. This approach yields expressive nonlinear\noperators in u, and we hypothesize contributes, together with other mechanisms (Olsson et al., 2022), to the\nability of certain operators to learn in-context i.e., to adapt to unseen tasks by leveraging context. In deep\nlearning, the projections take on speciﬁc names: query q = uMq, key k = uMk and value v = uMv. We often\nrewrite the attention operator as y = A(q, k)v.\nRemark 2.1. Similarly to implicit convolutions, SelfAttention does not entangle its ability to access distant\ninformation with the number of parameters: it looks at the whole sequence at the price of O(L2) operations.\nSubquadratic Operators\nExisting approaches to subquadratic alternatives to attention can be summa-\nrized by altering the way the data control is implemented i.e., how the operator is nonlinearly deﬁned by\nu, and then applied to v. For example, a layer of Attention-Free Transformers (AFTs) (Zhai et al., 2021)\nconstructs the operator through a combination of gating and SoftMax (AFT full) or gating and a single\nexplicit convolution (AFT conv). Gated State Spaces (GSS) instead compose the operator via gating and a\nlong convolution parametrized via SSMs. Taking this idea further, Hungry Hungry Hippo (H3) (Dao et al.,\n2022c), motivated by gaps of GSS on associative recall, extend the mechanism to include an additional gate\nand a short convolution obtained via a shift SSM. Hyena generalizes this body of work by introducing a\nrecurrence of gates and implicit long convolutions, evaluated eﬃciently.\n3\nHyena: Deﬁnition and Properties\nIn this section, we deﬁne Hyena, a class of data-controlled operators consisting of a recurrence of multiplicative\ngating interactions and long convolutions. Instead of seeking an approximation to attention, we guide our\ndesign by intentionally incorporating key computational properties of attention, including the decoupling of\nsequence length and parameter counts.\n3.1\nHyena Recurrences\nAt a high level, Hyena consists of the following steps (setting D = 1 for clarity):\ni. Compute a set of N +1 linear projections of the input, similarly to attention. The number of projections\n(vt, x1\nt, . . . , xN\nt ) need not be three. One projection takes the role of value, such that a linear input-output\nfunction can be deﬁned as y = H(u)v for some H(u).\n5\n",
  "6": "ii. The matrix H(u) is deﬁned by interleaving implicit long convolutions and element-wise multiplication\nwith one projection xi at a time, until all projections are exhausted.\nEvaluation of H(u)v is done\neﬃciently without materializing H(u). By doing so, we implicitly deﬁne a data-controlled operator\nas a factorization of a matrix. The long convolutions forming H(u) are parametrized implicitly to retain\nsublinear parameter scaling in sequence length.\nNext, we formally deﬁne Hyena, starting with its computational model. We leave the analysis of its data-\ncontrolled matrix form for the latter part of the section.\nDeﬁnition 3.1 (Order–N Hyena Operator). Let (v, x1, · · · , xN) be projections of the input and let\nh1, . . . , hN be a set of learnable ﬁlters. The HyenaN operator is deﬁned by the recurrence:\nz1\nt = vt\nzn+1\nt\n= xn\nt (hn ∗zn)t\nn = 1, . . . , N\nyt = zN+1\nt\n(4)\nRemark 3.1. The time complexity of a Hyena recurrence is O(NL log2 L). The input-output map can be\nrewritten as\ny = xN · (hN ∗(xN−1 · (hN−1 ∗(· · · ))))\nwhere each convolution is performed through the Fourier domain in O(L log2 L).\nInterestingly, the element-wise product in time domain corresponds to convolution in frequency domain,\ni.e.\nxtut = (ˆx ∗ˆu)t,\nwhere ˆx, ˆu denote the DFT of x and u, respectively. Thus, Hyena is alternatively applying convolutions in\nthe time and then the frequency domain (or alternatively applying element-wise products in the time and\nfrequency domain). One potential explanation for the eﬀectiveness of this procedure is that the convolution\nin the time domain (element-wise multiplication in the frequency domain) increases the memory length,\nallowing for a broader context to be taken into account. On the other hand, the element-wise multiplication\nin the time domain (convolution in the frequency domain) allows for more ﬁne-grained selection of speciﬁc\nfrequency components of the signal.\n3.2\nHyena Matrices\nHyena operators build on the H3 mechanism developed by (Dao et al., 2022c). For clarity of exposition, we\nonce again consider the SISO case (D = 1). Let Dq and Dk be the L-by-L diagonal matrices whose respective\nmain diagonal entries are the respective entries of q and k. H3 realizes a surrogate attention matrix with a\ndata-controlled, parametrized decomposition in four terms:\nA(q, k) = DqSψDkSϕ\nH3(q, k, v) = A(q, k)v\n(5)\nwhere Sϕ, Sψ are the Toeplitz matrices of learnable causal ﬁlters ϕ, ψ parametrized via SSMs4. Alongside the\nqkv-projections the ﬁlters constitute our degrees of freedom in the layer design. This decomposition allows\nevaluation of (8) in just O(L log2 L) time (two FFT convolutions and two element-wise products), i.e.\nzt = kt(ϕ ∗v)t\nyt = qt(ψ ∗z)t\n(6)\nHyena represents a generalization of (8) for an arbitrary number of projections – not limited to three – and\nwith implicit free-form long ﬁlters for the convolutions. The resulting recurrence (4) can be also represented\nin matrix form y = H(u)v. Let Dn\nx = diag(xn) ∈RL×L and let Sn\nh be the Toeplitz matrix corresponding to\nﬁlter hn. The resulting Hyena recurrence is linear in v and can be rewritten in matrix form:\ny = H(u)v = DN\nx SN\nh · · · D2\nxS2\nhD1\nxS1\nhv\nFigure 2.1 visualizes an example decomposition.\n4For consistency with our discussion, we have swapped k and v compared to the notation in (Dao et al., 2022c).\n6\n",
  "7": "Sequence Length\nFFN(t)\nSequence Length\nWindow\nSequence Length\nWindow ◦FFN(t)\nFigure 3.1:\n[Top]:\nExample of long convolution parametrization for Hyena operators, with a decay\nWindow(t) = exp{−αt}. Parameter α is modiﬁed across the independent channels of Hyena to regularize\nﬁlters to be of diﬀerent lengths. In practice, we add a bias term to our window, so that the ﬁlters are not\nconstrained to be zeros after a length determined by the decay rate.\nRemark 3.2 (Hyena generalizes H3 and GSS.). The H3 mechanism (Dao et al., 2022c) corresponds to Hyena2\nand GSS (Mehta et al., 2022) is Hyena1, with a particular choice of parametrization for the long convolutions\n(SSMs).\nAnalysis of the H3 mechanism as a decomposition DqSψDkSϕ of its surrogate attention matrix5 clariﬁes\na connection to fast evaluation algorithms for matrix-vector multiplications. In particular, the generalization\nof (8) to an arbitrary order is inspired by fast evaluation algorithms for structured dense matrices based on\nbutterﬂy decompositions (Li et al., 2015; Dao et al., 2019, 2022a), with length of the decomposition closely\ntied to its expressivity (in the classes of matrices it can represent). The Hyena operator blends data control\nwith a special case of butterﬂy decomposition.\nRemark 3.3. Hyena operators have unbounded context. Namely, they are not artiﬁcially restricted by e.g.,\nlocality, and can learn long-range dependencies between any of the elements of v via long convolutions, which\nwe discuss next.\n3.3\nHyena Filters\nHere we provide details on the convolution parametrization. We represent the ﬁlters of each Hyena operator\nas a map from the time (or space) domain t to values ht, and learn it with a shallow feed-forward neural\nnetwork (FFN):\nht = Window(t) · (FFN ◦PositionalEncoding)(t)\n(7)\nThis approach builds on the neural implicit representation literature (Mildenhall et al., 2021; Sitzmann et al.,\n2020), which has found application in long convolution layers (Romero et al., 2021b,a). One advantage of\n(7) is given by the decoupling of ﬁlter length and parameter cost.\nSpecializing ﬁlters in Hyena\nThe window and positional encoding functions are used to specialize ﬁlters\nin Hyena operators, biasing them towards a speciﬁc type. Figure 3.1 provides an important example: we\nchoose at least one of the convolutions in Hyena to be shaped towards exponential decay, mirroring the\nﬁndings of (Li et al., 2022) in other applications. Interestingly, we ﬁnd that long exponentially decaying\nﬁlters display synergy with high-frequency ﬁlters, as they enable the operator to select speciﬁc inputs at\nspeciﬁc steps6. Similarly to (Romero et al., 2021b), we use high-frequency periodic activations (sine) in the\nFFN. This allows (7) to learn ﬁlters with high-frequency content, addressing the low-frequency bias of neural\nnetworks (Basri et al., 2020). Owing to the FFN, the parametrization in (7) can approximate ﬁlters obtained\nthrough other means, such as S4 (Gu et al., 2020, 2021), CKConv (Romero et al., 2021b), SGConv (Li et al.,\n2022) and Fourier Neural Operator (FNO) (Li et al., 2020).\nPreserving causality\nCausality is necessary to train autoregressive language models, in order for the\noutput at a given position to depend only on the past.\nFor example, Transformers mask the attention\nmatrix to be lower triangular. In the case of Hyena, causality can be guaranteed by parametrizing causal\nconvolutions:\n5Some of this analysis is reported in the Appendix.\n6This observation ﬁnds mirrors in the parametrization of the convolutions in H3 (Dao et al., 2022c) as a shift SSM and a\ndiagonal SSM.\n7\n",
  "8": "Proposition 3.1 (Causal Hyenas). If each ﬁlter hn, n = 1, . . . , N is causal, then the corresponding HyenaN\noperator is causal.\nIn practice, we need not constrain the learning of the ﬁlter (7) to ensure its numerical causality. If we\nuse FFT-based convolution algorithms, all we need is to evaluate the ﬁlter at t = 0, . . . , L −1 and zero-pad\nthe input and ﬁlter sequences to 2L −1 before taking FFT.\nEﬃciency\nOne bottleneck of long convolution models can be their low utilization of hardware accelerators,\nespecially when they involve iterative numerical methods to materialize the ﬁlter7. Evaluation of 7 is fast,\nsince it involves a single forward pass of an FFN, and can be performed in parallel across sequence length and\nall orders of an Hyena operator as displayed in Algorithm 2, increasing hardware utilization. An additional\nsource of low utilization is the FFT, which is also shared by other long other convolutional layers. This\nbottleneck can be partially addressed by blocking (Selesnick and Burrus, 2017), and optimization of the\nunderlying routines (Dao et al., 2022c). We benchmark runtime in Section 4.5.\n3.4\nHyena Algorithm\nA forward pass of Hyena is summarized below.\nAlgorithm 1 Projection\nRequire: Input sequence u ∈RL×D\n1. In parallel across L: ˆz = Linear(u), Linear : RD →R(N+1)D\n2. In parallel across D: z = DepthwiseConv1d(h, ˆz), h is a short convolution ﬁlter\n3. Reshape and split z into x1, . . . , xN, v. Dimensions of one element are xn ∈RD×L\nReturn x1, . . . , xN, v, xn\nAlgorithm 2 Hyena Filter\nRequire: Sequence length L, positional embedding dimension De\n1. t = PositionalEncoding(L), t ∈RL×De\n2. In parallel across N, L: ˆh = FFN(t), FFN : RDe →RND, ˆh ∈RL×ND\n3. Reshape to ˆh ∈RN×D×L\n4. h = ˆh · Window(t), h ∈RN×D×L\n5. Split h into h1, . . . , hN\nReturn h1, . . . , hN\nAlgorithm 3 Forward pass of Hyena\nRequire: Input sequence u ∈RL×D, order N, model width D, sequence length L, positional embedding\ndimension De\n1. x1, . . . , xN, v = Projection(u)\n2. h1, . . . , hN = HyenaFilter(L, De)\nfor n = 1, . . . , N do\n3. In parallel across D: vt ←xn\nt · FFTConv(hn, v)t\nend for\nReturn y = v\nProposition 3.2 (Computational Complexity). The computational cost of processing an input u ∈RL×D\nwith an order-N Hyena operator is\nO(NDL(log2 L + D))\n7In contrast, deep learning primitives are designed for high GPU utilization, with FFNs and attention usually reaching\n50 −70% or higher, if optimized.\n8\n",
  "9": "27\n29\n211\n213\n215\n0\n20\n40\n60\n80\n100\nSequence Length\nVocabulary Size: 10\n27\n29\n211\n213\n215\nSequence Length\nVocabulary Size: 20\n27\n29\n211\n213\n215\nSequence Length\nVocabulary Size: 30\n27\n29\n211\n213\n215\nSequence Length\nVocabulary Size: 40\nHyena\nCKConv\nTransfer Function\nH3\nConv1D\nFNO\nAssociative Recall\nFigure 4.1: Benchmark of long convolution parametrizations in order 2 Hyena operators on associative recall\n(%). Our results show that implicit parametrizations scale more favorably in vocabulary size (number of\npossible values of tokens in the input) and length of the sequence.\n4\nExperiments\n4.1\nShrinking the gap on in-context learning\nWe begin by empirically motivating the Hyena design, including the choice of long convolution parametriza-\ntion. We consider the suite of tasks described in Table 4.1.\nOur evaluation is grounded in recent work\non mechanistic interpretability of Transformers (Elhage et al., 2021; Power et al., 2022; Olsson et al., 2022;\nZhang et al., 2022). Recently, associative recall, in particular, has been successfully used to guide the design\nof H3 (Dao et al., 2022c). We extend the suite of tasks from these works and include benchmarking more\nchallenging versions of each task . For example, solving associative recall with a vocabulary size of only 10\nreveals whether a model is structurally capable of performing recall. Testing on much longer sequences and\nlarger vocabularies reveals additional gaps in performance that are otherwise hidden.\nHow to parametrize long convolutions\nWe compare the performance of the following long convolution\nparametrizations for S1 and S2 in an order 2 Hyena:\n• Conv1d: Explicit convolutions (regular convolution layers with ﬁxed ﬁlter size).\n• FNO: Filters parametrized explicitly in the frequency-domain (Li et al., 2020).\n• H3: Implicit parametrization using state-space models (SSMs), in particular the standard S4 (Gu et al.,\n2021).\n• TransferFunc: Implicit parametrization via transfer functions, a classical system-theoretic generalization of\nSSMs8\n• CKConv: Implicit parametrization using FFNs (Romero et al., 2021b).\n8Transfer functions roughly correspond to a frequency-domain representation of SSMs.\nTable 4.1: A selection of our mechanistic design benchmarks.\nTask\nPrompt\nTarget\nAssociative Recall\na, 1, b, e, 3, f, b\ne\nMajority\na, g, g, g, e, f, g\ng\nCounting\na, b, b, b, a, c, b\n4\nICL of Functions\nx0, f(x0), . . . xn\nf(xn)\nArithmetic\n1, 3, 5, +, 6, 8, 3\n8, 1, 8\n9\n",
  "10": "Table 4.2: Test accuracy (%) for associative recall on longer sequences, vocabulary size 30. The symbol \u0017 is\nused to mark settings where the model does not ﬁt in memory.\nSequence length\nHyena\nFlashTransformer\nTransformer\nGSS\nH3\nAFT\nRWKV\n30k\n100.0\n32.4\n\u0017\n5.3\n8.4\n2.3\n12.4\n64k\n100.0\n26.7\n\u0017\n2.1\n4.3\n1.2\n6.5\n131k\n97.2\n\u0017\n\u0017\n0.1\n0.6\n0.8\n2.3\n• Hyena: Combination of implicit parametrizations via FFNs (with exponential decay modulation as shown\nin Figure 3.1), and short explicit ﬁlters.\nAll models have the same width and 2 layers. Figure 4.1 shows implicit approaches based on FFNs outperform\nother long convolutions, with the gap widening on longer sequences and larger vocabulary sizes. We train\na diﬀerent model on each setting of sequence length and vocabulary size. The ranking is correlated with\nthe ability to decouple sequence length from parameter count (Hyena, CKConv, TransferFunc, H3) and\nexpressivity (Hyena, CKConv). We observe similar trends on the other tasks.\nPushing sequence length to the limit\nNext, we evaluate associative recall performance on extremely\nlong sequences of length 131k. To the best of our knowledge, these represent the ﬁrst empirical display of\nattention-free in-context learning on sequences of this length. The gap between parametrization schemes\nwidens as shown in Appendix A, with Hyena outperforming CKConv by 80 points.\nComparing operators\nWe repeat our associative recall experiment, this time benchmarking diﬀerent 2\nlayer models rather than changing the convolution parametrization: an order 2 Hyena, GSS (Mehta et al.,\n2022), H3 (Dao et al., 2022c), AFT-conv (Zhai et al., 2021), RWKV (Peng, 2021), and a standard GPT\n(Brown et al., 2020) using FlashAttention (Dao et al., 2022b). As shown in Table 4.2, Hyena is the only\noperator able to solve the task. Our results challenge the observation that only Transformers are capable of\nchallenging in-context learning. Surprisingly, rankings of model performance at a ﬁxed sequence length on\nThe Pile are consistent with rankings on aggregate scores on our synthetics (Appendix C).\nGenerality of Hyena operators and ﬁlters\nHyena operators and ﬁlters can also applied successfully\nbeyond language tasks. We experiment on sequential CIFAR, where pixels are ﬂattened as a sequence, and\nuse the same operator deﬁned for language. We reach the accuracy of standard S4 (Gu et al., 2021) with same\nmodel size (91%). In Section 4.5 and Appendix A, we discuss larger-scale image classiﬁcation experiments\nwith Hyena.\n4.2\nLanguage Modeling\nNext, we verify the scaling of Hyena on autoregressive language modeling. We evaluate the perplexity on\nWikiText103 (Table 4.3) and The Pile (Table 4.4). On the The Pile, we train diﬀerent models for\n5, 10, 15 billion tokens (diﬀerent runs), adjusting the learning rate scheduler. Hyena is the ﬁrst attention-free,\nconvolution architecture to match GPT quality with a 20%9 reduction in total FLOPs. Preliminary scaling\nlaws are shown in Figure 4.2, collecting the training runs at 5, 10, 15 billion tokens. Each curve represents a\ndiﬀerent training run. In Appendix A, we provide results on the PG-19 long-range benchmark (Rae et al.,\n2019).\n4.3\nDownstream Evaluation\nWe perform a downstream evaluation on SuperGLUE (Wang et al., 2019) tasks. We compare Hyena (trained\nfor 137 billion tokens) with the best available pre-trained attention-free model, RWKV (Peng, 2021) (trained\n9The FLOP reduction consists in the non-parametric FLOPs of SelfAttention devoted to attention matrix computation. The\nratio of parametric to non-parametric FLOPs (and hence the gains) depend on the ratio of model width D and sequence length\nL used in training.\n10\n",
  "11": "1.3 1.6\n2.6\n3.2\n3.9\n4.9\n·1019\n2.44\n2.29\n2.21\nFLOPs\nLoss\nData Scaling on The Pile, 355M parameters\nHyena\nGPT\nFigure 4.2: Preliminary \"scaling law\" of language models on The Pile. Comparison of our approach (red)\nbased on long convolutions and gating (Hyena) and a standard GPT (blue) (Brown et al., 2020). We reach\nperplexity of GPT with a smaller training FLOP budget.\nTable 4.3: Perplexity on WikiText103\n(same tokenizer). ∗are results from (Dao\net al., 2022c). Deeper and thinner models\n(Hyena-slim) achieve lower perplexity.\nModel\nPerplexity\nTransformer (125M)\n18.6\nHybrid H3 (125M)\n18.5∗\nPerformer (125M)\n26.8∗\nReformer (125M)\n25.6∗\nAFT-conv (125M)\n28.2\nLinear Attention (125M)\n25.6∗\nHyena-3 (125M)\n18.6\nHyena-3-slim (125M)\n18.5\nTable 4.4: Perplexity on The Pile for models trained until a\ntotal number of tokens e.g., 5 billion (diﬀerent runs for each\ntoken total). All models use the same tokenizer (GPT2).\nFLOP count is for the 15 billion token run.\nModel\n5B\n10B\n15B\nFLOPs (1019)\nGPT (125M)\n13.3\n11.9\n11.2\n1.88\nHyena-2 (153M)\n13.3\n11.8\n11.1\n1.87\nGPT (355M)\n11.4\n9.8\n9.1\n4.77\nHyena-2 (355M)\n11.3\n9.8\n9.2\n3.93\nfor 332 billion tokens), and a reference GPTNeo (Black et al., 2021) (trained for 300 billion tokens) of the\nsame size.\nTables 4.5 and 4.6 summarize the results.\nHyena performs similarly to other models despite\nhaving been trained on less than half the number of total tokens. We observe Hyena to display characteristic\nfew-shot capabilities of standard Transformers, with some tasks e.g., MultiRC seeing a lift of more than 20%\naccuracy over zero-shot when the model is provided additional prompts as context. The improvements are\nmore noticeable in generation tasks, where the additional prompts can instruct the model on how it should\nbe responding to the questions. We report an additional downstream evaluation on the LAMBADA task\n(Paperno et al., 2016) in Appendix A.\nTable 4.5: Zero-shot accuracy (%) on SuperGLUE tasks for small models.\nModel\nWSC\nWIC\nRTE\nCB\nMultiRC\nReCoRD\nBoolQ\nCOPA\nAverage\nGPTNeo (Black et al., 2021)\n27.9\n50.0\n45.1\n41.1\n0.0\n61.7\n62.2\n62.0\n43.8\nRWKV (Peng, 2021)\n13.4\n52.3\n46.9\n25.0\n0.0\n58.5\n59.2\n66.0\n40.2\nHyena\n21.2\n50.5\n46.6\n39.3\n1.1\n59.4\n51.8\n70.0\n41.5\n11\n",
  "12": "Table 4.6: Few-shot (3) accuracy (%) on SuperGLUE tasks for small models.\nModel\nWSC\nWIC\nRTE\nCB\nMultiRC\nReCoRD\nBoolQ\nCOPA\nAverage\nGPTNeo (Black et al., 2021)\n38.5\n50.0\n53.8\n42.9\n22.4\n61.4\n61.0\n63.0\n49.1\nRWKV (Peng, 2021)\n32.7\n49.4\n47.2\n37.5\n0.0\n58.3\n55.0\n64.0\n43.0\nHyena\n39.4\n50.1\n47.6\n46.4\n26.7\n58.1\n56.0\n70.0\n49.3\n103\n104\n105\n0\n50\n100\nSequence Length\nRuntime [ms]\n103\n103.2\n103.4\n103.6\n103.8\n0\n2\n4\n6\nSequence Length\nHyena\nAttention\nFlashAttention\nBenchmarking Hyena\nFigure 4.3: Benchmarking runtime of Hyena, Attention and FlashAttention with varying sequence lengths.\nBatch size is set to 64. The ﬁgure on the right is an inset showing a zoomed-in portion of the ﬁgure on the\nleft.\n4.4\nBenchmarking\nWe benchmark runtime of an order 2 Hyena operator compared to attention and FlashAttention layers (Dao\net al., 2022b). Hyena uses a fused CUDA kernel to perform FFTConv (Dao et al., 2022c). We set batch\nsize to 64 and measure runtime (in milliseconds). Results are provided in Figure 4.3. Hyena speedups reach\n100× at sequence length 64K. Crossover points for Hyena and attention is at length 2048, and for Hyena and\nFlashAttention is between 4096 and 8196. Despite the absolute reduction in FLOPs, speedups are achieved\nonly on longer sequences when the gap grows suﬃciently large. This occurs because hardware utilization of\nHyena is lower than FlashAttention. We expect the gap between theoretical maximum speedup to shrink\nwith improved implementations of FFTConv and specialized hardware.\n4.5\nLarge-Scale Image Classiﬁcation\nFinally, we demonstrate the potential of Hyena as a general deep learning operator by applying it to image\nclassiﬁcation. On ImageNet, we drop-in replace attention layers in the Vision Transformer (ViT) (Dosovitskiy\net al., 2020) with the Hyena operator (without changes from its language counterpart) and match performance\nwith ViT. We also show that using smaller image patches boosts performance in both attention and Hyena.\nSince this results in longer sequence lengths, we expect Hyena to outperform in speed as patches get more\nﬁne-grained approaching pixel-level. On CIFAR-2D, we test a 2D version of Hyena long convolution ﬁlters in\na standard convolutional architecture, which improves on the 2D long convolutional model S4ND (Nguyen\net al., 2022) in accuracy with a 8% speedup and 25% fewer parameters. See Appendix A.4 for additional\nvision architectures and training procedure details.\nTable 4.7: Image classiﬁcation top-1 accuracy.\nModel\nPatch Size\nSeq Len\nDataset\nAcc (%)\nViT (87M)\n16x16\n196\nImageNet-1k\n78.5\nHyena-ViT (88M)\n16x16\n196\nImageNet-1k\n78.5\nViT (87M)\n8x8\n1024\nImageNet-1k\n80.0\nHyena-ViT (88M)\n8x8\n1024\nImageNet-1k\n79.8\nS4ND-ISO (268k)\n-\n-\nCIFAR-10\n89.9\nHyena-ISO (202k)\n-\n-\nCIFAR-10\n91.2\n12\n",
  "13": "5\nDiscussion and Conclusion\nIn this work, we introduced an attention-free drop-in replacement to the core building block of many large-\nscale language models. Hyena operators are a recurrence of gating and implicitly parametrized long convo-\nlutions, can be evaluated eﬃciently in subquadratic time, and can learn in-context on very long sequences.\nOn The Pile, deep stacks of Hyena operators constitute one of the ﬁrst attention-free, convolutional archi-\ntectures to match perplexity and downstream performance of Transformers with a signiﬁcant reduction in\ntraining compute. Our promising results at the sub-billion parameter scale suggest that attention may not\nbe all we need, and that simpler subquadratic designs such as Hyena, informed by a set of simple guiding\nprinciples and evaluation on mechanistic interpretability benchmarks, may form the basis for eﬃcient large\nmodels. We are excited about what new capabilities Hyena opens up as we scale and optimize the inference\nspeed of these models.\nAcknowledgments\nWe would like to thank Karan Goel, Albert Gu, Avanika Narayan, Khaled Saab, Michael Zhang, Elliot Ep-\nstein and Sabri Eyuboglu for helpful discussion and feedback on earlier drafts, and Together Computer and\nCrusoe for providing the compute used to train models in this paper. We gratefully acknowledge the support\nof NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078\n(Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under No. W911NF-21-2-0251 (Interactive\nHuman-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480:\nUnderstanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE);\nNXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture,\nEricsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for\nResearch program, the Stanford Data Science Initiative (SDSI), Department of Defense (DoD) through the\nNational Defense Science and Engineering Graduate Fellowship (NDSEG) Program, and members of the Stan-\nford DAWN project: Facebook, Google, and VMWare. This work is supported by NSF (1651565), AFOSR\n(FA95501910024), ARO (W911NF-21-1-0125), ONR, DOE (DE-SC0022222), CZ Biohub, and Sloan Fellow-\nship. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes\nnotwithstanding any copyright notation thereon. Any opinions, ﬁndings, and conclusions or recommenda-\ntions expressed in this material are those of the authors and do not necessarily reﬂect the views, policies, or\nendorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.\nReferences\nS. Arora, A. Narayan, M. F. Chen, L. J. Orr, N. Guha, K. Bhatia, I. Chami, F. Sala, and C. Ré. Ask me\nanything: A simple strategy for prompting language models. arXiv preprint arXiv:2210.02441, 2022.\nR. Basri, M. Galun, A. Geifman, D. Jacobs, Y. Kasten, and S. Kritchman. Frequency bias in neural networks\nfor input of non-uniform density. In International Conference on Machine Learning, pages 685–694. PMLR,\n2020.\nS. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman. GPT-Neo: Large Scale Autoregressive Language\nModeling with Mesh-Tensorﬂow, Mar. 2021. URL https://doi.org/10.5281/zenodo.5297715. If you\nuse this software, please cite it using these metadata.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems,\n33:1877–1901, 2020.\nC.-T. Chen. Linear system theory and design. Saunders college publishing, 1984.\nR. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019.\n13\n",
  "14": "P. Cramer. Alphafold2 and the future of structural biology. Nature structural & molecular biology, 28(9):\n704–705, 2021.\nE. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation\nwith a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition workshops, pages 702–703, 2020.\nT. Dao, A. Gu, M. Eichhorn, A. Rudra, and C. Ré. Learning fast algorithms for linear transforms using\nbutterﬂy factorizations. In International conference on machine learning, pages 1517–1527. PMLR, 2019.\nT. Dao, B. Chen, N. S. Sohoni, A. Desai, M. Poli, J. Grogan, A. Liu, A. Rao, A. Rudra, and C. Ré. Monarch:\nExpressive structured matrices for eﬃcient and accurate training. In International Conference on Machine\nLearning, pages 4690–4721. PMLR, 2022a.\nT. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention: Fast and memory-eﬃcient exact attention\nwith io-awareness. arXiv preprint arXiv:2205.14135, 2022b.\nT. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, and C. Ré. Hungry hungry hippos: Towards\nlanguage modeling with state space models. arXiv preprint arXiv:2212.14052, 2022c.\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint arXiv:2010.11929, 2020.\nN. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly,\net al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021.\nK. Fukushima and S. Miyake. Neocognitron: A self-organizing neural network model for a mechanism of\nvisual pattern recognition. In Competition and cooperation in neural nets, pages 267–285. Springer, 1982.\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima,\net al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027,\n2020.\nS. Garg, D. Tsipras, P. Liang, and G. Valiant. What can transformers learn in-context? a case study of\nsimple function classes. arXiv preprint arXiv:2208.01066, 2022.\nA. Gu, T. Dao, S. Ermon, A. Rudra, and C. Ré.\nHippo: Recurrent memory with optimal polynomial\nprojections. Advances in Neural Information Processing Systems, 33:1474–1487, 2020.\nA. Gu, K. Goel, and C. Ré. Eﬃciently modeling long sequences with structured state spaces. arXiv preprint\narXiv:2111.00396, 2021.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\nD. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Lakshminarayanan. Augmix: A simple data\nprocessing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019.\nJ. Hoﬀmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.\nHendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint\narXiv:2203.15556, 2022.\nG. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger. Deep networks with stochastic depth. In European\nconference on computer vision, pages 646–661. Springer, 2016.\nN. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The eﬃcient transformer. arXiv preprint arXiv:2001.04451,\n2020.\nY. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278–2324, 1998.\n14\n",
  "15": "Y. Li, H. Yang, E. R. Martin, K. L. Ho, and L. Ying.\nButterﬂy factorization.\nMultiscale Modeling &\nSimulation, 13(2):714–732, 2015.\nY. Li, T. Cai, Y. Zhang, D. Chen, and D. Dey. What makes convolutional models great on long sequence\nmodeling? arXiv preprint arXiv:2210.09298, 2022.\nZ. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Fourier\nneural operator for parametric partial diﬀerential equations. arXiv preprint arXiv:2010.08895, 2020.\nP. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu,\nA. Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\nS. Massaroli, M. Poli, J. Park, A. Yamashita, and H. Asama. Dissecting neural odes. Advances in Neural\nInformation Processing Systems, 33:3952–3963, 2020.\nH. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long range language modeling via gated state spaces.\narXiv preprint arXiv:2206.13947, 2022.\nB. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing\nscenes as neural radiance ﬁelds for view synthesis. Communications of the ACM, 65(1):99–106, 2021.\nE. Nguyen, K. Goel, A. Gu, G. W. Downs, P. Shah, T. Dao, S. A. Baccus, and C. Ré. S4nd: Modeling\nimages and videos as multidimensional signals using state spaces. arXiv preprint arXiv:2210.06583, 2022.\nC. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen,\net al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.\nA. V. Oppenheim, A. S. Willsky, S. H. Nawab, and J.-J. Ding. Signals and systems, volume 2. Prentice hall\nUpper Saddle River, NJ, 1997.\nD. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and\nR. Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint\narXiv:1606.06031, 2016.\nB. Peng. RWKV-LM, 8 2021. URL https://github.com/BlinkDL/RWKV-LM.\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on\ncontrol and optimization, 30(4):838–855, 1992.\nA. Power, Y. Burda, H. Edwards, I. Babuschkin, and V. Misra. Grokking: Generalization beyond overﬁtting\non small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.\nA. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via\nlarge-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022.\nJ. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap. Compressive transformers for\nlong-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507.\nD. W. Romero, R.-J. Bruintjes, J. M. Tomczak, E. J. Bekkers, M. Hoogendoorn, and J. C. van Gemert.\nFlexconv: Continuous kernel convolutions with diﬀerentiable kernel sizes. arXiv preprint arXiv:2110.08059,\n2021a.\nD. W. Romero, A. Kuzina, E. J. Bekkers, J. M. Tomczak, and M. Hoogendoorn. Ckconv: Continuous kernel\nconvolution for sequential data. arXiv preprint arXiv:2102.02611, 2021b.\nO. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation.\nIn International Conference on Medical image computing and computer-assisted intervention, pages 234–\n241. Springer, 2015.\nA. Roy, M. Saﬀar, A. Vaswani, and D. Grangier.\nEﬃcient content-based sparse attention with routing\ntransformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021.\n15\n",
  "16": "I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers. In Inter-\nnational Conference on Machine Learning, pages 9355–9366. PMLR, 2021.\nI. W. Selesnick and C. S. Burrus. Fast convolution and ﬁltering. In The Digital Signal Processing Handbook,\npages 8–1. CRC Press, 2017.\nV. Sitzmann, J. N. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein. Implicit neural representations\nwith periodic activation functions. arXiv preprint arXiv:2006.09661, 2020.\nC. Szegedy, V. Vanhoucke, S. Ioﬀe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for\ncomputer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n2818–2826, 2016.\nZ. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, and Y. Li. Maxvit: Multi-axis vision transformer.\narXiv preprint arXiv:2204.01697, 2022.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.\nA. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. Superglue:\nA stickier benchmark for general-purpose language understanding systems. Advances in neural information\nprocessing systems, 32, 2019.\nS. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768, 2020.\nL. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z.-H. Jiang, F. E. Tay, J. Feng, and S. Yan. Tokens-to-token\nvit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 558–567, 2021.\nS. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cutmix: Regularization strategy to train strong\nclassiﬁers with localizable features. In Proceedings of the IEEE/CVF international conference on computer\nvision, pages 6023–6032, 2019.\nS. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind.\nAn attention free\ntransformer. arXiv preprint arXiv:2105.14103, 2021.\nH. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv\npreprint arXiv:1710.09412, 2017.\nY. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. Unveiling transformers with lego:\na synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022.\nZ. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang. Random erasing data augmentation. In Proceedings of the\nAAAI conference on artiﬁcial intelligence, volume 34, pages 13001–13008, 2020.\n16\n",
  "17": "Hyena Hierarchy\nSupplementary Material\nContents\n1\nIntroduction\n1\n2\nPreliminaries and Related Work\n3\n2.1\nExplicit and Implicit Convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nThe Self-Attention Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nHyena: Deﬁnition and Properties\n5\n3.1\nHyena Recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2\nHyena Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.3\nHyena Filters\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.4\nHyena Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4\nExperiments\n9\n4.1\nShrinking the gap on in-context learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.2\nLanguage Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n4.3\nDownstream Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n4.4\nBenchmarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4.5\nLarge-Scale Image Classiﬁcation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5\nDiscussion and Conclusion\n13\nA Experimental Details\n18\nA.1 Mechanistic Design Synthetic Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA.2 Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nA.3 Downstream Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nA.4 Image Classiﬁcation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nB Theoretical Results and Details\n21\nB.1\nProofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nB.2\nAnalysis of Data-Controlled Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nC Discussion and Additional Results\n24\nC.1 Learning Arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nD Samples and Visualizations\n26\nD.1 Hyena Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nD.2 Hyena Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nD.3 Positional Encoding and Filters Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nD.4 Downstream Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n17\n",
  "18": "A\nExperimental Details\nAn implementation of Hyena can be found at this link.\nA.1\nMechanistic Design Synthetic Benchmarks\nOur synthetic reasoning are inspired by mechanistic interpretability (Elhage et al., 2021), in-context learning\n(ICL) (Garg et al., 2022) and language model benchmarking (Liang et al., 2022) research. The evaluation\nrevolves around 4 main tasks:\n• Associative recall: Each string is produced by concatenating key-value tuples from a diﬀerent random\ndictionary.\nThis test veriﬁes whether a model is able to extract right value given a key as prompt,\neﬀectively applying a data-controlled shift (delay).\n• Majority voting and counting: Testing if a model can densely activate its data-controlled matrix i.e.,\nthrough many non-zero entries (consider the string ’a a a a a a a a a a b →a’).\n• ICL of linear functions: Verifying whether a model can perform ICL on real-valued inputs. Prompts\nare generated as x1, wkx1, . . . , xn →wkxn, where both xk and wk ∈Rno are sampled from a normal\ndistribution.\n• Arithmetic: Basic capability check.\nFor each task, we train models using the hyperparameters shown in Table A.1. We consider increasing set-\ntings of diﬃculty controlled by sequence length, spanning values 1024, 2048, 4098, 8196, 16392, 32784, 65568, 131136\nand vocabulary sizes 10, 20, 30, 40. For ICL of functions, we vary instead the dimension no.\nNote that for associative recall on longer sequences, multiple copies of key-value tuples appear in the\nprompt. To see this, consider how likely it is to sample multiple copies of a particular key-value pair with\na vocabulary size of 40, in order to form a sequence of 100k characters. Models capable of looking further\nback in the sequence eﬀectively see more data, and can solve challenging versions of the in-context learning\ntask. Increasing the vocabulary size has the increasing the average distance between instances of the same\nkey-value pair in each prompt, highlighting performance gaps between diﬀerent approaches.\nTable A.1: (Hyperparameter settings for reasoning and in-context learning tasks.).\nOptimizer\nAdamW\nOptimizer momentum\nβ1, β2 = 0.9, 0.98\nBase learning rate\n0.0005\nWeight decay\n0.1\nDropout\nNone\nBatch size\n32\nTraining epochs\n200\nNum samples\n2000\nLearning rate schedule\ncosine decay\nWarmup epochs\n10\nWarmup schedule\nlinear\nNumber of layers\n2\nWidth\n64\nLong convolution comparisons:\nWe compare diﬀerent convolution parametrizations, embedding them\nin an order 2 Hyena operator.\nAll convolutions are applied separately to input channels (referred to as\nsingle-input single-output (SISO) in signal processing, or depthwise in other machine learning contexts).\n• Conv1d: Explicit convolutions (regular convolution layers with ﬁxed ﬁlter size). We use a ﬁxed ﬁlter size\nof 64, to match parameters of the other approaches.\n18\n",
  "19": "• FNO: Filters parametrized explicitly in the frequency-domain (Li et al., 2020). We set the number of modes\nto 64.\n• H3: Implicit parametrization using state-space models (SSMs), and in particular the standard S4 (Gu et al.,\n2021). We set the state dimension to 64.\n• TransferFunc: Implicit parametrization via transfer functions, a classical system-theoretic generalization\nof SSMs. Transfer functions are deﬁned by a ratio of polynomials (we parametrize the coeﬃcients, and\nevaluate the polynomials eﬃciently via FFTs). We set the order to 64.\n• CKConv: Implicit parametrization using FFNs (Romero et al., 2021b).\n• item Hyena: Combination of implicit parametrizations via FFNs (with exponential decay modulation as\nshown in Figure 3.1), and short explicit ﬁlters.\nCKConv and Hyena use the same size of FFNs (width 32 to match in parameters).\nIn Table A.1, we report additional results on the challenging setting of sequence length 131072 and vocab-\nulary size 30. Implicit parametrizations of convolutions outperform explicit parametrizations on associative\nrecall, with CKConv and Hyena greatly improving on the ability to extract the right key, value relations\nfrom diﬀerent inputs. In Appendix C, we discuss how results on our synthetic tasks can be indicative of\nperformance at a larger scale.\nTable A.2: Test accuracy (%) in associative recall on sequences of length 131072, vocabulary size 30.\nHyena\nCKConv\nTransferFunc\nH3\nFNO\nConv1d\n97.2\n14.3\n0.5\n0.6\n0.3\n0.5\nOperator comparisons:\nWe compare diﬀerent models on the same associative recall task, using hyper-\nparameters in Table A.1. Hyena uses our ﬁlter parametrization with decay windowing for long convolutions,\nand short explicit convolutions of size 3 after the dense input projections. All other models use defaults from\ntheir largest scale experiment, while keeping the size to 2 layers and width 64.\nA note on Transformer performance\nTransformers can solve associative recall tasks with longer se-\nquences, provided the length does not prevent them from ﬁtting in memory, and enough examples are present\nin the training data. In all our experiments, we keep the number of samples ﬁxed (2000), a regime where\nTransformers struggle to ﬁnd the generalizing solution (see Table A.1).\nFor shorter sequences (see Appendix C), Transformers solve the task easily even with limited data, com-\nparably to Hyena.\nMore broadly, these diﬀerent properties of attention and attention-free token-mixing layers may explain\nimproved performance when they are combined in hybrid architectures (Dao et al., 2022c). The focus on\nthis work has been identifying an architecture capable of performing without attention, which is necessary\nto tackle domains where long sequences are common. However, when training with shorter sequences (up\nto 8k), if ﬁnal downstream performance is the only metric of interest, improved results can be obtained by\nhybridizing our models similarly to H3 (Dao et al., 2022c).\nA.2\nLanguage Modeling\nWikiText103:\nWe train 125M parameter models on WikiText103 and compare perplexity to Trans-\nformers, hybrid models such as H3 (Dao et al., 2022c), and other variants of subquadratic attention. All\nmodels use the same GPT2 tokenizer with vocabulary size 50257. We test order 3 Hyena with our proposed\nﬁlter parametrization for two long convolutions, and a shorter explicit convolution on the third. We also\nconsider Hyena (slim) that are 1.5x deeper than Transformers (12 versus 18 layers), with width multiplier of\nthe FFNs set to 2. We ﬁnd trading-oﬀwidth for depth to be generally favourable. These modiﬁcations are\nmade possible by the reduction in overall FLOPs of Hyena operators compared to self-attention, in particular\nnon-parametric FLOPs which include materialization of the attention matrix, application of softmax, and\nmatrix-value reduction.\n19\n",
  "20": "Table A.3: Hyperparameter settings for The Pile, 125M).\nOptimizer\nAdamW\nOptimizer momentum\nβ1, β2 = 0.9, 0.98\nPeak learning rate\n0.0006\nWarmup learning rate init\n0.000001\nLearning rate min\n0.00006\nWeight decay\n0.1\nDropout\nNone\nBatch size\n256\nLearning rate schedule\ncosine decay\nWarmup schedule\nlinear\nThe Pile:\nWe follow a same procedure and train 125M and 355M-sized models on The Pile (Gao et al.,\n2020).\nHyperparameters are reported in Table A.3.\nHyperparameters for 355M are the same beyond a\nreduction in peak learning rate to 4 · 10−4. For larger models (1.3B), we set a learning rate of 2.2 · 10−4.\nWe perform three experiments for each model type and size, and train for 5, 10, 15 billion tokens at a\nsequence length 2024 and global batch size 256. All models are trained on a single node of 8 A100 80GB\nGPUs. We use order 2 Hyenas, with the same architectural considerations described above for WikiText103.\nIn addition to our data scaling experiments at 5, 10 and 15 billion tokens, we provide preliminary results\nfor models at the 1.3B parameter scale (10.8 perplexity after 5 billion tokens), and train a 153M model (130\nbillion tokens), reaching a perplexity of 9.8. The 153M is the same used in our downstream evaluation on\nSuperGLUE.\nTraining hyperparameters match those of standard GPT training pipelines, and are thus likely suboptimal\nfor new attention-free architectures such as Hyena. We run some preliminary experiments and ﬁnd that e.g.,\nsome modiﬁcations to the learning rate schedule, currently involving linear warmup and cosine decay, to\nimprove perplexity at convergence of Hyena models (we recommend slightly lower learning rates for Hyena\nmodels compared to GPT of a similar size). Despite these ﬁndings, we use standard GPT hyperparameters\nfor both GPT and Hyena.\nPG-19\nWe also report results of additional training runs on other datasets. We train a Hyena 153M model\non the standard PG-19 long-range corpus (Rae et al., 2019), with a context length of 16k tokens, reaching a\ntest perplexity of 14.6 (using the standard GPT2 tokenizer) in 8 epochs.\nArchitectures\nArchitectural hyperparameters for Hyena are shown in Table A.4. We use sine as an acti-\nvation function for the FFN of Hyena ﬁlters.\nTable A.4: Hyena architecture hyperparameters.\nSize\ndepth\nwidth\nFFN width\nﬁlter FFN width\nﬁlter FFN depth\nsine freq.\n125M\n12\n768\n3072\n64\n4\n14\n125M-slim\n18\n768\n1536\n64\n4\n14\n153M\n18\n864\n1728\n64\n4\n14\n355M\n36\n1024\n2048\n64\n4\n14\n1.3B\n36\n2048\n4096\n64\n4\n14\nFLOP computation\nThe number of ﬂoating point operations (FLOPs) reported in the main text are\ncomputed using the same strategy as in (Hoﬀmann et al., 2022). For GPT, we do not use the approximation,\nopting instead for the more accurate formula based on FLOP counts of individual layers. In the case of\nHyena, FLOPs are computed using the same method, except attention layers are replaced by:\ni. Projections: order × d_model × d_model × seq_len.\nii. Short conv on projections: order × d_model × seq_len × ﬁlter_len (usually 3).\n20\n",
  "21": "iii. FFTConv: 5 × (order - 1) × d_model × log(seq_len) × seq_len.\niv. Output: d_model × d_model × seq_len.\nwith a leading factor 2 to account for both additions and multiplications.\nA.3\nDownstream Evaluation\nSuperGLUE:\nWe evaluate models on the SuperGLUE (Wang et al., 2019) with the parsing pipeline of\n(Arora et al., 2022). For all tasks except WIC, CB and BoolQ, we generate a response using greedy decoding,\nthen check for the gold label. WIC, CB and BoolQ use logit scoring instead of generation.\nModels\nThe models considered are the open-source checkpoint of GPTNeo 125M trained for 300B tokens\nThe Pile, and the RWKV-v4 169M checkpoint trained for 332B tokens on The Pile. Hyena is a 153M\nmodel trained for 137B tokens on The Pile.\nLAMBADA:\nWe evaluate Hyena on the LAMBADA (Paperno et al., 2016) task. We apply a stop word\nﬁlter and check whether predictions for all tokens corresponding to the last word agree with the ground truth.\nThe small Hyena model trained on 137B tokens reaches 44.64% accuracy.\nA.4\nImage Classiﬁcation\na\nImageNet: We use ImageNet-1k which consists of 1000 classes and 1.3M images and train from scratch\nwith no outside data on 8 Nvidia A100 GPUs. In our ViT benchmark, we swap the attention layers with the\nHyena operator deﬁned in our language experiments, and remove the class token and positional embeddings,\nsimilar to S4ND (Nguyen et al., 2022). The parameter count is kept similar at 87M ViT-B (base) vs 88M\nHyena-ViT. The training procedure from T2T-ViT (Yuan et al., 2021) is used, including augmentations such\nas RandAugment (Cubuk et al., 2020), Mixup (Zhang et al., 2017), and AugMix (Hendrycks et al., 2019).\nSee table A.5 for hyperparameter settings used.\nCIFAR-10:\nWe use CIFAR-10 in sequential and 2D experiments. For sequential, we use the Hyena operator\ndeﬁned in our language tasks and compare with an S4 model (Gu et al., 2021) of the same size by swapping\nlayers in the residual blocks. In 2D, we learn Hyena ﬁlters (in both x and y dimensions) that are equal to the\nsize of the input shape, and forgo the gating mechanism from our language experiments. We window (i.e.,\napply a soft mask spatially to) the Hyena ﬁlters with a decay term. The rate of decay varies across channels,\nensuring diﬀerent sizes of the ﬁlters at initialization. We compare with another implicit 2D convolution,\nS4ND (Nguyen et al., 2022), by swapping the model layers with the 2D Hyena ﬁlters. The \"isometric\" model\nconsists of 4 residual blocks of model dimension 128. We use basic image augmentations, 0.1 dropout, 0.03\nweight decay and train for 100 epochs using a Nvidia T4 GPU.\nB\nTheoretical Results and Details\nB.1\nProofs\nProof of Proposition 3.1\nProof. A discrete L-by-L operator is causal if it is lower triangular, i.e., when there is no leakage of future\ninput information to the output. The Hyena operator H is the product of alternating diagonal and Toeplitz\nmatrices. Thus, if all the Toeplitz matrices Sn\nh are lower triangular then H is lower triangular. In turn, each\nSn\nh is lower triangular if and only if the ﬁlter h is causal, concluding the proof.\n21\n",
  "22": "Table A.5: ViT and ViT-Hyena settings for ImageNet-1k).\nImage size\n2242\nOptimizer\nAdamW\nOptimizer momentum\nβ1, β2 = 0.9, 0.999\nWeight init\ntrunc. normal (std=0.02)\nViT base learning rate\n1e−3\nHyena-ViT base learning rate\n2e−4\nViT weight decay\n0.05\nHyena-ViT weight decay\n0.01\nDropout\nNone\nBatch size\n1024\nTraining epochs\n300\nLearning rate schedule\ncosine decay\nWarmup epochs\n10\nWarmup schedule\nlinear\nRandaugment (Cubuk et al., 2020)\n(9,0.5,layers=2)\nMixup (Zhang et al., 2017)\n0.8\nCutmix (Yun et al., 2019)\n1.0\nRandom erasing (Zhong et al., 2020)\n0.25\nLabel smoothing (Szegedy et al., 2016)\n0.1\nStochastic depth (Huang et al., 2016)\n0.1\nExp.mov. avg (EMA) (Polyak and Juditsky, 1992)\nNone\nB.2\nAnalysis of Data-Controlled Mechanisms\nWe discuss the surrogate attention mechanism of Hyena-2: q, k, v 7→y:\nzt = kt(ϕ ∗v)t\nyt = qt(ψ ∗z)t\n(8)\nIf ϕ and ψ are convolutions parametrized via state-space models (SSMs), the above resembles the H3 mecha-\nnism (Dao et al., 2022c). We investigate the eﬀect of the convolutional kernels ϕ and ψ on the attention layer.\nWe start by introducing a matrix representation of the layer, and we isolate the attention matrix Aψ\nϕ(q, k)\nsuch that\ny = Aψ\nϕ(q, k)v.\n(9)\nIsolating the surrogate attention matrix\nIn the case of length-L discrete sequences\nzt = kt\nL−1\nX\nm=0\nϕt−mvm\nyt = qt\nL−1\nX\nm=0\nψt−mzm\n(10)\nTherefore we can rewrite (8) as\nyt = qt\nL−1\nX\nm=0\nψt−mkm\nL−1\nX\nn=0\nϕm−nvn\n= qt\nL−1\nX\nm=0\nL−1\nX\nn=0\nψt−mkmϕm−nvn\nMove ψ, k inside inner sum\n= qt\nL−1\nX\nn=0\nL−1\nX\nm=0\nψt−mkmϕm−nvn\nIndex shift\n=\nL−1\nX\nn=0\nqt\nL−1\nX\nm=0\nψt−mkmϕm−nvn\n(11)\n22\n",
  "23": "And we can deﬁne the surrogate attention matrix Aψ\nϕ(q, k)\n[Aψ\nϕ(q, k))]t,t′ = qt\nL−1\nX\nm=0\nψt−mkmϕm−t′.\n(12)\nContinuous Signals: We can also consider the case of continuous signals on a group G. In the\ncontinuous case, we can expand the convolutions in (8) as\n(ϕ ∗v)t =\nZ\nG\nϕt−gvgdg,\n(ψ ∗z)t =\nZ\nG\nψt−gzgdg\n(13)\nThis allows us to rewrite (8) as\nyt = qt(ψ ∗k(ϕ ∗v))t\n= qt\nZ\nG\nψt−g\n\u0014\nkg\nZ\nG\nϕg−τvτdτ\n\u0015\ndg\n= qt\nZ\nG\n\u0014Z\nG\nψt−gkgϕg−τvτdτ\n\u0015\ndg\n= qt\nZ\nG\n\u0014Z\nG\nψt−gkgϕg−τvτdg\n\u0015\ndτ\nVariable swap\n=\nZ\nG\n\u0014\nqt\nZ\nG\nψt−gkgϕg−τvτdg\n\u0015\ndτ\nPull qt in τ integral\n=\nZ\nG\n\u0014\nqt\nZ\nG\nψt−gkgϕg−τdg\n\u0015\nvτdτ\nPull vτ out of g integral.\n(14)\nThere is a linear operator A : v 7→y = Av which we interpret as the surrogate attention operator. A\nis conditioned on the query q, key k and ﬁlters ϕ and ψ, A = Aψ\nϕ(q, k). The kernel K of the operator\nis given by\nK(t, t′) = qt\nZ\nG\nψt−gkgϕg−t′dg\n(15)\nOperator decomposition of the surrogate attention matrix\nWe can decompose the linear map\nv 7→y; y = Aψ\nϕ(q, k)v into a sequence of factors, each dependent on a projection of the input Aψ\nϕ(q, k) =\nAψ(q)Aϕ(k). Let Dq and Dk be the L-by-L diagonal matrices whose respective main diagonal entries are the\nrespective entries of q and k. Then, we have that\nAψ(q) = DqSψ,\nDq = diag(q),\nAϕ(k) = DkSϕ,\nDk = diag(k).\n(16)\nThe matrix has been decomposed into two terms Aψ(q) and Aϕ(k) constructed by multiplying the diagonal\nmatrices Dq and Dk with the Toeplitz matrices Sψ and Sϕ. Sψ and Sϕ are the kernels of the convolution\noperators with ﬁlter’s impulse responses ψ and ϕ respectively. In the current applications of interest, ψ and\nϕ are chosen to be causal, i.e. ψ[t] = 0 for t < 0 and ϕ[t] = 0 for t < 0. This results in Sψ and Sϕ to be lower\ntriangular matrices\nSψ =\n\n\nψ0\n0\n· · ·\n0\nψ1\nψ0\n· · ·\n0\n...\n...\n...\n...\nψL−1\nψL−2\n· · ·\nψ0\n\n,\nSϕ =\n\n\nϕ0\n0\n· · ·\n0\nϕ1\nϕ0\n· · ·\n0\n...\n...\n...\n...\nϕL−1\nϕL−2\n· · ·\nϕ0\n\n.\n(17)\nThe surrogate attention matrix is then given by\nAψ\nϕ(q, k) = DqSψDkSϕ\n(18)\n23\n",
  "24": "We can expand the matrix multiplications in (16) in the case of causal ﬁlters ϕ and ψ as\nDq\n\n\nq0\nq1\n...\nqL−1\n\n\nSψ\n\n\nψ0\nψ1\nψ0\n...\n...\n...\nψL−1\nψL−2\n· · ·\nψ0\n\n\nDk\n\n\nk0\nk1\n...\nkL−1\n\n\nSϕ\n\n\nϕ0\nϕ1\nϕ0\n...\n...\n...\nϕL−1\nϕL−2\n· · ·\nϕ0\n\n\n=\n\n\nq0ψ0\nq1ψ1\nq1ψ0\n...\n...\n...\nqL−1ψL−1\nqL−1ψL−2\n· · ·\nqL−1ψ0\n\n\nAψ(q)\n\n\nk0ϕ0\nk1ϕ1\nk1ϕ0\n...\n...\n...\nkL−1ϕL−1\nkL−1ϕL−2\n· · ·\nkL−1ϕ0\n\n\nAϕ(k)\n(19)\nFourier decomposition of convolution operators: The kernels of the convolution operators Sψ\nand Sϕ are diagonalized by the Fourier transform matrix W ∈CL×L, Wnm = zm, z = ej2πn/L. The\nFourier transform of the convolution operator Sψ is given by\nSψ = W∗DΨW,\nSΦ = W∗DΦW\n(20)\nwhere DΨ, DΦ ∈CL×L are diagonal matrices constructed from the frequency responses (the discrete\nFourier transform) Ψ = Wψ, Φ = Wϕ, respectively. This decomposition can be used to simplify the\nmatrix multiplication in (19):\nA = DqSψDkSϕ = DqW∗DΨWDkW∗DΦW\n(21)\nAn important property of the above is the non-commutativity of Dq and Sk with W∗. If the two\noperators commuted, we would obtain\nA = DqW∗DΨWDkW∗DΦW = W∗DqDΨDkDΦW\n(22)\nwhich reduces the entire layer to a simple convolution. The non-commutativity of the gating term\nacts as a non-linearity in chain of convolution operators.\nC\nDiscussion and Additional Results\nVocabulary size scaling\nTable C.1 showcases interesting correlation between associative recall perfor-\nmance for varying vocabulary sizes and loss on the The Pile.\nIn this case, we ﬁx sequence length for\nassociative recall to be 2048, the same sequence length used to train all models on the The Pile.\nWe observe a similar phenomenon on other slices of tasks from our mechanistic design benchmarks, indi-\ncating that it may be possible to derive predictive laws for performance at scale, based on fast experimentation\non synthetic tasks with models of 1 or 2 layers. Surprisingly, performance on our language synthetics appears\nto be further linked to performance as attention replacement in other domains (Appendix A.4 for results on\nimage classiﬁcation).\nTable C.1: Hyena Accuracy on associative recall with varying vocabulary size 10, 20, 30, 40 in relation to\ntest loss on The Pile after 5 billion tokens. We notice a correlation between the two performance metrics,\nsuggesting that slices of our mechanistic design synthetics may be potentially predictive of performance at\nscale.\nModel\nAcc @ 10\nAcc @ 20\nAcc @ 30\nAcc @ 40\nLoss @ 5B on The Pile\nConv1d\n32\n11\n10\n8\n4.21\nAFT-conv\n55\n21\n12\n10\n3.57\nH3\n92\n60\n13\n10\n2.69\nTransformer\n100\n100\n92\n82\n2.59\nHyena\n100\n100\n98\n85\n2.59\n24\n",
  "25": "0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 1, Digits: 2\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 1, Digits: 4\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 1, Digits: 8\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 1, Digits: 16\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 2, Digits: 2\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 2, Digits: 4\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 2, Digits: 8\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 2, Digits: 16\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 3, Digits: 2\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 3, Digits: 4\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 3, Digits: 8\n0\n20\n40\n60\n80\n0\n1\n2\n3\nepochs\nLayers: 3, Digits: 16\nFigure C.1: Test loss and accuracy of Hyena on addition with diﬀerent numbers of digits and model depths.\nEach plot reports the results of a diﬀerent experiment, with the curve tracing test results during training.\nSingle layer recall\nAll experiments on our synthetic tasks default to 2 layer models. We choose 2 as\nit is the canonical number for mechanistic analysis of Transformers (Elhage et al., 2021) based on circuits.\nInterestingly, a single layer of Hyena (width 64) is capable of performing associative recall, solving the task\ncompletely even in the challenging setting with vocabulary size 40. Reverse engineering exactly how the\nsingle Hyena operator is able to perform recall is left for future work.\nC.1\nLearning Arithmetic\nWe showcase an additional task in our mechanistic design benchmark: learning arithmetic. We train Hyena\nmodels of increasing depth (1, 2 and 3 layers) on a dataset of Dn-digit addition. As an example, a 3-digit\naddition input sample is given by the sequence\n1, 2, 3, 9, 5, 4, 1, 0, 7, 7\nwhere the ﬁrst 6 digits contain the two 3 digits numbers to add, and the last 4 the result. Our models\nare optimized using standard autoregressive training i.e., predicting the next token, since they are causal.\nIn particular, we optimize models to learn a map x 7→y where x is the original prompt without the last\nelement, and y equal to x shifted right by one position. We mask the ﬁrst 2Dn −1 elements of the loss for\neach sequence since they contain predictions for addends and not results.\nWe report results in Figure C.1. A single layer of Hyena is able to learn to perform addition with up\nto 4 digits. Longer numbers require deeper models. In our experiments, alternative architectures such as\nAFT-conv struggle to learn arithmetic, signaling a cap in capability.\n25\n",
  "26": "D\nSamples and Visualizations\nD.1\nHyena Matrices\nWe provide visualizations of attention and Hyena matrices activated by test strings. In D.1, D.2, we compare\nGPTNeo (Black et al., 2021) attention matrices with Hyena matrices extracted by our pre-trained small\nHyena model. In D.3 and D.4, we provide additional Hyena matrices for the 355M model, activated by test\nstrings of diﬀerent length.\nFor attention, we visualize the raw post-softmax matrix. For Hyena matrices, we plot the (element-wise)\nabsolute value of H(u):\nH(u) = DN\nx SN\nh · · · D2\nxS2\nhD1\nxS1\nh\nˆH(u)ij = |H(u)ij|\nSince Hyena does not normalize the entries of its matrices with e.g., softmax, there are notable diﬀerences with\nattention: (1) the entries of H(u) can be either positive and negative, and (2) the magnitude is unconstrained.\nWe observe the magnitude of matrices in pre-trained Hyena models to be around 10−3.\n26\n",
  "27": "Figure D.1: Attention matrices from a GPTNeo small model. \"We use the test string \"Attention is all you\nneed. Attention is\".\n27\n",
  "28": "Figure D.2: Hyena matrices from a Hyena small (same model used for SuperGLUE downstream evaluations).\n\"We use the test string \"Attention is all you need.\nAttention is\".\nWe note that Hyena has a diﬀerent\ndata-controlled matrix for each channel i.e. for each dimension in its width, since it does not use heads.\n28\n",
  "29": "Figure D.3: Data-controlled Hyena matrices (355M model), activated by the string \"When a doctor doctors\na doctor, does the doctor doing the doctoring doctor as the doctor being doctored wants to be doctored or does\nthe doctor doing the doctoring doctor as they want to doctor?\". Rows in the plot are matrices from diﬀerent\nlayers, columns are matrices from diﬀerent channels. The operator shows characteristic patterns of attention\nmatrices, without attention.\n29\n",
  "30": "Figure D.4: Data-controlled Hyena matrices (355M model), activated by the string \"Mrs.\nDursley, Mr.\nDursley, Dudley Dursley\", from Causal scrubbing: results on induction heads. Rows in the plot are matrices\nfrom diﬀerent layers, columns are matrices from diﬀerent channels.\n30\n",
  "31": "D.2\nHyena Filters\nFigure D.5 provides a visualization of Hyena long convolution ﬁlters at initialization and after training to\ncompletion on The Pile.\nWe ﬁnd a substantial performance diﬀerence (up to 5% perplexity) between initialization schemes. If the\nﬁlters at initialization are excessively smooth (see Appendix D.3 for a discussion of positional encoding and\nactivation), the model ﬁnds a worse solution and takes longer to converge. Further, we observe initialization\nschemes that regularize ﬁlters towards typical ﬁlters learned at convergence to decrease performance. These\nobservations are in line with performance gaps between convolution parametrization schemes discussed in\nmain text and Appendix A.1. In particular, the performance improvements obtained via Hyena ﬁlters could\nbe due to easier optimization in the space of convolutional ﬁlters.\nAt convergence, Hyena learns a collection of lower-order ﬁlters with a similar structure, which can be\nexploited to further speed up inference after training.\nD.3\nPositional Encoding and Filters Initialization\nThe positional encoding chosen for the Hyena ﬁlters is a truncated complex exponential basis. Speciﬁcally,\nwith ρk(t) = ei2πkt/L for k = 0, . . . K −1, the positional encoding is deﬁned as a map from R to R2K+1 such\nthat\nPositionalEncoding(t) =\n\u0002\nt\nR[ρ0](t)\n· · ·\nR[ρK−1](t)\nI[ρ0](t)\n· · ·\nI[ρK−1](t)\n\u0003\nwhere R[·], I[·] denote the real and imaginary part of their argument, respectively. In the main text, we use\nDe = 2K + 1 to denote the size of a positional encoding with K features. The number of features of the\npositional encoding has an impact on the ﬁlter initialization and training performances. In particular, we\nshow how K leads to a preconditioning of the spectrum of the ﬁlter at initialization. Figures D.6, D.7, D.8\ndisplay the initialized ﬁlters (with no Window function) for diﬀerent values of K ({8, 32, 64}) for L = 128 and\nfrequency ωa of sinusoidal activation σ(·) = sin(ωa·) set to 1. We notice how the choice of K induces a bias\nin the modeled frequencies at initialization. Speciﬁcally the ﬁlters resemble low-pass ﬁlters with a cut-oﬀ\nfrequency of approximatively 2K + 1.\nThis cut-oﬀfrequency is strongly related to the smoothness of the ﬁlter; as previously mentioned, we\nempirically observe better training dynamics of ﬁlters initialized to be non-smooth, i.e. with a rich high-\nfrequency content. While we can achieve good initializations by increasing K, this results in larger FFNs\n(its input dimension is 2K + 1, i.e. the number of positional encoding features) which come with a higher\nparameter count.\nA more eﬃcient solution is to increase the frequency ωa of the sinusoidal activation.\nFigure D.9 show how with K = 8 we can cover the full spectrum simply by setting ωa = 10.\n31\n",
  "32": "Figure D.5: [Top]: Long convolution Hyena ﬁlters at initialization (153M parameters, 18 layer model).\n[Bottom]: Filters after training for 130 billion tokens on The Pile.\n32\n",
  "33": "0\n8\n16\n128\n96\n64\n32\n0\nPositional Encoding Feature\nSequence Index\nPositional Encoding\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nImpulse Response ht\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nMagnitude Response |FFT[h]|\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nPhase Response ̸ FFT[h]\nFigure D.6: Hyena ﬁlters at initialization with 17 positional encoding features K = 8.\n0\n16\n32\n48\n64\n128\n96\n64\n32\n0\nPositional Encoding Feature\nSequence Index\nPositional Encoding\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nImpulse Response ht\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nMagnitude Response |FFT[h]|\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nPhase Response ̸ FFT[h]\nFigure D.7: Hyena ﬁlters at initialization with 65 positional encoding features K = 32.\n33\n",
  "34": "0\n32\n64\n96\n128\n128\n96\n64\n32\n0\nPositional Encoding Feature\nSequence Index\nPositional Encoding\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nImpulse Response ht\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nMagnitude Response |FFT[h]|\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nPhase Response ̸ FFT[h]\nFigure D.8: Hyena ﬁlters at initialization with 65 positional encoding features K = 64.\n0\n8\n16\n128\n96\n64\n32\n0\nPositional Encoding Feature\nSequence Index\nPositional Encoding\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nImpulse Response ht\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nMagnitude Response |FFT[h]|\n0\n8\n16 24 32\n128\n96\n64\n32\n0\nFilter Index\nPhase Response ̸ FFT[h]\nFigure D.9: Hyena ﬁlters at initialization with 17 positional encoding features K = 8 and frequency of\nsinusodial activation set to 10.\n34\n",
  "35": "D.4\nDownstream Examples\nMultiRC\nWe report examples of downstream evaluation of small models on the MultiRC question-answering\ntask. We report answers of small Hyena (153M, trained for 130B tokens on The Pile) and the public check-\npoint RWKV-v4 (Peng, 2021) (169M, trained for 332B tokens on The Pile). We select randomized examples\nwith indices being powers of 2. Alignment of Hyena’s responses to the task format is greatly improved by\nproviding few-shot examples in the prompt, which may be a promising sign for larger models based on the\nproposed attention-free architecture. Each example shows responses for speciﬁc examples in the validation\nset (example indices are listed at the beginning of each example).\nMultiRC index: 2, 4\nPassage:\nWhat causes a change in motion?\nThe application of a force.\nAny time an object\nchanges motion, a force has been applied.\nIn what ways can this happen?\nForce can cause an\nobject at rest to start moving.\nForces can cause objects to speed up or slow down.\nForces\ncan cause a moving object to stop.\nForces can also cause a change in direction.\nIn short,\nforces cause changes in motion.\nThe moving object may change its speed, its direction,\nor both.\nWe know that changes in motion require a force.\nWe know that the size of the\nforce determines the change in motion.\nHow much an object’s motion changes when a force is\napplied depends on two things.\nIt depends on the strength of the force.\nIt also depends on\nthe object’s mass.\nThink about some simple tasks you may regularly do.\nYou may pick up a\nbaseball.\nThis requires only a very small force.\nQuestion:\nBased on the previous passage, Would the mass of a baseball affect how much force\nyou have to use to pick it up?\nIs \"Less the mass, less the force applied\" a correct answer?\nAnswer:\nYes\nRWKV zero-shot:\nThe mass of a baseball depends on the force applied.\nif the mass of the\nbaseball is small, the force applied is small.\nHyena zero-shot:\nYes it is.\nRWKV few-shot:\nThe mass, less the force applied, is a more important force.\nit is the force\nthat changes the mass of a baseball.\nHyena few-shot:\nYes.\nQuestion:\nBased on the previous passage, What do you apply to an object to make it move or\nstop?\nIs \"Strength\" a correct answer?\nAnswer:\nNo\nRWKV zero-shot:\nThe force applied to the object is the force applied to the object.\nHyena zero-shot:\nYes.\nRWKV few-shot:\nThe force applied to a baseball is a force.\nit is a force that is applied to\na baseball.\nHyena few-shot:\nYes.\n35\n",
  "36": "MultiRC index: 32, 64\nPassage:\nThe film opens with Sunita, a medical student, and her friends working on a project\nabout the human brain.\nShe wants to investigate the curious case of Sanjay Singhania,\na notable city businessman, who is reported to have anterograde amnesia.\nHer professor\ndenies access to Sanjay’s records as it is currently under criminal investigation.\nSunita,\nnonetheless, decides to investigate the matter herself.\nSanjay is introduced as he brutally\nmurders a man.\nHe takes a Polaroid picture of the man, and writes on it “done”.\nIt is\nrevealed that Sanjay has anterograde amnesia where he loses his memory every 15 minutes.\nSanjay uses a system of photographs, notes, and tattoos on his body to recover his memory\nafter each cycle.\nIt is revealed that Sanjay is ultimately out to avenge the death of his\nsweetheart Kalpana, and that he is systematically killing the people who were responsible\nfor it.\nHis main target is “Ghajini”, a notable social personality in the city.\nPolice\nInspector Arjun Yadav, on the case of the serial murders, tracks Sanjay down to his flat\nand attacks and disables him.\nYadav finds two diaries where Sanjay has chronicled the\nevents of 2005 and 2006 .\nThe film flashes back to 2005 as Yadav reads the diary.\nSanjay\nSinghania is shown as the owner of the Air Voice mobile telephone company.\nIn the course of\nhis business, Sanjay sends his men to meet Kalpana, a struggling model, about putting up a\nbillboard above her apartment.\nThe owner of Kalpana’s advertising firm misinterprets this\nas a romantic advance, and in view of a possible lucrative Air Voice ad campaign and other\nbenefits, encourages Kalpana to accept the overture.\nQuestion:\nBased on the previous passage, Why did Sanjay murder a man?\nIs \"Sanjay\" a correct\nanswer?\nAnswer:\nYes\nRWKV zero-shot:\nThe answer is \"Sanjay\".\nHyena zero-shot:\nThe answer is\nRWKV few-shot:\nThe question is not about Sanjay’s death.\nHyena few-shot:\nNo.\nQuestion:\nBased on the previous passage, Every 15 minutes, Sanjay goes through what process,\nWhich frustrates his attempts to avenge the death of his sweetheart?\nIs \"He forgets about\nfacts\" a correct answer?\nAnswer:\nYes\nRWKV zero-shot:\nYes, it is a correct answer..\nHyena zero-shot:\nThe answer is.\nRWKV few-shot:\nThe answer is \"yes\".\nHyena few-shot:\nThe answer is \"yes.\n36\n",
  "37": "MultiRC index: 128\nPassage:\nIn 1863, Alexander II re-convened the Diet of Finland and initiated several reforms\nincreasing Finlandś autonomy from Russia including establishment of its own currency,\nthe markka.\nLiberation of business led to increased foreign investment and industrial\ndevelopment.\nFinland also got its first railways, separately established under Finnish\nadministration.\nFinally, the elevation of Finnish from a language of the common people to\na national language equal to Swedish opened opportunities for a larger proportion of the\nsociety.\nAlexander II is still regarded as \"The Good Tsar\" in Finland.\nThese reforms could\nbe seen as results of a genuine belief that reforms were easier to test in an underpopulated,\nhomogeneous country, than in the whole of Russia.\nThey may also be seen as a reward for the\nloyalty of its relatively western-oriented population during the Crimean War and during the\nPolish uprising.\nEncouraging Finnish nationalism and language can also be seen as an attempt\nto dilute ties with Sweden.\nQuestion:\nBased on the previous passage, Alexander II is considered what in Finland since\n1863?\nIs \"The good tsar\" a correct answer?\nAnswer:\nYes\nRWKV zero-shot:\nThe correct answer is \"yes\".\nHyena zero-shot:\nYes, the question is correct.\nRWKV few-shot:\nThe Finnish language is a language of the people.\nHyena few-shot:\nYes.\n37\n",
  "38": "MultiRC index: 1024\nPassage:\nEinstein and Maric married in January 1903.\nIn May 1904, the couple’s first son,\nHans Albert Einstein, was born in Bern, Switzerland.\nTheir second son, Eduard, was born\nin Zurich in July 1910.\nIn 1914, the couple separated; Einstein moved to Berlin and his\nwife remained in Zurich with their sons.\nThey divorced on 14 February 1919, having lived\napart for five years.\nEduard, whom his father called \"Tete\" (for petit), had a breakdown at\nabout age 20 and was diagnosed with schizophrenia.\nHis mother cared for him and he was also\ncommitted to asylums for several periods, including full-time after her death.\nThe marriage\nwith Maric does not seem to have been very happy.\nIn letters revealed in 2015, Einstein\nwrote to his early love, Marie Winteler, about his marriage and his still strong feelings\nfor Marie.\nIn 1910 he wrote to her that \"I think of you in heartfelt love every spare minute\nand am so unhappy as only a man can be\" while his wife was pregnant with their second child.\nEinstein spoke about a \"misguided love\" and a \"missed life\" regarding his love for Marie.\nEinstein married Elsa Lowenthal on 2 June 1919, after having had a relationship with her\nsince 1912.\nShe was a first cousin maternally and a second cousin paternally.\nIn 1933, they\nemigrated to the United States.\nIn 1935, Elsa Einstein was diagnosed with heart and kidney\nproblems; she died in December 1936.\nQuestion:\nBased on the previous passage, How much time passed, after Albert Einstein’s\nfather divorced his mother, that he re-married?\nIs \"3 months and 18 days\" a correct answer?\nAnswer:\nYes\nRWKV zero-shot:\nAlbert Einstein was born on 3 march 1916 in Gerlin, Germany.\nhe was the son\nof a German doctor and a german woman.\nHyena zero-shot:\n\"3 months and 18 days\"\nRWKV few-shot:\nIt is not a correct answer.\nThe exact date is not known.\nHyena few-shot:\nYes, according to the previous passage.\n38\n"
}