{
  "1": "Token-level Direct Preference Optimization\nYongcheng Zeng 1 2 Guoqing Liu 3 Weiyu Ma 1 2 Ning Yang 1 Haifeng Zhang 1 Jun Wang 4\nAbstract\nFine-tuning pre-trained Large Language Models\n(LLMs) is essential to align them with human\nvalues and intentions.\nThis process often uti-\nlizes methods like pairwise comparisons and KL\ndivergence against a reference LLM, focusing\non the evaluation of full answers generated by\nthe models. However, the generation of these\nresponses occurs in a token level, following a\nsequential, auto-regressive fashion. In this pa-\nper, we introduce Token-level Direct Preference\nOptimization (TDPO), a novel approach to align\nLLMs with human preferences by optimizing pol-\nicy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency,\nTDPO incorporates forward KL divergence con-\nstraints for each token, improving alignment and\ndiversity. Utilizing the Bradley-Terry model for\na token-based reward system, TDPO enhances\nthe regulation of KL divergence, while preserv-\ning simplicity without the need for explicit re-\nward modeling. Experimental results across vari-\nous text tasks demonstrate TDPO’s superior per-\nformance in balancing alignment with genera-\ntion diversity. Notably, fine-tuning with TDPO\nstrikes a better balance than DPO in the controlled\nsentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of\ngenerated responses compared to both DPO and\nPPO-based RLHF methods. Our code is open-\nsourced at https://github.com/Vance0124/Token-\nlevel-Direct-Preference-Optimization.\n1Institute of Automation, Chinese Academy of Sciences\n2School of Artificial Intelligence, University of Chinese Academy\nof Sciences 3Microsoft Research AI4Science 4University College\nLondon. Correspondence to: Jun Wang <jun.wang@cs.ucl.ac.uk>,\nHaifeng Zhang <haifeng.zhang@ia.ac.cn>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\n1. Introduction\nLarge language models (LLMs) (Achiam et al., 2023;\nBubeck et al., 2023) have demonstrated significant gen-\neralization capabilities in various domains including text\nsummarization (Stiennon et al., 2022; Koh et al., 2022), cod-\ning writing (Chen et al., 2021; Gao et al., 2023), and even\nfollowing human instructions (Chung et al., 2022; Ouyang\net al., 2022). In order to align LLMs with human intentions,\nReinforcement Learning from Human Feedback (RLHF)\n(Christiano et al., 2017; Ouyang et al., 2022; Dong et al.,\n2023; Yuan et al., 2023; Liu et al., 2023) has emerged as a\nhighly effective method, embodying both stylistic and eth-\nical values (Bai et al., 2022; Ganguli et al., 2022). These\napproaches typically involve the training of a reward model\nfollowed by the fine-tuning of the policy model using rein-\nforcement learning (RL).\nDirect Preference Optimization (DPO) (Rafailov et al.,\n2023) introduces a straightforward and effective technique\nfor training LLMs using pairwise comparisons, without\nthe need for explicitly establishing a reward model. DPO\nutilizes KL divergence to ensure that the training process\nremains closely aligned with a reference Large Language\nModel (LLM), preventing significant deviations. In DPO,\nKL divergence is assessed at the sentence level, reflecting\nthe fact that evaluations are based on complete responses\n(answers), typically comprising several sentences. How-\never, the generation of these responses occurs sequentially,\nfollowing an auto-regressive approach. A potential benefit\nis to examine divergence in relation to a reference LLM\non a more granular, token-by-token basis. One approach\ninvolves using sequential KL divergence (as defined in Defi-\nnition 4.3), which monitors the trajectory of the generated\nresponses. As illustrated in Figure 1, DPO demonstrates\na significantly faster increase in KL divergence within the\nsubset of less preferred responses when compared to the\nsubset that is preferred. This results in an expanding gap\nbetween the two subsets and also indicates that DPO does\nnot effectively control the KL divergence of the dispreferred\nresponse subset. This impacts the model’s divergence effi-\nciency and ultimately affects its linguistic capabilities and\ngenerative diversity. Such a limitation highlights the de-\ncreased effectiveness of employing KL divergence within\nthe DPO framework, suggesting an area for improvement in\nits methodology.\n1\narXiv:2404.11999v5  [cs.CL]  30 Aug 2024\n",
  "2": "Token-level Direct Preference Optimization\n0\n5000\n10000\n15000\n20000\n25000\nstep\n0\n10\n20\n30\n40\n50\nDSeqKL(x, yw;\nref)\n(a)\n0\n5000\n10000\n15000\n20000\n25000\nstep\n0\n10\n20\n30\n40\n50\nDSeqKL(x, yl;\nref)\n(b)\n0\n5000\n10000\n15000\n20000\n25000\nstep\n0\n5\n10\n15\n20\n25\nmargin\nDPO\n(c)\nFigure 1. Sequential KL (SeqKL) divergence of both preferred response and dispreferred responses on IMDb dataset. Figure 1(a)\nshows the progression of SeqKL divergence on the preferred responses over training steps. Figure 1(b) depicts the evolution of SeqKL\ndivergence on the dispreferred responses over the training steps. Figure 1(c) illustrates the difference between the SeqKL divergence of\nthe dispreferred responses and that of the preferred responses during the training process, namely margin = |DSeqKL(x, yw; πref∥πθ) −\nDSeqKL(x, yl; πref∥πθ)|. The definition of SeqKL divergence refers to Definition 4.3.\nThe imbalance in the growth rates of the sequential KL di-\nvergence is potentially related to the reverse KL divergence\nconstraint employed by DPO. The mode-seeking property\nof reverse KL divergence tends to induce diversity reduction\nduring generation, limiting the model’s potential to produce\ndiverse and effective responses (Wiher et al., 2022; Khalifa\net al., 2020; Glaese et al., 2022; Perez et al., 2022). Built\nupon DPO, the f-DPO method (Wang et al., 2023) studies\nthe trade-off between alignment performance and generation\ndiversity of LLMs under different divergence constraints. It\nhighlights the advantages of the mass-covering behavior of\nforward KL divergence in enhancing model diversity and\nexplores the impact of different divergence constraints. Nev-\nertheless, f-DPO only independently discusses the changes\nin model behavior under either the reverse KL divergence\nor the forward KL divergence constraints. Essentially, it\ndoes not fundamentally enhance the DPO algorithm itself\nbut rather strikes a balance between alignment performance\nand generating diversity by simply swapping different KL\ndivergence constraints.\nInspired by the aforementioned observations, we define\nand examine the problem of aligning with human prefer-\nences from a sequential and token-level standpoint. Some\nconcurrent work has also been conducted in this direction\n(Rafailov et al., 2024; Zhong et al., 2024). We introduce a\nnew method, referred to as Token-level Direct Preference\nOptimization (TDPO), which aims to strike a better balance\nbetween alignment performance and generation diversity\nby controlling the KL divergence for each token. In order\nto achieve this, we redefine the objective of maximising\nrestricted rewards in a sequential manner. The connection\nbetween sentence-level reward and token-level generation\nis established by the use of the Bellman equation. After-\nwards, the Bradley-Terry model (Bradley & Terry, 1952) is\nconverted into a representation at the token level, demon-\nstrating its close relationship with the Regret Preference\nModel (Knox et al., 2022; 2023). By utilizing this method,\nwe effectively integrate forward KL divergence restrictions\nfor each token in the final objective function, resulting in\nimproved regulation of KL divergence.\nTDPO maintains the simplicity of DPO while offering im-\nproved regulation of KL divergence for aligning LLMs\nwith human preferences. Echoing the strategy of DPO, our\nmethod directly optimizes the policy without necessitating\nexplicit reward model learning or policy sampling through-\nout the training phase. Our experimental results demonstrate\nthe effectiveness of TDPO across multiple text tasks, and\ngain a notable enhancement in the quality of generated re-\nsponses in comparison to both DPO and PPO-based RLHF\nmethods. In conclusion, TDPO stands out for its ability\nto not only effectively address the issue of excessive KL\ndivergence but also greatly improve divergence efficiency.\n2. Related Works\nThe emergence of ChatGPT has catalyzed significant ad-\nvancements in the field of Large Language Models (LLMs),\nsuch as OpenAI’s GPT-4 (Achiam et al., 2023), Mistral\n(Jiang et al., 2023), and Google’s Gemini (Team et al., 2023).\nGenerally, the training of LLMs involves three stages: ini-\ntial unsupervised pre-training on massive text corpora to\ngrasp linguistic structures (Raffel et al., 2020; Brown et al.,\n2020; Workshop et al., 2022; Touvron et al., 2023), fol-\nlowed by supervised fine-tuning with task-specific datasets\nto enhance the LLMs’ probability of producing desired re-\nsponses (Taori et al., 2023; Chiang et al., 2023; Vu et al.,\n2023). However, due to the typically limited and expen-\nsive availability of labeled datasets during the supervised\n2\n",
  "3": "Token-level Direct Preference Optimization\nfine-tuning stage, the model may retain biases and inaccu-\nracies, manifesting as societal biases (Sheng et al., 2021),\nethical concerns (Weidinger et al., 2021), toxicity (Rauh\net al., 2022), and hallucinations (Huang et al., 2023), which\nnecessitates a subsequent AI alignment phase. Notewor-\nthy models achieving significant alignment, such as Zephyr\n(Tunstall et al., 2023) and GPT-4 (Achiam et al., 2023), have\ndemonstrated the effectiveness of techniques like Reinforce-\nment Learning from Human Feedback (RLHF) and Direct\nPreference Optimization (DPO) algorithms.\nReinforcement Learning from Human Feedback (RLHF)\nhas emerged as a cornerstone in aligning LLMs with hu-\nman values, providing a mechanism to refine model out-\nputs based on qualitative feedback (Christiano et al., 2017;\nOuyang et al., 2022; Bai et al., 2022; Song et al., 2023;\nTouvron et al., 2023). This approach has shown consider-\nable promise in making models more responsive to human\nexpectations and ethical considerations by iteratively im-\nproving their performance through human-generated feed-\nback. However, the complexity of implementing RLHF,\ncompounded by the inaccuracies in human-generated re-\nward models (Wu et al., 2023), has prompted the exploration\nof alternative strategies. Methods like Reward Ranked Fine-\nTuning (RAFT) (Dong et al., 2023) and Rank Responses to\nalign Human Feedback (RRHF) (Yuan et al., 2023) offer\nstreamlined approaches to alignment, circumventing some\nof RLHF’s inherent challenges. Particularly, Direct Prefer-\nence Optimization (DPO) (Rafailov et al., 2023) represents\na breakthrough in direct policy optimization, addressing the\nintricacies of balancing model behavior through a nuanced\napproach to reward function optimization. Nevertheless, the\nchallenge of maintaining linguistic diversity while aligning\nwith human preferences remains a pivotal concern, prompt-\ning our proposed Token-level Direct Preference Optimiza-\ntion (TDPO), which seeks to harmonize the dual objectives\nof alignment accuracy and expressive range in model out-\nputs.\n3. Preliminaries\nFor language generation, a language model (LM) is\nprompted with prompt (question) x to generate a response\n(answer) y, where both x and y consist of a sequence of\ntokens. Direct Preference Optimization (DPO) (Rafailov\net al., 2023) commences with the RL objective from the\nRLHF:\nmax\nπθ\nEx∼D,y∼πθ(·|x)\n\u0002\nr(x, y)\n−βDKL\n\u0000πθ(· | x)\n\r\rπref(· | x)\n\u0001\u0003\n,\n(1)\nwhere D represents the human preference dataset, r(x, y)\ndenotes the reward function, πref(·|x) serves as a reference\nmodel, typically chosen the language model after supervised\nfine-tuning, πθ represents the model undergoing RL fine-\ntuning, initialized with πθ = πref, and β is the coefficient\nfor the reverse KL divergence penalty.\nBy directly deriving from Eq. 1, DPO establishes a mapping\nbetween the reward model and the optimal policy under the\nreverse KL divergence, obtaining a representation of the\nreward function concerning the policy:\nr(x, y) = β log πθ(y|x)\nπref(y|x) + β log Z(x).\n(2)\nHere, Z(x) is the partition function.\nTo align with human preference, DPO uses the Bradley-\nTerry model for pairwise comparisons:\nPBT(y1 ≻y2|x) =\nexp(r(x, y1))\nexp(r(x, y1)) + exp(r(x, y2)). (3)\nBy substituting Eq. 2 into Eq. 3 and leveraging the negative\nlog-likelihood loss, DPO derives the objective function:\nu(x, yw, yl) = β log πθ(yw | x)\nπref(yw | x) −β log πθ(yl | x)\nπref(yl | x),\nLDPO(πθ; πref) = −E(x,yw,yl)∼D [log σ (u(x, yw, yl))] ,\n(4)\nand the derivative is given as follows:\n∇θLDPO(πθ; πref) = −E(x,yw,yl)∼D [σ (−u) ∇θu] , (5)\nwhere u is the abbreviation of u(x, yw, yl), yw and yl de-\nnotes the preferred and dispreferred completion.\n4. Methodology\nIn this section, we initially reformulate the constrained re-\nward maximization problem into a token-level form. From\nthis, we derive the mapping between the state-action func-\ntion and the optimal policy. Subsequently, we convert the\nBradley-Terry model into token-level representation, estab-\nlishing its equivalence with the Regret Preference Model.\nBy substituting the mapping relationship into the reward\nmodel in token-level format, we obtain the optimization\nobjective solely related to the policy. Finally, we conduct a\nformalized analysis of this optimization objective in terms\nof derivatives and, based on this, derive the ultimate loss\nfunction for TDPO.\n4.1. Markov Decision Process under Token Rewards\nTo model the sequential, auto-regressive generation, we ex-\ntend the sentence-level formulation in Section 3 by consid-\nering that the response consists of T tokens y = y<T +1 :=\n[y1, y2, ..., yT ], where yt ∈Y, and Y represents the alpha-\nbet (vocabulary). Additionally, we assume y<1 = [ ]. Given\na prompt x and the first t −1 tokens y<t of the response\n3\n",
  "4": "Token-level Direct Preference Optimization\ny, the LM predicts the probability distribution of the next\ntoken πθ(·|[x, y<t]).\nWhen modeling text generation as a Markov decision pro-\ncess (Puterman, 2014), a state is a combination of the prompt\nand the generated response up to the current step, denoted as\nst = [x, y<t]. An action corresponds to the next generated\ntoken, denoted as at = yt, and the token-wise reward is\ndefined as Rt := R(st, at) = R([x, y<t], yt).\nExpanding on the provided definitions, we establish the\nstate-action function Qπ, the state value function Vπ and\nthe advantage function Aπ for a policy π:\nQπ([x, y<t], yt) = Eπ\n\" ∞\nX\nk=0\nγkRt+k\n\f\f\f\fst = [x, y<t], at = yt\n#\n,\nVπ([x, y<t]) = Eπ\n\u0002\nQπ([x, y<t], yt)\n\f\fst = [x, y<t]\n\u0003\n,\nAπ([x, y<t], yt) = Qπ([x, y<t], yt) −Vπ([x, y<t]).\n(6)\nwhere γ represents the discount factor. In this paper, we set\nγ = 1.\n4.2. Token-Level Optimization\nDPO’s objective function in Eq. 1 operates at the sentence\nlevel. In contrast, we propose an alternative token-level\nobjective function:\nmax\nπθ\nEx,y<t∼D,z∼πθ(·|[x,y<t])\n\u0002\nAπref([x, y<t], z)\n−βDKL\n\u0000πθ(·|[x, y<t])||πref(·|[x, y<t])\n\u0001 \u0003\n.\n(7)\nThe objective function is inspired by Trust Region Policy\nOptimization (TRPO) (Schulman et al., 2015). As demon-\nstrated in Lemma 4.1, maximizing the objective function in\nEq. 7 will result in policy improvements in terms of expected\nreturn.\nLemma 4.1. Given two policies π and ˜π, if for any state\nst = [x, y<t], Ez∼˜π [Aπ([x, y<t], z)] ≥0, then we can\nconclude:\nEx∼D [V˜π([x])] ≥Ex∼D [Vπ([x])] ,\nThe proof is provided in Appendix A.1.\nNotably, to maintain generation diversity and prevent the\nmodel from hacking some high-reward answers, we incorpo-\nrate reverse KL divergence for each token in our token-level\nobjective function, which prevents the model from deviating\ntoo far from the reference model distribution.\nStarting from the token-level objective function in Eq. 7,\nwe can directly derive the mapping between the state-action\nfunction Qπ and the optimal policy π∗\nθ. We summarize this\nrelationship in the following lemma.\nLemma 4.2. The constrained problem in Eq. 7 has the\nclosed-form solution:\nπ∗\nθ(z|[x, y<t]) =\nπref(z|[x, y<t]) exp\n\u0010\n1\nβ Qπref([x, y<t], z)\n\u0011\nZ([x, y<t]; β)\n,\n(8)\nwhere Z([x, y<t]; β) = Ez∼πref(·|[x,y<t])e\n1\nβ Qπref ([x,y<t],z)\nis the partition function.\nSee Appendix A.2 for more details.\nTo obtain the optimal policy π∗\nθ from Eq. 8, we must esti-\nmate the state-action function Qπref and the partition func-\ntion Z(·). However, ensuring the accuracy of the state-\naction function Qπ at each state and action is challenging,\nand estimating the partition function Z(·) is also difficult.\nTherefore, we reorganize Eq. 8 to obtain the expression of\nthe state-value function in terms of the policy:\nQπref([x, y<t], z) =\nβ log π∗\nθ(z|[x, y<t])\nπref(z|[x, y<t]) + β log Z([x, y<t]; β).\n(9)\n4.3. BT Model Reformulation via Advantage Function\nTo facilitate subsequent derivations, we first introduce the\nsequential KL divergence, as defined in Definition 4.3.\nDefinition 4.3. Given two language models π1 and π2, with\nthe input prompt x and output response y, the sequential KL\ndivergence is defined as:\nDSeqKL(x,y; π1∥π2) =\nT\nX\nt=1\nDKL(π1(·|[x, y<t])∥π2(·|[x, y<t])).\n(10)\nGiven prompts x and pairwise responses (y1, y2), the\nBradley-Terry model expresses the human preference prob-\nability. However, since the Bradley-Terry model is formu-\nlated at the sentence level, it cannot establish a connection\nwith the token-level mapping presented in Eq. 9. Conse-\nquently, we need to derive a token-level preference model.\nInitiating from the Bradley-Terry model, we transform it\ninto a token-level formulation and demonstrate its equiva-\nlence with the Regret Preference Model (Knox et al., 2023;\n2022), as shown in the Lemma 4.4.\nLemma 4.4.\nGiven a reward function r(x, y),\nas-\nsuming\na\nrelationship\nbetween\ntoken-wise\nrewards\nand the reward function represented by r(x, y)\n=\nPT\nt=1 γt−1R([x, y<t], yt), we can establish the equiva-\nlence between the Bradley-Terry model and the Regret Pref-\n4\n",
  "5": "Token-level Direct Preference Optimization\nerence Model in the task of text generation alignment, i.e.,\nPBT(y1 ≻y2|x) =\nσ\n T1\nX\nt=1\nγt−1Aπ([x, y<t\n1 ], yt\n1) −\nT2\nX\nt=1\nγt−1Aπ([x, y<t\n2 ], yt\n2)\n!\n,\n(11)\nwhere σ(x) = 1/(1 + exp(−x)) is the logistic sigmoid\nfunction.\nWe prove this lemma in A.3.\nIn\nLemma\n4.4,\nwe\nassume\nthat\nr(x, y)\n=\nPT\nt=1 γt−1R([x, y<t], yt).\nThis assumption is natu-\nral in the context of RL, where r(x, y) represents the overall\nreward for response y given the prompt x. Considering text\ngeneration as a sequential decision-making problem, r(x, y)\ncan be viewed as the cumulative reward for the generated\ntext.\nAccording to the definition of the advantage function in Sec-\ntion 4.1, we can directly establish the relationship between\nthe optimal solution in Eq. 9 and preference optimization\nobjective in Eq. 11. One intractable aspect is that the state-\naction function Qπ depends on a partition function, which\nis contingent on both the input prompt x and the output\nresponse y. This results in non-identical values of the par-\ntition function for a pair of responses (yw, yl), specifically,\nZ([x, y<t\nw ]; β) ̸= Z([x, y<t\nl ]; β). As a result, we cannot em-\nploy a cancellation strategy similar to DPO, which relies on\nthe property that the Bradley-Terry model depends only on\nthe difference in rewards between two completions.\nFortunately, by expanding the advantage function Aπ and\nconverting the state-value function Vπ into a form exclu-\nsively related to the state-action function Qπ, we can offset\nthe partition function naturally. In this way, we ultimately\nreformulate the Bradley-Terry model to be directly tied to\nthe optimal policy π∗\nθ and the reference policy πref. This is\nsummarized in the following theorem.\nTheorem 4.5. In the KL-constrainted advantage function\nmaximization problem corresponding to Eq.7, the Bradley-\nTerry model express the human preference probability in\nterms of the optimal policy π∗\nθ and reference policy πref:\nP ∗\nBT(y1 ≻y2|x) = σ(u∗(x, y1, y2) −δ∗(x, y1, y2)),\n(12)\nwhere, u(x, y1, y2) refers to the difference in rewards im-\nplicitly defined by the language model πθ and the reference\nmodel πref (Rafailov et al., 2023), represented as\nu(x, y1, y2) = β log πθ(y1 | x)\nπref(y1 | x) −β log πθ(y2 | x)\nπref(y2 | x),\n(13)\nand δ(x, y1, y2) refers to the difference in sequential for-\nward KL divergence between two pairs (x, y1) and (x, y2),\nweighted by β, expressed as\nδ(x, y1, y2) =βDSeqKL (x, y2; πref∥πθ)\n−βDSeqKL (x, y1; πref∥πθ) .\n(14)\nThe proof is provided in the Appendix A.4.\n4.4. Loss Function and Formal Analysis\nDrawing on Eq. 12, we reformulate the Bradley-Terry model\ninto a structure solely relevant to the policy. This allows\nus to formulate a likelihood maximization objective for a\nparametrized policy πθ, leading to the derivation of the loss\nfunction for the initial version of our method, TDPO1:\nLTDPO1 (πθ; πref) =\n−E(x,yw,yl)∼D [log σ (u(x, yw, yl) −δ(x, yw, yl))] .\n(15)\nThrough this approach, we explicitly introduce sequential\nforward KL divergence into the loss function. Coupled with\nthe implicitly integrated reverse KL divergence, we enhance\nour ability to balance alignment performance and generation\ndiversity of LLMs.\nSubsequently, we conduct a derivative analysis of our\nmethod and make specific modifications to the loss function\nof TDPO. For convenience, we use u to denote u(x, yw, yl),\nand δ to represent δ(x, yw, yl). By employing the formula-\ntion of the loss function presented in Eq.15, we compute the\ngradient of the loss function with respect to the parameters\nθ:\n∇θLTDPO1(πθ; πref) =\n−E(x,yw,yl)∼D [σ (−u + δ) [∇θu −∇θδ]] .\n(16)\nIn Eq. 16, σ(−u + δ) serves as the weighting factor for\nthe gradient. The first part (−u) corresponds to the weight\nfactor in the loss function of DPO. When the language\nmodel makes errors in predicting human preferences, i.e.,\nlog πθ(yl|x)\nπref(yl|x) > log πθ(yw|x)\nπref(yw|x), the value of (−u) will be-\ncome larger, applying a stronger update for the comparison\n(yw, yl). While the second part δ is a distinctive component\nof our method. As shown in Figure 1, the KL divergence\ngrowth rate for the dispreferred response subset is faster\nthan that for the preferred response subset. With the increas-\ning disparity, the corresponding value of δ rises, thereby\namplifying the weight factor σ(−u + δ). Combined with\nthe subsequent gradient term, our objective function can ef-\nfectively suppress the difference in KL divergence between\npairs of responses with large disparities in KL divergence.\nThrough the collaborative influence of the weight factor δ\nand the gradient term (−∇θδ), our method achieves the pur-\npose of automatic control over the KL divergence balance.\nThe gradient of the loss function in Eq. 16 also consists\nof two components, ∇θu and (−∇θδ). ∇θu represents\nthe optimization direction of the gradient in DPO. Intu-\nitively, ∇θu increases the likelihood of preferred comple-\ntions yw and decreases the likelihood of dispreferred com-\n5\n",
  "6": "Token-level Direct Preference Optimization\npletions yl. While (−∇θδ) tends to narrow the gap between\nDSeqKL (x, yw; πref∥πθ) and DSeqKL (x, yl; πref∥πθ).\nHowever, when considered separately, the gradient of\nDSeqKL (x, yw; πref|πθ) in the loss function tends to in-\ncrease the sequential KL divergence between πref and πθ\nat (x, yw) during the optimization process. This is because\nthe sequential forward KL divergence in the loss function is\nintroduced through the state-value function Vπ, inherently\nintroducing an expectation Ez∼πref\nh\nlog πθ(z|[x,y<t])\nπref(z|[x,y<t])\ni\nas a\nbaseline at each token. The negative value of this expec-\ntation corresponds precisely to a forward KL divergence\nDKL (πref(·|[x, y<t])|πθ(·|[x, y<t])), which can be used to\nconstrain the unbalanced growth of KL divergence. For\nthe prompt x and the preferred response yw, at each token,\nthe loss function in Eq. 16 tends to increase the likelihood\nof log\nπ(yt\nw|[x,y<t\nw ])\nπref(ytw|[x,y<t\nw ]) while simultaneously decreasing the\nexpectation, enlarging the gap between the specified term\nyt\nw and the baseline to expedite training. The impact of\ndecreasing the expectation is an increase in the forward\nKL divergence DKL (πref(·|[x, y<t\nw ])|πθ(·|[x, y<t\nw ])) at each\ntoken, leading to an increase in DSeqKL (x, yw; πref|πθ).\nAs we do not aim to accelerate the training speed and\nprefer to ensure training stability, we modify the loss\nfunction by discontinuing the gradient propagation of\nDSeqKL (x, yw; πref|πθ) and treating it as a baseline term\nfor alignment of DSeqKL (x, yl; πref|πθ).\nDifferent from DSeqKL (x, yw; πref|πθ), the gradient of\nDSeqKL (x, yl; πref|πθ) tends to reduce the sequential\nKL divergence between πref and πθ at (x, yl).\nFor\nthe prompt x and the rejected response yl, the loss\nfunction in Eq.16 tends to decrease the likelihood of\nlog\nπ(yt\nl |[x,y<t\nl\n])\nπref(yt\nl |[x,y<t\nl\n]) at each token while increasing the ex-\npectation Ez∼πref\nh\nlog πθ(z|[x,y<t\nl\n])\nπref(z|[x,y<t\nl\n])\ni\n. The increase in the\nexpectation implies a smaller forward KL divergence at that\ntoken, thereby acting to constrain the growth rate of sequen-\ntial forward KL divergence. Therefore, for this term, we\nchoose to retain its gradient updates.\nIn conclusion, we only propagate the gradient of the\nDSeqKL (x, yl; πref|πθ) in (−∇θδ). When the second part\nweight factor δ becomes larger, it imposes a stronger sup-\npression on DSeqKL (x, yl; πref∥πθ) to control the balance\nof KL divergence.\nFurthermore, to achieve a better balance between align-\nment performance and generation diversity in TDPO, we\nintroduce an additional parameter α into the loss func-\ntion.\nBy adjusting the magnitude of α, we can con-\ntrol the deviation between DSeqKL (x, yw; πref∥πθ) and\nDSeqKL (x, yl; πref∥πθ).\nIn summary, we modify the loss function of TDPO1, re-\nsulting in the second version of our method, TDPO2, as\nfollows:\nLTDPO2 (πθ; πref) =\n−E(x,yw,yl)∼D [log σ (u(x, yw, yl) −αδ2(x, yw, yl))] ,\n(17)\nwhere α is a parameter, and\nδ2(x, y1, y2) =βDSeqKL (x, y2; πref∥πθ)\n−sg (βDSeqKL (x, y1; πref∥πθ)) .\n(18)\nThe sg represents the stop-gradient operator, which blocks\nthe propagation of gradients.\nWe summarize the comparison of the loss functions for\nDPO, TDPO1, and TDPO2, as presented in Figure 2.\n−𝛼𝛽𝐷SeqKL(𝑥, 𝑦𝑙; 𝜋ref||𝜋𝜃) −𝑠𝑔𝛽𝐷SeqKL 𝑥, 𝑦𝑤; 𝜋ref| 𝜋𝜃\n−𝛽𝐷SeqKL(𝑥, 𝑦𝑙; 𝜋ref||𝜋𝜃) −𝛽𝐷SeqKL(𝑥, 𝑦𝑤; 𝜋ref||𝜋𝜃)\n𝓛𝐃𝐏𝐎𝜋𝜃; 𝜋ref = −𝔼log 𝜎𝛽log 𝜋𝜃(𝑦𝑤|𝑥)\n𝜋ref(𝑦𝑤|𝑥) −𝛽log 𝜋𝜃(𝑦𝑙|𝑥)\n𝜋ref(𝑦𝑙|𝑥)\n𝓛𝐓𝐃𝐏𝐎𝟐𝜋𝜃; 𝜋ref = −𝔼log 𝜎\n𝛽log 𝜋𝜃(𝑦𝑤|𝑥)\n𝜋ref(𝑦𝑤|𝑥) −𝛽log 𝜋𝜃(𝑦𝑙|𝑥)\n𝜋ref(𝑦𝑙|𝑥)\n𝓛𝐓𝐃𝐏𝐎𝟏𝜋𝜃; 𝜋ref = −𝔼log 𝜎\n𝛽log 𝜋𝜃(𝑦𝑤|𝑥)\n𝜋ref(𝑦𝑤|𝑥) −𝛽log 𝜋𝜃(𝑦𝑙|𝑥)\n𝜋ref(𝑦𝑙|𝑥)\nFigure 2. Comparison of Loss Functions for DPO, TDPO1 and\nTDPO2 Methods. The sg denotes the stop-gradient operator.\nBoth TDPO1 and TDPO2 incorporate an additional term for\nfiner-grained control over the KL divergence, compared to DPO.\nLeveraging the parameter β to regulate the deviation of the\nlanguage model from the base reference model, and α to\ncontrol the balance of sequential KL divergence within the\nlanguage model, our approach achieves superior alignment\nwith human preferences while preserving model generation\ndiversity effectively. We provided the pseudocode in Algo-\nrithm 1 and the Pytorch implementation version of TDPO\nloss in Appendix B.\n5. Experiments\nIn this section, we demonstrate the superior performance\nof our algorithm in three different open-sourced datasets:\nthe IMDb sentiment dataset (Maas et al., 2011), the An-\nthropic HH dataset (Bai et al., 2022), and MT-bench (Zheng\net al., 2023). The IMDb dataset serves as a controlled se-\nmantic generation dataset where the model is presented\nwith prompts consisting of prefixes from movie reviews,\nand required to generate responses with positive sentiment.\nThe Anthropic HH dataset is a single-turn dialogue dataset\nwhere the model receives human queries, covering various\n6\n",
  "7": "Token-level Direct Preference Optimization\nAlgorithm 1 Token-level Direct Preference Optimization (TDPO)\n1: Input: Reference model πref, Policy model πθ, Coefficient α, β, Learning rate η\n2: Input: Dataset D = {(x, yw, yl)i}N\ni=1 of size N, Method M\n3: Initialize: πθ ←πref\n4: for each epoch do\n5:\nSample mini-batch Dm = {(x, yw, yl)m}M\nm=1 from D\n6:\nPredict the probabilities πθ(yw|x) and πθ(yl|x) for (x, yw, yl) in the mini-batch Dm using the policy model\n7:\nPredict the probabilities πref(yw|x) and πref(yl|x) for (x, yw, yl) in the mini-batch Dm using the reference model\n8:\nCalculate the function u(x, yw, yl) = β log πθ(yw|x)\nπref(yw|x) −β log πθ(yl|x)\nπref(yl|x)\n▷Eq.13\n9:\nCompute the sequential KL divergence DSeqKL (x, yw; πref∥πθ) for (x, yw) in the mini-batch Dm\n10:\nCompute the sequential KL divergence DSeqKL (x, yl; πref∥πθ) for (x, yl) in the mini-batch Dm\n11:\nif Method M is TDPO1 then\n12:\nCalculate the function δ(x, yw, yl) = βDSeqKL (x, yl; πref∥πθ) −βDSeqKL (x, yw; πref∥πθ)\n▷Eq.14\n13:\nθ ←θ + η∇θE(x,yw,yl)∼Dm [log σ (u(x, yw, yl) −δ(x, yw, yl))]\n▷Eq.15\n14:\nelse if Method M is TDPO2 then\n15:\nCalculate the function δ2(x, yw, yl) = βDSeqKL (x, yl; πref∥πθ) −sg (βDSeqKL (x, yw; πref∥πθ))\n▷Eq.18\n16:\nθ ←θ + η∇θE(x,yw,yl)∼Dm [log σ (u(x, yw, yl) −αδ2(x, yw, yl))]\n▷Eq.17\n17:\nend if\n18: end for\n19: Output: πθ\ntopics such as academic questions or life guidance. The\ntrained model is tasked with providing helpful answers to\nthese questions while avoiding toxic responses. Finally,\nMT-Bench is a GPT-4-based evaluation benchmark, assess-\ning the proficiency of LLMs in handling multi-turn open-\nended questions. Questions in MT-Bench span eight distinct\nknowledge domains, from areas such as writing, mathe-\nmatical reasoning, and humanities. Experimental results\ndemonstrate that MT-Bench achieves consistency with hu-\nman preferences exceeding 80%.\n5.1. Experiments on IMDb Dataset\nIn this experiment, besides our proposed methods TDPO1\nand TDPO2, we also implemented the DPO algorithm\nfor fair comparison. We employed GPT-2 Large (Radford\net al., 2019) as our base model and the model checkpoint:\ninsub/gpt2-large-IMDb-fine-tuned1 as the SFT model. Dur-\ning the evaluation, we utilized the pre-trained sentiment\nclassifier siebert/sentiment-roberta-large-english2 to com-\npute rewards. For DPO, we followed the official implemen-\ntation (Rafailov et al., 2023), setting β at 0.1. To analyze\nthe effectiveness of each algorithm in optimizing the con-\nstrained reward maximization objective, we evaluated each\nalgorithm after 100 training steps until convergence, com-\nputing its frontier of average reward and average sequential\nKL divergence with the reference policy.\n1https://huggingface.co/insub/\ngpt2-large-IMDb-fine-tuned\n2https://huggingface.co/siebert/\nsentiment-roberta-large-english\nThe results are depicted in Figure 3(a). We implement\nthe DPO, TDPO1, and different versions of TDPO2 algo-\nrithms with varying the parameter α. From the figure, we\nnotice that although DPO establishes an efficient frontier,\nTDPO1 and TDPO2 outperform DPO in terms of diver-\ngence versus reward on the frontier, achieving higher reward\nwhile maintaining low KL divergence. We also implemented\nversions of TDPO2 with α ∈{1, 1.5, 2, 5}. However, we\nfound that higher values of α made it difficult to optimize\nthe reward. In Figures 3(b) to 3(d), we illustrate the curves\nportraying the sequential KL divergence for different al-\ngorithms during the training process. The sequential KL\ndivergence growth rate of DPO on the dispreferred response\nsubset is significantly higher than that on the preferred re-\nsponse subset, leading to an increasing offset between them.\nIn contrast, TDPO2 exhibits superior control over KL di-\nvergence, achieving better divergence efficiency compared\nto DPO. As analyzed in Section 4.4, TDPO1 tends to result\nin an increased sequential KL divergence on the preferred\nresponse subset, thereby exhibiting a weaker capacity for\nKL divergence adjustment compared to TDPO2. TDPO2\nmaintains a more balanced sequential KL divergence on\nboth dispreferred and preferred response subsets, contribut-\ning to its ability to achieve a superior frontier. Although a\nlarger α enhances control over the sequential KL divergence,\nit also affects the speed and difficulty of optimization. For\nthe remainder of this paper, we set α = 0.5. In Appendix C,\nwe also present graphs of the frontier between the reward\nand forward KL divergence and the progression curves of\nthe forward KL divergence throughout the training process.\n7\n",
  "8": "Token-level Direct Preference Optimization\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nDKL(\nref)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nReward\nDPO\nTDPO1\nTDPO2  \n= 0.1\nTDPO2  \n= 0.3\nTDPO2  \n= 0.5\nTDPO2  \n= 0.7\n(a)\n0\n5000\n10000\n15000\n20000\n25000\nstep\n0\n10\n20\n30\n40\n50\nDSeqKL(x, yw;\nref)\n(b)\n0\n5000\n10000\n15000\n20000\n25000\nstep\n0\n10\n20\n30\n40\n50\nDSeqKL(x, yl;\nref)\n(c)\n0\n5000\n10000\n15000\n20000\n25000\nstep\n0\n5\n10\n15\n20\n25\nmargin\n(d)\nFigure 3. The experiment on IMDb dataset. Figure 3(a) represents the frontier of expected reward and KL divergence with respect to the\nreference model. We implemented DPO, TDPO1, and different versions of TDPO2 with respect to the parameter α. Both TDPO1 and\nTDPO2 outperform DPO in terms of the frontier, with TDPO2 showing further improvement over TDPO1. This demonstrates the\neffectiveness of our analysis and modifications. Figure 3(b) and Figure 3(c) present the progression of sequential KL divergence on the\npreferred and dispreferred responses subset over training steps respectively. Figure 3(d) illustrates the difference between the sequential\nKL divergence on the dispreferred responses subset and that on the preferred responses subset throughout the training process, namely\nmargin = |DSeqKL(x, yw; πref∥πθ) −DSeqKL(x, yl; πref∥πθ)|. TDPO2 exhibit superior regulation over KL divergence compared to\nthe TDPO1 and DPO algorithm.\nTable 1. Comparison of DPO, TDPO1 and TDPO2 in terms of\nthe trade-off between Alignment (accuracy) and Diversity (en-\ntropy) on the Anthropic HH dataset. The ↑indicates higher values\nare preferable.\nMethod\nAlignment\nDiversity\nAccuracy(%) ↑\nEntropy ↑\nf-DPO(FKL)\n54.71\n4.708\nDPO\n59.43\n3.196\nTDPO1(ours)\n60.08\n4.727\nTDPO2(ours)\n67.33\n4.915\n5.2. Experiments on Anthropic HH Dataset\nNext, we evaluate the performance of TDPO1 and TDPO2\non the Anthropic HH dataset. We use Pythia-2.8B (Bider-\nman et al., 2023) as the base model and fine-tune the base\nmodel on chosen completions to train a reference model,\nsuch that completions are within the distribution of the\nmodel. Subsequently, we train TDPO1, TDPO2, DPO\n(Rafailov et al., 2023) and f-DPO with forward KL diver-\ngence constraint (Wang et al., 2023) on this reference model.\nIn this experiment, our primary focus is on two aspects: 1)\nthe trade-off between alignment and diversity in generating\nresponses among different algorithms, and 2) the ability of\ndifferent algorithms to align with human preferences. For\nthe first part, we utilize automatic metrics for evaluation,\nwhile for the second part, we rely on the GPT-4 evalua-\ntion. Both evaluations were conducted on the test set of the\nAnthropic HH dataset.\nTo assess the alignment performance of different algorithms\nin generating responses, we compute the accuracy of gen-\n8\n",
  "9": "Token-level Direct Preference Optimization\nerated responses relative to chosen completions in the test\ndataset. To measure the diversity, we employ nucleus sam-\npling with p = 0.95 to generate 25 responses and utilize\nthe predictive entropy as the evaluation metric. The trade-\noff between alignment accuracy and diversity for different\nalgorithms is summarized in Table 1. TDPO2 not only sur-\npasses DPO, f-DPO and TDPO1 in terms of accuracy but\nalso excels in entropy, achieving a superior balance between\nalignment and diversity.\nTo further assess the ability of TDPO1 and TDPO2 to\nalign with human preferences, we evaluated the win rates\nof responses generated by models trained with different\nalgorithms against chosen responses on the test set of the HH\ndataset, the result is illustrated in the Figure 4. Compared to\nthe SFT model, the DPO, TDPO1, and TDPO2 algorithms\nbetter align with human preferences, achieving win rates not\nless than 50% against chosen responses at temperature 0.75.\nThis demonstrates that both TDPO1, and TDPO2 possess\na strong capability to align with human preferences.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSampling temperature\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nWin rate\nSFT\nTDPO1\nDPO\nTDPO2\nf\nDPO(FKL)\nFigure 4. The win rates, computed by GPT-4, in comparison to the\nchosen responses for Anthropic-HH one-step dialogue.\n5.3. Experiments on MT-Bench\nTo comprehensively evaluate TDPO1, and TDPO2 in\nterms of generation quality, we conducted pairwise com-\nparisons on the MT-Bench using models trained on the\nAnthropic HH dataset. Following the official MT-Bench\nimplementation, we sampled responses with a temperature\ncoefficient of 0.7 and constrained the maximum number of\nnewly generated tokens to 512. For the PPO baseline, we\nemployed the trlx framework (Havrilla et al., 2023), utiliz-\ning the proxy reward model Dahoas/gptj-rm-static3 during\ntraining. The result is depicted in the Figure 5. It reveals\nthat TDPO2 achieves a higher win rate compared to other\nalgorithms, indicating its ability to assist LLMs in generat-\ning higher-quality responses. This advantage is attributed\n3https://huggingface.co/Dahoas/\ngptj-rm-static\nto its exceptional ability to regulate KL divergence, facili-\ntating a better balance between performance alignment and\ngeneration diversity.\n0\n20\n40\n60\n80\n100\n120\n140\n160\nTDPO2\nvs TDPO1\nTDPO2\nvs DPO\nTDPO2\nvs PPO\nTDPO2\nvs SFT\n27.1%\n54.8%\n18.1%\n28.7%\n59.1%\n12.2%\n28.8%\n60.4%\n10.8%\n31.3%\n58.7%\n10.0%\nWin\nTie\nLose\nFigure 5. MT-Bench comparison between SFT, PPO, DPO,\nTDPO1 and TDPO2 methods. The win, tie, and lose rates are\nevaluated based on GPT-4.\n6. Conclusion\nIn this work, we introduced Token-level Direct Preference\nOptimization (TDPO), an innovative token-level fine-tuning\napproach for Large Language Models (LLMs) aimed at\naligning more closely with human preferences. By employ-\ning the token-wise optimization with forward KL divergence\nconstraints and converting the Bradley-Terry model into a\ntoken-level preference model, TDPO addresses key chal-\nlenges in divergence efficiency and content diversity, sur-\npassing traditional methods like Direct Preference Optimiza-\ntion (DPO) and PPO-based RLHF in tasks such as controlled\nsentiment generation and single-turn dialogues. This marks\na substantial advancement in LLM training methodologies,\ndemonstrating the potential of token-level optimization to\nenhance the alignment, quality, and diversity of LLM out-\nputs, setting a new direction for AI alignment research and\nthe development of nuanced, human-aligned AI systems.\nRegarding the future prospects of alignment methodologies,\nwe anticipate that iterative refinement approaches and multi-\nturn conversational alignment strategies will significantly\nimprove the alignment of large language models with hu-\nman values. By continuously refining these models, we can\nachieve more precise alignment with complex human pref-\nerences. Moreover, multi-turn conversations enable deeper\nand more nuanced interactions, fostering comprehensive\nattunement to human intentions. These approaches aim to\nenhance the quality and relevance of AI responses, mak-\ning AI systems more harmonized with human values and\nexpectations.\n9\n",
  "10": "Token-level Direct Preference Optimization\nAcknowledgements\nThe research leading to these results received funding from\nNational Key R&D Program of China (2022ZD0116402). In\naddition, it received funding from Science and Technology\nResearch and Development Project of China State Railway\nGroup Corporation Limited (P2022X012).\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\net al. Training a helpful and harmless assistant with rein-\nforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022.\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley,\nH., O’Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\nPrashanth, U. S., Raff, E., et al. Pythia: A suite for ana-\nlyzing large language models across training and scaling.\nIn International Conference on Machine Learning, pp.\n2397–2430. PMLR, 2023.\nBradley, R. A. and Terry, M. E. Rank analysis of incom-\nplete block designs: I. the method of paired comparisons.\nBiometrika, 39(3/4):324–345, 1952.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877–1901, 2020.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y.,\nLundberg, S., et al. Sparks of artificial general intel-\nligence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712, 2023.\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,\nH. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,\nBrockman, G., Ray, A., Puri, R., Krueger, G., Petrov,\nM., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,\nS., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-\nian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D.,\nPlappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A.,\nGuss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,\nJ., Babuschkin, I., Balaji, S., Jain, S., Saunders, W.,\nHesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,\nV., Morikawa, E., Radford, A., Knight, M., Brundage,\nM., Murati, M., Mayer, K., Welinder, P., McGrew, B.,\nAmodei, D., McCandlish, S., Sutskever, I., and Zaremba,\nW. Evaluating large language models trained on code,\n2021.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\net al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023), 2023.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg,\nS., and Amodei, D. Deep reinforcement learning from\nhuman preferences. Advances in neural information pro-\ncessing systems, 30, 2017.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, Y., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nDong, H., Xiong, W., Goyal, D., Pan, R., Diao, S., Zhang,\nJ., Shum, K., and Zhang, T. Raft: Reward ranked fine-\ntuning for generative foundation model alignment. arXiv\npreprint arXiv:2304.06767, 2023.\nGanguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y.,\nKadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse,\nK., et al. Red teaming language models to reduce harms:\nMethods, scaling behaviors, and lessons learned. arXiv\npreprint arXiv:2209.07858, 2022.\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y.,\nCallan, J., and Neubig, G. Pal: Program-aided language\nmodels, 2023.\nGlaese, A., McAleese, N., Tr˛ebacz, M., Aslanides, J., Firoiu,\nV., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M.,\nThacker, P., et al.\nImproving alignment of dialogue\nagents via targeted human judgements. arXiv preprint\narXiv:2209.14375, 2022.\nHavrilla, A., Zhuravinskyi, M., Phung, D., Tiwari, A., Tow,\nJ., Biderman, S., Anthony, Q., and Castricato, L. trlx: A\nframework for large scale reinforcement learning from\nhuman feedback. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing,\npp. 8578–8595, 2023.\nHuang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H.,\nChen, Q., Peng, W., Feng, X., Qin, B., et al. A survey\non hallucination in large language models: Principles,\n10\n",
  "11": "Token-level Direct Preference Optimization\ntaxonomy, challenges, and open questions. arXiv preprint\narXiv:2311.05232, 2023.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., de las Casas, D., Bressand, F., Lengyel,\nG., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-\nA., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,\nT., and Sayed, W. E. Mistral 7b, 2023.\nKhalifa, M., Elsahar, H., and Dymetman, M.\nA distri-\nbutional approach to controlled text generation. arXiv\npreprint arXiv:2012.11635, 2020.\nKnox, W. B., Hatgis-Kessell, S., Booth, S., Niekum, S.,\nStone, P., and Allievi, A.\nModels of human prefer-\nence for learning reward functions.\narXiv preprint\narXiv:2206.02231, 2022.\nKnox, W. B., Hatgis-Kessell, S., Adalgeirsson, S. O., Booth,\nS., Dragan, A., Stone, P., and Niekum, S.\nLearning\noptimal advantage from preferences and mistaking it for\nreward. arXiv preprint arXiv:2310.02456, 2023.\nKoh, H. Y., Ju, J., Liu, M., and Pan, S. An empirical survey\non long document summarization: Datasets, models, and\nmetrics. ACM Computing Surveys, 55(8):1–35, December\n2022. ISSN 1557-7341. doi: 10.1145/3545176. URL\nhttp://dx.doi.org/10.1145/3545176.\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000), pp. 1207–1216, Stan-\nford, CA, 2000. Morgan Kaufmann.\nLiu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M.,\nLiu, P. J., and Liu, J.\nStatistical rejection sam-\npling improves preference optimization. arXiv preprint\narXiv:2309.06657, 2023.\nMaas, A., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and\nPotts, C. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th annual meeting of the asso-\nciation for computational linguistics: Human language\ntechnologies, pp. 142–150, 2011.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730–27744, 2022.\nPerez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides,\nJ., Glaese, A., McAleese, N., and Irving, G. Red team-\ning language models with language models, 2022. URL\nhttps://arxiv. org/abs/2202.03286, 2022.\nPuterman, M. L.\nMarkov decision processes: discrete\nstochastic dynamic programming. John Wiley & Sons,\n2014.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\nC. D., and Finn, C. Direct preference optimization: Your\nlanguage model is secretly a reward model. arXiv preprint\narXiv:2305.18290, 2023.\nRafailov, R., Hejna, J., Park, R., and Finn, C. From r to Q∗:\nYour Language Model is Secretly a Q-Function. arXiv\npreprint arXiv:2404.12358, 2024.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485–5551, 2020.\nRauh, M., Mellor, J., Uesato, J., Huang, P.-S., Welbl, J., Wei-\ndinger, L., Dathathri, S., Glaese, A., Irving, G., Gabriel, I.,\nIsaac, W., and Hendricks, L. A. Characteristics of harm-\nful text: Towards rigorous benchmarking of language\nmodels, 2022.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,\nP. Trust region policy optimization. In International\nconference on machine learning, pp. 1889–1897. PMLR,\n2015.\nSheng, E., Chang, K.-W., Natarajan, P., and Peng, N. So-\ncietal biases in language generation: Progress and chal-\nlenges. arXiv preprint arXiv:2105.04054, 2021.\nSong, F., Yu, B., Li, M., Yu, H., Huang, F., Li, Y., and Wang,\nH. Preference ranking optimization for human alignment.\narXiv preprint arXiv:2306.17492, 2023.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,\nR., Voss, C., Radford, A., Amodei, D., and Christiano, P.\nLearning to summarize from human feedback, 2022.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpaca: A\nstrong, replicable instruction-following model. Stanford\nCenter for Research on Foundation Models. https://crfm.\nstanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023.\nTeam, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu,\nJ., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al.\nGemini: a family of highly capable multimodal models.\narXiv preprint arXiv:2312.11805, 2023.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\n11\n",
  "12": "Token-level Direct Preference Optimization\nTunstall, L., Beeching, E., Lambert, N., Rajani, N., Ra-\nsul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier,\nC., Habib, N., et al. Zephyr: Direct distillation of lm\nalignment. arXiv preprint arXiv:2310.16944, 2023.\nVu, T.-T., He, X., Haffari, G., and Shareghi, E. Koala: An\nindex for quantifying overlaps with pre-training corpora,\n2023.\nWang, C., Jiang, Y., Yang, C., Liu, H., and Chen, Y. Be-\nyond reverse kl: Generalizing direct preference optimiza-\ntion with diverse divergence constraints. arXiv preprint\narXiv:2309.16240, 2023.\nWeidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato,\nJ., Huang, P.-S., Cheng, M., Glaese, M., Balle, B.,\nKasirzadeh, A., et al. Ethical and social risks of harm\nfrom language models. arXiv preprint arXiv:2112.04359,\n2021.\nWiher, G., Meister, C., and Cotterell, R.\nOn decoding\nstrategies for neural text generators. Transactions of the\nAssociation for Computational Linguistics, 10:997–1012,\n2022.\nWorkshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E.,\nIli´c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon,\nF., et al. Bloom: A 176b-parameter open-access multilin-\ngual language model. arXiv preprint arXiv:2211.05100,\n2022.\nWu, Z., Hu, Y., Shi, W., Dziri, N., Suhr, A., Ammanabrolu,\nP., Smith, N. A., Ostendorf, M., and Hajishirzi, H. Fine-\ngrained human feedback gives better rewards for language\nmodel training. arXiv preprint arXiv:2306.01693, 2023.\nYuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S., and\nHuang, F. Rrhf: Rank responses to align language mod-\nels with human feedback without tears. arXiv preprint\narXiv:2304.05302, 2023.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. arXiv\npreprint arXiv:2306.05685, 2023.\nZhong, H., Feng, G., Xiong, W., Zhao, L., He, D., Bian,\nJ., and Wang, L.\nDpo meets ppo: Reinforced token\noptimization for rlhf. arXiv preprint arXiv:2404.18922,\n2024.\n12\n",
  "13": "Token-level Direct Preference Optimization\nA. Mathematical Derivations\nA.1. Proving the Relationship between Maximizing the Advantage Function and Enhancing the Expected Returns\nLemma A.1. Given two policies π and ˜π, if for any state st = [x, y<t], Ez∼˜π [Aπ([x, y<t], z)] ≥0, then we can conclude:\nEx∼D [V˜π([x])] ≥Ex∼D [Vπ([x])] ,\nProof. Let trajectory τ := (x, y1, y2, ...), and the notation Eτ|˜π[·] indicates that actions are sampled from ˜π to generate τ.\nSo we can get\nEx∼D [V˜π([x])] −Ex∼D [Vπ([x])]\n(19)\n=Eτ|˜π\n\" ∞\nX\nt=1\nγt−1Rt −Vπ([x])\n#\n(20)\n=Eτ|˜π\n\" ∞\nX\nt=1\nγt−1 \u0000Rt + γVπ([x, y<t+1]) −Vπ([x, y<t])\n\u0001\n#\n(21)\n=Eτ|˜π\n\" ∞\nX\nt=1\nγt−1Aπ([x, y<t], yt)\n#\n(22)\n=Eτ|˜π\n\" ∞\nX\nt=1\nγt−1Eyt∼˜π\n\u0002\nAπ([x, y<t], yt)\n\u0003\n#\n(23)\nSince for any state st = [x, y<t], Ez∼˜π [Aπ([x, y<t], z)] ≥0, so we can obtain\nEx∼D [V˜π([x])] −Ex∼D [Vπ([x])] ≥0\n(24)\nOur goal is to maximize the expected return of a parameterized policy πθ. According to Eq.23, what we need to do is\nmax\nπθ\nEx,y<t∼D,z∼πθ(·|[x,y<t]) [Aπref([x, y<t], z)]. To prevent the excessive degradation of language models, we introduce a\nreverse KL divergence constraint, forming our objective function:\nmax\nπθ\nEx,y<t∼D,z∼πθ(·|[x,y<t])\n\u0002\nAπref([x, y<t], z) −βDKL\n\u0000πθ(·|[x, y<t])||πref(·|[x, y<t])\n\u0001\u0003\n(25)\nA.2. Deriving the Mapping between the State-Action Function and the Optimal Policy\nLemma A.2. The constrained problem in Eq. 7 has the closed-form solution:\nπ∗\nθ(z|[x, y<t]) =\nπref(z|[x, y<t]) exp\n\u0010\n1\nβ Qπref([x, y<t], z)\n\u0011\nZ([x, y<t]; β)\n,\n(26)\nwhere Z([x, y<t]; β) = Ez∼πref(·|[x,y<t])e\n1\nβ Qπref ([x,y<t],z) is the partition function.\n13\n",
  "14": "Token-level Direct Preference Optimization\nProof.\nmax\nπθ\nEz∼πθ(·|[x,y<t])Aπref([x, y<t], z) −βDKL\n\u0000πθ(·|[x, y<t])∥πref(·|[x, y<t])\n\u0001\n(27)\n= max\nπθ\nEz∼πθ(·|[x,y<t])\n\u0012\u0000Qπref([x, y<t], z) −Vπref([x, y<t])\n\u0001\n+ β log\n\u0000πref(z|[x, y<t])\nπθ(z|[x, y<t])\n\u0001\u0013\n(28)\n= max\nπθ\nβEz∼πθ(·|[x,y<t]) log\n \np(z|[x, y<t])e\n1\nβ Qπref ([x,y<t],z)\nπθ(z|[x, y<t])\n!\n−Vπref([x, y<t])\n(29)\n= max\nπθ\nβEz∼πθ(·|[x,y<t]) log\n \nπref(z|[x, y<t])e\n1\nβ Qπref ([x,y<t],z)\nZ([x, y<t]; β)πθ(z|[x, y<t])\n!\n−Vπref([x, y<t]) + β log Z([x, y<t]; β)\n(30)\n= max\nπθ\n−βDKL\n \nπθ(z|[x, y<t])\n\r\r\r\r\nπref(z|[x, y<t])e\n1\nβ Qπref ([x,y<t],z)\nZ([x, y<t]; β)\n!\n−Vπref([x, y<t]) + β log Z([x, y<t]; β)\n(31)\nwhere Z([x, y<t]; β) is the partition function:\nZ([x, y<t]; β) = Ez∼πref(·|[x,y<t]) exp\n\u0012 1\nβ Qπref([x, y<t], z)\n\u0013\n(32)\nBased on the property of KL divergence, we can derive the relationship between the optimal policy and the state-action\nfunction:\nπ∗\nθ(z|[x, y<t]) =\nπref(z|[x, y<t]) exp\n\u0010\n1\nβ Qπref([x, y<t], z)\n\u0011\nZ([x, y<t]; β)\n(33)\nA.3. Proving the Equivalence of the Bradley-Terry Model and the Regret Preference Model\nLemma A.3. Given a reward function r(x, y), assuming a relationship between token-wise rewards and the reward function\nrepresented by r(x, y) = PT\nt=1 γt−1R([x, y<t], yt), we can establish the equivalence between the Bradley-Terry model\nand the Regret Preference Model in the task of text generation alignment, i.e.,\nPBT(y1 ≻y2|x) = σ\n T1\nX\nt=1\nγt−1Aπ([x, y<t\n1 ], yt\n1) −\nT2\nX\nt=1\nγt−1Aπ([x, y<t\n2 ], yt\n2)\n!\n,\n(34)\nwhere σ(x) = 1/(1 + exp(−x)) is the logistic sigmoid function.\nProof. According to the Bradley-Terry model, we have\nPBT(y1 ≻y2|x) =\nexp(r(x, y1))\nexp(r(x, y1)) + exp(r(x, y2)),\n(35)\nwhere r(x, y) represents the overall reward of the pair (x, y).\nBased on assumption that r(x, y) = PT\nt=1 γt−1R([x, y<t], yt), we can get:\nr(x, y) =\nT\nX\nt=1\nγt−1R([x, y<t], yt)\n(36)\n=\nT\nX\nt=1\nγt−1(R([x, y<t], yt) + γVπ([x, y<t+1]) −γVπ([x, y<t+1]))\n(37)\n= Vπ([x, y<1]) +\nT\nX\nt=1\nγt−1 \u0000R([x, y<t], yt) + γVπ([x, y<t+1]) −Vπ([x, y<t])\n\u0001\n−γT Vπ([x, y<T +1])\n(38)\n14\n",
  "15": "Token-level Direct Preference Optimization\nText generation is analogous to a deterministic contextual bandit, where the transition to the next state is certain given the\ncurrent state and action, i.e., p(st+1 = [x, y<t+1]|st = [x, y<t], at = yt) = 1, so we have:\nQπ([x, y<t], yt) = R([x, y<t], yt) + Vπ([x, y<t+1])\n(39)\nAπ([x, y<t], yt) = Qπ([x, y<t], yt) −Vπ([x, y<t])\n(40)\nNext, note that yT = EOS denotes the end of the text sequence. Therefore,\nVπ([x, y<T +1]) = Eπ\n\" ∞\nX\nk=0\nγkR([x, y<T +1+k], yT +1+k)\n\f\f\f\fst = [x, y<T +1]\n#\n= 0\n(41)\nSubstituting Eq.38 to Eq.41 into the Bradley-Terry model, we obtain\nPBT(y1 ≻y2|x) =\nexp(r(x, y1))\nexp(r(x, y1)) + exp(r(x, y2))\n=σ\n  \nVπ([x, y<1\n1 ]) +\nT1\nX\nt=1\n\u0000γt−1Aπ([x, y<t\n1 ], yt)\n\u0001\n!\n−\n \nVπ([x, y<1\n2 ]) +\nT2\nX\nt=1\n\u0000γt−1Aπ([x, y<t\n2 ], yt\n2)\n\u0001\n!!\n(42)\nAdditionally, note that y<1 = [ ], so we can get\nVπ([x, y<1\n1 ]) = Vπ([x, [ ]]) = Vπ([x, y<1\n2 ])\nTherefore,\nPBT(y1 ≻y2|x) = σ\n  \nVπ([x, y<1\n1 ]) +\nT1\nX\nt=1\n\u0000γt−1Aπ([x, y<t\n1 ], yt\n1)\n\u0001\n!\n−\n \nVπ([x, y<1\n2 ]) +\nT2\nX\nt=1\n\u0000γt−1Aπ([x, y<t\n2 ], yt\n2)\n\u0001\n!!\n(43)\n= σ\n T1\nX\nt=1\n\u0000γt−1Aπ([x, y<t\n1 ], yt\n1)\n\u0001\n−\nT2\nX\nt=1\n\u0000γt−1Aπ([x, y<t\n2 ], yt\n2)\n\u0001\n!\n(44)\nA.4. Deriving the TDPO Objective Under the Bradley-Terry Model\nTheorem A.4. In the KL-constrainted advantage function maximization problem corresponding to Eq.7, the Bradley-Terry\nmodel express the human preference probability in terms of the optimal policy π∗\nθ and reference policy πref:\nP ∗\nBT(y1 ≻y2|x) = σ(u∗(x, y1, y2) −δ∗(x, y1, y2)),\n(45)\nwhere, u(x, y1, y2) refers to the difference in rewards implicitly defined by the language model πθ and the reference model\nπref (Rafailov et al., 2023), represented as\nu(x, y1, y2) = β log πθ(y1 | x)\nπref(y1 | x) −β log πθ(y2 | x)\nπref(y2 | x),\n(46)\nand δ(x, y1, y2) refers to the difference in sequential forward KL divergence between two pairs (x, y1) and (x, y2), weighted\nby β, expressed as\nδ(x, y1, y2) = βDSeqKL (x, y2; πref∥πθ) −βDSeqKL (x, y1; πref∥πθ) .\n(47)\nProof. According to the Lemma 4.2, we have\nπ∗\nθ(z|[x, y<t]) =\nπref(z|[x, y<t]) exp\n\u0010\n1\nβ Qπref([x, y<t], z)\n\u0011\nZ([x, y<t]; β)\n(48)\n15\n",
  "16": "Token-level Direct Preference Optimization\nwhere Z([x, y<t]; β) = Ez∼πref(·|[x,y<t])e\n1\nβ Qπref ([x,y<t],z) is the partition function.\nRearrange Eq.48, we obtain\nQπref([x, y<t], z) = β log π∗\nθ(z|[x, y<t])\nπref(z|[x, y<t]) + β log Z([x, y<t]; β)\n(49)\nFrom Lemma 4.4, We can get\nPBT(y1 ≻y2|x) = σ\n T1\nX\nt=1\n\u0000γt−1Aπ([x, y<t\n1 ], yt\n1)\n\u0001\n−\nT2\nX\nt=1\n\u0000γt−1Aπ([x, y<t\n2 ], yt\n2)\n\u0001\n!\n(50)\nBy leveraging Eq.49, we can derive\nT\nX\nt=1\nγt−1Aπref([x, y<t], yt)\n=\nT\nX\nt=1\nγt−1\u0010\nQπref([x, y<t], yt) −Vπref([x, y<t])\n\u0011\n(51)\n=\nT\nX\nt=1\nγt−1\u0010\nQπref([x, y<t], yt) −Ez∼πref\n\u0002\nQπref([x, y<t], z)\n\u0003 \u0011\n(52)\n=\nT\nX\nt=1\nγt−1\n\u0012\nβ log π∗\nθ(yt|[x, y<t])\nπref(yt|[x, y<t]) + β log Z([x, y<t]; β) −Ez∼πref\n\u0014\nβ log π∗\nθ(z|[x, y<t])\nπref(z|[x, y<t]) + β log Z([x, y<t]; β)\n\u0015\u0013\n(53)\nNote that\nEz∼πref\n\u0002\nβ log Z([x, y<t]; β)\n\u0003\n= β log Z([x, y<t]; β)\n(54)\nTherefore,\nT\nX\nt=1\nγt−1Aπref([x, y<t], yt) = β\nT\nX\nt=1\nγt−1\n\u0012\nlog π∗\nθ(yt|[x, y<t])\nπref(yt|[x, y<t]) −Ez∼πref\n\u0014\nlog π∗\nθ(z|[x, y<t])\nπref(z|[x, y<t])\n\u0015\u0013\n(55)\n= β\nT\nX\nt=1\nγt−1\n\u0012\nlog π∗\nθ(yt|[x, y<t])\nπref(yt|[x, y<t]) + DKL\n\u0000πref(·|[x, y<t])∥π∗\nθ(·|[x, y<t])\n\u0001\u0013\n(56)\n= β\nT\nX\nt=1\nγt−1 log π∗\nθ(yt|[x, y<t])\nπref(yt|[x, y<t]) + β\nT\nX\nt=1\nγt−1DKL\n\u0000πref(·|[x, y<t])∥π∗\nθ(·|[x, y<t])\n\u0001\n(57)\nWhen substituting γ = 1 into the expression, we obtain a more concise form:\nT\nX\nt=1\nAπref([x, y<t], yt) = β\nT\nX\nt=1\nlog π∗\nθ(yt|[x, y<t])\nπref(yt|[x, y<t]) + β\nT\nX\nt=1\nDKL\n\u0000πref(·|[x, y<t])∥π∗\nθ(·|[x, y<t])\n\u0001\n(58)\n= β\n\u0012\nlog π∗\nθ(y|x)\nπref(y|x) + DSeqKL (x, y; πref∥π∗\nθ)\n\u0013\n(59)\nWe let\nu(x, y1, y2) = β log πθ(y1 | x)\nπref(y1 | x) −β log πθ(y2 | x)\nπref(y2 | x),\n(60)\nδ(x, y1, y2) = βDSeqKL (x, y2; πref∥πθ) −βDSeqKL (x, y1; πref∥πθ) .\n(61)\nSubstituting Eq.59 to Eq.61 into Eq.50, we arrive at:\nP ∗\nBT(y1 ≻y2|x) = σ(u∗(x, y1, y2) −δ∗(x, y1, y2)).\n(62)\n16\n",
  "17": "Token-level Direct Preference Optimization\nB. TDPO Implementation Details and Hyperparameters\nPyTorch code for the TDPO loss is provided below:\nimport torch\nimport torch.nn.functional as F\ndef tdpo_loss(pi_logits, ref_logits, yw_idxs, yl_idxs, labels, beta, alpha, if_tdpo2):\n\"\"\"\npi_logits: policy logits. Shape: (batch_size, sequence_length, vocab_size),\nref_logits: reference logits. Shape: (batch_size, sequence_length, vocab_size)\nyw_idxs: preferred completion indices in [0,B-1], shape (T,)\nyl_idxs: dispreferred completion indices in [0,B-1], shape (T,)\nlabels: labels for which to compute the log probabilities, Shape: (batch_size,\nsequence_length)\nbeta: temperature controlling strength of KL penalty\nEach pair of (yw_idxs[i], yl_idxs[i]) represents the indices of a single preference\npair.\nalpha: The weight factor adjusts the influence weight of kl divergence at each token\nif_tdpo2: Use method TDPO2 by default; if False, switch to TDPO1\n\"\"\"\npi_vocab_logps = pi_logits.log_softmax(-1)\nref_vocab_ps = ref_logits.softmax(-1)\nref_vocab_logps = ref_vocab_ps.log()\npi_per_token_logps = torch.gather(pi_vocab_logps, dim=2, index=labels.unsqueeze(2)).\nsqueeze(2)\nref_per_token_logps = torch.gather(ref_vocab_logps, dim=2, index=labels.unsqueeze(2)).\nsqueeze(2)\nper_position_rewards = pi_per_token_logps - ref_per_token_logps\nyw_rewards, yl_rewards = per_position_rewards[yw_idxs], per_position_rewards[yl_idxs]\nrewards = yw_rewards - yl_rewards\n# losses = -F.logsigmoid(beta * rewards)\n# DPO loss function\n# =============================Difference with DPO=================================\nper_position_kl = (ref_vocab_ps * (ref_vocab_logps - pi_vocab_logps)).sum(-1)\nyw_kl, yl_kl = per_position_kl[yw_idxs], per_position_kl[yl_idxs]\nif not if_tdpo2:\nvalues = yw_rewards - yl_rewards - (yl_kl - yw_kl)\nelse:\nvalues = yw_rewards - yl_rewards - alpha * (yl_kl - yw_kl.detach())\nlosses = -F.logsigmoid(beta * values)\n# =================================================================================\nreturn losses\nUnless specified otherwise, we use a α = 0.5, β = 0.1, batch size of 64, and the RMSprop optimizer with a learning rate of\n5e-6. We linearly warm up the learning rate from 0 to 5e-6 over 150 steps.\n17\n",
  "18": "Token-level Direct Preference Optimization\nC. Additional Experimental Results\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nDKL(\nref\n)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nReward\nDPO\nTDPO1\nTDPO2  \n= 0.1\nTDPO2  \n= 0.3\nTDPO2  \n= 0.5\nTDPO2  \n= 0.7\n(a)\n0\n5000\n10000\n15000\n20000\n25000\nstep\n0\n10\n20\n30\n40\n50\nDSeqKL(x, yw;\nref\n)\n(b)\n0\n5000\n10000\n15000\n20000\n25000\nstep\n0\n10\n20\n30\n40\n50\nDSeqKL(x, yl;\nref\n)\n(c)\n0\n5000\n10000\n15000\n20000\n25000\nstep\n0\n5\n10\n15\n20\n25\nmargin\n(d)\nFigure 6. The experiment on IMDb dataset. Figure 6(a) represents the frontier of expected reward and forward KL divergence with respect\nto the reference model. Figure 6(b) and Figure 6(c) present the progression of sequential forward KL divergence on the preferred and\ndispreferred responses subset over training steps respectively. Figure 6(d) illustrates the difference between the sequential forward KL\ndivergence on the dispreferred responses subset and that on the preferred responses subset throughout the training process.\n18\n"
}