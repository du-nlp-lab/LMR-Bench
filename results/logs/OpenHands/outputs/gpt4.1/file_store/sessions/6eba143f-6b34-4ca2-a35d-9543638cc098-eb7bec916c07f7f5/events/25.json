{"id": 25, "timestamp": "2025-05-19T19:42:07.346665", "source": "agent", "message": "Code executed in IPython cell.", "cause": 24, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_zMup9QeKdOc99EtAoJYVW6jV", "model_response": {"id": "chatcmpl-BZ5LO4Iu5ENgBTjzxsaUotJmLCPmg", "created": 1747701722, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_ccc6ec921d", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"# Try to extract the table of contents or section describing the synthetic dataset/task generation\\nfor i in range(15, 25):\\n    page = ''\\n    with open(pdf_path, 'rb') as f:\\n        reader = PyPDF2.PdfReader(f)\\n        page = reader.pages[i].extract_text()\\n    if 'dataset' in page.lower() or 'task' in page.lower() or 'synthetic' in page.lower():\\n        print(page[:2000])\"}", "name": "execute_ipython_cell"}, "id": "call_zMup9QeKdOc99EtAoJYVW6jV", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 121, "prompt_tokens": 8925, "total_tokens": 9046, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 8192}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "D Training Transformers from Scratch: TNP models decision boundaries\nWe trained TNP models of four different sizes as shown in the Table 1 below. We plot how does\nthe TNP models decision boudnary changes as more in-context examples are added in Figure 15.\nTNP models learn smooth deicision boundary for this moon-shaped non-linear task. And we did not\nobserve a scaling law of transformer sizes versus the decision boundary smoothness. In contrast the\nsmaller model generalize better than the larger model.\nTable 1: TNP transformers model sizes and architectures.\nModel Parameters (M) Input embed dim feedforward dim num heads num layers\nSmall 0.1 64 64 2 3\nMedium 0.6 128 128 4 6\nLarge 1.6 128 256 8 12\nX-Large 9.7 256 512 16 18\n20 30 40 50 60 70\nFeature 120406080X-Large8 examples\n20 40 60 80\nFeature 120406080Feature 216 examples\n20 40 60 80\nFeature 120406080Feature 232 examples\n20 40 60 80\nFeature 120406080Feature 264 examples\n20 40 60 80\nFeature 120406080Feature 2128 examples\n20 40 60 80\nFeature 120406080Feature 2256 examples\n20 40 60 80\nFeature 120406080Feature 2512 examples\n20 30 40 50 60 70\nFeature 120406080Large8 examples\n20 40 60 80\nFeature 120406080Feature 216 examples\n20 40 60 80\nFeature 120406080Feature 232 examples\n20 40 60 80\nFeature 120406080Feature 264 examples\n20 40 60 80\nFeature 120406080Feature 2128 examples\n20 40 60 80\nFeature 120406080Feature 2256 examples\n20 40 60 80\nFeature 120406080Feature 2512 examples\n20 30 40 50 60 70\nFeature 120406080Medium8 examples\n20 40 60 80\nFeature 120406080Feature 216 examples\n20 40 60 80\nFeature 120406080Feature 232 examples\n20 40 60 80\nFeature 120406080Feature 264 examples\n20 40 60 80\nFeature 120406080Feature 2128 examples\n20 40 60 80\nFeature 120406080Feature 2256 examples\n20 40 60 80\nFeature 120406080Feature 2512 examples\n20 30 40 50 60 70\nFeature 120406080Small8 examples\n20 40 60 80\nFeature 120406080Feature 216 examples\n20 40 60 80\nFeature 120406080Feature 232 examples\n20 40 60 80\nFeature 120406080Feature 264 examples\n20 4\nF Prompt Format for binary classification\nGiven pairs of numbers and their labels, predict the label for a new\ninput pair of numbers based on the provided data.\nAnswer with only one of the labels \u2018Foo\u2019 and \u2018Bar\u2019:\nInput: 64 24\nLabel: Bar\nInput: 34 41\nLabel: Bar\nInput: 71 66\nLabel: Bar\n...\nInput: 96 49\nLabel: Foo\nInput: 21 56\nLabel: Foo\nWhat is the label for this input?\nInput: 2 3\nLabel:\nFigure 16: Few-shot in-context prompt with ncontext questions.\nG Classification Datasets\nWe use three types of classification tasks from scikit-learn [Pedregosa et al., 2011] to probe the\ndecision boundary of LLMs and transformers: linear, circle, and moon classification problems. For\nlinear classification tasks, we utilize the make_classification function, which generates random\nclassification problems by creating clusters of points normally distributed around the vertices of a\nhypercube with sides of length 2\u00d7class_sep . Circle classification tasks are generated using the\nmake_circles function, creating a binary classification problem with a large circle containing a\nsmaller circle. The factor parameter controls the scale of the inner circle relative to the outer\ncircle. Moon classification tasks are generated using the make_moons function, creating a binary\nclassification problem with two interleaving half circles. The noise parameter controls the standard\ndeviation of Gaussian noise added to the data points.\nFor training tasks, the class_sep parameter is randomly sampled from the range [1.5,2], and the\nfactor parameter for circular tasks is sampled from [0.1,0.4]. For testing tasks, the class_sep\nparameter is sampled from [1,1.4], and the factor parameter from [0.5,0.9], ensuring that testing\ntasks differ from training tasks. The noise parameter for moon-shaped tasks is sampled from\n[0.05,0.1]for training and [0.1,0.2]for testing, introducing varying levels of complexity in the\nclassification problems.\nH Decision Boundary of LLMs on NLP tasks.\nWe extend our analysis to multi-class\nand news topic classification (AG_NEWS) [Zhang et al., 2015]. The results, presented in Figure 17,\ndemonstrate that the non-smooth decision boundary characteristics observed in our synthetic datasets\npersist in these more complex NLP tasks.\nFigure 17: Decision boundaries of Llama-3-8b on six NLP tasks, ranging from binary to multi-class\nclassification. Since text embeddings are natively high-dimensional, we projected text embeddings\nonto a 2D space using t-SNE. The irregular, non-smooth behaviors are also seen in these tasks.\nIUncertainty Aware Active Sampling For Smoother Decision Boundary and\nBetter Test set Accuracy\n20 40 60 80\nFeature 110203040506070Feature 232 examples\n Test Accuracy: 0.88\n20 40 60 80\nFeature 110203040506070Feature 264 examples\n Test Accuracy: 0.81\n20 40 60 80\nFeature 110203040506070Feature 296 examples\n Test Accuracy: 0.80\n20 40 60 80\nFeature 110203040506070Feature 2128 examples\n Test Accuracy: 0.85\n20 40 60 80\nFeature 110203040506070Feature 2160 examples\n Test Accuracy: 0.86\n20 40 60 80\nFeature 110203040506070Feature 2192 examples\n Test Accuracy: 0.85\n20 40 60 80\nFeature 110203040506070Feature 2224 examples\n Test Accuracy: 0.92\n20 40 60 80\nFeature 110203040506070Feature 2256 examples\n Test Accuracy: 0.96\n20 40 60 80 100\nFeature 110203040506070Feature 2Uncertainty\n20 40 60 80 100\nFeature 110203040506070Feature 2Uncertainty\n20 40 60 80 100\nFeature 110203040506070Feature 2Uncertainty\n20 40 60 80 100\nFeature 110203040506070Feature 2Uncertainty\n20 40 60 80 100\nFeature 110203040506070Feature 2Uncertainty\n20 40 60 80 100\nFeature 110203040506070Feature 2Uncertainty\n20 40 60 80 100\nFeature 110203040506070Feature 2Uncertainty\n20 40 60 80 100\nFeature 110203040506070Feature 2Uncertainty\n0.3 0.4 0.5 0.6Class 1 (In-Context Examples) Class 2 (In-Context Examples) Class 1 (Model Prediction) Class 2 (Model Prediction)\n(a) Active sampling\n20 40 60 80\nFeature 110203040506070Feature 232 examples\n Test Accuracy: 0.88\n20 40 60 80\nFeature 110203040506070Feature 264\nNeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper\u2019s contributions and scope?\nAnswer: [Yes]\nJustification: It reflects our proposed method and experimental findings.\nGuidelines:\n\u2022The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\u2022The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n\u2022The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n\u2022It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We provide limitation in the appendix section.\nGuidelines:\n\u2022The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n\u2022The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n\u2022The authors should reflect on the factors that influence the performance of the approach.\nFor ex\nJustification: The paper does not include theoretical studies.\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n\u2022All assumptions should be clearly stated or referenced in the statement of any theorems.\n\u2022The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n\u2022Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\n4.Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We present complete method and experiment details in Section 3 and 4.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n\u2022If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n\u2022Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to \n\u2022The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n\u2022The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n\u2022If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11.Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: NA\nGuidelines:\n\u2022 The answer NA means that the paper poses no such risks.\n\u2022Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n\u2022Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n\u2022We recognize that providing ef\n\u2022If this information is not available online, the authors are encouraged to reach out to\nthe asset\u2019s creators.\n13.New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: We don\u2019t release new dataset.\nGuidelines:\n\u2022 The answer NA means that the paper does not release new assets.\n\u2022Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n\u2022The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n\u2022At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14.Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: we don\u2019t have any.\nGuidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n\u2022According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country o\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "# Try to extract the table of contents or section describing the synthetic dataset/task generation\nfor i in range(15, 25):\n    page = ''\n    with open(pdf_path, 'rb') as f:\n        reader = PyPDF2.PdfReader(f)\n        page = reader.pages[i].extract_text()\n    if 'dataset' in page.lower() or 'task' in page.lower() or 'synthetic' in page.lower():\n        print(page[:2000])"}, "success": true}