{"id": 17, "timestamp": "2025-05-16T17:10:59.559040", "source": "agent", "message": "Code executed in IPython cell.", "cause": 16, "observation": "run_ipython", "tool_call_metadata": {"function_name": "execute_ipython_cell", "tool_call_id": "call_vwBJIXQi4BUNu33CyITp3Obn", "model_response": {"id": "chatcmpl-BXxYX3DMDJOgBYXeGxa42cgbXRaxS", "created": 1747433457, "model": "gpt-4.1-2025-04-14", "object": "chat.completion", "system_fingerprint": "fp_b38e740b47", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": null, "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"code\":\"import PyPDF2\\npdf_path = '/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/paper.pdf'\\nreader = PyPDF2.PdfReader(pdf_path)\\npages = [reader.pages[i].extract_text() for i in range(min(10, len(reader.pages)))]\\nprint('\\\\n---PAGE BREAK---\\\\n'.join(pages))\"}", "name": "execute_ipython_cell"}, "id": "call_vwBJIXQi4BUNu33CyITp3Obn", "type": "function"}], "function_call": null, "annotations": []}}], "usage": {"completion_tokens": 96, "prompt_tokens": 6913, "total_tokens": 7009, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 6144}}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "Token-level Direct Preference Optimization\nYongcheng Zeng1 2Guoqing Liu3Weiyu Ma1 2Ning Yang1Haifeng Zhang1Jun Wang4\nAbstract\nFine-tuning pre-trained Large Language Models\n(LLMs) is essential to align them with human\nvalues and intentions. This process often uti-\nlizes methods like pairwise comparisons and KL\ndivergence against a reference LLM, focusing\non the evaluation of full answers generated by\nthe models. However, the generation of these\nresponses occurs in a token level, following a\nsequential, auto-regressive fashion. In this pa-\nper, we introduce Token-level Direct Preference\nOptimization (TDPO), a novel approach to align\nLLMs with human preferences by optimizing pol-\nicy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency,\nTDPO incorporates forward KL divergence con-\nstraints for each token, improving alignment and\ndiversity. Utilizing the Bradley-Terry model for\na token-based reward system, TDPO enhances\nthe regulation of KL divergence, while preserv-\ning simplicity without the need for explicit re-\nward modeling. Experimental results across vari-\nous text tasks demonstrate TDPO\u2019s superior per-\nformance in balancing alignment with genera-\ntion diversity. Notably, fine-tuning with TDPO\nstrikes a better balance than DPO in the controlled\nsentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of\ngenerated responses compared to both DPO and\nPPO-based RLHF methods. Our code is open-\nsourced at https://github.com/Vance0124/Token-\nlevel-Direct-Preference-Optimization.\n1Institute of Automation, Chinese Academy of Sciences\n2School of Artificial Intelligence, University of Chinese Academy\nof Sciences3Microsoft Research AI4Science4University College\nLondon. Correspondence to: Jun Wang <jun.wang@cs.ucl.ac.uk>,\nHaifeng Zhang <haifeng.zhang@ia.ac.cn>.\nProceedings of the 41stInternational Conference on Machine\nLearning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).1. Introduction\nLarge language models (LLMs) (Achiam et al., 2023;\nBubeck et al., 2023) have demonstrated significant gen-\neralization capabilities in various domains including text\nsummarization (Stiennon et al., 2022; Koh et al., 2022), cod-\ning writing (Chen et al., 2021; Gao et al., 2023), and even\nfollowing human instructions (Chung et al., 2022; Ouyang\net al., 2022). In order to align LLMs with human intentions,\nReinforcement Learning from Human Feedback (RLHF)\n(Christiano et al., 2017; Ouyang et al., 2022; Dong et al.,\n2023; Yuan et al., 2023; Liu et al., 2023) has emerged as a\nhighly effective method, embodying both stylistic and eth-\nical values (Bai et al., 2022; Ganguli et al., 2022). These\napproaches typically involve the training of a reward model\nfollowed by the fine-tuning of the policy model using rein-\nforcement learning (RL).\nDirect Preference Optimization (DPO) (Rafailov et al.,\n2023) introduces a straightforward and effective technique\nfor training LLMs using pairwise comparisons, without\nthe need for explicitly establishing a reward model. DPO\nutilizes KL divergence to ensure that the training process\nremains closely aligned with a reference Large Language\nModel (LLM), preventing significant deviations. In DPO,\nKL divergence is assessed at the sentence level, reflecting\nthe fact that evaluations are based on complete responses\n(answers), typically comprising several sentences. How-\never, the generation of these responses occurs sequentially,\nfollowing an auto-regressive approach. A potential benefit\nis to examine divergence in relation to a reference LLM\non a more granular, token-by-token basis. One approach\ninvolves using sequential KL divergence (as defined in Defi-\nnition 4.3), which monitors the trajectory of the generated\nresponses. As illustrated in Figure 1, DPO demonstrates\na significantly faster increase in KL divergence within the\nsubset of less preferred responses when compared to the\nsubset that is preferred. This results in an expanding gap\nbetween the two subsets and also indicates that DPO does\nnot effectively control the KL divergence of the dispreferred\nresponse subset. This impacts the model\u2019s divergence effi-\nciency and ultimately affects its linguistic capabilities and\ngenerative diversity. Such a limitation highlights the de-\ncreased effectiveness of employing KL divergence within\nthe DPO framework, suggesting an area for improvement in\nits methodology.\n1arXiv:2404.11999v5  [cs.CL]  30 Aug 2024\n---PAGE BREAK---\nToken-level Direct Preference Optimization\n/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013\n/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013DSeqKL(x,yw; ref)\n(a)\n/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013\n/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013DSeqKL(x,yl; ref)\n (b)\n/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013\n/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000050/uni00000044/uni00000055/uni0000004a/uni0000004c/uni00000051\nDPO (c)\nFigure 1. Sequential KL (SeqKL) divergence of both preferred response and dispreferred responses on IMDb dataset. Figure 1(a)\nshows the progression of SeqKL divergence on the preferred responses over training steps. Figure 1(b) depicts the evolution of SeqKL\ndivergence on the dispreferred responses over the training steps. Figure 1(c) illustrates the difference between the SeqKL divergence of\nthe dispreferred responses and that of the preferred responses during the training process, namely margin =|DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8)\u2212\nDSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)|. The definition of SeqKL divergence refers to Definition 4.3.\nThe imbalance in the growth rates of the sequential KL di-\nvergence is potentially related to the reverse KL divergence\nconstraint employed by DPO. The mode-seeking property\nof reverse KL divergence tends to induce diversity reduction\nduring generation, limiting the model\u2019s potential to produce\ndiverse and effective responses (Wiher et al., 2022; Khalifa\net al., 2020; Glaese et al., 2022; Perez et al., 2022). Built\nupon DPO, the f-DPO method (Wang et al., 2023) studies\nthe trade-off between alignment performance and generation\ndiversity of LLMs under different divergence constraints. It\nhighlights the advantages of the mass-covering behavior of\nforward KL divergence in enhancing model diversity and\nexplores the impact of different divergence constraints. Nev-\nertheless, f-DPO only independently discusses the changes\nin model behavior under either the reverse KL divergence\nor the forward KL divergence constraints. Essentially, it\ndoes not fundamentally enhance the DPO algorithm itself\nbut rather strikes a balance between alignment performance\nand generating diversity by simply swapping different KL\ndivergence constraints.\nInspired by the aforementioned observations, we define\nand examine the problem of aligning with human prefer-\nences from a sequential and token-level standpoint. Some\nconcurrent work has also been conducted in this direction\n(Rafailov et al., 2024; Zhong et al., 2024). We introduce a\nnew method, referred to as Token-level Direct Preference\nOptimization (TDPO), which aims to strike a better balance\nbetween alignment performance and generation diversity\nby controlling the KL divergence for each token. In order\nto achieve this, we redefine the objective of maximising\nrestricted rewards in a sequential manner. The connection\nbetween sentence-level reward and token-level generation\nis established by the use of the Bellman equation. After-\nwards, the Bradley-Terry model (Bradley & Terry, 1952) isconverted into a representation at the token level, demon-\nstrating its close relationship with the Regret Preference\nModel (Knox et al., 2022; 2023). By utilizing this method,\nwe effectively integrate forward KL divergence restrictions\nfor each token in the final objective function, resulting in\nimproved regulation of KL divergence.\nTDPO maintains the simplicity of DPO while offering im-\nproved regulation of KL divergence for aligning LLMs\nwith human preferences. Echoing the strategy of DPO, our\nmethod directly optimizes the policy without necessitating\nexplicit reward model learning or policy sampling through-\nout the training phase. Our experimental results demonstrate\nthe effectiveness of TDPO across multiple text tasks, and\ngain a notable enhancement in the quality of generated re-\nsponses in comparison to both DPO and PPO-based RLHF\nmethods. In conclusion, TDPO stands out for its ability\nto not only effectively address the issue of excessive KL\ndivergence but also greatly improve divergence efficiency.\n2. Related Works\nThe emergence of ChatGPT has catalyzed significant ad-\nvancements in the field of Large Language Models (LLMs),\nsuch as OpenAI\u2019s GPT-4 (Achiam et al., 2023), Mistral\n(Jiang et al., 2023), and Google\u2019s Gemini (Team et al., 2023).\nGenerally, the training of LLMs involves three stages: ini-\ntial unsupervised pre-training on massive text corpora to\ngrasp linguistic structures (Raffel et al., 2020; Brown et al.,\n2020; Workshop et al., 2022; Touvron et al., 2023), fol-\nlowed by supervised fine-tuning with task-specific datasets\nto enhance the LLMs\u2019 probability of producing desired re-\nsponses (Taori et al., 2023; Chiang et al., 2023; Vu et al.,\n2023). However, due to the typically limited and expen-\nsive availability of labeled datasets during the supervised\n2\n---PAGE BREAK---\nToken-level Direct Preference Optimization\nfine-tuning stage, the model may retain biases and inaccu-\nracies, manifesting as societal biases (Sheng et al., 2021),\nethical concerns (Weidinger et al., 2021), toxicity (Rauh\net al., 2022), and hallucinations (Huang et al., 2023), which\nnecessitates a subsequent AI alignment phase. Notewor-\nthy models achieving significant alignment, such as Zephyr\n(Tunstall et al., 2023) and GPT-4 (Achiam et al., 2023), have\ndemonstrated the effectiveness of techniques like Reinforce-\nment Learning from Human Feedback (RLHF) and Direct\nPreference Optimization (DPO) algorithms.\nReinforcement Learning from Human Feedback (RLHF)\nhas emerged as a cornerstone in aligning LLMs with hu-\nman values, providing a mechanism to refine model out-\nputs based on qualitative feedback (Christiano et al., 2017;\nOuyang et al., 2022; Bai et al., 2022; Song et al., 2023;\nTouvron et al., 2023). This approach has shown consider-\nable promise in making models more responsive to human\nexpectations and ethical considerations by iteratively im-\nproving their performance through human-generated feed-\nback. However, the complexity of implementing RLHF,\ncompounded by the inaccuracies in human-generated re-\nward models (Wu et al., 2023), has prompted the exploration\nof alternative strategies. Methods like Reward Ranked Fine-\nTuning (RAFT) (Dong et al., 2023) and Rank Responses to\nalign Human Feedback (RRHF) (Yuan et al., 2023) offer\nstreamlined approaches to alignment, circumventing some\nof RLHF\u2019s inherent challenges. Particularly, Direct Prefer-\nence Optimization (DPO) (Rafailov et al., 2023) represents\na breakthrough in direct policy optimization, addressing the\nintricacies of balancing model behavior through a nuanced\napproach to reward function optimization. Nevertheless, the\nchallenge of maintaining linguistic diversity while aligning\nwith human preferences remains a pivotal concern, prompt-\ning our proposed Token-level Direct Preference Optimiza-\ntion (TDPO), which seeks to harmonize the dual objectives\nof alignment accuracy and expressive range in model out-\nputs.\n3. Preliminaries\nFor language generation, a language model (LM) is\nprompted with prompt (question) xto generate a response\n(answer) y, where both xandyconsist of a sequence of\ntokens. Direct Preference Optimization (DPO) (Rafailov\net al., 2023) commences with the RL objective from the\nRLHF:\nmax\n\u03c0\u03b8Ex\u223cD,y\u223c\u03c0\u03b8(\u00b7|x)\u0002\nr(x, y)\n\u2212\u03b2DKL\u0000\n\u03c0\u03b8(\u00b7 |x)\r\r\u03c0ref(\u00b7 |x)\u0001\u0003\n,(1)\nwhere Drepresents the human preference dataset, r(x, y)\ndenotes the reward function, \u03c0ref(\u00b7|x)serves as a reference\nmodel, typically chosen the language model after supervised\nfine-tuning, \u03c0\u03b8represents the model undergoing RL fine-tuning, initialized with \u03c0\u03b8=\u03c0ref, and \u03b2is the coefficient\nfor the reverse KL divergence penalty.\nBy directly deriving from Eq. 1, DPO establishes a mapping\nbetween the reward model and the optimal policy under the\nreverse KL divergence, obtaining a representation of the\nreward function concerning the policy:\nr(x, y) =\u03b2log\u03c0\u03b8(y|x)\n\u03c0ref(y|x)+\u03b2logZ(x). (2)\nHere, Z(x)is the partition function.\nTo align with human preference, DPO uses the Bradley-\nTerry model for pairwise comparisons:\nPBT(y1\u227by2|x) =exp(r(x, y1))\nexp(r(x, y1)) + exp( r(x, y2)).(3)\nBy substituting Eq. 2 into Eq. 3 and leveraging the negative\nlog-likelihood loss, DPO derives the objective function:\nu(x, yw, yl) =\u03b2log\u03c0\u03b8(yw|x)\n\u03c0ref(yw|x)\u2212\u03b2log\u03c0\u03b8(yl|x)\n\u03c0ref(yl|x),\nLDPO(\u03c0\u03b8;\u03c0ref) =\u2212E(x,yw,yl)\u223cD[log\u03c3(u(x, yw, yl))],\n(4)\nand the derivative is given as follows:\n\u2207\u03b8LDPO(\u03c0\u03b8;\u03c0ref) =\u2212E(x,yw,yl)\u223cD[\u03c3(\u2212u)\u2207\u03b8u],(5)\nwhere uis the abbreviation of u(x, yw, yl),ywandylde-\nnotes the preferred and dispreferred completion.\n4. Methodology\nIn this section, we initially reformulate the constrained re-\nward maximization problem into a token-level form. From\nthis, we derive the mapping between the state-action func-\ntion and the optimal policy. Subsequently, we convert the\nBradley-Terry model into token-level representation, estab-\nlishing its equivalence with the Regret Preference Model.\nBy substituting the mapping relationship into the reward\nmodel in token-level format, we obtain the optimization\nobjective solely related to the policy. Finally, we conduct a\nformalized analysis of this optimization objective in terms\nof derivatives and, based on this, derive the ultimate loss\nfunction for TDPO.\n4.1. Markov Decision Process under Token Rewards\nTo model the sequential, auto-regressive generation, we ex-\ntend the sentence-level formulation in Section 3 by consid-\nering that the response consists of Ttokens y=y<T+1:=\n[y1, y2, ..., yT], where yt\u2208 Y, andYrepresents the alpha-\nbet (vocabulary). Additionally, we assume y<1= [ ]. Given\na prompt xand the first t\u22121tokens y<tof the response\n3\n---PAGE BREAK---\nToken-level Direct Preference Optimization\ny, the LM predicts the probability distribution of the next\ntoken \u03c0\u03b8(\u00b7|[x, y<t]).\nWhen modeling text generation as a Markov decision pro-\ncess (Puterman, 2014), a state is a combination of the prompt\nand the generated response up to the current step, denoted as\nst= [x, y<t]. An action corresponds to the next generated\ntoken, denoted as at=yt, and the token-wise reward is\ndefined as Rt:=R(st, at) =R([x, y<t], yt).\nExpanding on the provided definitions, we establish the\nstate-action function Q\u03c0, the state value function V\u03c0and\nthe advantage function A\u03c0for a policy \u03c0:\nQ\u03c0([x, y<t], yt) =E\u03c0\"\u221eX\nk=0\u03b3kRt+k\f\f\f\fst= [x, y<t], at=yt#\n,\nV\u03c0([x, y<t]) =E\u03c0\u0002\nQ\u03c0([x, y<t], yt)\f\fst= [x, y<t]\u0003\n,\nA\u03c0([x, y<t], yt) =Q\u03c0([x, y<t], yt)\u2212V\u03c0([x, y<t]).\n(6)\nwhere \u03b3represents the discount factor. In this paper, we set\n\u03b3= 1.\n4.2. Token-Level Optimization\nDPO\u2019s objective function in Eq. 1 operates at the sentence\nlevel. In contrast, we propose an alternative token-level\nobjective function:\nmax\n\u03c0\u03b8Ex,y<t\u223cD,z\u223c\u03c0\u03b8(\u00b7|[x,y<t])\u0002\nA\u03c0ref([x, y<t], z)\n\u2212\u03b2DKL\u0000\n\u03c0\u03b8(\u00b7|[x, y<t])||\u03c0ref(\u00b7|[x, y<t])\u0001\u0003\n.(7)\nThe objective function is inspired by Trust Region Policy\nOptimization (TRPO) (Schulman et al., 2015). As demon-\nstrated in Lemma 4.1, maximizing the objective function in\nEq. 7 will result in policy improvements in terms of expected\nreturn.\nLemma 4.1. Given two policies \u03c0and\u02dc\u03c0, if for any state\nst= [x, y<t],Ez\u223c\u02dc\u03c0[A\u03c0([x, y<t], z)]\u22650, then we can\nconclude:\nEx\u223cD[V\u02dc\u03c0([x])]\u2265Ex\u223cD[V\u03c0([x])],\nThe proof is provided in Appendix A.1.\nNotably, to maintain generation diversity and prevent the\nmodel from hacking some high-reward answers, we incorpo-\nrate reverse KL divergence for each token in our token-level\nobjective function, which prevents the model from deviating\ntoo far from the reference model distribution.\nStarting from the token-level objective function in Eq. 7,\nwe can directly derive the mapping between the state-action\nfunction Q\u03c0and the optimal policy \u03c0\u2217\n\u03b8. We summarize this\nrelationship in the following lemma.Lemma 4.2. The constrained problem in Eq. 7 has the\nclosed-form solution:\n\u03c0\u2217\n\u03b8(z|[x, y<t]) =\n\u03c0ref(z|[x, y<t]) exp\u0010\n1\n\u03b2Q\u03c0ref([x, y<t], z)\u0011\nZ([x, y<t];\u03b2),(8)\nwhere Z([x, y<t];\u03b2) =Ez\u223c\u03c0ref(\u00b7|[x,y<t])e1\n\u03b2Q\u03c0ref([x,y<t],z)\nis the partition function.\nSee Appendix A.2 for more details.\nTo obtain the optimal policy \u03c0\u2217\n\u03b8from Eq. 8, we must esti-\nmate the state-action function Q\u03c0refand the partition func-\ntionZ(\u00b7). However, ensuring the accuracy of the state-\naction function Q\u03c0at each state and action is challenging,\nand estimating the partition function Z(\u00b7)is also difficult.\nTherefore, we reorganize Eq. 8 to obtain the expression of\nthe state-value function in terms of the policy:\nQ\u03c0ref([x, y<t], z) =\n\u03b2log\u03c0\u2217\n\u03b8(z|[x, y<t])\n\u03c0ref(z|[x, y<t])+\u03b2logZ([x, y<t];\u03b2).(9)\n4.3. BT Model Reformulation via Advantage Function\nTo facilitate subsequent derivations, we first introduce the\nsequential KL divergence, as defined in Definition 4.3.\nDefinition 4.3. Given two language models \u03c01and\u03c02, with\nthe input prompt xand output response y, the sequential KL\ndivergence is defined as:\nDSeqKL (x,y;\u03c01\u2225\u03c02) =\nTX\nt=1DKL(\u03c01(\u00b7|[x, y<t])\u2225\u03c02(\u00b7|[x, y<t])).(10)\nGiven prompts xand pairwise responses ( y1,y2), the\nBradley-Terry model expresses the human preference prob-\nability. However, since the Bradley-Terry model is formu-\nlated at the sentence level, it cannot establish a connection\nwith the token-level mapping presented in Eq. 9. Conse-\nquently, we need to derive a token-level preference model.\nInitiating from the Bradley-Terry model, we transform it\ninto a token-level formulation and demonstrate its equiva-\nlence with the Regret Preference Model (Knox et al., 2023;\n2022), as shown in the Lemma 4.4.\nLemma 4.4. Given a reward function r(x,y), as-\nsuming a relationship between token-wise rewards\nand the reward function represented by r(x, y) =PT\nt=1\u03b3t\u22121R([x, y<t], yt), we can establish the equiva-\nlence between the Bradley-Terry model and the Regret Pref-\n4\n---PAGE BREAK---\nToken-level Direct Preference Optimization\nerence Model in the task of text generation alignment, i.e.,\nPBT(y1\u227by2|x) =\n\u03c3 T1X\nt=1\u03b3t\u22121A\u03c0([x, y<t\n1], yt\n1)\u2212T2X\nt=1\u03b3t\u22121A\u03c0([x, y<t\n2], yt\n2)!\n,\n(11)\nwhere \u03c3(x) = 1 /(1 + exp(\u2212x))is the logistic sigmoid\nfunction.\nWe prove this lemma in A.3.\nIn Lemma 4.4, we assume that r(x, y) =PT\nt=1\u03b3t\u22121R([x, y<t], yt). This assumption is natu-\nral in the context of RL, where r(x, y)represents the overall\nreward for response ygiven the prompt x. Considering text\ngeneration as a sequential decision-making problem, r(x, y)\ncan be viewed as the cumulative reward for the generated\ntext.\nAccording to the definition of the advantage function in Sec-\ntion 4.1, we can directly establish the relationship between\nthe optimal solution in Eq. 9 and preference optimization\nobjective in Eq. 11. One intractable aspect is that the state-\naction function Q\u03c0depends on a partition function, which\nis contingent on both the input prompt xand the output\nresponse y. This results in non-identical values of the par-\ntition function for a pair of responses ( yw,yl), specifically,\nZ([x, y<t\nw];\u03b2)\u0338=Z([x, y<t\nl];\u03b2). As a result, we cannot em-\nploy a cancellation strategy similar to DPO, which relies on\nthe property that the Bradley-Terry model depends only on\nthe difference in rewards between two completions.\nFortunately, by expanding the advantage function A\u03c0and\nconverting the state-value function V\u03c0into a form exclu-\nsively related to the state-action function Q\u03c0, we can offset\nthe partition function naturally. In this way, we ultimately\nreformulate the Bradley-Terry model to be directly tied to\nthe optimal policy \u03c0\u2217\n\u03b8and the reference policy \u03c0ref. This is\nsummarized in the following theorem.\nTheorem 4.5. In the KL-constrainted advantage function\nmaximization problem corresponding to Eq.7, the Bradley-\nTerry model express the human preference probability in\nterms of the optimal policy \u03c0\u2217\n\u03b8and reference policy \u03c0ref:\nP\u2217\nBT(y1\u227by2|x) =\u03c3(u\u2217(x, y1, y2)\u2212\u03b4\u2217(x, y1, y2)),(12)\nwhere, u(x, y1, y2)refers to the difference in rewards im-\nplicitly defined by the language model \u03c0\u03b8and the reference\nmodel \u03c0ref(Rafailov et al., 2023), represented as\nu(x, y1, y2) =\u03b2log\u03c0\u03b8(y1|x)\n\u03c0ref(y1|x)\u2212\u03b2log\u03c0\u03b8(y2|x)\n\u03c0ref(y2|x),(13)\nand\u03b4(x, y1, y2)refers to the difference in sequential for-\nward KL divergence between two pairs (x, y1)and(x, y2),\nweighted by \u03b2, expressed as\n\u03b4(x, y1, y2) =\u03b2DSeqKL (x, y2;\u03c0ref\u2225\u03c0\u03b8)\n\u2212\u03b2DSeqKL (x, y1;\u03c0ref\u2225\u03c0\u03b8).(14)The proof is provided in the Appendix A.4.\n4.4. Loss Function and Formal Analysis\nDrawing on Eq. 12, we reformulate the Bradley-Terry model\ninto a structure solely relevant to the policy. This allows\nus to formulate a likelihood maximization objective for a\nparametrized policy \u03c0\u03b8, leading to the derivation of the loss\nfunction for the initial version of our method, TDPO 1:\nLTDPO 1(\u03c0\u03b8;\u03c0ref) =\n\u2212E(x,yw,yl)\u223cD[log\u03c3(u(x, yw, yl)\u2212\u03b4(x, yw, yl))].\n(15)\nThrough this approach, we explicitly introduce sequential\nforward KL divergence into the loss function. Coupled with\nthe implicitly integrated reverse KL divergence, we enhance\nour ability to balance alignment performance and generation\ndiversity of LLMs.\nSubsequently, we conduct a derivative analysis of our\nmethod and make specific modifications to the loss function\nof TDPO. For convenience, we use uto denote u(x, yw, yl),\nand\u03b4to represent \u03b4(x, yw, yl). By employing the formula-\ntion of the loss function presented in Eq.15, we compute the\ngradient of the loss function with respect to the parameters\n\u03b8:\n\u2207\u03b8LTDPO 1(\u03c0\u03b8;\u03c0ref) =\n\u2212E(x,yw,yl)\u223cD[\u03c3(\u2212u+\u03b4) [\u2207\u03b8u\u2212 \u2207 \u03b8\u03b4]].\n(16)\nIn Eq. 16, \u03c3(\u2212u+\u03b4)serves as the weighting factor for\nthe gradient. The first part (\u2212u)corresponds to the weight\nfactor in the loss function of DPO. When the language\nmodel makes errors in predicting human preferences, i.e.,\nlog\u03c0\u03b8(yl|x)\n\u03c0ref(yl|x)>log\u03c0\u03b8(yw|x)\n\u03c0ref(yw|x), the value of (\u2212u)will be-\ncome larger, applying a stronger update for the comparison\n(yw, yl). While the second part \u03b4is a distinctive component\nof our method. As shown in Figure 1, the KL divergence\ngrowth rate for the dispreferred response subset is faster\nthan that for the preferred response subset. With the increas-\ning disparity, the corresponding value of \u03b4rises, thereby\namplifying the weight factor \u03c3(\u2212u+\u03b4). Combined with\nthe subsequent gradient term, our objective function can ef-\nfectively suppress the difference in KL divergence between\npairs of responses with large disparities in KL divergence.\nThrough the collaborative influence of the weight factor \u03b4\nand the gradient term (\u2212\u2207\u03b8\u03b4), our method achieves the pur-\npose of automatic control over the KL divergence balance.\nThe gradient of the loss function in Eq. 16 also consists\nof two components, \u2207\u03b8uand(\u2212\u2207\u03b8\u03b4).\u2207\u03b8urepresents\nthe optimization direction of the gradient in DPO. Intu-\nitively, \u2207\u03b8uincreases the likelihood of preferred comple-\ntions ywand decreases the likelihood of dispreferred com-\n5\n---PAGE BREAK---\nToken-level Direct Preference Optimization\npletions yl. While (\u2212\u2207\u03b8\u03b4)tends to narrow the gap between\nDSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8)andDSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8).\nHowever, when considered separately, the gradient of\nDSeqKL (x, yw;\u03c0ref|\u03c0\u03b8)in the loss function tends to in-\ncrease the sequential KL divergence between \u03c0refand\u03c0\u03b8\nat(x, yw)during the optimization process. This is because\nthe sequential forward KL divergence in the loss function is\nintroduced through the state-value function V\u03c0, inherently\nintroducing an expectation Ez\u223c\u03c0refh\nlog\u03c0\u03b8(z|[x,y<t])\n\u03c0ref(z|[x,y<t])i\nas a\nbaseline at each token. The negative value of this expec-\ntation corresponds precisely to a forward KL divergence\nDKL(\u03c0ref(\u00b7|[x, y<t])|\u03c0\u03b8(\u00b7|[x, y<t])), which can be used to\nconstrain the unbalanced growth of KL divergence. For\nthe prompt xand the preferred response yw, at each token,\nthe loss function in Eq. 16 tends to increase the likelihood\noflog\u03c0(yt\nw|[x,y<t\nw])\n\u03c0ref(ytw|[x,y<t\nw])while simultaneously decreasing the\nexpectation, enlarging the gap between the specified term\nyt\nwand the baseline to expedite training. The impact of\ndecreasing the expectation is an increase in the forward\nKL divergence DKL(\u03c0ref(\u00b7|[x, y<t\nw])|\u03c0\u03b8(\u00b7|[x, y<t\nw]))at each\ntoken, leading to an increase in DSeqKL (x, yw;\u03c0ref|\u03c0\u03b8).\nAs we do not aim to accelerate the training speed and\nprefer to ensure training stability, we modify the loss\nfunction by discontinuing the gradient propagation of\nDSeqKL (x, yw;\u03c0ref|\u03c0\u03b8)and treating it as a baseline term\nfor alignment of DSeqKL (x, yl;\u03c0ref|\u03c0\u03b8).\nDifferent from DSeqKL (x, yw;\u03c0ref|\u03c0\u03b8), the gradient of\nDSeqKL (x, yl;\u03c0ref|\u03c0\u03b8)tends to reduce the sequential\nKL divergence between \u03c0refand\u03c0\u03b8at(x, yl). For\nthe prompt xand the rejected response yl, the loss\nfunction in Eq.16 tends to decrease the likelihood of\nlog\u03c0(yt\nl|[x,y<t\nl])\n\u03c0ref(yt\nl|[x,y<t\nl])at each token while increasing the ex-\npectation Ez\u223c\u03c0refh\nlog\u03c0\u03b8(z|[x,y<t\nl])\n\u03c0ref(z|[x,y<t\nl])i\n. The increase in the\nexpectation implies a smaller forward KL divergence at that\ntoken, thereby acting to constrain the growth rate of sequen-\ntial forward KL divergence. Therefore, for this term, we\nchoose to retain its gradient updates.\nIn conclusion, we only propagate the gradient of the\nDSeqKL (x, yl;\u03c0ref|\u03c0\u03b8)in(\u2212\u2207\u03b8\u03b4). When the second part\nweight factor \u03b4becomes larger, it imposes a stronger sup-\npression on DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)to control the balance\nof KL divergence.\nFurthermore, to achieve a better balance between align-\nment performance and generation diversity in TDPO, we\nintroduce an additional parameter \u03b1into the loss func-\ntion. By adjusting the magnitude of \u03b1, we can con-\ntrol the deviation between DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8)and\nDSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8).\nIn summary, we modify the loss function of TDPO 1, re-\nsulting in the second version of our method, TDPO 2, asfollows:\nLTDPO 2(\u03c0\u03b8;\u03c0ref) =\n\u2212E(x,yw,yl)\u223cD[log\u03c3(u(x, yw, yl)\u2212\u03b1\u03b42(x, yw, yl))],\n(17)\nwhere \u03b1is a parameter, and\n\u03b42(x, y1, y2) =\u03b2DSeqKL (x, y2;\u03c0ref\u2225\u03c0\u03b8)\n\u2212sg(\u03b2DSeqKL (x, y1;\u03c0ref\u2225\u03c0\u03b8)).\n(18)\nThesgrepresents the stop-gradient operator, which blocks\nthe propagation of gradients.\nWe summarize the comparison of the loss functions for\nDPO ,TDPO 1, and TDPO 2, as presented in Figure 2.\n\u2212\ud835\udefc\ud835\udefd\ud835\udc37SeqKL(\ud835\udc65,\ud835\udc66\ud835\udc59;\ud835\udf0bref||\ud835\udf0b\ud835\udf03)\u2212\ud835\udc60\ud835\udc54\ud835\udefd\ud835\udc37SeqKL\ud835\udc65,\ud835\udc66\ud835\udc64;\ud835\udf0bref|\ud835\udf0b\ud835\udf03|\ud835\udc65)\n|\ud835\udc65)\u2212\ud835\udefd\ud835\udc37SeqKL(\ud835\udc65,\ud835\udc66\ud835\udc59;\ud835\udf0bref||\ud835\udf0b\ud835\udf03)\u2212\ud835\udefd\ud835\udc37SeqKL(\ud835\udc65,\ud835\udc66\ud835\udc64;\ud835\udf0bref||\ud835\udf0b\ud835\udf03)\ud835\udcdb\ud835\udc03\ud835\udc0f\ud835\udc0e\ud835\udf0b\ud835\udf03;\ud835\udf0bref=\u2212\ud835\udd3clog\ud835\udf0e\ud835\udefdlog\ud835\udf0b\ud835\udf03(\ud835\udc66\ud835\udc64|\ud835\udc65)\n\ud835\udf0bref(\ud835\udc66\ud835\udc64|\ud835\udc65)\u2212\ud835\udefdlog\ud835\udf0b\ud835\udf03(\ud835\udc66\ud835\udc59|\ud835\udc65)\n\ud835\udf0bref(\ud835\udc66\ud835\udc59|\ud835\udc65)\n\ud835\udcdb\ud835\udc13\ud835\udc03\ud835\udc0f\ud835\udc0e\ud835\udfd0\ud835\udf0b\ud835\udf03;\ud835\udf0bref=\u2212\ud835\udd3clog\ud835\udf0e\ud835\udefdlog\ud835\udf0b\ud835\udf03(\ud835\udc66\ud835\udc64|\ud835\udc65)\n\ud835\udf0bref(\ud835\udc66\ud835\udc64|\ud835\udc65)\u2212\ud835\udefdlog\ud835\udf0b\ud835\udf03(\ud835\udc66\ud835\udc59|\ud835\udc65)\n\ud835\udf0bref(\ud835\udc66\ud835\udc59|\ud835\udc65)\ud835\udcdb\ud835\udc13\ud835\udc03\ud835\udc0f\ud835\udc0e\ud835\udfcf\ud835\udf0b\ud835\udf03;\ud835\udf0bref=\u2212\ud835\udd3clog\ud835\udf0e\ud835\udefdlog\ud835\udf0b\ud835\udf03(\ud835\udc66\ud835\udc64|\ud835\udc65)\n\ud835\udf0bref(\ud835\udc66\ud835\udc64|\ud835\udc65)\u2212\ud835\udefdlog\ud835\udf0b\ud835\udf03(\ud835\udc66\ud835\udc59|\ud835\udc65)\n\ud835\udf0bref(\ud835\udc66\ud835\udc59|\ud835\udc65)\nFigure 2. Comparison of Loss Functions for DPO ,TDPO 1and\nTDPO 2Methods. The sgdenotes the stop-gradient operator.\nBoth TDPO 1andTDPO 2incorporate an additional term for\nfiner-grained control over the KL divergence, compared to DPO.\nLeveraging the parameter \u03b2to regulate the deviation of the\nlanguage model from the base reference model, and \u03b1to\ncontrol the balance of sequential KL divergence within the\nlanguage model, our approach achieves superior alignment\nwith human preferences while preserving model generation\ndiversity effectively. We provided the pseudocode in Algo-\nrithm 1 and the Pytorch implementation version of TDPO\nloss in Appendix B.\n5. Experiments\nIn this section, we demonstrate the superior performance\nof our algorithm in three different open-sourced datasets:\nthe IMDb sentiment dataset (Maas et al., 2011), the An-\nthropic HH dataset (Bai et al., 2022), and MT-bench (Zheng\net al., 2023). The IMDb dataset serves as a controlled se-\nmantic generation dataset where the model is presented\nwith prompts consisting of prefixes from movie reviews,\nand required to generate responses with positive sentiment.\nThe Anthropic HH dataset is a single-turn dialogue dataset\nwhere the model receives human queries, covering various\n6\n---PAGE BREAK---\nToken-level Direct Preference Optimization\nAlgorithm 1 Token-level Direct Preference Optimization (TDPO)\n1:Input: Reference model \u03c0ref, Policy model \u03c0\u03b8, Coefficient \u03b1, \u03b2, Learning rate \u03b7\n2:Input: Dataset D={(x, yw, yl)i}N\ni=1of size N, Method M\n3:Initialize: \u03c0\u03b8\u2190\u03c0ref\n4:foreach epoch do\n5: Sample mini-batch Dm={(x, yw, yl)m}M\nm=1fromD\n6: Predict the probabilities \u03c0\u03b8(yw|x)and\u03c0\u03b8(yl|x)for(x, yw, yl)in the mini-batch Dmusing the policy model\n7: Predict the probabilities \u03c0ref(yw|x)and\u03c0ref(yl|x)for(x, yw, yl)in the mini-batch Dmusing the reference model\n8: Calculate the function u(x, yw, yl) =\u03b2log\u03c0\u03b8(yw|x)\n\u03c0ref(yw|x)\u2212\u03b2log\u03c0\u03b8(yl|x)\n\u03c0ref(yl|x)\u25b7Eq.13\n9: Compute the sequential KL divergence DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8)for(x, yw)in the mini-batch Dm\n10: Compute the sequential KL divergence DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)for(x, yl)in the mini-batch Dm\n11: ifMethod MisTDPO 1then\n12: Calculate the function \u03b4(x, yw, yl) =\u03b2DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)\u2212\u03b2DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8) \u25b7Eq.14\n13: \u03b8\u2190\u03b8+\u03b7\u2207\u03b8E(x,yw,yl)\u223cDm[log\u03c3(u(x, yw, yl)\u2212\u03b4(x, yw, yl))] \u25b7Eq.15\n14: else if Method MisTDPO 2then\n15: Calculate the function \u03b42(x, yw, yl) =\u03b2DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)\u2212sg(\u03b2DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8)) \u25b7Eq.18\n16: \u03b8\u2190\u03b8+\u03b7\u2207\u03b8E(x,yw,yl)\u223cDm[log\u03c3(u(x, yw, yl)\u2212\u03b1\u03b42(x, yw, yl))] \u25b7Eq.17\n17: end if\n18:end for\n19:Output: \u03c0\u03b8\ntopics such as academic questions or life guidance. The\ntrained model is tasked with providing helpful answers to\nthese questions while avoiding toxic responses. Finally,\nMT-Bench is a GPT-4-based evaluation benchmark, assess-\ning the proficiency of LLMs in handling multi-turn open-\nended questions. Questions in MT-Bench span eight distinct\nknowledge domains, from areas such as writing, mathe-\nmatical reasoning, and humanities. Experimental results\ndemonstrate that MT-Bench achieves consistency with hu-\nman preferences exceeding 80%.\n5.1. Experiments on IMDb Dataset\nIn this experiment, besides our proposed methods TDPO 1\nandTDPO 2, we also implemented the DPO algorithm\nfor fair comparison. We employed GPT-2 Large (Radford\net al., 2019) as our base model and the model checkpoint:\ninsub/gpt2-large-IMDb-fine-tuned1as the SFT model. Dur-\ning the evaluation, we utilized the pre-trained sentiment\nclassifier siebert/sentiment-roberta-large-english2to com-\npute rewards. For DPO, we followed the official implemen-\ntation (Rafailov et al., 2023), setting \u03b2at 0.1. To analyze\nthe effectiveness of each algorithm in optimizing the con-\nstrained reward maximization objective, we evaluated each\nalgorithm after 100 training steps until convergence, com-\nputing its frontier of average reward and average sequential\nKL divergence with the reference policy.\n1https://huggingface.co/insub/\ngpt2-large-IMDb-fine-tuned\n2https://huggingface.co/siebert/\nsentiment-roberta-large-englishThe results are depicted in Figure 3(a). We implement\nthe DPO, TDPO 1, and different versions of TDPO 2algo-\nrithms with varying the parameter \u03b1. From the figure, we\nnotice that although DPO establishes an efficient frontier,\nTDPO 1andTDPO 2outperform DPO in terms of diver-\ngence versus reward on the frontier, achieving higher reward\nwhile maintaining low KL divergence. We also implemented\nversions of TDPO 2with\u03b1\u2208 {1,1.5,2,5}. However, we\nfound that higher values of \u03b1made it difficult to optimize\nthe reward. In Figures 3(b) to 3(d), we illustrate the curves\nportraying the sequential KL divergence for different al-\ngorithms during the training process. The sequential KL\ndivergence growth rate of DPO on the dispreferred response\nsubset is significantly higher than that on the preferred re-\nsponse subset, leading to an increasing offset between them.\nIn contrast, TDPO 2exhibits superior control over KL di-\nvergence, achieving better divergence efficiency compared\nto DPO. As analyzed in Section 4.4, TDPO 1tends to result\nin an increased sequential KL divergence on the preferred\nresponse subset, thereby exhibiting a weaker capacity for\nKL divergence adjustment compared to TDPO 2.TDPO 2\nmaintains a more balanced sequential KL divergence on\nboth dispreferred and preferred response subsets, contribut-\ning to its ability to achieve a superior frontier. Although a\nlarger \u03b1enhances control over the sequential KL divergence,\nit also affects the speed and difficulty of optimization. For\nthe remainder of this paper, we set \u03b1= 0.5. In Appendix C,\nwe also present graphs of the frontier between the reward\nand forward KL divergence and the progression curves of\nthe forward KL divergence throughout the training process.\n7\n---PAGE BREAK---\nToken-level Direct Preference Optimization\n/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015/uni00000011/uni00000018/uni00000014/uni00000018/uni00000011/uni00000013/uni00000014/uni0000001a/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000013\nDKL( ref)\n/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047DPO\nTDPO1\nTDPO2/uni00000003/uni00000003=0.1\nTDPO2/uni00000003/uni00000003=0.3\nTDPO2/uni00000003/uni00000003=0.5\nTDPO2/uni00000003/uni00000003=0.7\n(a)\n/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013\n/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013DSeqKL(x,yw; ref)\n (b)\n/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013\n/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013DSeqKL(x,yl; ref)\n(c)\n/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013\n/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000050/uni00000044/uni00000055/uni0000004a/uni0000004c/uni00000051 (d)\nFigure 3. The experiment on IMDb dataset. Figure 3(a) represents the frontier of expected reward and KL divergence with respect to the\nreference model. We implemented DPO, TDPO 1, and different versions of TDPO 2with respect to the parameter \u03b1. Both TDPO 1and\nTDPO 2outperform DPO in terms of the frontier, with TDPO 2showing further improvement over TDPO 1. This demonstrates the\neffectiveness of our analysis and modifications. Figure 3(b) and Figure 3(c) present the progression of sequential KL divergence on the\npreferred and dispreferred responses subset over training steps respectively. Figure 3(d) illustrates the difference between the sequential\nKL divergence on the dispreferred responses subset and that on the preferred responses subset throughout the training process, namely\nmargin =|DSeqKL (x, yw;\u03c0ref\u2225\u03c0\u03b8)\u2212DSeqKL (x, yl;\u03c0ref\u2225\u03c0\u03b8)|.TDPO 2exhibit superior regulation over KL divergence compared to\ntheTDPO 1and DPO algorithm.\nTable 1. Comparison of DPO, TDPO 1andTDPO 2in terms of\nthe trade-off between Alignment (accuracy) and Diversity (en-\ntropy) on the Anthropic HH dataset. The \u2191indicates higher values\nare preferable.\nMethodAlignment Diversity\nAccuracy (%)\u2191Entropy \u2191\nf-DPO(FKL) 54.71 4.708\nDPO 59.43 3.196\nTDPO 1(ours ) 60.08 4.727\nTDPO 2(ours ) 67.33 4.915\n5.2. Experiments on Anthropic HH Dataset\nNext, we evaluate the performance of TDPO 1andTDPO 2\non the Anthropic HH dataset. We use Pythia-2.8B (Bider-man et al., 2023) as the base model and fine-tune the base\nmodel on chosen completions to train a reference model,\nsuch that completions are within the distribution of the\nmodel. Subsequently, we train TDPO 1,TDPO 2, DPO\n(Rafailov et al., 2023) and f-DPO with forward KL diver-\ngence constraint (Wang et al., 2023) on this reference model.\nIn this experiment, our primary focus is on two aspects: 1)\nthe trade-off between alignment and diversity in generating\nresponses among different algorithms, and 2) the ability of\ndifferent algorithms to align with human preferences. For\nthe first part, we utilize automatic metrics for evaluation,\nwhile for the second part, we rely on the GPT-4 evalua-\ntion. Both evaluations were conducted on the test set of the\nAnthropic HH dataset.\nTo assess the alignment performance of different algorithms\nin generating responses, we compute the accuracy of gen-\n8\n---PAGE BREAK---\nToken-level Direct Preference Optimization\nerated responses relative to chosen completions in the test\ndataset. To measure the diversity, we employ nucleus sam-\npling with p= 0.95to generate 25 responses and utilize\nthe predictive entropy as the evaluation metric. The trade-\noff between alignment accuracy and diversity for different\nalgorithms is summarized in Table 1. TDPO 2not only sur-\npasses DPO, f-DPO and TDPO 1in terms of accuracy but\nalso excels in entropy, achieving a superior balance between\nalignment and diversity.\nTo further assess the ability of TDPO 1andTDPO 2to\nalign with human preferences, we evaluated the win rates\nof responses generated by models trained with different\nalgorithms against chosen responses on the test set of the HH\ndataset, the result is illustrated in the Figure 4. Compared to\nthe SFT model, the DPO, TDPO 1, and TDPO 2algorithms\nbetter align with human preferences, achieving win rates not\nless than 50% against chosen responses at temperature 0.75.\nThis demonstrates that both TDPO 1, and TDPO 2possess\na strong capability to align with human preferences.\n/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013\n/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni00000048/uni00000050/uni00000053/uni00000048/uni00000055/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni0000003a/uni0000004c/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048\nSFT\nTDPO1DPO\nTDPO2fDPO(FKL)\nFigure 4. The win rates, computed by GPT-4, in comparison to the\nchosen responses for Anthropic-HH one-step dialogue.\n5.3. Experiments on MT-Bench\nTo comprehensively evaluate TDPO 1, and TDPO 2in\nterms of generation quality, we conducted pairwise com-\nparisons on the MT-Bench using models trained on the\nAnthropic HH dataset. Following the official MT-Bench\nimplementation, we sampled responses with a temperature\ncoefficient of 0.7and constrained the maximum number of\nnewly generated tokens to 512. For the PPO baseline, we\nemployed the trlx framework (Havrilla et al., 2023), utiliz-\ning the proxy reward model Dahoas/gptj-rm-static3during\ntraining. The result is depicted in the Figure 5. It reveals\nthatTDPO 2achieves a higher win rate compared to other\nalgorithms, indicating its ability to assist LLMs in generat-\ning higher-quality responses. This advantage is attributed\n3https://huggingface.co/Dahoas/\ngptj-rm-staticto its exceptional ability to regulate KL divergence, facili-\ntating a better balance between performance alignment and\ngeneration diversity.\n0 20 40 60 80 100 120 140 160TDPO2\nvs TDPO1TDPO2\nvs DPOTDPO2\nvs PPOTDPO2\nvs SFT\n27.1% 54.8% 18.1%28.7% 59.1% 12.2%28.8% 60.4% 10.8%31.3% 58.7% 10.0%\nWin Tie Lose\nFigure 5. MT-Bench comparison between SFT ,PPO ,DPO ,\nTDPO 1andTDPO 2methods. The win, tie, and lose rates are\nevaluated based on GPT-4.\n6. Conclusion\nIn this work, we introduced Token-level Direct Preference\nOptimization (TDPO), an innovative token-level fine-tuning\napproach for Large Language Models (LLMs) aimed at\naligning more closely with human preferences. By employ-\ning the token-wise optimization with forward KL divergence\nconstraints and converting the Bradley-Terry model into a\ntoken-level preference model, TDPO addresses key chal-\nlenges in divergence efficiency and content diversity, sur-\npassing traditional methods like Direct Preference Optimiza-\ntion (DPO) and PPO-based RLHF in tasks such as controlled\nsentiment generation and single-turn dialogues. This marks\na substantial advancement in LLM training methodologies,\ndemonstrating the potential of token-level optimization to\nenhance the alignment, quality, and diversity of LLM out-\nputs, setting a new direction for AI alignment research and\nthe development of nuanced, human-aligned AI systems.\nRegarding the future prospects of alignment methodologies,\nwe anticipate that iterative refinement approaches and multi-\nturn conversational alignment strategies will significantly\nimprove the alignment of large language models with hu-\nman values. By continuously refining these models, we can\nachieve more precise alignment with complex human pref-\nerences. Moreover, multi-turn conversations enable deeper\nand more nuanced interactions, fostering comprehensive\nattunement to human intentions. These approaches aim to\nenhance the quality and relevance of AI responses, mak-\ning AI systems more harmonized with human values and\nexpectations.\n9\n---PAGE BREAK---\nToken-level Direct Preference Optimization\nAcknowledgements\nThe research leading to these results received funding from\nNational Key R&D Program of China (2022ZD0116402). In\naddition, it received funding from Science and Technology\nResearch and Development Project of China State Railway\nGroup Corporation Limited (P2022X012).\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774 , 2023.\nBai, Y ., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\net al. Training a helpful and harmless assistant with rein-\nforcement learning from human feedback. arXiv preprint\narXiv:2204.05862 , 2022.\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley,\nH., O\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\nPrashanth, U. S., Raff, E., et al. Pythia: A suite for ana-\nlyzing large language models across training and scaling.\nInInternational Conference on Machine Learning , pp.\n2397\u20132430. PMLR, 2023.\nBradley, R. A. and Terry, M. E. Rank analysis of incom-\nplete block designs: I. the method of paired comparisons.\nBiometrika , 39(3/4):324\u2013345, 1952.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems , 33:\n1877\u20131901, 2020.\nBubeck, S., Chandrasekaran, V ., Eldan, R., Gehrke, J.,\nHorvitz, E., Kamar, E., Lee, P., Lee, Y . T., Li, Y .,\nLundberg, S., et al. Sparks of artificial general intel-\nligence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712 , 2023.\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,\nH. P., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N.,\nBrockman, G., Ray, A., Puri, R., Krueger, G., Petrov,\nM., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,\nS., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-\nian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D.,Plappert, M., Chantzis, F., Barnes, E., Herbert-V oss, A.,\nGuss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,\nJ., Babuschkin, I., Balaji, S., Jain, S., Saunders, W.,\nHesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,\nV ., Morikawa, E., Radford, A., Knight, M., Brundage,\nM., Murati, M., Mayer, K., Welinder, P., McGrew, B.,\nAmodei, D., McCandlish, S., Sutskever, I., and Zaremba,\nW. Evaluating large language models trained on code,\n2021.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y ., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y ., Gonzalez, J. E.,\net al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023) , 2023.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg,\nS., and Amodei, D. Deep reinforcement learning from\nhuman preferences. Advances in neural information pro-\ncessing systems , 30, 2017.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\nFedus, W., Li, Y ., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416 , 2022.\nDong, H., Xiong, W., Goyal, D., Pan, R., Diao, S., Zhang,\nJ., Shum, K., and Zhang, T. Raft: Reward ranked fine-\ntuning for generative foundation model alignment. arXiv\npreprint arXiv:2304.06767 , 2023.\nGanguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y .,\nKadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse,\nK., et al. Red teaming language models to reduce harms:\nMethods, scaling behaviors, and lessons learned. arXiv\npreprint arXiv:2209.07858 , 2022.\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y .,\nCallan, J., and Neubig, G. Pal: Program-aided language\nmodels, 2023.\nGlaese, A., McAleese, N., Tr\u02db ebacz, M., Aslanides, J., Firoiu,\nV ., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M.,\nThacker, P., et al. Improving alignment of dialogue\nagents via targeted human judgements. arXiv preprint\narXiv:2209.14375 , 2022.\nHavrilla, A., Zhuravinskyi, M., Phung, D., Tiwari, A., Tow,\nJ., Biderman, S., Anthony, Q., and Castricato, L. trlx: A\nframework for large scale reinforcement learning from\nhuman feedback. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing ,\npp. 8578\u20138595, 2023.\nHuang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H.,\nChen, Q., Peng, W., Feng, X., Qin, B., et al. A survey\non hallucination in large language models: Principles,\n10\n[Jupyter current working directory: /workspace/benchmark/datasets]\n[Jupyter Python interpreter: /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python]", "extras": {"code": "import PyPDF2\npdf_path = '/workspace/benchmark/datasets/22-TokenDPO/TokenDPO-main/paper.pdf'\nreader = PyPDF2.PdfReader(pdf_path)\npages = [reader.pages[i].extract_text() for i in range(min(10, len(reader.pages)))]\nprint('\\n---PAGE BREAK---\\n'.join(pages))"}, "success": true}